[
  {
    "title": "Introduction to JAX",
    "concepts": [
      "JAX is a NumPy-like library for numerical computation.",
      "JAX supports automatic differentiation and JIT compilation.",
      "JAX aims to enable high-performance machine learning research.",
      "JAX provides a unified interface for CPU, GPU, and TPU.",
      "JAX includes built-in Just-In-Time (JIT) compilation via Open XLA.",
      "JAX functions can be automatically vectorized."
    ],
    "code_examples": [
      {
        "description": "Installing JAX for CPU",
        "code": "pip\ninstall\njax"
      },
      {
        "description": "Installing JAX for NVIDIA GPU",
        "code": "pip\ninstall\n-\nU\n\"jax[cuda12]\""
      },
      {
        "description": "Importing jax.numpy as jnp",
        "code": "import\njax.numpy\nas\njnp"
      }
    ]
  },
  {
    "title": "JAX NumPy API and Basic Usage",
    "concepts": [
      "JAX provides a jax.numpy API similar to NumPy.",
      "JAX arrays can be used with NumPy-style array creation functions, Python functions, operators, attributes and methods.",
      "JAX runs transparently on GPU or TPU (falling back to CPU)."
    ],
    "code_examples": [
      {
        "description": "Example of using jax.numpy for a selu function",
        "code": "import\njax.numpy\nas\njnp\n\ndef selu(x, alpha=1.67, lmbda=1.05):\n    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = jnp.arange(5.0)\nprint(selu(x))"
      }
    ]
  },
  {
    "title": "Just-In-Time (JIT) Compilation",
    "concepts": [
      "jax.jit() compiles a sequence of operations together using XLA.",
      "JIT compilation can significantly speed up JAX code execution.",
      "JIT compilation happens the first time a function is called and is cached thereafter."
    ],
    "code_examples": [
      {
        "description": "Benchmarking the selu function without JIT compilation",
        "code": "from jax import random\n\nkey = random.key(1701)\nx = random.normal(key, (1_000_000,))\n%timeit selu(x).block_until_ready()"
      },
      {
        "description": "JIT compiling the selu function",
        "code": "from jax import jit\n\nselu_jit = jit(selu)\n_ = selu_jit(x)  # compiles on first call\n%timeit selu_jit(x).block_until_ready()"
      }
    ]
  },
  {
    "title": "Automatic Differentiation",
    "concepts": [
      "JAX provides automatic differentiation via jax.grad().",
      "The grad() transformation can be used to compute derivatives of functions.",
      "The grad() and jit() transformations can be composed and mixed arbitrarily.",
      "jax.jacobian() computes the Jacobian matrix for vector-valued functions.",
      "jax.vjp() is for reverse-mode vector-Jacobian products.",
      "jax.jvp() and jax.linearize() are for forward-mode Jacobian-vector products.",
      "jax.jacfwd() computes Jacobians in forward-mode.",
      "jax.jacrev() computes Jacobians in reverse-mode."
    ],
    "code_examples": [
      {
        "description": "Example of using jax.grad() to compute the derivative of a function",
        "code": "from jax import grad\nimport jax.numpy as jnp\n\ndef sum_logistic(x):\n    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\nx_small = jnp.arange(3.)\nderivative_fn = grad(sum_logistic)\nprint(derivative_fn(x_small))"
      },
      {
        "description": "Verifying derivative with finite differences",
        "code": "import jax.numpy as jnp\n\ndef first_finite_differences(f, x, eps=1E-3):\n    return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n                      for v in jnp.eye(len(x))])\n\nprint(first_finite_differences(sum_logistic, x_small))"
      },
      {
        "description": "Composition of grad() and jit()",
        "code": "from jax import grad, jit\nimport jax.numpy as jnp\n\ndef sum_logistic(x):\n    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\nprint(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
      },
      {
        "description": "Computing the Jacobian matrix using jax.jacobian()",
        "code": "from jax import jacobian\nimport jax.numpy as jnp\n\nx_small = jnp.arange(3.)\nprint(jacobian(jnp.exp)(x_small))"
      },
      {
        "description": "Defining the hessian function using jacfwd and jacrev",
        "code": "from jax import jacfwd, jacrev, jit\nimport jax.numpy as jnp\n\ndef sum_logistic(x):\n    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\ndef hessian(fun):\n    return jit(jacfwd(jacrev(fun)))\n\nx_small = jnp.arange(3.)\nprint(hessian(sum_logistic)(x_small))"
      }
    ]
  },
  {
    "title": "Automatic Vectorization with vmap()",
    "concepts": [
      "vmap() transforms a function into a natively vectorized version.",
      "vmap() maps a function along array axes.",
      "Composing vmap() with jit() can be as performant as manually rewriting the function.",
      "vmap() can be arbitrarily composed with jit(), grad(), and other JAX transformations."
    ],
    "code_examples": [
      {
        "description": "Defining the apply_matrix function",
        "code": "from jax import random\nimport jax.numpy as jnp\n\nkey = random.key(0)\nkey1, key2 = random.split(key)\nmat = random.normal(key1, (150, 100))\nbatched_x = random.normal(key2, (10, 100))\n\ndef apply_matrix(x):\n    return jnp.dot(mat, x)"
      },
      {
        "description": "Naively batching the apply_matrix function using a Python loop",
        "code": "import jax.numpy as jnp\n\ndef naively_batched_apply_matrix(v_batched):\n    return jnp.stack([apply_matrix(v) for v in v_batched])\n\nprint('Naively batched')\n%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
      },
      {
        "description": "Manually batching the apply_matrix function using jnp.dot",
        "code": "from jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n@jit\ndef batched_apply_matrix(batched_x):\n    return jnp.dot(batched_x, mat.T)\n\nnp.testing.assert_allclose(\n    naively_batched_apply_matrix(batched_x),\n    batched_apply_matrix(batched_x),\n    atol=1E-4, rtol=1E-4\n)\n\nprint('Manually batched')\n%timeit batched_apply_matrix(batched_x).block_until_ready()"
      },
      {
        "description": "Automatically vectorizing the apply_matrix function using vmap()",
        "code": "from jax import vmap, jit\nimport jax.numpy as jnp\nimport numpy as np\n\n@jit\ndef vmap_batched_apply_matrix(batched_x):\n    return vmap(apply_matrix)(batched_x)\n\nnp.testing.assert_allclose(\n    naively_batched_apply_matrix(batched_x),\n    vmap_batched_apply_matrix(batched_x),\n    atol=1E-4, rtol=1E-4\n)\n\nprint('Auto-vectorized with vmap')\n%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
      }
    ]
  },
  {
    "title": "Advanced tutorials",
    "concepts": [],
    "code_examples": []
  },
  {
    "title": "JAX Arrays",
    "concepts": [
      "The default array implementation in JAX is jax.Array.",
      "JAX arrays are similar to NumPy arrays but have important differences.",
      "JAX arrays are typically created via JAX API functions like jax.numpy.zeros(), jax.numpy.linspace(), and jax.numpy.arange().",
      "jax.Array is the appropriate type annotation for JAX array objects.",
      "JAX arrays have a devices method to inspect where the array is stored.",
      "JAX arrays can be sharded across multiple devices or hosts.",
      "The sharding attribute reveals how an array is distributed across devices."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to create a JAX array using jnp.arange() and checks its type.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nisinstance(x, jax.Array)"
      },
      {
        "description": "Demonstrates how to inspect the device of a JAX array.",
        "code": "import jax\nimport jax.numpy as jnp\nx = jnp.arange(5)\nx.devices()"
      },
      {
        "description": "Demonstrates how to inspect the sharding of a JAX array.",
        "code": "import jax\nimport jax.numpy as jnp\nx = jnp.arange(5)\nx.sharding"
      }
    ]
  },
  {
    "title": "JAX Transformations",
    "concepts": [
      "JAX includes transformations that operate on JAX functions.",
      "Common transformations include jax.jit(), jax.vmap(), and jax.grad().",
      "Transformations accept a function as input and return a new, transformed function.",
      "Transformations can be applied using Python's decorator syntax."
    ],
    "code_examples": [
      {
        "description": "Defines a simple SELU function and JIT-compiles it using jax.jit().",
        "code": "import jax\nimport jax.numpy as jnp\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nselu_jit = jax.jit(selu)\nprint(selu_jit(1.0))"
      },
      {
        "description": "Defines a simple SELU function and JIT-compiles it using decorator syntax.",
        "code": "import jax\nimport jax.numpy as jnp\n@jax.jit\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)"
      }
    ]
  },
  {
    "title": "Tracers and JAXPRs",
    "concepts": [
      "Tracers are abstract stand-ins for array objects.",
      "Tracers are passed to JAX functions to extract the sequence of operations.",
      "JAX uses tracers to determine the sequence of operations encoded by a function before execution.",
      "JAXPR (JAX exPRession) is JAX's intermediate representation for sequences of operations.",
      "JAXPRs are a simple representation of a functional program, comprising a sequence of primitive operations."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how JAX uses tracers by printing an array value within a JIT-compiled function.",
        "code": "import jax\nimport jax.numpy as jnp\n@jax.jit\ndef f(x):\n  print(x)\n  return x + 1\n\nx = jnp.arange(5)\nresult = f(x)"
      },
      {
        "description": "Converts a function into a JAXPR using jax.make_jaxpr().",
        "code": "import jax\nimport jax.numpy as jnp\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = jnp.arange(5.0)\njax.make_jaxpr(selu)(x)"
      }
    ]
  },
  {
    "title": "Pytrees",
    "concepts": [
      "JAX uses the pytree abstraction to handle collections of arrays in a uniform manner.",
      "Pytrees can be lists, tuples, dictionaries, or named tuples containing arrays.",
      "jax.tree.structure() reveals the structure of a pytree.",
      "jax.tree.leaves() extracts the leaf values (arrays) from a pytree.",
      "JAX provides utilities for working with PyTrees, like jax.tree.map() and jax.tree.reduce()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to inspect the structure and leaves of a nested list pytree.",
        "code": "import jax\nimport jax.numpy as jnp\n# (nested) list of parameters\nparams = [1, 2, (jnp.arange(3), jnp.ones(2))]\nprint(jax.tree.structure(params))\nprint(jax.tree.leaves(params))"
      },
      {
        "description": "Demonstrates how to inspect the structure and leaves of a dictionary pytree.",
        "code": "import jax\nimport jax.numpy as jnp\n# Dictionary of parameters\nparams = {'n': 5, 'W': jnp.ones((2, 2)), 'b': jnp.zeros(2)}\nprint(jax.tree.structure(params))\nprint(jax.tree.leaves(params))"
      },
      {
        "description": "Demonstrates how to inspect the structure and leaves of a NamedTuple pytree.",
        "code": "import jax\nfrom typing import NamedTuple\nclass Params(NamedTuple):\n  a: int\n  b: float\n\nparams = Params(1, 5.0)\nprint(jax.tree.structure(params))\nprint(jax.tree.leaves(params))"
      }
    ]
  },
  {
    "title": "Pseudo Random Number Generation",
    "concepts": [
      "JAX differs from NumPy in its approach to pseudo random number generation.",
      "NumPy uses a global state, while JAX tracks state explicitly via a random key.",
      "Random functions consume the key but do not modify it.",
      "Keys should not be reused unless identical outputs are desired.",
      "jax.random.split() should be used to generate different and independent random samples.",
      "This approach is thread-safe."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to initialize a JAX random key.",
        "code": "from jax import random\n\nkey = random.key(43)\nprint(key)"
      },
      {
        "description": "Illustrates that reusing the same key produces the same random numbers.",
        "code": "from jax import random\n\nkey = random.key(43)\nprint(random.normal(key))\nprint(random.normal(key))\nprint(random.normal(key))"
      },
      {
        "description": "Demonstrates how to generate different and independent random samples using jax.random.split().",
        "code": "from jax import random\n\nkey = random.key(43)\nfor i in range(3):\n  new_key, subkey = random.split(key)\n  del key  # The old key is consumed by split() -- we must never use it again.\n  val = random.normal(subkey)\n  del subkey # The subkey is consumed by normal().\n  print(f\"draw {i}: {val}\")\n  key = new_key  # new_key is safe to use in the next iteration."
      }
    ]
  },
  {
    "title": "Understanding JAX Transformations and jaxpr",
    "concepts": [
      "JAX transforms Python functions into sequences of primitive operations.",
      "jax.make_jaxpr() displays the sequence of primitives behind a function.",
      "jaxpr does not capture side effects like modifying global variables.",
      "JAX transformations work best with side-effect-free (pure) code.",
      "Impure functions can cause unexpected behavior under JAX transformations.",
      "JAX wraps function arguments with tracer objects during tracing.",
      "Tracers record JAX operations and reconstruct the function as a jaxpr.",
      "Python code runs at least once during tracing.",
      "A jaxpr captures the function execution based on the parameters given.",
      "jaxpr only knows about the branch taken in a Python conditional.",
      "Python print() calls will only happen during tracing, and will not appear in the jaxpr."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating jax.make_jaxpr() to inspect the primitive operations of a function.",
        "code": "import jax\nimport jax.numpy as jnp\n\nglobal_list = []\n\ndef log2(x):\n  global_list.append(x)\n  ln_x = jnp.log(x)\n  ln_2 = jnp.log(2.0)\n  return ln_x / ln_2\n\nprint(jax.make_jaxpr(log2)(3.0))"
      },
      {
        "description": "Example demonstrating that Python print() calls will only happen during tracing and will not appear in the jaxpr.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef log2_with_print(x):\n  print(\"printed x:\", x)\n  ln_x = jnp.log(x)\n  ln_2 = jnp.log(2.0)\n  return ln_x / ln_2\n\nprint(jax.make_jaxpr(log2_with_print)(3.))"
      },
      {
        "description": "Example demonstrating that a jaxpr captures the function execution based on the parameters given, taking only the taken branch of a conditional into account.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef log2_if_rank_2(x):\n  if x.ndim == 2:\n    ln_x = jnp.log(x)\n    ln_2 = jnp.log(2.0)\n    return ln_x / ln_2\n  else:\n    return x\n\nprint(jax.make_jaxpr(log2_if_rank_2)(jax.numpy.array([1, 2, 3])))"
      }
    ]
  },
  {
    "title": "Improving Performance with jax.jit()",
    "concepts": [
      "JAX enables operations to execute on CPU/GPU/TPU using the same code.",
      "Sending operations one at a time to the accelerator limits optimization.",
      "jax.jit() performs Just In Time (JIT) compilation of a JAX function.",
      "JIT compilation allows the XLA compiler to optimize the function.",
      "The first call to a JIT-compiled function triggers tracing and compilation.",
      "Subsequent calls to a JIT-compiled function use the cached compiled code.",
      "block_until_ready() is required due to JAX\u2019s Asynchronous dispatch.",
      "Conditionals on value can cause errors within jitted functions.",
      "Traced values can only affect control flow via static attributes.",
      "One way to avoid these errors is to rewrite code to avoid conditionals on value or use control flow operators.",
      "Another approach is to JIT-compile only parts of the function.",
      "static_argnums or static_argnames can be used to specify arguments that are treated as static. JAX will recompile when these static values change.",
      "functools.partial() can be used to specify static arguments when jit is used as a decorator."
    ],
    "code_examples": [
      {
        "description": "Example of computing a Scaled Exponential Linear Unit (SELU) without JIT compilation.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = jnp.arange(1000000)\n%timeit selu(x).block_until_ready()"
      },
      {
        "description": "Example of using jax.jit() to speed up the SELU function.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n\nx = jnp.arange(1000000)\n\nselu_jit = jax.jit(selu)\n# Pre-compile the function before timing...\nselu_jit(x).block_until_ready()\n%timeit selu_jit(x).block_until_ready()"
      },
      {
        "description": "Example demonstrating errors when using conditionals on the value of the input within a jitted function.",
        "code": "import jax\n\ndef f(x):\n  if x > 0:\n    return x\n  else:\n    return 2 * x\n\n# Raises an error\njax.jit(f)(10)"
      },
      {
        "description": "Example demonstrating errors when using while loops conditioned on the value of the input within a jitted function.",
        "code": "import jax\n\ndef g(x, n):\n  i = 0\n  while i < n:\n    i += 1\n  return x + i\n\n# Raises an error\njax.jit(g)(10, 20)"
      },
      {
        "description": "Example demonstrating how to JIT-compile only the inner part of a function with a while loop to avoid errors caused by conditionals on the value of the input.",
        "code": "import jax\n\n@jax.jit\ndef loop_body(prev_i):\n  return prev_i + 1\n\ndef g_inner_jitted(x, n):\n  i = 0\n  while i < n:\n    i = loop_body(i)\n  return x + i\n\ng_inner_jitted(10, 20)"
      },
      {
        "description": "Example demonstrating how to use static_argnums to correctly JIT-compile a function with a conditional on the value of an input.",
        "code": "import jax\n\ndef f(x):\n  if x > 0:\n    return x\n  else:\n    return 2 * x\n\nf_jit_correct = jax.jit(f, static_argnums=0)\nprint(f_jit_correct(10))"
      },
      {
        "description": "Example demonstrating how to use static_argnames to correctly JIT-compile a function with a conditional on the value of an input.",
        "code": "import jax\n\ndef g(x, n):\n  i = 0\n  while i < n:\n    i += 1\n  return x + i\n\ng_jit_correct = jax.jit(g, static_argnames=['n'])\nprint(g_jit_correct(10, 20))"
      },
      {
        "description": "Example demonstrating how to use functools.partial() to specify static arguments when jit is used as a decorator.",
        "code": "import jax\nfrom functools import partial\n\n@partial(jax.jit, static_argnames=['n'])\ndef g_jit_decorated(x, n):\n  i = 0\n  while i < n:\n    i += 1\n  return x + i\n\nprint(g_jit_decorated(10, 20))"
      }
    ]
  },
  {
    "title": "Understanding jax.jit() Caching",
    "concepts": [
      "jax.jit() caches compiled XLA code for reuse in subsequent calls.",
      "Caching makes up for the up-front compilation cost.",
      "Code is only reused if the values of arguments labelled as static do not change.",
      "Calling jax.jit() on temporary functions inside loops can lead to recompilation.",
      "Avoid calling jax.jit() on temporary functions defined inside loops.",
      "The cache relies on the hash of the function, so equivalent functions that are redefined will cause recompilation."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating how defining equivalent functions inside loops causes unnecessary recompilation when using jax.jit().",
        "code": "import jax\nfrom functools import partial\n\ndef unjitted_loop_body(prev_i):\n  return prev_i + 1\n\ndef g_inner_jitted_partial(x, n):\n  i = 0\n  while i < n:\n    # Don't do this! each time the partial returns\n    # a function with different hash\n    i = jax.jit(partial(unjitted_loop_body))(i)\n  return x + i\n\ndef g_inner_jitted_lambda(x, n):\n  i = 0\n  while i < n:\n    # Don't do this!, lambda will also return\n    # a function with a different hash\n    i = jax.jit(lambda x: unjitted_loop_body(x))(i)\n  return x + i\n\ndef g_inner_jitted_normal(x, n):\n  i = 0\n  while i < n:\n    # this is OK, since JAX can find the\n    # cached, compiled function\n    i = jax.jit(unjitted_loop_body)(i)\n  return x + i\n\nprint(\"jit called in a loop with partials:\")\n%timeit g_inner_jitted_partial(10, 20).block_until_ready()\nprint(\"jit called in a loop with lambdas:\")\n%timeit g_inner_jitted_lambda(10, 20).block_until_ready()\nprint(\"jit called in a loop with caching:\")\n%timeit g_inner_jitted_normal(10, 20).block_until_ready()"
      }
    ]
  },
  {
    "title": "Introduction to jax.vmap()",
    "concepts": [
      "jax.vmap() is used for automatic vectorization of functions.",
      "Vectorization avoids explicit looping in Python for better performance.",
      "Manually vectorizing functions can be complex and error-prone."
    ],
    "code_examples": [
      {
        "description": "Define a 1D convolution function using JAX numpy.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nw = jnp.array([2., 3., 4.])\n\ndef convolve(x, w):\n  output = []\n  for i in range(1, len(x) - 1):\n    output.append(jnp.dot(x[i-1:i+2], w))\n  return jnp.array(output)\n\nconvolve(x, w)"
      }
    ]
  },
  {
    "title": "Manual Batching and Vectorization",
    "concepts": [
      "A naive approach to batching involves looping over inputs in Python.",
      "Manual vectorization requires rewriting the function to handle batch dimensions directly.",
      "Manual vectorization improves performance compared to naive Python loops.",
      "Stacking is used to create batches of inputs.",
      "Manual vectorization involves correctly handling indices and axes."
    ],
    "code_examples": [
      {
        "description": "Manually batch the convolution function using a Python loop.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nw = jnp.array([2., 3., 4.])\n\ndef convolve(x, w):\n  output = []\n  for i in range(1, len(x) - 1):\n    output.append(jnp.dot(x[i-1:i+2], w))\n  return jnp.array(output)\n\nxs = jnp.stack([x, x])\nws = jnp.stack([w, w])\n\ndef manually_batched_convolve(xs, ws):\n  output = []\n  for i in range(xs.shape[0]):\n    output.append(convolve(xs[i], ws[i]))\n  return jnp.stack(output)\n\nmanually_batched_convolve(xs, ws)"
      },
      {
        "description": "Manually vectorize the convolution function to handle batch dimensions.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nw = jnp.array([2., 3., 4.])\n\ndef convolve(x, w):\n  output = []\n  for i in range(1, len(x) - 1):\n    output.append(jnp.dot(x[i-1:i+2], w))\n  return jnp.array(output)\n\nxs = jnp.stack([x, x])\nws = jnp.stack([w, w])\n\ndef manually_vectorized_convolve(xs, ws):\n  output = []\n  for i in range(1, xs.shape[-1] - 1):\n    output.append(jnp.sum(xs[:, i-1:i+2] * ws, axis=1))\n  return jnp.stack(output, axis=1)\n\nmanually_vectorized_convolve(xs, ws)"
      }
    ]
  },
  {
    "title": "Automatic Vectorization with jax.vmap()",
    "concepts": [
      "jax.vmap() automatically generates a vectorized implementation of a function.",
      "jax.vmap() adds batch axes at the beginning of each input.",
      "The in_axes and out_axes arguments specify the location of the batch dimension.",
      "in_axes can be an integer or a list depending on the input structure.",
      "jax.vmap() supports the case where only one argument is batched by setting in_axes to None for non-batched arguments."
    ],
    "code_examples": [
      {
        "description": "Automatically batch the convolution function using jax.vmap().",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nw = jnp.array([2., 3., 4.])\n\ndef convolve(x, w):\n  output = []\n  for i in range(1, len(x) - 1):\n    output.append(jnp.dot(x[i-1:i+2], w))\n  return jnp.array(output)\n\nxs = jnp.stack([x, x])\nws = jnp.stack([w, w])\n\nauto_batch_convolve = jax.vmap(convolve)\nauto_batch_convolve(xs, ws)"
      },
      {
        "description": "Use jax.vmap() with in_axes and out_axes to specify the batch dimension.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nw = jnp.array([2., 3., 4.])\n\ndef convolve(x, w):\n  output = []\n  for i in range(1, len(x) - 1):\n    output.append(jnp.dot(x[i-1:i+2], w))\n  return jnp.array(output)\n\nxs = jnp.stack([x, x])\nws = jnp.stack([w, w])\n\nauto_batch_convolve_v2 = jax.vmap(convolve, in_axes=1, out_axes=1)\nxst = jnp.transpose(xs)\nwst = jnp.transpose(ws)\nauto_batch_convolve_v2(xst, wst)"
      },
      {
        "description": "Use jax.vmap() where only one argument is batched.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nw = jnp.array([2., 3., 4.])\n\ndef convolve(x, w):\n  output = []\n  for i in range(1, len(x) - 1):\n    output.append(jnp.dot(x[i-1:i+2], w))\n  return jnp.array(output)\n\nxs = jnp.stack([x, x])\nws = jnp.stack([w, w])\n\nbatch_convolve_v3 = jax.vmap(convolve, in_axes=[0, None])\nbatch_convolve_v3(xs, w)"
      }
    ]
  },
  {
    "title": "Composing jax.jit() and jax.vmap()",
    "concepts": [
      "jax.jit() and jax.vmap() are composable transformations.",
      "You can wrap a vmapped function with jit or a jitted function with vmap."
    ],
    "code_examples": [
      {
        "description": "Combine jax.jit() and jax.vmap() by wrapping a vmapped function with jit.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nw = jnp.array([2., 3., 4.])\n\ndef convolve(x, w):\n  output = []\n  for i in range(1, len(x) - 1):\n    output.append(jnp.dot(x[i-1:i+2], w))\n  return jnp.array(output)\n\nxs = jnp.stack([x, x])\nws = jnp.stack([w, w])\n\nauto_batch_convolve = jax.vmap(convolve)\njitted_batch_convolve = jax.jit(auto_batch_convolve)\njitted_batch_convolve(xs, ws)"
      }
    ]
  },
  {
    "title": "Introduction to Automatic Differentiation in JAX",
    "concepts": [
      "JAX has a general autodiff system.",
      "Computing gradients is crucial for modern machine learning.",
      "jax.grad is used for taking gradients of scalar-valued functions.",
      "jax.value_and_grad is used for evaluating a function and its gradient.",
      "Numerical differences can be used to check gradients."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jax.grad",
    "concepts": [
      "jax.grad takes a function and returns a function that computes its gradient.",
      "Higher-order derivatives can be computed by repeatedly applying jax.grad.",
      "The derivative of a function can be computed easily using jax.grad()."
    ],
    "code_examples": [
      {
        "description": "Computes the gradient of the tanh function at x=2.0 using jax.grad.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import grad\n\ngrad_tanh = grad(jnp.tanh)\nprint(grad_tanh(2.0))"
      },
      {
        "description": "Computes higher-order derivatives of the tanh function at x=2.0 by repeatedly applying jax.grad.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import grad\n\nprint(grad(grad(jnp.tanh))(2.0))\nprint(grad(grad(grad(jnp.tanh)))(2.0))"
      },
      {
        "description": "Computes the first four derivatives of the function f(x) = x^3 + 2x^2 - 3x + 1 at x=1 using jax.grad.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import grad\n\nf = lambda x: x**3 + 2 * x**2 - 3 * x + 1\ndfdx = jax.grad(f)\nd2fdx = jax.grad(dfdx)\nd3fdx = jax.grad(d2fdx)\nd4fdx = jax.grad(d3fdx)\n\nprint(dfdx(1.))\nprint(d2fdx(1.))\nprint(d3fdx(1.))\nprint(d4fdx(1.))"
      }
    ]
  },
  {
    "title": "Gradients in Linear Logistic Regression",
    "concepts": [
      "Linear logistic regression model.",
      "The loss function is the negative log-likelihood of the training examples.",
      "jax.grad() can differentiate with respect to specific positional arguments using the argnums argument.",
      "Tuples can be used to specify multiple arguments for differentiation."
    ],
    "code_examples": [
      {
        "description": "Compute gradients of the loss function with respect to model parameters (W and b) in a linear logistic regression using jax.grad.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import grad\n\nkey = jax.random.key(0)\n\ndef sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12, 0.77],\n                    [0.88, -1.08, 0.15],\n                    [0.52, 0.06, -1.30],\n                    [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n    preds = predict(W, b, inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\n# Initialize random model coefficients\nkey, W_key, b_key = jax.random.split(key, 3)\nW = jax.random.normal(W_key, (3,))\nb = jax.random.normal(b_key, ())\n\n# Differentiate `loss` with respect to the first positional argument:\nW_grad = grad(loss, argnums=0)(W, b)\nprint(f'{W_grad=}')\n\n# Since argnums=0 is the default, this does the same thing:\nW_grad = grad(loss)(W, b)\nprint(f'{W_grad=}')\n\n# But you can choose different values too, and drop the keyword:\nb_grad = grad(loss, 1)(W, b)\nprint(f'{b_grad=}')\n\n# Including tuple values\nW_grad, b_grad = grad(loss, (0, 1))(W, b)\nprint(f'{W_grad=}')\nprint(f'{b_grad=}')"
      }
    ]
  },
  {
    "title": "Differentiating with Respect to PyTrees",
    "concepts": [
      "JAX supports differentiating with respect to standard Python containers like lists, tuples, and dictionaries.",
      "JAX's PyTree abstraction enables automatic differentiation with nested data structures."
    ],
    "code_examples": [
      {
        "description": "Compute the gradient of the loss function with respect to a dictionary of model parameters.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import grad\n\nkey = jax.random.key(0)\n\ndef sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12, 0.77],\n                    [0.88, -1.08, 0.15],\n                    [0.52, 0.06, -1.30],\n                    [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss2(params_dict):\n    W = params_dict['W']\n    b = params_dict['b']\n    preds = predict(W, b, inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\nkey, W_key, b_key = jax.random.split(key, 3)\nW = jax.random.normal(W_key, (3,))\nb = jax.random.normal(b_key, ())\n\nprint(grad(loss2)({'W': W, 'b': b}))"
      }
    ]
  },
  {
    "title": "Evaluating Function Value and Gradient Simultaneously",
    "concepts": [
      "jax.value_and_grad() efficiently computes both the function value and its gradient in one pass.",
      "This is more efficient than calling jax.grad() and the function separately."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of jax.value_and_grad to compute the loss value and gradients of W and b simultaneously.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import grad, value_and_grad\n\nkey = jax.random.key(0)\n\ndef sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12, 0.77],\n                    [0.88, -1.08, 0.15],\n                    [0.52, 0.06, -1.30],\n                    [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n    preds = predict(W, b, inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\nkey, W_key, b_key = jax.random.split(key, 3)\nW = jax.random.normal(W_key, (3,))\nb = jax.random.normal(b_key, ())\n\nloss_value, Wb_grad = jax.value_and_grad(loss, (0, 1))(W, b)\nprint('loss value', loss_value)\nprint('loss value', loss(W, b))"
      }
    ]
  },
  {
    "title": "Checking Gradients with Numerical Differences",
    "concepts": [
      "Derivatives can be verified using finite differences.",
      "Finite differences approximate the derivative using small perturbations.",
      "jax.test_util.check_grads provides a convenient function for checking gradients up to a specified order."
    ],
    "code_examples": [
      {
        "description": "Checks the gradient of the loss function with respect to b using scalar finite differences and W using finite differences in a random direction.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import grad\n\nkey = jax.random.key(0)\n\ndef sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12, 0.77],\n                    [0.88, -1.08, 0.15],\n                    [0.52, 0.06, -1.30],\n                    [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n    preds = predict(W, b, inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\nkey, W_key, b_key = jax.random.split(key, 3)\nW = jax.random.normal(W_key, (3,))\nb = jax.random.normal(b_key, ())\n\n# Set a step size for finite differences calculations\neps = 1e-4\n# Check b_grad with scalar finite differences\nb_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps\nprint('b_grad_numerical', b_grad_numerical)\nprint('b_grad_autodiff', grad(loss, 1)(W, b))\n\n# Check W_grad with finite differences in a random direction\nkey, subkey = jax.random.split(key)\nvec = jax.random.normal(subkey, W.shape)\nunitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\nW_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps\nprint('W_dirderiv_numerical', W_grad_numerical)\nprint('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec))"
      },
      {
        "description": "Demonstrates the usage of jax.test_util.check_grads to verify the gradients of the loss function.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import grad\nfrom jax.test_util import check_grads\n\nkey = jax.random.key(0)\n\ndef sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12, 0.77],\n                    [0.88, -1.08, 0.15],\n                    [0.52, 0.06, -1.30],\n                    [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n    preds = predict(W, b, inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\nkey, W_key, b_key = jax.random.split(key, 3)\nW = jax.random.normal(W_key, (3,))\nb = jax.random.normal(b_key, ())\n\ncheck_grads(loss, (W, b), order=2)  # check up to 2nd order derivatives"
      }
    ]
  },
  {
    "title": "Advanced Automatic Differentiation",
    "concepts": [
      "The Advanced automatic differentiation tutorial provides more detailed explanations.",
      "Custom derivative rules depend on understanding advanced automatic differentiation."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Debugging Methods",
    "concepts": [
      "JAX provides built-in debugging methods: jax.debug.print(), jax.debug.breakpoint(), and jax.debug.callback().",
      "These methods can be used with various JAX transformations like jax.jit() and jax.vmap()."
    ],
    "code_examples": []
  },
  {
    "title": "Using jax.debug.print() for Debugging",
    "concepts": [
      "Use jax.debug.print() for traced (dynamic) array values with jax.jit(), jax.vmap() and others.",
      "Use Python print() for static values, such as dtypes and array shapes.",
      "Python's print() executes at trace-time with abstract tracers when using jax.jit().",
      "jax.debug.print() prints the actual runtime values.",
      "jax.debug.print() can be used within jax.vmap() to print values being mapped over.",
      "The `ordered=True` parameter ensures jax.debug.print() statements are executed in the original order.",
      "jax.debug.print() only prints the forward pass when used with `jax.grad()`"
    ],
    "code_examples": [
      {
        "description": "Illustrates the behavior of Python print() with jax.jit(). It prints the tracer value instead of the runtime value.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef f(x):\n  print(\"print(x) ->\", x)\n  y = jnp.sin(x)\n  print(\"print(y) ->\", y)\n  return y\n\nresult = f(2.)"
      },
      {
        "description": "Demonstrates how to use jax.debug.print() to print runtime values within a jitted function.",
        "code": "@jax.jit\ndef f(x):\n  jax.debug.print(\"jax.debug.print(x) -> {x}\", x=x)\n  y = jnp.sin(x)\n  jax.debug.print(\"jax.debug.print(y) -> {y}\", y=y)\n  return y\n\nresult = f(2.)"
      },
      {
        "description": "Shows the usage of jax.debug.print() within jax.vmap() to print values being mapped over.",
        "code": "def f(x):\n  jax.debug.print(\"jax.debug.print(x) -> {}\", x)\n  y = jnp.sin(x)\n  jax.debug.print(\"jax.debug.print(y) -> {}\", y)\n  return y\n\nxs = jnp.arange(3.)\nresult = jax.vmap(f)(xs)"
      },
      {
        "description": "Example using jax.lax.map() and jax.debug.print().",
        "code": "result = jax.lax.map(f, xs)"
      },
      {
        "description": "Example of using jax.grad() and jax.debug.print().",
        "code": "def f(x):\n  jax.debug.print(\"jax.debug.print(x) -> {}\", x)\n  return x ** 2\n\nresult = jax.grad(f)(1.)"
      },
      {
        "description": "Demonstrates the use of the `ordered=True` parameter with jax.debug.print() to ensure the printing order is preserved.",
        "code": "@jax.jit\ndef f(x, y):\n  jax.debug.print(\"jax.debug.print(x) -> {}\", x, ordered=True)\n  jax.debug.print(\"jax.debug.print(y) -> {}\", y, ordered=True)\n  return x + y\n\nf(1, 2)"
      }
    ]
  },
  {
    "title": "Using jax.debug.breakpoint() for Interactive Debugging",
    "concepts": [
      "jax.debug.breakpoint() pauses the execution of a JAX program to inspect values interactively.",
      "The prompt is similar to Python's pdb.",
      "jax.debug.breakpoint() is an application of jax.debug.callback() that captures information about the call stack.",
      "It's possible to set breakpoints conditionally based on runtime values using jax.lax.cond()."
    ],
    "code_examples": [
      {
        "description": "Shows how to insert a breakpoint in a jitted function using jax.debug.breakpoint().",
        "code": "@jax.jit\ndef f(x):\n  y, z = jnp.sin(x), jnp.cos(x)\n  jax.debug.breakpoint()\n  return y * z\n\nf(2.)\n# ==> Pauses during execution"
      },
      {
        "description": "Demonstrates conditional breakpointing based on whether a value is finite using jax.lax.cond().",
        "code": "def breakpoint_if_nonfinite(x):\n  is_finite = jnp.isfinite(x).all()\n\n  def true_fn(x):\n    pass\n\n  def false_fn(x):\n    jax.debug.breakpoint()\n\n  jax.lax.cond(is_finite, true_fn, false_fn, x)\n\n\n@jax.jit\ndef f(x, y):\n  z = x / y\n  breakpoint_if_nonfinite(z)\n  return z\n\nf(2., 1.)\n# ==> No breakpoint\nf(2., 0.)\n# ==> Pauses during execution"
      }
    ]
  },
  {
    "title": "Using jax.debug.callback() for Custom Debugging",
    "concepts": [
      "jax.debug.callback() provides greater control over host-side logic executed via a Python callback.",
      "It is compatible with transformations like jax.jit(), jax.vmap(), and jax.grad().",
      "jax.debug.callback() can be useful for general-purpose debugging."
    ],
    "code_examples": [
      {
        "description": "Example of using jax.debug.callback() to log a value using the logging module.",
        "code": "import logging\n\ndef log_value(x):\n  logging.warning(f'Logged value: {x}')\n\n@jax.jit\ndef f(x):\n  jax.debug.callback(log_value, x)\n  return x\n\nf(1.0);"
      },
      {
        "description": "Demonstrates the compatibility of jax.debug.callback() with jax.vmap().",
        "code": "x = jnp.arange(5.0)\njax.vmap(f)(x);"
      },
      {
        "description": "Demonstrates the compatibility of jax.debug.callback() with jax.grad().",
        "code": "jax.grad(f)(1.0);"
      }
    ]
  },
  {
    "title": "Introduction to Pseudo Random Number Generation",
    "concepts": [
      "Pseudo random number generation (PRNG) is the algorithmic generation of number sequences that approximate random numbers.",
      "PRNG sequences are determined by their initial value (seed).",
      "JAX's PRNG approach differs significantly from NumPy's.",
      "JAX strives to be reproducible, parallelizable, and vectorizable in its PRNG design."
    ],
    "code_examples": []
  },
  {
    "title": "NumPy's PRNG with Global State",
    "concepts": [
      "NumPy uses the numpy.random module for PRNG.",
      "NumPy's PRNG is based on a global state.",
      "The global state can be set using numpy.random.seed().",
      "Repeated calls to NumPy's PRNGs mutate the global state.",
      "NumPy uses the Mersenne Twister PRNG.",
      "The state can be inspected using numpy.random.get_state().",
      "NumPy provides a sequential equivalence guarantee."
    ],
    "code_examples": [
      {
        "description": "Sets the seed for NumPy's random number generator.",
        "code": "import numpy as np\nnp.random.seed(0)"
      },
      {
        "description": "Demonstrates the stateful nature of NumPy's PRNG, where subsequent calls return different random numbers.",
        "code": "print(np.random.random())\nprint(np.random.random())\nprint(np.random.random())\nprint(np.random.random())\nprint(np.random.random())\nprint(np.random.random())"
      },
      {
        "description": "Defines a function to print a truncated version of NumPy's random state.",
        "code": "def print_truncated_random_state():\n    \"\"\"To avoid spamming the outputs, print only part of the state.\"\"\"\n    full_random_state = np.random.get_state()\n    print(str(full_random_state)[:460], '...')\n\nprint_truncated_random_state()"
      },
      {
        "description": "Shows how the random state changes with each call to a random function after setting the seed.",
        "code": "np.random.seed(0)\nprint_truncated_random_state()\n_\n_ = np.random.uniform()\nprint_truncated_random_state()"
      },
      {
        "description": "Samples a vector of 3 scalars from a uniform distribution using NumPy.",
        "code": "np.random.seed(0)\nprint(np.random.uniform(size=3))"
      },
      {
        "description": "Demonstrates NumPy's sequential equivalence, showing that sampling individually or all at once produces the same result.",
        "code": "np.random.seed(0)\nprint(\"individually:\", np.stack([np.random.uniform() for _ in range(3)]))\nnp.random.seed(0)\nprint(\"all at once: \", np.random.uniform(size=3))"
      }
    ]
  },
  {
    "title": "JAX's PRNG with Explicit State",
    "concepts": [
      "JAX avoids implicit global random state and uses explicit random keys.",
      "Random keys are created using jax.random.key().",
      "Keys are arrays with a special dtype.",
      "Random functions in JAX consume the key but do not modify it.",
      "Reusing the same key results in identical samples.",
      "JAX uses the Threefry PRNG.",
      "Keys must be split using jax.random.split() to generate independent samples.",
      "JAX does not provide a sequential equivalence guarantee.",
      "jax.vmap() can be used for vectorized random sampling."
    ],
    "code_examples": [
      {
        "description": "Creates a random key in JAX using jax.random.key().",
        "code": "from jax import random\nkey = random.key(42)\nprint(key)"
      },
      {
        "description": "Demonstrates that using the same key results in the same sample.",
        "code": "from jax import random\nkey = random.key(42)\nprint(random.normal(key))\nprint(random.normal(key))\nprint(random.normal(key))"
      },
      {
        "description": "Shows how to split a key to generate independent samples using jax.random.split().",
        "code": "from jax import random\nkey = random.key(42)\n\nfor i in range(3):\n    new_key, subkey = random.split(key)\n    del key  # The old key is consumed by split() -- we must never use it again.\n    val = random.normal(subkey)\n    del subkey  # The subkey is consumed by normal().\n    print(f\"draw {i}: {val}\")\n    key = new_key  # new_key is safe to use in the next iteration."
      },
      {
        "description": "Illustrates the lack of sequential equivalence in JAX by comparing individual and batched sampling.",
        "code": "import jax\nfrom jax import random\nimport numpy as np\n\nkey = random.key(42)\nsubkeys = random.split(key, 3)\nsequence = np.stack([random.normal(subkey) for subkey in subkeys])\nprint(\"individually:\", sequence)\n\nkey = random.key(42)\nprint(\"all at once: \", random.normal(key, shape=(3,)))"
      },
      {
        "description": "Demonstrates vectorized random sampling using jax.vmap().",
        "code": "import jax\nfrom jax import random\nimport numpy as np\n\nkey = random.key(42)\nsubkeys = random.split(key, 3)\nprint(\"vectorized:\", jax.vmap(random.normal)(subkeys))"
      }
    ]
  },
  {
    "title": "Introduction to Device Parallelism and SPMD in JAX",
    "concepts": [
      "Device parallelism enables running the same computation on different data in parallel on different devices.",
      "SPMD (Single-Program Multi-Data) is a parallelism technique used for device parallelism.",
      "This tutorial covers automatic parallelism with jax.jit(), semi-automated parallelism using jax.jit() and jax.lax.with_sharding_constraint(), and fully manual parallelism with jax.experimental.shard_map.shard_map().",
      "Make sure your hardware accelerator is set to TPU v2 if you are running these examples in Google Colab."
    ],
    "code_examples": [
      {
        "description": "This code imports the jax.devices() to inspect the available devices.",
        "code": "import\njax\njax\n.\ndevices\n()"
      },
      {
        "description": "This code prints the TPU devices available.",
        "code": "import\njax\njax\n.\ndevices\n()\n[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\nTpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\nTpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\nTpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\nTpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\nTpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\nTpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\nTpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      },
      {
        "description": "This code prints the TPU devices available.",
        "code": "import\njax\njax\n.\ndevices\n()\n[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\nTpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\nTpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\nTpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\nTpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\nTpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\nTpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\nTpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      }
    ]
  },
  {
    "title": "Data Sharding and jax.Array",
    "concepts": [
      "Data sharding describes how data is laid out on available devices.",
      "jax.Array represents arrays with physical storage spanning one or multiple devices.",
      "Every jax.Array has an associated jax.sharding.Sharding object.",
      "jax.sharding.Sharding object describes which shard of the global data is required by each global device."
    ],
    "code_examples": [
      {
        "description": "Creates a jax array and prints the devices it is on.",
        "code": "import\njax.numpy\nas\njnp\narr\n=\njnp\n.\narange\n(\n32.0\n)\n.\nreshape\n(\n4\n,\n8\n)\narr\n.\ndevices\n()"
      },
      {
        "description": "Creates a jax array and prints the devices it is on.",
        "code": "import\njax.numpy\nas\njnp\narr\n=\njnp\n.\narange\n(\n32.0\n)\n.\nreshape\n(\n4\n,\n8\n)\narr\n.\ndevices\n()\n{TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}"
      },
      {
        "description": "This code prints the sharding of a jax array, which is SingleDeviceSharding.",
        "code": "arr\n.\nsharding"
      },
      {
        "description": "This code prints the sharding of a jax array, which is SingleDeviceSharding.",
        "code": "arr\n.\nsharding\nSingleDeviceSharding(device=TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0))"
      },
      {
        "description": "Visualizes the array sharding using jax.debug.visualize_array_sharding().",
        "code": "jax\n.\ndebug\n.\nvisualize_array_sharding\n(\narr\n)"
      },
      {
        "description": "Visualizes the array sharding using jax.debug.visualize_array_sharding().",
        "code": "jax\n.\ndebug\n.\nvisualize_array_sharding\n(\narr\n)"
      }
    ]
  },
  {
    "title": "Creating Arrays with Non-Trivial Sharding",
    "concepts": [
      "jax.device_put() is used to create an array with a non-trivial sharding.",
      "jax.sharding.NamedSharding specifies an N-dimensional grid of devices with named axes.",
      "jax.sharding.Mesh allows for precise device placement."
    ],
    "code_examples": [
      {
        "description": "Defines a NamedSharding for the array.",
        "code": "from\njax.sharding\nimport\nPartitionSpec\nas\nP\nmesh\n=\njax\n.\nmake_mesh\n((\n2\n,\n4\n),\n(\n'x'\n,\n'y'\n))\nsharding\n=\njax\n.\nsharding\n.\nNamedSharding\n(\nmesh\n,\nP\n(\n'x'\n,\n'y'\n))\nprint\n(\nsharding\n)"
      },
      {
        "description": "Defines a NamedSharding for the array.",
        "code": "from\njax.sharding\nimport\nPartitionSpec\nas\nP\nmesh\n=\njax\n.\nmake_mesh\n((\n2\n,\n4\n),\n(\n'x'\n,\n'y'\n))\nsharding\n=\njax\n.\nsharding\n.\nNamedSharding\n(\nmesh\n,\nP\n(\n'x'\n,\n'y'\n))\nprint\n(\nsharding\n)\nNamedSharding(mesh=Mesh('x': 2, 'y': 4), spec=PartitionSpec('x', 'y'))"
      },
      {
        "description": "Creates a sharded array using jax.device_put().",
        "code": "arr_sharded\n=\njax\n.\ndevice_put\n(\narr\n,\nsharding\n)\nprint\n(\narr_sharded\n)\njax\n.\ndebug\n.\nvisualize_array_sharding\n(\narr_sharded\n)"
      },
      {
        "description": "Creates a sharded array using jax.device_put().",
        "code": "arr_sharded\n=\njax\n.\ndevice_put\n(\narr\n,\nsharding\n)\nprint\n(\narr_sharded\n)\njax\n.\ndebug\n.\nvisualize_array_sharding\n(\narr_sharded\n)\n[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n [ 8.  9. 10. 11. 12. 13. 14. 15.]\n [16. 17. 18. 19. 20. 21. 22. 23.]\n [24. 25. 26. 27. 28. 29. 30. 31.]]"
      },
      {
        "description": "Demonstrates the memory_kind parameter within NamedSharding and how to change it.",
        "code": "s_host\n=\njax\n.\nNamedSharding\n(\nmesh\n,\nP\n(\n'x'\n,\n'y'\n),\nmemory_kind\n=\n'pinned_host'\n)\ns_dev\n=\ns_host\n.\nwith_memory_kind\n(\n'device'\n)\narr_host\n=\njax\n.\ndevice_put\n(\narr\n,\ns_host\n)\narr_dev\n=\njax\n.\ndevice_put\n(\narr\n,\ns_dev\n)\nprint\n(\narr_host\n.\nsharding\n.\nmemory_kind\n)\nprint\n(\narr_dev\n.\nsharding\n.\nmemory_kind\n)"
      },
      {
        "description": "Demonstrates the memory_kind parameter within NamedSharding and how to change it.",
        "code": "s_host\n=\njax\n.\nNamedSharding\n(\nmesh\n,\nP\n(\n'x'\n,\n'y'\n),\nmemory_kind\n=\n'pinned_host'\n)\ns_dev\n=\ns_host\n.\nwith_memory_kind\n(\n'device'\n)\narr_host\n=\njax\n.\ndevice_put\n(\narr\n,\ns_host\n)\narr_dev\n=\njax\n.\ndevice_put\n(\narr\n,\ns_dev\n)\nprint\n(\narr_host\n.\nsharding\n.\nmemory_kind\n)\nprint\n(\narr_dev\n.\nsharding\n.\nmemory_kind\n)\npinned_host\ndevice"
      }
    ]
  },
  {
    "title": "Automatic Parallelization with jax.jit()",
    "concepts": [
      "Passing sharded data to a jax.jit()-compiled function enables parallel computation.",
      "The XLA compiler optimizes computations across multiple devices.",
      "Computation follows data: computation for each shard is performed on the device associated with that shard.",
      "The output sharding can differ from the input sharding by specifying the out_shardings parameter."
    ],
    "code_examples": [
      {
        "description": "Demonstrates auto-parallelization with a jax.jit()-decorated function.",
        "code": "@jax\n.\njit\ndef f_elementwise(x):\n    return 2 * jnp.sin(x) + 1\n\nresult = f_elementwise(arr_sharded)\nprint(\"shardings match:\", result.sharding == arr_sharded.sharding)"
      },
      {
        "description": "Demonstrates auto-parallelization with a jax.jit()-decorated function.",
        "code": "@jax\n.\njit\ndef f_elementwise(x):\n    return 2 * jnp.sin(x) + 1\n\nresult = f_elementwise(arr_sharded)\nprint(\"shardings match:\", result.sharding == arr_sharded.sharding)\nshardings match: True"
      },
      {
        "description": "Demonstrates compiler decisions on how to propagate data sharding with a sum operation.",
        "code": "@jax\n.\njit\ndef f_contract(x):\n    return x.sum(axis=0)\n\nresult = f_contract(arr_sharded)\njax.debug.visualize_array_sharding(result)\nprint(result)"
      },
      {
        "description": "Demonstrates compiler decisions on how to propagate data sharding with a sum operation.",
        "code": "@jax\n.\njit\ndef f_contract(x):\n    return x.sum(axis=0)\n\nresult = f_contract(arr_sharded)\njax.debug.visualize_array_sharding(result)\nprint(result)\n[48. 52. 56. 60. 64. 68. 72. 76.]"
      },
      {
        "description": "The output sharding of a jax.jit() function can differ from the input sharding if you specify the output sharding using the out_shardings parameter.",
        "code": "f = jax.jit(lambda x: x, out_shardings=s_dev)\nout_dev = f(arr_host)\nprint(out_dev)\nprint(out_dev.sharding.memory_kind)"
      },
      {
        "description": "The output sharding of a jax.jit() function can differ from the input sharding if you specify the output sharding using the out_shardings parameter.",
        "code": "f = jax.jit(lambda x: x, out_shardings=s_dev)\nout_dev = f(arr_host)\nprint(out_dev)\nprint(out_dev.sharding.memory_kind)\n[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n [ 8.  9. 10. 11. 12. 13. 14. 15.]\n [16. 17. 18. 19. 20. 21. 22. 23.]\n [24. 25. 26. 27. 28. 29. 30. 31.]]\ndevice"
      },
      {
        "description": "The output sharding of a jax.jit() function can differ from the input sharding if you specify the output sharding using the out_shardings parameter.",
        "code": "g = jax.jit(lambda x: x, out_shardings=s_host)\nout_host = g(arr_dev)\nprint(out_host)\nprint(out_host.sharding.memory_kind)"
      },
      {
        "description": "The output sharding of a jax.jit() function can differ from the input sharding if you specify the output sharding using the out_shardings parameter.",
        "code": "g = jax.jit(lambda x: x, out_shardings=s_host)\nout_host = g(arr_dev)\nprint(out_host)\nprint(out_host.sharding.memory_kind)\n[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n [ 8.  9. 10. 11. 12. 13. 14. 15.]\n [16. 17. 18. 19. 20. 21. 22. 23.]\n [24. 25. 26. 27. 28. 29. 30. 31.]]\npinned_host"
      }
    ]
  },
  {
    "title": "Semi-Automated Parallelism with jax.lax.with_sharding_constraint()",
    "concepts": [
      "jax.lax.with_sharding_constraint() provides more control over how the compiler constraints the distribution of intermediate values and outputs.",
      "It allows specifying the desired sharding for intermediate values within a computation."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of jax.lax.with_sharding_constraint() to control output sharding.",
        "code": "@jax\n.\njit\ndef f_contract_2(x):\n    out = x.sum(axis=0)\n    sharding = jax.sharding.NamedSharding(mesh, P('x'))\n    return jax.lax.with_sharding_constraint(out, sharding)\n\nresult = f_contract_2(arr_sharded)\njax.debug.visualize_array_sharding(result)\nprint(result)"
      },
      {
        "description": "Demonstrates the use of jax.lax.with_sharding_constraint() to control output sharding.",
        "code": "@jax\n.\njit\ndef f_contract_2(x):\n    out = x.sum(axis=0)\n    sharding = jax.sharding.NamedSharding(mesh, P('x'))\n    return jax.lax.with_sharding_constraint(out, sharding)\n\nresult = f_contract_2(arr_sharded)\njax.debug.visualize_array_sharding(result)\nprint(result)\n[48. 52. 56. 60. 64. 68. 72. 76.]"
      }
    ]
  },
  {
    "title": "Manual Parallelism with jax.experimental.shard_map.shard_map()",
    "concepts": [
      "With shard_map, the function handles a single shard of data, and shard_map constructs the full function.",
      "jax.sharding.Mesh allows for precise device placement.",
      "The in_specs argument determines the shard sizes.",
      "The out_specs argument identifies how the blocks are assembled back together.",
      "jax.experimental.shard_map.shard_map() code can work inside jax.jit().",
      "Aggregation-like functions require explicit collective operations like jax.lax.psum() when using shard_map."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of shard_map to apply a function to sharded data.",
        "code": "from\njax.experimental.shard_map\nimport\nshard_map\nmesh\n=\njax\n.\nmake_mesh\n((\n8\n,),\n(\n'x'\n,))\nf_elementwise_sharded\n=\nshard_map\n(\nf_elementwise\n,\nmesh\n=\nmesh\n,\nin_specs\n=\nP\n(\n'x'\n),\nout_specs\n=\nP\n(\n'x'\n))\narr\n=\njnp\n.\narange\n(\n32\n)\nf_elementwise_sharded\n(\narr\n)"
      },
      {
        "description": "Demonstrates the use of shard_map to apply a function to sharded data.",
        "code": "from\njax.experimental.shard_map\nimport\nshard_map\nmesh\n=\njax\n.\nmake_mesh\n((\n8\n,),\n(\n'x'\n,))\nf_elementwise_sharded\n=\nshard_map\n(\nf_elementwise\n,\nmesh\n=\nmesh\n,\nin_specs\n=\nP\n(\n'x'\n),\nout_specs\n=\nP\n(\n'x'\n))\narr\n=\njnp\n.\narange\n(\n32\n)\nf_elementwise_sharded\n(\narr\n)\nArray([ 1.        ,  2.682942  ,  2.818595  ,  1.28224   , -0.513605  ,\n       -0.9178486 ,  0.44116896,  2.3139732 ,  2.9787164 ,  1.824237  ,\n       -0.08804226, -0.99998045, -0.07314599,  1.8403342 ,  2.9812148 ,\n        2.3005757 ,  0.42419332, -0.92279506, -0.50197446,  1.2997544 ,\n        2.8258905 ,  2.6733112 ,  0.98229736, -0.69244075, -0.81115675,\n        0.7352965 ,  2.525117  ,  2.912752  ,  1.5418116 , -0.32726777,\n       -0.97606325,  0.19192469], dtype=float32)"
      },
      {
        "description": "Demonstrates how shard_map only 'sees' a single batch of the data.",
        "code": "x = jnp.arange(32)\nprint(f\"global shape: {x.shape=}\")\n\ndef f(x):\n    print(f\"device local shape: {x.shape=}\")\n    return x * 2\n\ny = shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)"
      },
      {
        "description": "Demonstrates how shard_map only 'sees' a single batch of the data.",
        "code": "x = jnp.arange(32)\nprint(f\"global shape: {x.shape=}\")\n\ndef f(x):\n    print(f\"device local shape: {x.shape=}\")\n    return x * 2\n\ny = shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)\nglobal shape: x.shape=(32,)\ndevice local shape: x.shape=(4,)"
      },
      {
        "description": "Demonstrates a shard_map of a jax.numpy.sum() function.",
        "code": "def f(x):\n    return jnp.sum(x, keepdims=True)\n\nshard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)"
      },
      {
        "description": "Demonstrates a shard_map of a jax.numpy.sum() function.",
        "code": "def f(x):\n    return jnp.sum(x, keepdims=True)\n\nshard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)\nArray([  6,  22,  38,  54,  70,  86, 102, 118], dtype=int32)"
      },
      {
        "description": "Demonstrates how to sum across shards using jax.lax.psum().",
        "code": "def f(x):\n    sum_in_shard = x.sum()\n    return jax.lax.psum(sum_in_shard, 'x')\n\nshard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P())(x)"
      },
      {
        "description": "Demonstrates how to sum across shards using jax.lax.psum().",
        "code": "def f(x):\n    sum_in_shard = x.sum()\n    return jax.lax.psum(sum_in_shard, 'x')\n\nshard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P())(x)\nArray(496, dtype=int32)"
      }
    ]
  },
  {
    "title": "Comparison of Parallelism Approaches with a Neural Network Layer",
    "concepts": [
      "Automatic parallelism with jax.jit() and appropriately sharded data.",
      "Semi-automatic parallelism with jax.lax.with_sharding_constraint() within a jax.jit()-decorated function.",
      "Manual parallelism with shard_map and explicit collective operations (jax.lax.psum())."
    ],
    "code_examples": [
      {
        "description": "Defines the canonical neural network layer function.",
        "code": "@jax\n.\njit\ndef layer(x, weights, bias):\n    return jax.nn.sigmoid(x @ weights + bias)"
      },
      {
        "description": "Defines the canonical neural network layer function.",
        "code": "@jax\n.\njit\ndef layer(x, weights, bias):\n    return jax.nn.sigmoid(x @ weights + bias)"
      },
      {
        "description": "Generates random input data for the layer.",
        "code": "import\nnumpy\nas\nnp\nrng\n=\nnp\n.\nrandom\n.\ndefault_rng\n(\n0\n)\nx\n=\nrng\n.\nnormal\n(\nsize\n=\n(\n32\n,))\nweights\n=\nrng\n.\nnormal\n(\nsize\n=\n(\n32\n,\n4\n))\nbias\n=\nrng\n.\nnormal\n(\nsize\n=\n(\n4\n,))\nlayer\n(\nx\n,\nweights\n,\nbias\n)"
      },
      {
        "description": "Generates random input data for the layer.",
        "code": "import\nnumpy\nas\nnp\nrng\n=\nnp\n.\nrandom\n.\ndefault_rng\n(\n0\n)\nx\n=\nrng\n.\nnormal\n(\nsize\n=\n(\n32\n,))\nweights\n=\nrng\n.\nnormal\n(\nsize\n=\n(\n32\n,\n4\n))\nbias\n=\nrng\n.\nnormal\n(\nsize\n=\n(\n4\n,))\nlayer\n(\nx\n,\nweights\n,\nbias\n)\nArray([0.02138912, 0.893112  , 0.59892005, 0.97742504], dtype=float32)"
      },
      {
        "description": "Automatically runs the layer in a distributed manner using jax.jit() and sharded data.",
        "code": "mesh = jax.make_mesh((8,), ('x',))\nsharding = jax.sharding.NamedSharding(mesh, P('x'))\nx_sharded = jax.device_put(x, sharding)\nweights_sharded = jax.device_put(weights, sharding)\nlayer(x_sharded, weights_sharded, bias)"
      },
      {
        "description": "Automatically runs the layer in a distributed manner using jax.jit() and sharded data.",
        "code": "mesh = jax.make_mesh((8,), ('x',))\nsharding = jax.sharding.NamedSharding(mesh, P('x'))\nx_sharded = jax.device_put(x, sharding)\nweights_sharded = jax.device_put(weights, sharding)\nlayer(x_sharded, weights_sharded, bias)\nArray([0.02138912, 0.893112  , 0.59892005, 0.97742504], dtype=float32)"
      },
      {
        "description": "Uses jax.lax.with_sharding_constraint() in the function to automatically distribute unsharded inputs.",
        "code": "@jax\n.\njit\ndef layer_auto(x, weights, bias):\n    x = jax.lax.with_sharding_constraint(x, sharding)\n    weights = jax.lax.with_sharding_constraint(weights, sharding)\n    return layer(x, weights, bias)\n\nlayer_auto(x, weights, bias)  # pass in unsharded inputs"
      },
      {
        "description": "Uses jax.lax.with_sharding_constraint() in the function to automatically distribute unsharded inputs.",
        "code": "@jax\n.\njit\ndef layer_auto(x, weights, bias):\n    x = jax.lax.with_sharding_constraint(x, sharding)\n    weights = jax.lax.with_sharding_constraint(weights, sharding)\n    return layer(x, weights, bias)\n\nlayer_auto(x, weights, bias)  # pass in unsharded inputs\nArray([0.02138914, 0.89311206, 0.5989201 , 0.97742516], dtype=float32)"
      },
      {
        "description": "Performs the same thing with shard_map, using jax.lax.psum() to indicate the cross-shard collective required for the matrix product.",
        "code": "from functools import partial\n\n@jax.jit\n@partial(\n    shard_map,\n    mesh=mesh,\n    in_specs=(P('x'), P('x', None), P(None)),\n    out_specs=P(None),\n)\ndef layer_sharded(x, weights, bias):\n    return jax.nn.sigmoid(jax.lax.psum(x @ weights, 'x') + bias)\n\nlayer_sharded(x, weights, bias)"
      },
      {
        "description": "Performs the same thing with shard_map, using jax.lax.psum() to indicate the cross-shard collective required for the matrix product.",
        "code": "from functools import partial\n\n@jax.jit\n@partial(\n    shard_map,\n    mesh=mesh,\n    in_specs=(P('x'), P('x', None), P(None)),\n    out_specs=P(None),\n)\ndef layer_sharded(x, weights, bias):\n    return jax.nn.sigmoid(jax.lax.psum(x @ weights, 'x') + bias)\n\nlayer_sharded(x, weights, bias)\nArray([0.02138914, 0.89311206, 0.5989201 , 0.97742516], dtype=float32)"
      }
    ]
  },
  {
    "title": "Conclusion and Further Resources",
    "concepts": [
      "This tutorial provided a brief introduction to sharded and parallel computation in JAX.",
      "Further resources include documentation on distributed arrays and automatic parallelization, and manual parallelism with shard_map."
    ],
    "code_examples": []
  },
  {
    "title": "Pure Functions and State in JAX",
    "concepts": [
      "JAX transformations like jit(), vmap(), grad() require pure functions.",
      "Pure functions' outputs depend solely on the inputs.",
      "Pure functions should have no side effects.",
      "State in machine learning can exist in model parameters, optimizer state, and stateful layers.",
      "This section provides advice on how to properly handle state in a JAX program."
    ],
    "code_examples": []
  },
  {
    "title": "Stateful Counter Example",
    "concepts": [
      "A stateful counter is implemented with a class.",
      "The counter's state is maintained in an attribute.",
      "The state is modified as a side effect of calling the count method.",
      "JIT-compiling a method with side effects leads to incorrect results because the side effects are executed only once during tracing."
    ],
    "code_examples": [
      {
        "description": "A simple stateful counter class that increments and resets a counter.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nclass\nCounter\n:\n    \"\"\"A simple counter.\"\"\"\n    def\n    __init__\n    (\n    self\n    ):\n        self\n        .n\n        =\n        0\n    def\n    count\n    (\n    self\n    )\n    ->\n    int\n    :\n        \"\"\"Increments the counter and returns the new value.\"\"\"\n        self\n        .n\n        +=\n        1\n        return\n        self\n        .n\n    def\n    reset\n    (\n    self\n    ):\n        \"\"\"Resets the counter to zero.\"\"\"\n        self\n        .n\n        =\n        0\n\ncounter\n=\nCounter\n()\nfor\n_\nin\nrange\n(\n3\n):\n    print\n    (\n    counter\n    .count\n    ())\n"
      },
      {
        "description": "Attempting to jit-compile the stateful counter's count method leads to incorrect results.",
        "code": "counter\n.reset\n()\nfast_count\n=\njax\n.jit\n(\ncounter\n.count\n)\nfor\n_\nin\nrange\n(\n3\n):\n    print\n    (\n    fast_count\n    ())\n"
      }
    ]
  },
  {
    "title": "Stateless Counter Example",
    "concepts": [
      "To fix the stateful counter issue, the state is made an argument of the count method.",
      "The count method returns a tuple containing the output and the updated state.",
      "The state is explicitly tracked by the user.",
      "This allows the count method to be safely jitted.",
      "This pattern of making state explicit is a common functional programming technique."
    ],
    "code_examples": [
      {
        "description": "A stateless counter class where the count method takes the counter state as an argument and returns the new state.",
        "code": "CounterState\n=\nint\nclass\nCounterV2\n:\n    def\n    count\n    (\n    self\n    ,\n    n\n    :\n    CounterState\n    )\n    ->\n    tuple\n    [\n    int\n    ,\n    CounterState\n    ]:\n        # You could just return n+1, but here we separate its role as\n        # the output and as the counter state for didactic purposes.\n        return\n        n\n        +\n        1\n        ,\n        n\n        +\n        1\n    def\n    reset\n    (\n    self\n    )\n    ->\n    CounterState\n    :\n        return\n        0\n\ncounter\n=\nCounterV2\n()\nstate\n=\ncounter\n.reset\n()\nfor\n_\nin\nrange\n(\n3\n):\n    value\n    ,\n    state\n    =\n    counter\n    .count\n    (\n    state\n    )\n    print\n    (\n    value\n    )\n"
      },
      {
        "description": "Jitting the stateless counter's count method works correctly.",
        "code": "state\n=\ncounter\n.reset\n()\nfast_count\n=\njax\n.jit\n(\ncounter\n.count\n)\nfor\n_\nin\nrange\n(\n3\n):\n    value\n    ,\n    state\n    =\n    fast_count\n    (\n    state\n    )\n    print\n    (\n    value\n    )\n"
      }
    ]
  },
  {
    "title": "Generalizing Stateful to Stateless Methods",
    "concepts": [
      "Stateful methods can be transformed into stateless methods by making the state an explicit argument and return value.",
      "The need for a class may become less clear when state is handled explicitly.",
      "OOP is a way to help programmers understand program state.",
      "The JAX pseudo-randomness API, jax.random, is an example of this strategy."
    ],
    "code_examples": []
  },
  {
    "title": "Linear Regression Example",
    "concepts": [
      "The strategy of threading state in and out of JAX functions is applied to a linear regression model.",
      "Model parameters are treated as state.",
      "Optimizer state and layer statistics for batchnorm are other examples of state that can be handled in this way.",
      "The update function carefully handles the input and output params."
    ],
    "code_examples": [
      {
        "description": "Defines the Params NamedTuple, init function, loss function, and the update function for linear regression using SGD.",
        "code": "from\ntyping\nimport\nNamedTuple\nclass\nParams\n(\nNamedTuple\n):\n    weight\n    :\n    jnp\n    .ndarray\n    bias\n    :\n    jnp\n    .ndarray\ndef\ninit\n(\nrng\n)\n->\nParams\n:\n    \"\"\"Returns the initial model params.\"\"\"\n    weights_key\n    ,\n    bias_key\n    =\n    jax\n    .random\n    .split\n    (\n    rng\n    )\n    weight\n    =\n    jax\n    .random\n    .normal\n    (\n    weights_key\n    ,\n    ())\n    bias\n    =\n    jax\n    .random\n    .normal\n    (\n    bias_key\n    ,\n    ())\n    return\n    Params\n    (\n    weight\n    ,\n    bias\n    )\ndef\nloss\n(\nparams\n:\nParams\n,\nx\n:\njnp\n.ndarray\n,\ny\n:\njnp\n.ndarray\n)\n->\njnp\n.ndarray\n:\n    \"\"\"Computes the least squares error of the model's predictions on x against y.\"\"\"\n    pred\n    =\n    params\n    .weight\n    *\n    x\n    +\n    params\n    .bias\n    return\n    jnp\n    .mean\n    ((\n    pred\n    -\n    y\n    )**\n    2\n    )\n\nLEARNING_RATE\n=\n0.005\n@jax\n.jit\ndef\nupdate\n(\nparams\n:\nParams\n,\nx\n:\njnp\n.ndarray\n,\ny\n:\njnp\n.ndarray\n)\n->\nParams\n:\n    \"\"\"Performs one SGD update step on params using the given data.\"\"\"\n    grad\n    =\n    jax\n    .grad\n    (\n    loss\n    )(\n    params\n    ,\n    x\n    ,\n    y\n    )\n    # If we were using Adam or another stateful optimizer,\n    # we would also do something like\n    #\n    #   updates, new_optimizer_state = optimizer(grad, optimizer_state)\n    #\n    # and then use `updates` instead of `grad` to actually update the params.\n    # (And we'd include `new_optimizer_state` in the output, naturally.)\n    new_params\n    =\n    jax\n    .tree\n    .map\n    (\n    lambda\n    param\n    ,\n    g\n    :\n    param\n    -\n    g\n    *\n    LEARNING_RATE\n    ,\n    params\n    ,\n    grad\n    )\n    return\n    new_params"
      },
      {
        "description": "Demonstrates how to fit the linear regression model and plot the results.",
        "code": "import\nmatplotlib.pyplot\nas\nplt\nrng\n=\njax\n.random\n.key\n(\n42\n)\n# Generate true data from y = w*x + b + noise\ntrue_w\n,\ntrue_b\n=\n2\n,\n-\n1\nx_rng\n,\nnoise_rng\n=\njax\n.random\n.split\n(\nrng\n)\nxs\n=\njax\n.random\n.normal\n(\nx_rng\n,\n(\n128\n,\n1\n))\nnoise\n=\njax\n.random\n.normal\n(\nnoise_rng\n,\n(\n128\n,\n1\n))\n*\n0.5\nys\n=\nxs\n*\ntrue_w\n+\ntrue_b\n+\nnoise\n# Fit regression\nparams\n=\ninit\n(\nrng\n)\nfor\n_\nin\nrange\n(\n1000\n):\n    params\n    =\n    update\n    (\n    params\n    ,\n    xs\n    ,\n    ys\n    )\nplt\n.scatter\n(\nxs\n,\nys\n)\nplt\n.plot\n(\nxs\n,\nparams\n.weight\n*\nxs\n+\nparams\n.bias\n,\nc\n=\n'red'\n,\nlabel\n=\n'Model Prediction'\n)\nplt\n.legend\n();"
      }
    ]
  },
  {
    "title": "Handling Many Parameters",
    "concepts": [
      "Handling parameters manually can become cumbersome with large neural networks.",
      "There are libraries that can help manage parameters automatically.",
      "See JAX Neural Network Libraries for examples."
    ],
    "code_examples": []
  },
  {
    "title": "JAX and Python Control Flow",
    "concepts": [
      "JAX code works with Python control flow and logical operators like NumPy code when executed eagerly (outside of JIT).",
      "Using control flow and logical operators with JIT is more complex.",
      "Python control flow and logical operators are evaluated at JIT compile time.",
      "The compiled function represents a single path through the control flow graph.",
      "If the path depends on the values of the inputs, the function cannot be JIT compiled by default.",
      "The path may depend on the shape or dtype of the inputs, causing recompilation on new shapes or dtypes."
    ],
    "code_examples": [
      {
        "description": "Demonstrates a simple loop that works with JIT because the loop range is known at compile time.",
        "code": "from\njax\nimport\ngrad\n,\njit\nimport\njax.numpy\nas\njnp\n@jit\ndef\nf\n(\nx\n):\n  for i in range(3):\n    x = 2 * x\n  return x\n\nprint(f(3))"
      },
      {
        "description": "Demonstrates another loop that works with JIT because the loop range depends on the shape of the input array, which is known at compile time.",
        "code": "from\njax\nimport\ngrad\n,\njit\nimport\njax.numpy\nas\njnp\n@jit\ndef g(x):\n  y = 0.\n  for i in range(x.shape[0]):\n    y = y + x[i]\n  return y\n\nprint(g(jnp.array([1., 2., 3.])))"
      },
      {
        "description": "Illustrates a case where JIT compilation fails due to a conditional statement that depends on the value of the input, resulting in a TracerBoolConversionError.",
        "code": "@jit\ndef f(x):\n  if x < 3:\n    return 3. * x**2\n  else:\n    return -4 * x\n\n# This will fail!\nf(2)"
      },
      {
        "description": "Shows another example where JIT compilation fails because of a logical operator that depends on the value of the input, resulting in a TracerBoolConversionError.",
        "code": "@jit\ndef g(x):\n  return (x > 0) and (x < 3)\n\n# This will fail!\ng(2)"
      }
    ]
  },
  {
    "title": "Controlling JIT Compilation with static_argnames",
    "concepts": [
      "JAX traces Python code with the ShapedArray abstraction as input, which represents the set of all array values with a fixed shape and dtype.",
      "When a conditional statement depends on a ShapedArray, the expression evaluates to an abstract ShapedArray, causing a TracerBoolConversionError.",
      "The static_argnames argument to jit can be used to specify tracing on concrete values of some arguments.",
      "Using static_argnames relaxes the traceability constraints and allows JIT to compile functions with control flow that depends on the values of static arguments.",
      "This involves a tradeoff: more refined abstract values allow more control flow in compiled functions but might require more recompilations."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use static_argnames to allow JIT to compile a function with a conditional statement that depends on the value of the input.",
        "code": "from\njax\nimport\ngrad\n,\njit\nimport\njax.numpy\nas\njnp\ndef f(x):\n  if x < 3:\n    return 3. * x**2\n  else:\n    return -4 * x\n\nf = jit(f, static_argnames='x')\n\nprint(f(2.))"
      },
      {
        "description": "Demonstrates using static_argnames with a loop, effectively unrolling the loop statically.",
        "code": "from\njax\nimport\ngrad\n,\njit\nimport\njax.numpy\nas\njnp\ndef f(x, n):\n  y = 0.\n  for i in range(n):\n    y = y + x[i]\n  return y\n\nf = jit(f, static_argnames='n')\n\nf(jnp.array([2., 3., 4.]), 2)"
      },
      {
        "description": "Illustrates a function where the output shape depends on the input value, leading to a TypeError when JIT-compiled without static_argnames.",
        "code": "from\njax\nimport\ngrad\n,\njit\nimport\njax.numpy\nas\njnp\ndef example_fun(length, val):\n  return jnp.ones((length,)) * val\n\n# un-jit'd works fine\nprint(example_fun(5, 4))\n\nbad_example_jit = jit(example_fun)\n# this will fail:\nbad_example_jit(10, 4)"
      },
      {
        "description": "Shows how to use static_argnames to resolve the TypeError in the previous example, allowing JIT to recompile when the 'length' argument changes.",
        "code": "from\njax\nimport\ngrad\n,\njit\nimport\njax.numpy\nas\njnp\ndef example_fun(length, val):\n  return jnp.ones((length,)) * val\n\n# un-jit'd works fine\nprint(example_fun(5, 4))\n\n# static_argnames tells JAX to recompile on changes at these argument positions:\ngood_example_jit = jit(example_fun, static_argnames='length')\n# first compile\nprint(good_example_jit(10, 4))\n# recompiles\nprint(good_example_jit(5, 4))"
      },
      {
        "description": "Demonstrates that global side-effects, like printing within a JIT compiled function, can lead to unexpected behavior.",
        "code": "from\njax\nimport\ngrad\n,\njit\nimport\njax.numpy\nas\njnp\n@jit\ndef f(x):\n  print(x)\n  y = 2 * x\n  print(y)\n  return y\n\nf(2)"
      }
    ]
  },
  {
    "title": "Structured Control Flow Primitives",
    "concepts": [
      "To avoid recompilations while using traceable control flow and avoiding unrolling large loops, JAX offers structured control flow primitives.",
      "These include lax.cond, lax.while_loop, lax.fori_loop, and lax.scan.",
      "lax.cond is a differentiable conditional.",
      "lax.while_loop is forward-mode-differentiable.",
      "lax.fori_loop is forward-mode-differentiable in general; forward and reverse-mode differentiable if endpoints are static.",
      "lax.scan is differentiable.",
      "jax.lax also provides lax.select (batched lax.cond) and lax.switch (multi-way conditional).",
      "jax.numpy provides numpy-style interfaces: jnp.where, jnp.piecewise, and jnp.select."
    ],
    "code_examples": [
      {
        "description": "Python equivalent of lax.cond",
        "code": "def cond(pred, true_fun, false_fun, operand):\n    if pred:\n        return true_fun(operand)\n    else:\n        return false_fun(operand)"
      },
      {
        "description": "Demonstrates the use of lax.cond for conditional execution.",
        "code": "from jax import lax\nimport jax.numpy as jnp\noperand = jnp.array([0.])\nlax.cond(True, lambda x: x + 1, lambda x: x - 1, operand)\n# --> array([1.], dtype=float32)\nlax.cond(False, lambda x: x + 1, lambda x: x - 1, operand)\n# --> array([-1.], dtype=float32)"
      },
      {
        "description": "Python equivalent of lax.while_loop",
        "code": "def while_loop(cond_fun, body_fun, init_val):\n    val = init_val\n    while cond_fun(val):\n        val = body_fun(val)\n    return val"
      },
      {
        "description": "Demonstrates the use of lax.while_loop for iterative execution.",
        "code": "from jax import lax\nimport jax.numpy as jnp\ninit_val = 0\ncond_fun = lambda x: x < 10\nbody_fun = lambda x: x + 1\nlax.while_loop(cond_fun, body_fun, init_val)\n# --> array(10, dtype=int32)"
      },
      {
        "description": "Python equivalent of lax.fori_loop",
        "code": "def fori_loop(start, stop, body_fun, init_val):\n    val = init_val\n    for i in range(start, stop):\n        val = body_fun(i, val)\n    return val"
      },
      {
        "description": "Demonstrates the use of lax.fori_loop for iterative execution with an index.",
        "code": "from jax import lax\nimport jax.numpy as jnp\ninit_val = 0\nstart = 0\nstop = 10\nbody_fun = lambda i, x: x + i\nlax.fori_loop(start, stop, body_fun, init_val)\n# --> array(45, dtype=int32)"
      }
    ]
  },
  {
    "title": "Logical Operators with JIT",
    "concepts": [
      "jax.numpy provides logical_and, logical_or, and logical_not, which operate element-wise on arrays and can be evaluated under jit without recompiling.",
      "Unlike Python's `and`, `or`, and `not`, these operators do not short-circuit.",
      "Bitwise operators (&, |, ~) can also be used with jit.",
      "Python's logical operators error when applied to JAX arrays of more than one element, even without jit."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the difference between Python's `and` and JAX's `jnp.logical_and` in the context of JIT.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\ndef python_check_positive_even(x):\n    is_even = x % 2 == 0\n    # `and` short-circults, so when `is_even` is `False`, `x > 0` is not evaluated.\n    return is_even and (x > 0)\n\n@jit\ndef jax_check_positive_even(x):\n    is_even = x % 2 == 0\n    # `logical_and` does not short circuit, so `x > 0` is always evaluated.\n    return jnp.logical_and(is_even, x > 0)\n\nprint(python_check_positive_even(24))\nprint(jax_check_positive_even(24))"
      },
      {
        "description": "Shows that JAX's logical_and returns element-wise values when applied to arrays.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef jax_check_positive_even(x):\n    is_even = x % 2 == 0\n    # `logical_and` does not short circuit, so `x > 0` is always evaluated.\n    return jnp.logical_and(is_even, x > 0)\n\nx = jnp.array([-1, 2, 5])\nprint(jax_check_positive_even(x))"
      }
    ]
  },
  {
    "title": "Control Flow with Grad (without JIT)",
    "concepts": [
      "The constraints on control flow and logical operators are relevant only with JIT.",
      "When using grad without jit, regular Python control-flow constructs can be used without problems, as if using Autograd, Pytorch, or TF Eager."
    ],
    "code_examples": [
      {
        "description": "Demonstrates that Python control flow works fine with grad without JIT.",
        "code": "from jax import grad\n\ndef f(x):\n  if x < 3:\n    return 3. * x**2\n  else:\n    return -4 * x\n\nprint(grad(f)(2.))\n# ok!\nprint(grad(f)(4.))\n# ok!"
      }
    ]
  },
  {
    "title": "Introduction to Advanced Autodiff in JAX",
    "concepts": [
      "This tutorial covers advanced applications of automatic differentiation (autodiff) in JAX.",
      "A basic understanding of JAX autodiff is assumed."
    ],
    "code_examples": [
      {
        "description": "Import necessary JAX libraries.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nkey = random.key(0)"
      }
    ]
  },
  {
    "title": "Higher-Order Derivatives",
    "concepts": [
      "JAX makes computing higher-order derivatives easy because derivative functions are themselves differentiable.",
      "The second-order derivative of a function is represented by its Hessian matrix.",
      "jax.jacfwd() and jax.jacrev() compute the Jacobian using forward- and reverse-mode autodiff respectively.",
      "The Hessian of a function can be computed as the Jacobian of its gradient."
    ],
    "code_examples": [
      {
        "description": "Define a function to compute the Hessian of a function f using jax.jacfwd and jax.grad.",
        "code": "def hessian(f):\n  return jax.jacfwd(jax.grad(f))"
      },
      {
        "description": "Define a function f(x) = x^T x and compute its Hessian.",
        "code": "def f(x):\n  return jnp.dot(x, x)\n\nhessian(f)(jnp.array([1., 2., 3.]))"
      }
    ]
  },
  {
    "title": "Differentiating Through Gradient Updates (Meta-Learning)",
    "concepts": [
      "Some meta-learning techniques, like MAML, require differentiating through gradient updates.",
      "JAX simplifies differentiating through gradient updates."
    ],
    "code_examples": [
      {
        "description": "Define a meta-loss function that computes the loss after one step of SGD and then compute the meta-gradients.",
        "code": "def meta_loss_fn(params, data):\n  \"\"\"Computes the loss after one step of SGD.\"\"\"\n  grads = jax.grad(loss_fn)(params, data)\n  return loss_fn(params - lr * grads, data)\n\nmeta_grads = jax.grad(meta_loss_fn)(params, data)"
      }
    ]
  },
  {
    "title": "Controlling Backpropagation with `jax.lax.stop_gradient()`",
    "concepts": [
      "jax.lax.stop_gradient() prevents backpropagation through a subset of the computational graph.",
      "This is useful for implementing algorithms like the TD(0) update in reinforcement learning.",
      "The TD(0) update is not the gradient of any loss function, but can be written as the gradient of a pseudo-loss function if the dependency of the target on the parameter is ignored."
    ],
    "code_examples": [
      {
        "description": "Define a value function and initial parameters for reinforcement learning.",
        "code": "# Value function and initial parameters\nvalue_fn = lambda theta, state: jnp.dot(theta, state)\ntheta = jnp.array([0.1, -0.1, 0.])"
      },
      {
        "description": "Define an example transition for reinforcement learning.",
        "code": "# An example transition.\ns_tm1 = jnp.array([1., 2., -1.])\nr_t = jnp.array(1.)\ns_t = jnp.array([2., 1., 0.])"
      },
      {
        "description": "Naive implementation of TD loss, which incorrectly includes the dependency of target on theta.",
        "code": "def td_loss(theta, s_tm1, r_t, s_t):\n  v_tm1 = value_fn(theta, s_tm1)\n  target = r_t + value_fn(theta, s_t)\n  return -0.5 * ((target - v_tm1) ** 2)\n\ntd_update = jax.grad(td_loss)\ndelta_theta = td_update(theta, s_tm1, r_t, s_t)\ndelta_theta"
      },
      {
        "description": "Correct TD loss implementation using jax.lax.stop_gradient to prevent backpropagation through the target.",
        "code": "def td_loss(theta, s_tm1, r_t, s_t):\n  v_tm1 = value_fn(theta, s_tm1)\n  target = r_t + value_fn(theta, s_t)\n  return -0.5 * ((jax.lax.stop_gradient(target) - v_tm1) ** 2)\n\ntd_update = jax.grad(td_loss)\ndelta_theta = td_update(theta, s_tm1, r_t, s_t)\ndelta_theta"
      },
      {
        "description": "Calculate delta_theta using the original TD(0) update expression.",
        "code": "s_grad = jax.grad(value_fn)(theta, s_tm1)\ndelta_theta_original_calculation = (r_t + value_fn(theta, s_t) - value_fn(theta, s_tm1)) * s_grad\ndelta_theta_original_calculation\n# [1.2, 2.4, -1.2], same as `delta_theta`"
      }
    ]
  },
  {
    "title": "Straight-Through Estimator",
    "concepts": [
      "The straight-through estimator defines a 'gradient' for non-differentiable functions.",
      "It pretends the function is the identity function during the backward pass.",
      "jax.lax.stop_gradient can be used to implement this trick."
    ],
    "code_examples": [
      {
        "description": "Implement the straight-through estimator for the round function.",
        "code": "def f(x):\n  return jnp.round(x)  # non-differentiable\n\ndef straight_through_f(x):\n  # Create an exactly-zero expression with Sterbenz lemma that has\n  # an exactly-one gradient.\n  zero = x - jax.lax.stop_gradient(x)\n  return zero + jax.lax.stop_gradient(f(x))\n\nprint(\"f(x): \", f(3.2))\nprint(\"straight_through_f(x):\", straight_through_f(3.2))\nprint(\"grad(f)(x):\", jax.grad(f)(3.2))\nprint(\"grad(straight_through_f)(x):\", jax.grad(straight_through_f)(3.2))"
      }
    ]
  },
  {
    "title": "Per-Example Gradients",
    "concepts": [
      "Computing per-example gradients is needed for techniques like prioritizing data based on gradient magnitude.",
      "JAX makes it easy to compute per-example gradients using jax.jit(), jax.vmap(), and jax.grad()."
    ],
    "code_examples": [
      {
        "description": "Compute per-example gradients using jax.jit(), jax.vmap(), and jax.grad().",
        "code": "perex_grads = jax.jit(jax.vmap(jax.grad(td_loss), in_axes=(None, 0, 0, 0)))\n\n# Test it:\nbatched_s_tm1 = jnp.stack([s_tm1, s_tm1])\nbatched_r_t = jnp.stack([r_t, r_t])\nbatched_s_t = jnp.stack([s_t, s_t])\nperex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)"
      },
      {
        "description": "First apply jax.grad() to td_loss to obtain a function that computes the gradient of the loss w.r.t. the parameters on single (unbatched) inputs",
        "code": "dtdloss_dtheta = jax.grad(td_loss)\ndtdloss_dtheta(theta, s_tm1, r_t, s_t)"
      },
      {
        "description": "Vectorize this function using jax.vmap(). This adds a batch dimension to all inputs and outputs.",
        "code": "almost_perex_grads = jax.vmap(dtdloss_dtheta)\nbatched_theta = jnp.stack([theta, theta])\nalmost_perex_grads(batched_theta, batched_s_tm1, batched_r_t, batched_s_t)"
      },
      {
        "description": "We fix this by adding in_axes to the jax.vmap() , specifying theta as None , and the other args as 0 . This makes the resulting function add an extra axis only to the other arguments, leaving theta unbatched, as we want:",
        "code": "inefficient_perex_grads = jax.vmap(dtdloss_dtheta, in_axes=(None, 0, 0, 0))\ninefficient_perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)"
      },
      {
        "description": "Now, you wrap the whole thing in a jax.jit() to get the compiled, efficient version of the same function",
        "code": "perex_grads = jax.jit(inefficient_perex_grads)\nperex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)"
      }
    ]
  },
  {
    "title": "Hessian-Vector Products",
    "concepts": [
      "A Hessian-vector product function is useful in truncated Newton Conjugate-Gradient algorithms.",
      "It avoids instantiating the full Hessian matrix.",
      "It can be implemented efficiently using jax.grad() and lexical closure.",
      "The key identity is: \u2202\u00b2f(x)v = \u2202[x \u21a6 \u2202f(x) \u22c5 v] = \u2202g(x), where g(x) = \u2202f(x) \u22c5 v."
    ],
    "code_examples": [
      {
        "description": "Implement a Hessian-vector product function using jax.grad() and jnp.vdot().",
        "code": "def hvp(f, x, v):\n  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
      }
    ]
  },
  {
    "title": "Jacobian Matrices",
    "concepts": [
      "Full Jacobian matrices can be computed using jax.jacfwd() and jax.jacrev().",
      "jax.jacfwd() uses forward-mode automatic differentiation, which is more efficient for 'tall' Jacobian matrices.",
      "jax.jacrev() uses reverse-mode, which is more efficient for 'wide' Jacobian matrices.",
      "These functions can also be used with container types (e.g., dictionaries)."
    ],
    "code_examples": [
      {
        "description": "Define a sigmoid function and a predict function.",
        "code": "from jax import jacfwd, jacrev\n# Define a sigmoid function.\ndef sigmoid(x):\n  return 0.5 * (jnp.tanh(x / 2) + 1)\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n  return sigmoid(jnp.dot(inputs, W) + b)"
      },
      {
        "description": "Build a toy dataset and initialize random model coefficients.",
        "code": "# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12, 0.77],\n                    [0.88, -1.08, 0.15],\n                    [0.52, 0.06, -1.30],\n                    [0.74, -2.49, 1.39]])\n# Initialize random model coefficients\nkey, W_key, b_key = random.split(key, 3)\nW = random.normal(W_key, (3,))\nb = random.normal(b_key, ())"
      },
      {
        "description": "Compute the Jacobian using jax.jacfwd() and jax.jacrev().",
        "code": "# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nJ = jacfwd(f)(W)\nprint(\"jacfwd result, with shape\", J.shape)\nprint(J)\n\nJ = jacrev(f)(W)\nprint(\"jacrev result, with shape\", J.shape)\nprint(J)"
      },
      {
        "description": "Compute the Jacobian with dictionary as input.",
        "code": "def predict_dict(params, inputs):\n  return predict(params['W'], params['b'], inputs)\n\nJ_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\nfor k, v in J_dict.items():\n  print(\"Jacobian from {} to logits is\".format(k))\n  print(v)"
      }
    ]
  },
  {
    "title": "Dense Hessian Matrices",
    "concepts": [
      "Dense Hessian matrices can be computed by composing jax.jacfwd() and jax.jacrev().",
      "Forward-over-reverse (jacfwd(jacrev(f))) is typically the most efficient composition."
    ],
    "code_examples": [
      {
        "description": "Define a function to compute the Hessian of a function f using jax.jacfwd and jax.jacrev.",
        "code": "def hessian(f):\n  return jacfwd(jacrev(f))\n\nH = hessian(f)(W)\nprint(\"hessian, with shape\", H.shape)\nprint(H)"
      }
    ]
  },
  {
    "title": "Forward-Mode Automatic Differentiation: Jacobian-Vector Products",
    "concepts": [
      "Forward-mode automatic differentiation computes Jacobian-vector products (JVPs).",
      "The Jacobian can be thought of as a linear map or a matrix.",
      "The Jacobian-vector product is written as (x, v) \u21a6 \u2202f(x)v.",
      "JAX's jax.jvp() function models this transformation.",
      "Forward-mode is memory-efficient, but computationally expensive for wide Jacobians."
    ],
    "code_examples": [
      {
        "description": "Compute the Jacobian-vector product using jax.jvp().",
        "code": "from jax import jvp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nkey, subkey = random.split(key)\nv = random.normal(subkey, W.shape)\n\n# Push forward the vector `v` along `f` evaluated at `W`\ny, u = jvp(f, (W,), (v,))"
      }
    ]
  },
  {
    "title": "Reverse-Mode Automatic Differentiation: Vector-Jacobian Products",
    "concepts": [
      "Reverse-mode automatic differentiation computes vector-Jacobian products (VJPs).",
      "The vector-Jacobian product is written as (x, v) \u21a6 v \u2202f(x).",
      "Reverse-mode is computationally efficient for wide Jacobians.",
      "The corresponding map on cotangent spaces is often called the pullback of f at x."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Checkpointing",
    "concepts": [
      "JAX automatic differentiation can be controlled using jax.checkpoint() (also known as jax.remat()).",
      "jax.checkpoint() trades off memory and FLOPs by controlling which intermediates are saved on the forward pass versus recomputed on the backward pass.",
      "Residuals are values saved during the forward pass for use in the backward pass.",
      "If jax.checkpoint() is not used, jax.grad(f)(x) stores Jacobian coefficients and other intermediates to use during the backward pass."
    ],
    "code_examples": [
      {
        "description": "Import JAX and JAX NumPy and define functions g and f to demonstrate checkpointing.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f(W1, W2, W3, x):\n    x = g(W1, x)\n    x = g(W2, x)\n    x = g(W3, x)\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\n# Inspect the 'residual' values to be saved on the forward pass\n# if you were to evaluate `jax.grad(f)(W1, W2, W3, x)`\nfrom jax.ad_checkpoint import print_saved_residuals\njax.ad_checkpoint.print_saved_residuals(f, W1, W2, W3, x)"
      }
    ]
  },
  {
    "title": "Applying jax.checkpoint()",
    "concepts": [
      "Applying jax.checkpoint() to a sub-function forces JAX not to save any of that sub-function\u2019s residuals.",
      "Only the inputs of a jax.checkpoint()-decorated function may be saved.",
      "Residuals consumed on the backward pass are re-computed from the inputs of the checkpointed function.",
      "Rematerialization policies control which values are saveable without having to edit the function definition."
    ],
    "code_examples": [
      {
        "description": "Apply jax.checkpoint() to the sub-function g to reduce the number of saved residuals.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f2(W1, W2, W3, x):\n    x = jax.checkpoint(g)(W1, x)\n    x = jax.checkpoint(g)(W2, x)\n    x = jax.checkpoint(g)(W3, x)\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\njax.ad_checkpoint.print_saved_residuals(f2, W1, W2, W3, x)"
      },
      {
        "description": "Use a rematerialization policy to save only the results of dot operations with no batch dimensions.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f(W1, W2, W3, x):\n    x = g(W1, x)\n    x = g(W2, x)\n    x = g(W3, x)\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\nf3 = jax.checkpoint(\n    f,\n    policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable\n)\njax.ad_checkpoint.print_saved_residuals(f3, W1, W2, W3, x)"
      },
      {
        "description": "Use policies to refer to intermediate values named using jax.ad_checkpoint.checkpoint_name().",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.ad_checkpoint import checkpoint_name\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f(W1, W2, W3, x):\n    x = checkpoint_name(g(W1, x), name='a')\n    x = checkpoint_name(g(W2, x), name='b')\n    x = checkpoint_name(g(W3, x), name='c')\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\nf4 = jax.checkpoint(\n    f,\n    policy=jax.checkpoint_policies.save_only_these_names('a')\n)\njax.ad_checkpoint.print_saved_residuals(f4, W1, W2, W3, x)"
      }
    ]
  },
  {
    "title": "Analyzing Forward and Backward Computations",
    "concepts": [
      "A custom print_fwd_bwd utility can be used to examine the forward and backward computations.",
      "jax.linearize() and jax.vjp() provide flexibility in when values are computed, offering memory vs. FLOPs tradeoffs.",
      "jax.checkpoint() controls these choices.",
      "Jacobian coefficient computations can be performed on the forward or backward pass.",
      "Function composition can be optimized for memory usage by recomputing values on the backward pass."
    ],
    "code_examples": [
      {
        "description": "Define a function to print the forward and backward computations of a JAX function.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import tree_flatten, tree_unflatten\nfrom rich.console import Console\nfrom rich.table import Table\nimport rich.text\n\ndef print_fwd_bwd(f, *args, **kwargs) -> None:\n    args, in_tree = tree_flatten((args, kwargs))\n\n    def f_(*args):\n        args, kwargs = tree_unflatten(in_tree, args)\n        return f(*args, **kwargs)\n\n    fwd = jax.make_jaxpr(lambda *args: jax.vjp(f_, *args))(*args).jaxpr\n    y, f_vjp = jax.vjp(f_, *args)\n    res, in_tree = tree_flatten(f_vjp)\n\n    def g_(*args):\n        *res, y = args\n        f_vjp = tree_unflatten(in_tree, res)\n        return f_vjp(y)\n\n    bwd = jax.make_jaxpr(g_)(*res, y).jaxpr\n    table = Table(show_header=False, show_lines=True, padding=(1, 2, 0, 2), box=None)\n    table.add_row(\"[bold green]forward computation:\", \"[bold green]backward computation:\")\n    table.add_row(\n        rich.text.Text.from_ansi(str(fwd)),\n        rich.text.Text.from_ansi(str(bwd))\n    )\n    console = Console(width=240, force_jupyter=True)\n    console.print(table)\n\ndef _renderable_repr(self):\n    return self.html\n\nimport rich.jupyter\nrich.jupyter.JupyterRenderable._repr_html_ = _renderable_repr"
      },
      {
        "description": "Demonstrate the usage of print_fwd_bwd with and without jax.checkpoint and different checkpoint policies.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import tree_flatten, tree_unflatten\nfrom rich.console import Console\nfrom rich.table import Table\nimport rich.text\n\ndef print_fwd_bwd(f, *args, **kwargs) -> None:\n    args, in_tree = tree_flatten((args, kwargs))\n\n    def f_(*args):\n        args, kwargs = tree_unflatten(in_tree, args)\n        return f(*args, **kwargs)\n\n    fwd = jax.make_jaxpr(lambda *args: jax.vjp(f_, *args))(*args).jaxpr\n    y, f_vjp = jax.vjp(f_, *args)\n    res, in_tree = tree_flatten(f_vjp)\n\n    def g_(*args):\n        *res, y = args\n        f_vjp = tree_unflatten(in_tree, res)\n        return f_vjp(y)\n\n    bwd = jax.make_jaxpr(g_)(*res, y).jaxpr\n    table = Table(show_header=False, show_lines=True, padding=(1, 2, 0, 2), box=None)\n    table.add_row(\"[bold green]forward computation:\", \"[bold green]backward computation:\")\n    table.add_row(\n        rich.text.Text.from_ansi(str(fwd)),\n        rich.text.Text.from_ansi(str(bwd))\n    )\n    console = Console(width=240, force_jupyter=True)\n    console.print(table)\n\ndef _renderable_repr(self):\n    return self.html\n\nimport rich.jupyter\nrich.jupyter.JupyterRenderable._repr_html_ = _renderable_repr\n\nimport jax\nimport jax.numpy as jnp\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f(W1, W2, W3, x):\n    x = g(W1, x)\n    x = g(W2, x)\n    x = g(W3, x)\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\nf3 = jax.checkpoint(\n    f,\n    policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable\n)\n\n# Without using `jax.checkpoint`:\nprint_fwd_bwd(f, W1, W2, W3, x)\n# Using `jax.checkpoint` with policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable:\nprint_fwd_bwd(f3, W1, W2, W3, x)"
      },
      {
        "description": "Alternative implementations of sin_vjp, showing different choices for computing Jacobian coefficients.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef sin_vjp(x):\n    y = jnp.sin(x)\n    cos_x = jnp.cos(x)\n    return y, lambda y_bar: cos_x * y_bar\n\ndef sin_vjp2(x):\n    y = jnp.sin(x)\n    return y, lambda y_bar: jnp.cos(x) * y_bar"
      },
      {
        "description": "VJP rule for a composition of two functions with and without checkpointing.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(x):\n    return x\n\ndef h(x):\n    return x\n\ndef f(x):\n    y = g(x)\n    z = h(y)\n    return z\n\ndef f_vjp(x):\n    y, g_vjp = jax.vjp(g, x)\n    z, h_vjp = jax.vjp(h, y)\n\n    def f_bwd(z_bar):\n        y_bar, = h_vjp(z_bar)\n        x_bar, = g_vjp(y_bar)\n        return x_bar\n\n    return z, f_bwd\n\ndef f_vjp_checkpoint(x):\n    y = g(x)\n    z, h_vjp = jax.vjp(h, y)\n\n    def f_bwd2(z_bar):\n        y_bar, = h_vjp(z_bar)\n        _, g_vjp = jax.vjp(g, x)\n        x_bar, = g_vjp(y_bar)\n        return x_bar\n\n    return z, f_bwd2"
      },
      {
        "description": "Using jax.checkpoint() in an alternative definition of the original function f.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(x):\n    return x\n\ndef h(x):\n    return x\n\ndef f_checkpoint(x):\n    y = jax.checkpoint(g)(x)\n    z = h(y)\n    return z"
      },
      {
        "description": "Illustrates the computation flow when evaluating jax.grad(f_checkpoint)(x).",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(x):\n    return x\n\ndef h(x):\n    return x\n\ndef f_checkpoint_grad(x):\n    y = g(x)  # step 1\n    _, h_vjp = jax.vjp(h)(y)  # step 2\n    y_bar, = h_vjp(1.0)  # step 3\n    _, g_vjp = jax.vjp(g, x)  # step 4\n    x_bar, = g_vjp(y_bar)  # step 5\n    return x_bar"
      },
      {
        "description": "Show that applying jax.checkpoint() to the whole function f won't save any memory.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x):\n    return x\n\ndef f_grad_bad(x):\n    _ = f(x)  # step 1\n    _, f_vjp = jax.vjp(f, x)  # step 2\n    x_bar, = f_vjp(1.0)  # step 3\n    return x_bar"
      },
      {
        "description": "Show that applying jax.checkpoint() to the last sub-function h won't save any memory.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(x):\n    return x\n\ndef h(x):\n    return x\n\ndef f_grad_bad2(x):\n    y, g_vjp = jax.vjp(g, x)  # step 1\n    z = h(y)  # step 2\n    _, h_vjp = jax.vjp(h, y)  # step 3\n    y_bar, = h_vjp(1.0)  # step 3\n    x_bar, = g_vjp(y_bar)  # step 5\n    return x_bar"
      }
    ]
  },
  {
    "title": "Policies for jax.checkpoint()",
    "concepts": [
      "Policies allow finer-grained control over what is saved vs. recomputed.",
      "A policy is a callable that determines whether a value should be saved as a residual.",
      "Policies should be selected from jax.checkpoint_policies attributes.",
      "Policies can refer to intermediate values named using jax.ad_checkpoint.checkpoint_name().",
      "jax.checkpoint_policies.offload_dot_with_no_batch_dims can offload to CPU memory instead of recomputing."
    ],
    "code_examples": [
      {
        "description": "Example loss, predict and layer functions for use with checkpointing policies.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef loss(params, x, y):\n    return jnp.sum((predict(params, x) - y)**2)\n\ndef predict(params, x):\n    *Ws, Wlast = params\n    for W in Ws:\n        x = layer(W, x)\n    x = jnp.dot(Wlast, x)\n    return x\n\ndef layer(W, x):\n    return jnp.sin(jnp.dot(W, x))\n\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)"
      },
      {
        "description": "Print the saved residuals of the loss function.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef loss(params, x, y):\n    return jnp.sum((predict(params, x) - y)**2)\n\ndef predict(params, x):\n    *Ws, Wlast = params\n    for W in Ws:\n        x = layer(W, x)\n    x = jnp.dot(Wlast, x)\n    return x\n\ndef layer(W, x):\n    return jnp.sin(jnp.dot(W, x))\n\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)\n\nfrom jax.ad_checkpoint import print_saved_residuals\n\nprint_saved_residuals(loss, params, x, y)"
      },
      {
        "description": "Use the dots_with_no_batch_dims_saveable policy with jax.checkpoint.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef loss(params, x, y):\n    return jnp.sum((predict(params, x) - y)**2)\n\ndef predict(params, x):\n    *Ws, Wlast = params\n    for W in Ws:\n        x = layer(W, x)\n    x = jnp.dot(Wlast, x)\n    return x\n\ndef layer(W, x):\n    return jnp.sin(jnp.dot(W, x))\n\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)\n\nfrom jax.ad_checkpoint import print_saved_residuals\n\nloss_checkpoint = jax.checkpoint(\n    loss,\n    policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable\n)\nprint_saved_residuals(loss_checkpoint, params, x, y)"
      },
      {
        "description": "Example demonstrating usage of checkpoint_name to name intermediate values, and then selectively checkpoint based on those names.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.ad_checkpoint import checkpoint_name\n\ndef loss(params, x, y):\n    return jnp.sum((predict(params, x) - y)**2)\n\ndef predict(params, x):\n    *Ws, Wlast = params\n    for i, W in enumerate(Ws):\n        x = layer(W, x)\n        x = checkpoint_name(x, name=f'layer{i}_output')\n    x = jnp.dot(Wlast, x)\n    return x\n\ndef layer(W, x):\n    return jnp.sin(jnp.dot(W, x))\n\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)\n\nfrom jax.ad_checkpoint import print_saved_residuals\n\nprint_saved_residuals(loss, params, x, y)"
      },
      {
        "description": "Using save_any_names_but_these policy to exclude certain named values from being saved.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.ad_checkpoint import checkpoint_name\n\ndef loss(params, x, y):\n    return jnp.sum((predict(params, x) - y)**2)\n\ndef predict(params, x):\n    *Ws, Wlast = params\n    for i, W in enumerate(Ws):\n        x = layer(W, x)\n        x = checkpoint_name(x, name=f'layer{i}_output')\n    x = jnp.dot(Wlast, x)\n    return x\n\ndef layer(W, x):\n    return jnp.sin(jnp.dot(W, x))\n\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)\n\nfrom jax.ad_checkpoint import print_saved_residuals\n\nloss_checkpoint2 = jax.checkpoint(\n    loss,\n    policy=jax.checkpoint_policies.save_any_names_but_these(\n        'layer1_output'\n    )\n)\nprint_saved_residuals(loss_checkpoint2, params, x, y)"
      },
      {
        "description": "Example of checkpoint offloading with jax.checkpoint_policies.offload_dot_with_no_batch_dims.",
        "code": "import functools\nimport jax\nimport jax.numpy as jnp\nfrom jax import lax\nfrom jax.ad_checkpoint import checkpoint\n\ndef checkpoint_offload_dot_with_no_batch_dims(self):\n    policy = jax.checkpoint_policies.offload_dot_with_no_batch_dims(\n        \"device\", \"pinned_host\"\n    )\n\n    @functools.partial(checkpoint, policy=policy)\n    def f(x):\n        x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n        x = jnp.sin(x)\n        x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n        x = jnp.sin(x)\n        x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n        x = jnp.sin(x)\n        x = jnp.sum(x)\n        return x"
      },
      {
        "description": "Example demonstrates usage of policies and jax.ad_checkpoint to save or offload certain checkpoint names.",
        "code": "import functools\nimport jax\nimport jax.numpy as jnp\nfrom jax import lax\nfrom jax.ad_checkpoint import checkpoint, checkpoint_name\nfrom jax._src import test_util as jtu\nimport numpy as np\nimport math\nfrom jax.sharding import NamedSharding, PartitionSpec as P\n\ndef checkpoint_names_saved_offloaded_recomputed(self):\n    mesh = jtu.create_mesh((2,), (\"x\",))\n    shape = (256, 128)\n    np_inp = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n    s = NamedSharding(mesh, P(\"x\"))\n    inp = jax.device_put(np_inp, s)\n    policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n        names_which_can_be_saved=[\"y\"],\n        names_which_can_be_offloaded=[\"z\"],\n        offload_src='device',\n        offload_dst='pinned_host'\n    )\n\n    @functools.partial(checkpoint, policy=policy)\n    def f(x):\n        def g(ys, _):\n            y, _ = ys\n            y = checkpoint_name(jnp.sin(y), \"y\")\n            z = checkpoint_name(jnp.sin(y), \"z\")\n            z = z.T\n            w = checkpoint_name(jnp.sin(z), \"w\")\n            return (w.T, jnp.sum(w)), None\n        _, scan_out = jax.lax.scan(g, (x, np.array(1, dtype=np.float32)), [np_inp])[0]\n        return scan_out"
      }
    ]
  },
  {
    "title": "Introduction to JAX Primitives",
    "concepts": [
      "A JAX primitive is the basic computational unit of a JAX program.",
      "JAX primitives support JAX transformations like jit, grad, and vmap.",
      "JAX primitives can operate on both concrete data values and abstract JAX values.",
      "JAX provides pre-defined primitives corresponding to most XLA operations and an implementation of NumPy functions.",
      "The set of JAX primitives is extensible.",
      "New functions can be defined using existing JAX primitives or by defining new primitives."
    ],
    "code_examples": []
  },
  {
    "title": "Defining Functions Using Existing JAX Primitives",
    "concepts": [
      "Functions can be defined using existing JAX primitives from jax.lax.",
      "This approach leverages the existing JAX infrastructure for transformations.",
      "The defined functions are JAX-traceable and transformable.",
      "Example: multiply-add can be implemented using jax.lax.add and jax.lax.mul."
    ],
    "code_examples": [
      {
        "description": "Implementation of multiply-add and square-add using `jax.lax` primitives, demonstrating their usage and differentiation.",
        "code": "from jax import lax\nfrom jax._src import api\n\ndef multiply_add_lax(x, y, z):\n    \"\"\"Implementation of multiply-add using the `jax.lax` primitives.\"\"\"\n    return lax.add(lax.mul(x, y), z)\n\ndef square_add_lax(a, b):\n    \"\"\"A square-add function using the newly defined multiply-add.\"\"\"\n    return multiply_add_lax(a, a, b)\n\nprint(\"square_add_lax = \", square_add_lax(2., 10.))\n# Differentiate w.r.t. the first argument\nprint(\"grad(square_add_lax) = \", api.grad(square_add_lax, argnums=0)(2.0, 10.))"
      }
    ]
  },
  {
    "title": "Helper Functions for Tracing",
    "concepts": [
      "Helper functions are introduced to trace function calls and understand how JAX uses primitives internally.",
      "The `trace` decorator helps in tracing arguments and results of functions.",
      "The `expectNotImplementedError` context manager checks for NotImplementedError exceptions.",
      "These functions are used for debugging and understanding JAX's internal workings."
    ],
    "code_examples": [
      {
        "description": "Helper functions for tracing function calls, including indentation management, succinct value printing, and a decorator for function tracing.",
        "code": "import functools\nimport traceback\n\n_indentation = 0\n\ndef _trace(msg=None):\n    \"\"\"Print a message at current indentation.\"\"\"\n    if msg is not None:\n        print(\"  \" * _indentation + msg)\n\n\ndef _trace_indent(msg=None):\n    \"\"\"Print a message and then indent the rest.\"\"\"\n    global _indentation\n    _trace(msg)\n    _indentation = 1 + _indentation\n\n\ndef _trace_unindent(msg=None):\n    \"\"\"Unindent then print a message.\"\"\"\n    global _indentation\n    _indentation = _indentation - 1\n    _trace(msg)\n\n\ndef trace(name):\n    \"\"\"A decorator for functions to trace arguments and results.\"\"\"\n    def trace_func(func):\n        # pylint: disable=missing-docstring\n        def pp(v):\n            \"\"\"Print certain values more succinctly\"\"\"\n            vtype = str(type(v))\n            if \"jax._src.xla_bridge._JaxComputationBuilder\" in vtype:\n                return \"<JaxComputationBuilder>\"\n            elif \"jaxlib.xla_extension.XlaOp\" in vtype:\n                return \"<XlaOp at 0x{:x}>\".format(id(v))\n            elif (\n                \"partial_eval.JaxprTracer\" in vtype\n                or \"batching.BatchTracer\" in vtype\n                or \"ad.JVPTracer\" in vtype\n            ):\n                return \"Traced<{}>\".format(v.aval)\n            elif isinstance(v, tuple):\n                return \"({})\".format(pp_values(v))\n            else:\n                return str(v)\n\n        def pp_values(args):\n            return \", \".join([pp(arg) for arg in args])\n\n        @functools.wraps(func)\n        def func_wrapper(*args):\n            _trace_indent(\"call {}({})\".format(name, pp_values(args)))\n            res = func(*args)\n            _trace_unindent(\"|<- {} = {}\".format(name, pp(res)))\n            return res\n\n        return func_wrapper\n\n    return trace_func\n\n\nclass expectNotImplementedError(object):\n    \"\"\"Context manager to check for NotImplementedError.\"\"\"\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, type, value, tb):\n        global _indentation\n        _indentation = 0\n        if type is NotImplementedError:\n            print(\"\\nFound expected exception:\")\n            traceback.print_exc(limit=3)\n            return True\n        elif type is None:\n            # No exception\n            assert False, \"Expected NotImplementedError\"\n        else:\n            return False"
      }
    ]
  },
  {
    "title": "Defining Functions Using `jax.numpy`",
    "concepts": [
      "Functions can be defined using `jax.numpy` which are already written in terms of `jax.lax` primitives.",
      "This provides a higher-level interface for defining JAX-traceable functions.",
      "The functions can then be differentiated using `jax.grad`.",
      "The tracing functions allow insight into the behavior when running and when calculating gradients."
    ],
    "code_examples": [
      {
        "description": "Implementation of multiply-add and square-add using jax.numpy primitives, demonstrating the use of tracing and differentiation.",
        "code": "import jax.numpy as jnp\nimport numpy as np\nfrom jax import api\n\n@trace(\"multiply_add_numpy\")\ndef multiply_add_numpy(x, y, z):\n    return jnp.add(jnp.multiply(x, y), z)\n\n\n@trace(\"square_add_numpy\")\ndef square_add_numpy(a, b):\n    return multiply_add_numpy(a, a, b)\n\n\nprint(\"\\nNormal evaluation:\")\nprint(\"square_add_numpy = \", square_add_numpy(2., 10.))\n\nprint(\"\\nGradient evaluation:\")\nprint(\"grad(square_add_numpy) = \", api.grad(square_add_numpy)(2.0, 10.))"
      }
    ]
  },
  {
    "title": "Defining New JAX Primitives",
    "concepts": [
      "New JAX primitives can be defined to encapsulate custom functionality.",
      "A `core.Primitive` object needs to be created and bound to a function.",
      "Initially, the newly defined primitive has no semantics defined, leading to errors when called.",
      "This section simulates the process of adding a new primitive for demonstration, even though it is better to use existing primitives if possible."
    ],
    "code_examples": [
      {
        "description": "Defining a new JAX primitive 'multiply_add' and its JAX-traceable usage, demonstrating the initial setup and the resulting error when trying to use it without further definitions.",
        "code": "from jax import core\n\nmultiply_add_p = core.Primitive(\"multiply_add\")  # Create the primitive\n\n\n@trace(\"multiply_add_prim\")\ndef multiply_add_prim(x, y, z):\n    \"\"\"The JAX-traceable way to use the JAX primitive.\n    Note that the traced arguments must be passed as positional arguments\n    to `bind`.\n    \"\"\"\n    return multiply_add_p.bind(x, y, z)\n\n\n@trace(\"square_add_prim\")\ndef square_add_prim(a, b):\n    \"\"\"A square-add function implemented using the new JAX-primitive.\"\"\"\n    return multiply_add_prim(a, a, b)"
      }
    ]
  },
  {
    "title": "Defining the Primitive Implementation",
    "concepts": [
      "The concrete implementation of the primitive needs to be defined using NumPy.",
      "This implementation is not JAX-traceable and operates on concrete values.",
      "The implementation is registered with JAX using `def_impl`.",
      "This provides the actual computation for the primitive when concrete values are provided."
    ],
    "code_examples": [
      {
        "description": "Defining the concrete implementation of the 'multiply_add' primitive using NumPy and registering it with JAX, enabling its use with concrete values.",
        "code": "import numpy as np\n\n@trace(\"multiply_add_impl\")\ndef multiply_add_impl(x, y, z):\n    \"\"\"Concrete implementation of the primitive.\n    This function does not need to be JAX traceable.\n    Args:\n    x, y, z: The concrete arguments of the primitive. Will only be called with\n        concrete values.\n    Returns:\n    the concrete result of the primitive.\n    \"\"\"\n    # Note: you can use the ordinary (non-JAX) NumPy, which is not JAX-traceable.\n    return np.add(np.multiply(x, y), z)\n\n\n# Now, register the primal implementation with JAX:\nmultiply_add_p.def_impl(multiply_add_impl)"
      }
    ]
  },
  {
    "title": "Abstract Evaluation",
    "concepts": [
      "For JIT and other transformations, JAX first evaluates functions abstractly.",
      "Abstract evaluation uses the shape and type of arguments to determine the sequence of primitives and compute the shape and type of operations.",
      "The abstraction can be `ShapedArray` (shape and type) or `ConcreteArray` (concrete value wrapped as abstract).",
      "The abstract evaluation function is registered using `def_abstract_eval`."
    ],
    "code_examples": [
      {
        "description": "Defining the abstract evaluation function for the 'multiply_add' primitive and registering it with JAX, enabling JAX to perform transformations like JIT.",
        "code": "from jax import core\n\n@trace(\"multiply_add_abstract_eval\")\ndef multiply_add_abstract_eval(xs, ys, zs):\n    \"\"\"Abstract evaluation of the primitive.\n    This function does not need to be JAX traceable. It will be invoked with\n    abstractions of the actual arguments\n    Args:\n    xs, ys, zs: Abstractions of the arguments.\n    Result:\n    a ShapedArray for the result of the primitive.\n    \"\"\"\n    assert xs.shape == ys.shape\n    assert xs.shape == zs.shape\n    return core.ShapedArray(xs.shape, xs.dtype)\n\n\n# Now, register the abstract evaluation with JAX:\nmultiply_add_p.def_abstract_eval(multiply_add_abstract_eval)"
      }
    ]
  },
  {
    "title": "XLA Compilation",
    "concepts": [
      "JAX compilation involves compiling each primitive into a graph of XLA operations.",
      "The set of XLA operations is limited, but XLA includes a `CustomCall` operation for arbitrary C++ functionality.",
      "The compilation to XLA of the primitive function does not need to be JAX-traceable.",
      "The compilation/lowering rule is registered using `register_lowering`."
    ],
    "code_examples": [
      {
        "description": "Defining the XLA lowering rule for the 'multiply_add' primitive and registering it with JAX for CPU platform, enabling JAX to compile the primitive into XLA operations.",
        "code": "from jax._src.lib.mlir.dialects import hlo\nfrom jax.interpreters import mlir\n\n@trace(\"multiply_add_lowering\")\ndef multiply_add_lowering(ctx, xc, yc, zc):\n    \"\"\"The compilation to XLA of the primitive.\n    Given an mlir.ir.Value for each argument, return the mlir.ir.Values for\n    the results of the function.\n    Does not need to be a JAX-traceable function.\n    \"\"\"\n    return [hlo.AddOp(hlo.MulOp(xc, yc), zc).result]\n\n\n# Now, register the lowering rule with JAX.\n# For GPU, refer to the https://jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html\nmlir.register_lowering(multiply_add_p, multiply_add_lowering, platform='cpu')"
      }
    ]
  },
  {
    "title": "Successful JIT Compilation",
    "concepts": [
      "After defining the implementation, abstract evaluation, and XLA compilation rules, `jax.jit` can be successfully applied.",
      "JAX first evaluates the function abstractly, then compiles the primitives.",
      "At the compilation point, the `multiply_add_lowering` function is invoked to translate to XLA."
    ],
    "code_examples": [
      {
        "description": "Demonstrating successful JIT compilation of the 'square_add_prim' function using the newly defined 'multiply_add' primitive, now fully integrated into the JAX ecosystem.",
        "code": "from jax import api\nassert api.jit(lambda x, y: square_add_prim(x, y))(2., 10.) == 14."
      }
    ]
  },
  {
    "title": "Introduction to Jaxprs",
    "concepts": [
      "Jaxprs are JAX's internal intermediate representation (IR) of programs.",
      "Jaxprs are explicitly typed, functional, first-order, and in algebraic normal form (ANF).",
      "JAX transformations trace-specialize Python functions into Jaxprs.",
      "Jaxprs represent a simple statically-typed expression language.",
      "Python interpreter is used to distill the computation into Jaxprs."
    ],
    "code_examples": []
  },
  {
    "title": "Jaxpr Structure and Representation",
    "concepts": [
      "A jaxpr instance represents a function with typed parameters and results.",
      "There are two related representations for jaxprs: jax.core.Jaxpr and jax.core.ClosedJaxpr.",
      "A jax.core.ClosedJaxpr represents a partially-applied jax.core.Jaxpr.",
      "The `jaxpr` field in a `ClosedJaxpr` represents the computation content.",
      "The `consts` field in a `ClosedJaxpr` is a list of constants.",
      "The parameters of the jaxpr are constvars (hoisted constants) and invars (inputs).",
      "Eqn* is a list of equations, defining intermediate variables and expressions.",
      "Expr+ is a list of output atomic expressions.",
      "Equations define variables as the result of applying a primitive on atomic expressions.",
      "unitvar or literal unit (*) represents a value that is not needed.",
      "Param* are zero or more named parameters to the primitive."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Jaxpr Primitives and Example",
    "concepts": [
      "Jaxpr primitives are first-order.",
      "Common jaxpr primitives are documented in the jax.lax module.",
      "JAX traces through normal Python conditionals, inlining control flow.",
      "Python level control-flow and Python-level functions execute normally."
    ],
    "code_examples": [
      {
        "description": "This code defines a simple function `func1` and uses `jax.make_jaxpr` to inspect the corresponding jaxpr.",
        "code": "from jax import make_jaxpr\nimport jax.numpy as jnp\n\ndef func1(first, second):\n  temp = first + jnp.sin(second) * 3.\n  return jnp.sum(temp)\n\nprint(make_jaxpr(func1)(jnp.zeros(8), jnp.ones(8)))"
      },
      {
        "description": "This example shows how JAX will inline the call to inner and the conditional if second.shape[0] > 4, and will produce the same jaxpr as before.",
        "code": "def func2(inner, first, second):\n  temp = first + inner(second) * 3.\n  return jnp.sum(temp)\n\ndef inner(second):\n  if second.shape[0] > 4:\n    return jnp.sin(second)\n  else:\n    assert False\n\ndef func3(first, second):\n  return func2(inner, first, second)\n\nprint(make_jaxpr(func3)(jnp.zeros(8), jnp.ones(8)))"
      },
      {
        "description": "This example demonstrates that JAX will flatten structured inputs and outputs. The `func4` function takes a tuple as input, but the resulting jaxpr has two input vars.",
        "code": "def func4(arg):\n  # The `arg` is a pair.\n  temp = arg[0] + jnp.sin(arg[1]) * 3.\n  return jnp.sum(temp)\n\nprint(make_jaxpr(func4)((jnp.zeros(8), jnp.ones(8))))"
      }
    ]
  },
  {
    "title": "Constants in Jaxprs",
    "concepts": [
      "Scalar constants are represented directly in the jaxpr equations.",
      "Non-scalar array constants are hoisted out to the top-level jaxpr as constvars."
    ],
    "code_examples": []
  },
  {
    "title": "Higher-Order Primitives: Conditionals",
    "concepts": [
      "Jaxpr includes several higher-order JAX primitives.",
      "To capture conditional expressions for dynamic execution, use `jax.lax.switch()` and `jax.lax.cond()`.",
      "`jax.lax.switch` and `jax.lax.cond` bind a primitive called `cond` internally.",
      "The cond primitive takes an integer denoting the index of the branch to execute.",
      "The cond primitive takes two operands. The first one is the branch index, and the second is the operand to be passed to selected branch."
    ],
    "code_examples": [
      {
        "description": "This code defines a function `one_of_three` using `jax.lax.switch` and prints its jaxpr. The jaxpr includes a `cond` primitive with three branches.",
        "code": "from jax import lax\n\ndef one_of_three(index, arg):\n  return lax.switch(index, [\n      lambda x: x + 1.,\n      lambda x: x - 2.,\n      lambda x: x + 3.\n  ], arg)\n\nprint(make_jaxpr(one_of_three)(1, 5.))"
      },
      {
        "description": "This code defines a function `func7` using `jax.lax.cond` and prints its jaxpr. The jaxpr includes a `cond` primitive with two branches for true and false conditions.",
        "code": "from jax import lax\n\ndef func7(arg):\n  return lax.cond(\n      arg >= 0.,\n      lambda xtrue: xtrue + 3.,\n      lambda xfalse: xfalse - 3.,\n      arg\n  )\n\nprint(make_jaxpr(func7)(5.))"
      },
      {
        "description": "This code defines a function `func8` using `jax.lax.cond` where the input to the branch functionals is a tuple, and the false branch functional contains a constant jnp.ones(1) that is hoisted as a constvar.",
        "code": "from jax import lax\nimport jax.numpy as jnp\n\ndef func8(arg1, arg2):\n  # Where `arg2` is a pair.\n  return lax.cond(\n      arg1 >= 0.,\n      lambda xtrue: xtrue[0],\n      lambda xfalse: jnp.array([1]) + xfalse[1],\n      arg2\n  )\n\nprint(make_jaxpr(func8)(5., (jnp.zeros(1), 2.)))"
      }
    ]
  },
  {
    "title": "Higher-Order Primitives: Loops",
    "concepts": [
      "Python loops are inlined during tracing.",
      "To capture a loop for dynamic execution, use `jax.lax.while_loop()` and `jax.lax.fori_loop()`.",
      "`jax.lax.while_loop()` is a primitive and `jax.lax.fori_loop()` is a helper that generates a while_loop primitive.",
      "C stands for the type of the loop \u201ccarry\u201d value."
    ],
    "code_examples": [
      {
        "description": "This code defines a function `func10` using `jax.lax.fori_loop` and prints its jaxpr. The jaxpr includes a `while` primitive representing the loop.",
        "code": "import numpy as np\nfrom jax import lax\nimport jax.numpy as jnp\n\ndef func10(arg, n):\n  ones = jnp.ones(arg.shape)  # A constant.\n  return lax.fori_loop(\n      0, n, lambda i, carry: carry + ones * 3. + arg, arg + ones\n  )\n\nprint(make_jaxpr(func10)(np.ones(16), 5))"
      }
    ]
  },
  {
    "title": "Higher-Order Primitives: Scan",
    "concepts": [
      "JAX supports loops over the elements of an array with statically known shape using `jax.lax.scan()`.",
      "The number of iterations is fixed, making the loop reverse-differentiable.",
      "C is the type of the scan carry, A is the element type of the input array(s), and B is the element type of the output array(s).",
      "The linear parameter describes whether the input variables are guaranteed to be used linearly in the body."
    ],
    "code_examples": [
      {
        "description": "This code defines a function `func11` using `jax.lax.scan` and prints its jaxpr. The jaxpr includes a `scan` primitive.",
        "code": "import numpy as np\nfrom jax import lax\nimport jax.numpy as jnp\n\ndef func11(arr, extra):\n  ones = jnp.ones(arr.shape)  #  A constant\n\n  def body(carry, aelems):\n    # carry: running dot-product of the two arrays\n    # aelems: a pair with corresponding elements from the two arrays\n    ae1, ae2 = aelems\n    return (carry + ae1 * ae2 + extra, carry)\n\n  return lax.scan(body, 0., (arr, ones))\n\nprint(make_jaxpr(func11)(np.ones(16), 5.))"
      }
    ]
  },
  {
    "title": "Higher-Order Primitives: Call",
    "concepts": [
      "The call primitive arises from JIT compilation.",
      "It encapsulates a sub-jaxpr along with parameters that specify the backend and device.",
      "The `pjit` primtive encapsulates a sub-jaxpr."
    ],
    "code_examples": [
      {
        "description": "This code defines a function `func12` with a JIT-compiled inner function and prints its jaxpr. The jaxpr includes a `pjit` primitive.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\ndef func12(arg):\n  @jit\n  def inner(x):\n    return x + arg * jnp.ones(1)  # Include a constant in the inner function.\n\n  return arg + inner(arg - 2.)\n\nprint(make_jaxpr(func12)(1.))"
      }
    ]
  },
  {
    "title": "Introduction to JAX and Functional Purity",
    "concepts": [
      "JAX is a language for transforming numerical programs.",
      "JAX can compile numerical programs for CPU or accelerators.",
      "JAX works best with functionally pure Python functions.",
      "Pure functions have inputs through parameters and outputs through results.",
      "Pure functions always return the same result for the same inputs."
    ],
    "code_examples": [
      {
        "description": "Import necessary libraries for JAX.",
        "code": "import numpy as np\nfrom jax import jit\nfrom jax import lax\nfrom jax import random\nimport jax\nimport jax.numpy as jnp"
      }
    ]
  },
  {
    "title": "Impure Functions and Side Effects in JAX",
    "concepts": [
      "JAX behaves differently than Python for impure functions.",
      "Impure functions can have side effects (e.g., printing or modifying globals).",
      "JAX may cache compiled functions, affecting when side effects occur.",
      "JAX re-runs Python functions when the type or shape of arguments change.",
      "JAX captures the value of global variables during the first run.",
      "Subsequent runs might use cached global values.",
      "JAX executes transformed functions with traced values for arguments.",
      "Modifying global variables is not guaranteed to work as expected with JAX."
    ],
    "code_examples": [
      {
        "description": "Example of a function with a printing side effect.  JAX may not print on every call due to caching.",
        "code": "def impure_print_side_effect(x):\n    print(\"Executing function\")  # This is a side-effect\n    return x\n\n# The side-effects appear during the first run\nprint(\"First call: \", jit(impure_print_side_effect)(4.))\n# Subsequent runs with parameters of same type and shape may not show the side-effect\n# This is because JAX now invokes a cached compilation of the function\nprint(\"Second call: \", jit(impure_print_side_effect)(5.))\n# JAX re-runs the Python function when the type or shape of the argument changes\nprint(\"Third call, different type: \", jit(impure_print_side_effect)(jnp.array([5.])))"
      },
      {
        "description": "Example of a function using a global variable.  JAX may cache the initial value of the global.",
        "code": "g = 0.\ndef impure_uses_globals(x):\n    return x + g\n\n# JAX captures the value of the global during the first run\nprint(\"First call: \", jit(impure_uses_globals)(4.))\ng = 10.\n# Update the global\n# Subsequent runs may silently use the cached value of the globals\nprint(\"Second call: \", jit(impure_uses_globals)(5.))\n# JAX re-runs the Python function when the type or shape of the argument changes\n# This will end up reading the latest value of the global\nprint(\"Third call, different type: \", jit(impure_uses_globals)(jnp.array([4.])))"
      },
      {
        "description": "Example of a function that saves to a global variable.  The global variable is only updated once with a JAX traced value.",
        "code": "g = 0.\ndef impure_saves_global(x):\n    global g\n    g = x\n    return x\n\n# JAX runs once the transformed function with special Traced values for arguments\nprint(\"First call: \", jit(impure_saves_global)(4.))\nprint(\"Saved global: \", g)"
      }
    ]
  },
  {
    "title": "Pure Functions with Internal State",
    "concepts": [
      "A Python function can be pure even if it uses stateful objects internally.",
      "The function should not read or write external state."
    ],
    "code_examples": [
      {
        "description": "Example of a pure function using internal state. The state dictionary is local to the function.",
        "code": "def pure_uses_internal_state(x):\n    state = dict(even=0, odd=0)\n    for i in range(10):\n        state['even' if i % 2 == 0 else 'odd'] += x\n    return state['even'] + state['odd']\n\nprint(jit(pure_uses_internal_state)(5.))"
      }
    ]
  },
  {
    "title": "Iterators and JAX",
    "concepts": [
      "It is not recommended to use iterators in JAX functions.",
      "Iterators introduce state, which is incompatible with JAX's functional model.",
      "Using iterators can lead to errors or unexpected results."
    ],
    "code_examples": [
      {
        "description": "Illustrates the behavior of using iterators within `lax.fori_loop`, showcasing an unexpected result of 0.",
        "code": "import jax.numpy as jnp\nfrom jax import make_jaxpr\nfrom jax import lax\n\n# lax.fori_loop\narray = jnp.arange(10)\nprint(lax.fori_loop(0, 10, lambda i, x: x + array[i], 0))\n# expected result 45\niterator = iter(range(10))\nprint(lax.fori_loop(0, 10, lambda i, x: x + next(iterator), 0))\n# unexpected result 0\n\n# lax.scan\ndef func11(arr, extra):\n  ones = jnp.ones(arr.shape)\n  def body(carry, aelems):\n    ae1, ae2 = aelems\n    return (carry + ae1 * ae2 + extra, carry)\n  return lax.scan(body, 0., (arr, ones))\n\nmake_jaxpr(func11)(jnp.arange(16), 5.)\n# make_jaxpr(func11)(iter(range(16)), 5.) # throws error\n\n# lax.cond\narray_operand = jnp.array([0.])\nlax.cond(True, lambda x: x + 1, lambda x: x - 1, array_operand)\niter_operand = iter(range(10))\n# lax.cond(True, lambda x: next(x)+1, lambda x: next(x)-1, iter_operand) # throws error"
      }
    ]
  },
  {
    "title": "Array Updates: Immutability and Functional Updates",
    "concepts": [
      "JAX arrays are immutable and do not support in-place updates.",
      "In-place mutation makes program analysis difficult.",
      "JAX provides functional array updates using the `.at` property.",
      "Array updates with `.at` operate out-of-place, returning a new array.",
      "Inside `jit`-compiled code, in-place optimization may occur if the original array is not reused.",
      "Indexed array updates can be used for addition, subtraction, etc."
    ],
    "code_examples": [
      {
        "description": "Illustrates in-place update in NumPy.",
        "code": "import numpy as np\n\nnumpy_array = np.zeros((3, 3), dtype=np.float32)\nprint(\"original array:\")\nprint(numpy_array)\n# In place, mutating update\nnumpy_array[1, :] = 1.0\nprint(\"updated array:\")\nprint(numpy_array)"
      },
      {
        "description": "Example of attempting to update a JAX array in-place, which will raise an error.",
        "code": "import jax.numpy as jnp\n\njax_array = jnp.zeros((3, 3), dtype=jnp.float32)\n# In place update of JAX's array will yield an error!\n# jax_array[1, :] = 1.0 # This will raise TypeError"
      },
      {
        "description": "Shows the correct way to update a JAX array using `.at`.",
        "code": "import jax.numpy as jnp\n\njax_array = jnp.zeros((3, 3), dtype=jnp.float32)\nupdated_array = jax_array.at[1, :].set(1.0)\nprint(\"updated array: \\n\", updated_array)"
      },
      {
        "description": "Demonstrates indexed addition using `.at`.",
        "code": "import jax.numpy as jnp\n\nprint(\"original array:\")\njax_array = jnp.ones((5, 6))\nprint(jax_array)\nnew_jax_array = jax_array.at[::2, 3:].add(7.)\nprint(\"new array post-addition:\")\nprint(new_jax_array)"
      }
    ]
  },
  {
    "title": "Out-of-Bounds Indexing",
    "concepts": [
      "JAX handles out-of-bounds indexing differently than NumPy.",
      "Out-of-bounds indexing does not raise errors in JAX.",
      "For array index updates (e.g., `index_add`), out-of-bounds updates are skipped.",
      "For array index retrieval, out-of-bounds indices are clamped to the bounds of the array.",
      "The `.at` property offers optional parameters for finer control over out-of-bound indices.",
      "Reverse-mode automatic differentiation may not preserve the semantics of out-of-bounds indexing."
    ],
    "code_examples": [
      {
        "description": "Out-of-bounds indexing in NumPy raises an error.",
        "code": "# np.arange(10)[11] # This will raise IndexError"
      },
      {
        "description": "Out-of-bounds indexing in JAX returns the last element of the array.",
        "code": "import jax.numpy as jnp\n\njnp.arange(10)[11]"
      },
      {
        "description": "Shows how to use the `.at` property to get a value with a specified fill value for out-of-bounds indices.",
        "code": "import jax.numpy as jnp\n\njnp.arange(10.0).at[11].get(mode='fill', fill_value=jnp.nan)"
      }
    ]
  },
  {
    "title": "Input Type Restrictions: Lists and Tuples",
    "concepts": [
      "JAX generally does not accept Python lists or tuples as inputs to its API functions.",
      "Passing lists or tuples can lead to silent performance degradation.",
      "Explicitly convert lists and tuples to JAX arrays before passing them to JAX functions."
    ],
    "code_examples": [
      {
        "description": "Example of a permissive sum function that converts a list to a JAX array.",
        "code": "import jax.numpy as jnp\nfrom jax import make_jaxpr\n\ndef permissive_sum(x):\n    return jnp.sum(jnp.array(x))\n\nx = list(range(10))\npermissive_sum(x)"
      },
      {
        "description": "Demonstrates how to explicitly convert a list to a JAX array before summing.",
        "code": "import jax.numpy as jnp\n\nx = list(range(10))\njnp.sum(jnp.array(x))"
      }
    ]
  },
  {
    "title": "Static Shapes and Dynamic Array Sizes",
    "concepts": [
      "JAX requires all output and intermediate arrays within transforms to have static shapes.",
      "Array shapes cannot depend on values within other arrays.",
      "Dynamically-sized arrays can cause errors with `jax.jit` and other transforms.",
      "Workarounds often exist to avoid the need for dynamically-sized arrays."
    ],
    "code_examples": [
      {
        "description": "Example of a `nansum` function that fails under JIT due to dynamic shape.",
        "code": "import jax.numpy as jnp\n\ndef nansum(x):\n    mask = ~jnp.isnan(x)\n    # boolean mask selecting non-nan values\n    x_without_nans = x[mask]\n    return x_without_nans.sum()\n\nx = jnp.array([1, 2, jnp.nan, 3, 4])\nprint(nansum(x))\n\n# from jax import jit\n# jit(nansum)(x)  # This will raise NonConcreteBooleanIndexError"
      },
      {
        "description": "Example of a `nansum` function that works under JIT by using `jnp.where` to avoid dynamic shapes.",
        "code": "import jax.numpy as jnp\nfrom jax import jit\n\n@jit\ndef nansum_2(x):\n    mask = ~jnp.isnan(x)\n    # boolean mask selecting non-nan values\n    return jnp.where(mask, x, 0).sum()\n\nx = jnp.array([1, 2, jnp.nan, 3, 4])\nprint(nansum_2(x))"
      }
    ]
  },
  {
    "title": "NaN Checking and Debugging",
    "concepts": [
      "JAX provides a NaN-checker for debugging NaN occurrences.",
      "The NaN-checker can be enabled via an environment variable, configuration setting, or command-line flag.",
      "Enabling the NaN-checker causes computations to error out immediately on production of a NaN.",
      "The NaN-checker adds a NaN check to every floating point type value.",
      "The NaN-checker may introduce performance regressions and device-host round-trips.",
      "The NaN-checker doesn't work with `pmap`; consider using `vmap` instead."
    ],
    "code_examples": [
      {
        "description": "Example showing how the NaN checker will error out on encountering a NaN value. (Requires JAX_DEBUG_NANS=True)",
        "code": "import jax.numpy as jnp\n\n# The following will raise FloatingPointError if JAX_DEBUG_NANS=True\n# jnp.divide(0., 0.)"
      },
      {
        "description": "Example showing how the NaN checker works with jit functions, triggering a de-optimized execution and providing a clear stack trace. (Requires JAX_DEBUG_NANS=True)",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef f(x, y):\n    a = x * y\n    b = (x + y) / (x - y)\n    c = a + 2\n    return a + b * c\n\nx = jnp.array([2., 0.])\ny = jnp.array([3., 0.])\n#The following will raise FloatingPointError if JAX_DEBUG_NANS=True\n#f(x, y)"
      }
    ]
  },
  {
    "title": "Default Floating Point Precision",
    "concepts": [
      "JAX defaults to single-precision numbers (float32).",
      "This is different from NumPy, which often promotes operands to double-precision.",
      "The single-precision default is suitable for many machine learning applications."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how the explicitly requested dtype float64 is truncated to float32 by default",
        "code": "from jax import random\nimport jax.numpy as jnp\n\nx = random.uniform(random.key(0), (1000,), dtype=jnp.float64)\nx.dtype"
      }
    ]
  },
  {
    "title": "Impure Functions and Global State",
    "concepts": [
      "Functions using global state or side-effects can exhibit different behavior after JIT compilation.",
      "jax.jit compiles and caches the function after the first execution, leading to consistent but potentially unexpected behavior with global variables.",
      "JIT compilation captures the initial value of global variables and uses it for subsequent executions, ignoring later changes."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how using a global variable and printing inside a JIT-compiled function results in different behavior compared to non-JIT execution.",
        "code": "y = 0\n# @jit   # Different behavior with jit\ndef impure_func(x):\n    print(\"Inside:\", y)\n    return x + y\nfor y in range(3):\n    print(\"Result:\", impure_func(y))\n\ny = 0\n# @jit   # Different behavior with jit\ndef impure_func(x):\n    print(\"Inside:\", y)\n    return x + y\nfor y in range(3):\n    print(\"Result:\", impure_func(y))"
      }
    ]
  },
  {
    "title": "XLA Compiler Optimizations and Floating-Point Arithmetic",
    "concepts": [
      "JIT compilation can change function outputs due to XLA compiler optimizations.",
      "XLA rearranges or elides operations for efficiency, potentially leading to slight differences in floating-point results.",
      "XLA can perform algebraic simplifications, such as replacing log(sqrt(x)) with 0.5 * log(x).",
      "Floating-point arithmetic is an approximation, and different computations of the same expression can yield subtly different results.",
      "XLA can sometimes produce more accurate results by avoiding overflows through recognizing and removing inverse operations (e.g., log(exp(x)))."
    ],
    "code_examples": [
      {
        "description": "Illustrates the slight difference in output due to XLA's algebraic simplification of log(sqrt(x)).",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\ndef f(x):\n    return jnp.log(jnp.sqrt(x))\n\nx = jnp.pi\nprint(f(x))\nprint(jit(f)(x))\nprint(jit(f)(x))"
      },
      {
        "description": "Demonstrates how XLA can remove inverse operations like log(exp(x)), producing a more accurate result compared to non-JIT execution that results in overflow.",
        "code": "def f(x):\n    return jnp.log(jnp.exp(x))\n\nx = 100.0\nprint(f(x))\nprint(jit(f)(x))\nprint(jit(f)(x))"
      }
    ]
  },
  {
    "title": "Long Compilation Times and Python Control Flow",
    "concepts": [
      "Long initial compilation times for JIT-compiled functions often indicate excessive code generation due to heavy use of Python control flow (e.g., for loops).",
      "For many loop iterations, JAX's structured control flow primitives (e.g., lax.scan()) should be preferred over Python loops.",
      "Wrapping loops with jit should be avoided if possible; use JIT-decorated functions inside the loop instead.",
      "jax.make_jaxpr() can be used to check the amount of code generated; long outputs indicate potential compilation issues.",
      "When dealing with arrays of different shapes, use functions like jax.numpy.where() for computation on padded arrays with fixed shapes."
    ],
    "code_examples": []
  },
  {
    "title": "JIT-compiling Class Methods",
    "concepts": [
      "Decorating class methods with jax.jit directly can lead to errors because JAX doesn't know how to handle the self argument.",
      "The self argument, being of the class type, is not a valid JAX type by default.",
      "One approach is to create a helper function outside the class and decorate it with jit, passing relevant class attributes as arguments.",
      "static_argnums can be used to mark the self argument as static, but this requires careful consideration of object mutations.",
      "Mutating a 'static' object after the first method call can lead to incorrect results because JAX caches the compilation based on object identity (hash).",
      "Overriding __hash__ and __eq__ can partially address this issue, but mutations of objects used as hash keys can still lead to problems.",
      "The most flexible approach is to register the class as a custom PyTree object using tree_flatten and tree_unflatten to define which components are static and dynamic."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the TypeError that arises when directly JIT-compiling a class method without handling the `self` argument.",
        "code": "import jax.numpy as jnp\nfrom jax import jit\n\nclass CustomClass:\n    def __init__(self, x: jnp.ndarray, mul: bool):\n        self.x = x\n        self.mul = mul\n\n    @jit\n    # <---- How to do this correctly?\n    def calc(self, y):\n        if self.mul:\n            return self.x * y\n        return y\n\nc = CustomClass(2, True)\n# This will raise a TypeError\n# c.calc(3)"
      },
      {
        "description": "Shows how to use a helper function outside the class and decorate it with jit, marking the first argument (mul) as static using partial.",
        "code": "from functools import partial\n\nclass CustomClass:\n    def __init__(self, x: jnp.ndarray, mul: bool):\n        self.x = x\n        self.mul = mul\n\n    def calc(self, y):\n        return _calc(self.mul, self.x, y)\n\n@partial(jit, static_argnums=0)\ndef _calc(mul, x, y):\n    if mul:\n        return x * y\n    return y\n\nc = CustomClass(2, True)\nprint(c.calc(3))"
      },
      {
        "description": "Illustrates the broken example of using static_argnums directly on the class method, leading to incorrect results after object mutation.",
        "code": "class CustomClass:\n    def __init__(self, x: jnp.ndarray, mul: bool):\n        self.x = x\n        self.mul = mul\n\n    # WARNING: this example is broken, as we'll see below. Don't copy & paste!\n    @partial(jit, static_argnums=0)\n    def calc(self, y):\n        if self.mul:\n            return self.x * y\n        return y\n\nc = CustomClass(2, True)\nprint(c.calc(3))\nc.mul = False\nprint(c.calc(3))  # Incorrect result due to object mutation"
      },
      {
        "description": "Demonstrates how to override __hash__ and __eq__ to partially address the object mutation problem, but still not ideal for truly mutable objects.",
        "code": "class CustomClass:\n    def __init__(self, x: jnp.ndarray, mul: bool):\n        self.x = x\n        self.mul = mul\n\n    @partial(jit, static_argnums=0)\n    def calc(self, y):\n        if self.mul:\n            return self.x * y\n        return y\n\n    def __hash__(self):\n        return hash((self.x, self.mul))\n\n    def __eq__(self, other):\n        return (isinstance(other, CustomClass) and\n                (self.x, self.mul) == (other.x, other.mul))"
      },
      {
        "description": "Shows the most flexible approach: registering the class as a custom PyTree object, handling both static and dynamic attributes, to enable proper JIT compilation of class methods.",
        "code": "from jax import tree_util\n\nclass CustomClass:\n    def __init__(self, x: jnp.ndarray, mul: bool):\n        self.x = x\n        self.mul = mul\n\n    @jit\n    def calc(self, y):\n        if self.mul:\n            return self.x * y\n        return y\n\n    def _tree_flatten(self):\n        children = (self.x,)  # arrays / dynamic values\n        aux_data = {'mul': self.mul}  # static values\n        return (children, aux_data)\n\n    @classmethod\n    def _tree_unflatten(cls, aux_data, children):\n        return cls(*children, **aux_data)\n\ntree_util.register_pytree_node(\n    CustomClass,\n    CustomClass._tree_flatten,\n    CustomClass._tree_unflatten\n)\n\nc = CustomClass(2, True)\nprint(c.calc(3))\nc.mul = False  # mutation is detected\nprint(c.calc(3))\nc = CustomClass(jnp.array(2), True)  # non-hashable x is supported\nprint(c.calc(3))"
      }
    ]
  },
  {
    "title": "Data and Computation Placement in JAX",
    "concepts": [
      "In JAX, computation follows data placement.",
      "JAX arrays have device placement and commit status properties.",
      "By default, JAX arrays are placed uncommitted on the default device.",
      "The default device is the first GPU or TPU, or CPU if neither is present.",
      "The default device can be overridden temporarily with jax.default_device() or permanently with the JAX_PLATFORMS environment variable.",
      "Computations involving uncommitted data are performed on the default device and the results are uncommitted on the default device.",
      "Data can be explicitly placed on a device using jax.device_put() with a device parameter, committing it to that device.",
      "Computations involving committed inputs happen on the committed device.",
      "Invoking an operation on arguments committed to multiple devices raises an error.",
      "jax.device_put() without a device parameter places data uncommitted on the default device if it isn't already on a device.",
      "Jitted functions follow data placement and raise errors if invoked on data committed to multiple devices."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how JAX arrays are created on the default device.",
        "code": "from jax import numpy as jnp\n\nprint(jnp.ones(3).devices())"
      },
      {
        "description": "Shows how to explicitly place data on a specific device using jax.device_put().",
        "code": "import jax\nfrom jax import device_put\n\narr = device_put(1, jax.devices()[2])\nprint(arr.devices())"
      }
    ]
  },
  {
    "title": "Benchmarking JAX Code",
    "concepts": [
      "When benchmarking JAX code, consider these differences from NumPy: JIT compilation, asynchronous dispatch, default 32-bit dtypes, and data transfer overhead.",
      "Apply jax.jit() to outer-most function calls for maximum performance.",
      "The first run of JAX code will be slower due to compilation.",
      "Call .block_until_ready() to ensure computation has completed due to asynchronous dispatch.",
      "Be mindful of data types: NumPy might use 64-bit by default. Use 32-bit dtypes in NumPy or enable 64-bit in JAX for a fair comparison.",
      "Transfer data to the target device before timing function evaluation.",
      "NumPy operations are executed eagerly, synchronously, and only on CPU.",
      "JAX operations may be executed eagerly or after compilation; they are dispatched asynchronously; and they can be executed on CPU, GPU, or TPU.",
      "Microbenchmarks of individual array operations on CPU typically favor NumPy due to lower dispatch overhead.",
      "JIT-compiled sequences of operations on CPU, GPU, or TPU often favor JAX."
    ],
    "code_examples": [
      {
        "description": "Illustrates a microbenchmark comparing JAX versus NumPy using IPython's %time and %timeit magics, considering JIT compilation, asynchronous dispatch, and data transfer.",
        "code": "import numpy as np\nimport jax.numpy as jnp\nimport jax\n\ndef f(x):\n    # function we're benchmarking (works in both NumPy & JAX)\n    return x.T @ (x - x.mean(axis=0))\n\nx_np = np.ones((1000, 1000), dtype=np.float32)  # same as JAX default dtype\n\n# %timeit f(x_np)  # measure NumPy runtime\n\n# %time x_jax = jax.device_put(x_np)  # measure JAX device transfer time\nf_jit = jax.jit(f)\n\n# %time f_jit(x_jax).block_until_ready()  # measure JAX compilation time\n\n# %timeit f_jit(x_jax).block_until_ready()  # measure JAX runtime"
      }
    ]
  },
  {
    "title": "Tracers and Abstract Values",
    "concepts": [
      "During JAX transformations, function arguments can be replaced with special tracer values.",
      "Tracer values carry abstract values (e.g., shape and dtype information), or concrete array data, used by JAX primitives.",
      "Abstract tracers are introduced by transformations like jax.jit(), jax.pmap(), jax.vmap(), and higher-order control flow primitives.",
      "Concrete tracers are introduced by jax.jvp() and jax.grad().",
      "Computations involving tracer values typically produce tracer values themselves.",
      "There are very few exceptions, such as casting tracer values to regular types (int(x), x.astype(float)), or getting the shape of a tracer.",
      "Conditional control flow based on data requires concrete values (regular or concrete tracers).",
      "jax.jit() introduces abstract tracers unless static_argnums is used to keep arguments as regular values.",
      "jax.grad() uses concrete tracers, allowing conditionals to be resolved based on the concrete value of the argument.",
      "Boolean expressions in control flow require concrete values."
    ],
    "code_examples": [
      {
        "description": "Shows that printing a function argument inside a jitted function will reveal it's a tracer value.",
        "code": "def func(x):\n    print(x)\n    return jnp.cos(x)\n\nres = jax.jit(func)(0.)"
      },
      {
        "description": "Demonstrates the need for static_argnums when using conditional control flow within a JIT-compiled function due to boolean expressions requiring concrete values.",
        "code": "def divide(x, y):\n    return x / y if y >= 1. else 0.\n\n# If we want to apply jax.jit(), we must ensure to specify static_argnums=1 to ensure y stays a regular value."
      }
    ]
  },
  {
    "title": "Buffer Donation",
    "concepts": [
      "Buffer donation can reduce memory consumption by reusing input buffers for outputs when the input is no longer needed after the computation.",
      "Use donate_argnums in jax.jit(), jax.pjit(), and jax.pmap() to specify which input buffers can be donated.",
      "Donated buffers must match the shape and element type of the output.",
      "If an argument is a pytree, all buffers for its components are donated.",
      "It is not allowed to use a donated buffer subsequently in the computation.",
      "A warning is issued if a donated buffer is not used because there are more donated buffers than needed for the outputs.",
      "Buffer donation currently does not work when calling your function with key-word arguments."
    ],
    "code_examples": [
      {
        "description": "Illustrates the use of donate_argnums in jax.jit() to donate the buffer for the `y` argument.",
        "code": "def add(x, y):\n    return x + y\n\nx = jax.device_put(np.ones((2, 3)))\ny = jax.device_put(np.ones((2, 3)))\n\n# Execute `add` with donation of the buffer for `y`. The result has\n# the same shape and type as `y`, so it will share its buffer.\nz = jax.jit(add, donate_argnums=(1,))(x, y)"
      },
      {
        "description": "Shows how buffer donation works with pytrees.",
        "code": "from typing import List\nfrom jax import Array\n\ndef add_ones(xs: List[Array]):\n    return [x + 1 for x in xs]\n\nxs = [jax.device_put(np.ones((2, 3))), jax.device_put(np.ones((3, 4)))]\n\n# Execute `add_ones` with donation of all the buffers for `xs`.\n# The outputs have the same shape and type as the elements of `xs`,\n# so they will share those buffers.\nz = jax.jit(add_ones, donate_argnums=0)(xs)"
      }
    ]
  },
  {
    "title": "Introduction to JAX User Guides",
    "concepts": [
      "User guides provide in-depth information on specific JAX topics.",
      "User guides are relevant for larger or deployed JAX projects."
    ],
    "code_examples": []
  },
  {
    "title": "Advanced JAX Topics",
    "concepts": [
      "Debugging and performance optimization are important aspects of JAX development.",
      "JAX provides interfaces for interacting with different parts of the system.",
      "Custom operations allow extending JAX functionality.",
      "Example applications showcase how to use JAX in real-world scenarios."
    ],
    "code_examples": []
  },
  {
    "title": "JAX and NumPy",
    "concepts": [
      "JAX provides a NumPy-inspired interface for convenience.",
      "JAX arrays can often be used as drop-in replacements of NumPy arrays due to duck-typing.",
      "Unlike NumPy arrays, JAX arrays are always immutable."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of NumPy for plotting.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\nx_np = np.linspace(0, 10, 1000)\ny_np = 2 * np.sin(x_np) * np.cos(x_np)\n\nplt.plot(x_np, y_np);"
      },
      {
        "description": "Demonstrates the use of JAX NumPy for plotting. Notice the similarity to the NumPy example.",
        "code": "import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx_jnp = jnp.linspace(0, 10, 1000)\ny_jnp = 2 * jnp.sin(x_jnp) * jnp.cos(x_jnp)\n\nplt.plot(x_jnp, y_jnp);"
      },
      {
        "description": "Example of mutating a NumPy array.",
        "code": "import numpy as np\n\n# NumPy: mutable arrays\nx = np.arange(10)\nx[0] = 10\nprint(x)"
      },
      {
        "description": "Demonstrates that JAX arrays are immutable and do not support in-place assignment.",
        "code": "import jax.numpy as jnp\n\n# JAX: immutable arrays\nx = jnp.arange(10)\nx[0] = 10"
      },
      {
        "description": "Shows how to update elements in a JAX array using indexed update syntax.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(10)\ny = x.at[0].set(10)\nprint(x)\nprint(y)"
      }
    ]
  },
  {
    "title": "jax.numpy vs jax.lax",
    "concepts": [
      "jax.numpy is a high-level wrapper that provides a familiar interface.",
      "jax.lax is a lower-level API that is stricter and often more powerful.",
      "All JAX operations are implemented in terms of operations in XLA \u2013 the Accelerated Linear Algebra compiler."
    ],
    "code_examples": [
      {
        "description": "Demonstrates implicit type promotion in jax.numpy.",
        "code": "import jax.numpy as jnp\n\njnp.add(1, 1.0)"
      },
      {
        "description": "Shows that jax.lax requires explicit type promotion.",
        "code": "from jax import lax\n\nlax.add(1, 1.0)"
      },
      {
        "description": "Demonstrates explicit type promotion with jax.lax.",
        "code": "import jax.numpy as jnp\nfrom jax import lax\n\nlax.add(jnp.float32(1), 1.0)"
      },
      {
        "description": "Example of 1D convolution using jax.numpy.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 1])\ny = jnp.ones(10)\njnp.convolve(x, y)"
      },
      {
        "description": "Demonstrates the use of `jax.lax.conv_general_dilated` for convolution, which is more general than the NumPy version.",
        "code": "import jax.numpy as jnp\nfrom jax import lax\n\nx = jnp.array([1, 2, 1])\ny = jnp.ones(10)\n\nresult = lax.conv_general_dilated(\n    x.reshape(1, 1, 3).astype(float),  # note: explicit promotion\n    y.reshape(1, 1, 10),\n    window_strides=(1,),\n    padding=[(len(y) - 1, len(y) - 1)],\n)\n# equivalent of padding='full' in NumPy\nresult[0, 0]"
      }
    ]
  },
  {
    "title": "Just-In-Time (JIT) Compilation",
    "concepts": [
      "By default JAX executes operations one at a time, in sequence.",
      "Using a just-in-time (JIT) compilation decorator, sequences of operations can be optimized together and run at once.",
      "Not all JAX code can be JIT compiled, as it requires array shapes to be static & known at compile time."
    ],
    "code_examples": [
      {
        "description": "Example of a function that normalizes the rows of a 2D matrix.",
        "code": "import jax.numpy as jnp\n\ndef norm(X):\n  X = X - X.mean(0)\n  return X / X.std(0)"
      },
      {
        "description": "Creates a just-in-time compiled version of the norm function using `jax.jit`.",
        "code": "from jax import jit\n\nnorm_compiled = jit(norm)"
      },
      {
        "description": "Compares the output of the original and JIT-compiled norm functions.",
        "code": "import jax.numpy as jnp\nimport numpy as np\n\nnp.random.seed(1701)\nX = jnp.array(np.random.rand(10000, 10))\nnp.allclose(norm(X), norm_compiled(X), atol=1E-6)"
      },
      {
        "description": "Defines a function to get negative values from an array. This function is not compatible with JIT compilation due to dynamic shape.",
        "code": "import jax.numpy as jnp\nimport numpy as np\n\ndef get_negatives(x):\n  return x[x < 0]\n\nx = jnp.array(np.random.randn(10))\nget_negatives(x)"
      },
      {
        "description": "Attempts to JIT-compile the get_negatives function, resulting in an error.",
        "code": "from jax import jit\n\njit(get_negatives)(x)"
      }
    ]
  },
  {
    "title": "JIT Tracing and Static Arguments",
    "concepts": [
      "JIT and other JAX transforms work by tracing a function to determine its effect on inputs of a specific shape and type.",
      "Variables that you don\u2019t want to be traced can be marked as static"
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the tracing behavior of JIT. The print statements show tracer objects instead of actual data.",
        "code": "import jax.numpy as jnp\nfrom jax import jit\nimport numpy as np\n\n@jit\ndef f(x, y):\n  print(\"Running f():\")\n  print(f\"  x = {x}\")\n  print(f\"  y = {y}\")\n  result = jnp.dot(x + 1, y + 1)\n  print(f\"  result = {result}\")\n  return result\n\nx = np.random.randn(3, 4)\ny = np.random.randn(4)\nf(x, y)"
      },
      {
        "description": "Shows the use of `jax.make_jaxpr` to view the JAX expression (jaxpr) of a function.",
        "code": "import jax.numpy as jnp\nfrom jax import make_jaxpr\n\ndef f(x, y):\n  return jnp.dot(x + 1, y + 1)\n\nx = np.random.randn(3, 4)\ny = np.random.randn(4)\nmake_jaxpr(f)(x, y)"
      },
      {
        "description": "Illustrates that control flow based on traced values is not allowed in JIT-compiled functions.",
        "code": "from jax import jit\n\n@jit\ndef f(x, neg):\n  return -x if neg else x\n\nf(1, True)"
      },
      {
        "description": "Shows how to mark a variable as static using `static_argnums` in `jax.jit`.",
        "code": "from functools import partial\nfrom jax import jit\n\n@partial(jit, static_argnums=(1,))\ndef f(x, neg):\n  return -x if neg else x\n\nf(1, True)"
      }
    ]
  },
  {
    "title": "Static vs. Traced Operations",
    "concepts": [
      "Just as values can be either static or traced, operations can be static or traced.",
      "Static operations are evaluated at compile-time in Python; traced operations are compiled & evaluated at run-time in XLA.",
      "Use numpy for operations that you want to be static; use jax.numpy for operations that you want to be traced."
    ],
    "code_examples": [
      {
        "description": "Example showing how static values can inadvertently become traced due to the use of jax.numpy operations, leading to errors in JIT-compiled code.",
        "code": "import jax.numpy as jnp\nfrom jax import jit\n\n@jit\ndef f(x):\n  return x.reshape(jnp.array(x.shape).prod())\n\nx = jnp.ones((2, 3))\nf(x)"
      },
      {
        "description": "Demonstrates the correct way to perform static operations using NumPy within a JIT-compiled function.",
        "code": "from jax import jit\nimport jax.numpy as jnp\nimport numpy as np\n\n@jit\ndef f(x):\n  return x.reshape((np.prod(x.shape),))\n\nx = jnp.ones((2, 3))\nf(x)"
      }
    ]
  },
  {
    "title": "Perfetto Profiler",
    "concepts": [
      "The JAX profiler can generate traces of a JAX program.",
      "Traces can be visualized using the Perfetto visualizer.",
      "jax.profiler.trace() blocks the program until the Perfetto UI loads the trace.",
      "An SSH tunnel on port 9001 is needed when profiling code running remotely.",
      "jax.profiler.start_server(<port>) starts a profiling server in the script.",
      "jax.profiler.stop_server() shuts down the profiling server.",
      "jax.collect_profile can manually capture traces.",
      "Traces are dumped into a temporary directory by default, which can be overridden using --log_dir.",
      "The Perfetto link can be disabled using --no_perfetto_link."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using jax.profiler.trace to profile a JAX program and generate a Perfetto link.",
        "code": "import jax\n\nwith jax.profiler.trace(\"/tmp/jax-trace\", create_perfetto_link=True):\n    # Run the operations to be profiled\n    key = jax.random.key(0)\n    x = jax.random.normal(key, (5000, 5000))\n    y = x @ x\n    y.block_until_ready()"
      },
      {
        "description": "Command to establish an SSH tunnel for remote profiling.",
        "code": "$ ssh -L 9001:127.0.0.1:9001 <user>@<host>"
      },
      {
        "description": "Command to establish an SSH tunnel for remote profiling using Google Cloud.",
        "code": "$ gcloud compute ssh <machine-name> -- -L 9001:127.0.0.1:9001"
      },
      {
        "description": "Command to manually capture a trace using jax.collect_profile.",
        "code": "$ python -m jax.collect_profile <port> <duration_in_ms>"
      }
    ]
  },
  {
    "title": "TensorBoard Profiling",
    "concepts": [
      "TensorBoard's profiler can be used to profile JAX programs.",
      "TensorBoard is a great way to acquire and visualize performance traces and profiles of your program, including activity on GPU and TPU.",
      "The TensorBoard profiler is only available with the version of TensorBoard bundled with TensorFlow.",
      "Install tensorflow and tensorboard-plugin-profile using pip.",
      "Install tf-nightly, tb-nightly, and tbp-nightly for the nightly version of TensorBoard profiler.",
      "jax.profiler.start_trace() and jax.profiler.stop_trace() capture a profiler trace.",
      "jax.profiler.trace() context manager can be used as an alternative to start_trace and stop_trace.",
      "Use block_until_ready() to ensure on-device execution is captured by the trace.",
      "Start TensorBoard using tensorboard --logdir=/tmp/tensorboard.",
      "Add custom events using jax.profiler.TraceAnnotation and jax.profiler.annotate_function()."
    ],
    "code_examples": [
      {
        "description": "Install tensorflow and tensorboard-plugin-profile.",
        "code": "pip install tensorflow tensorboard-plugin-profile"
      },
      {
        "description": "Install tf-nightly, tb-nightly, and tbp-nightly for the nightly version of TensorBoard profiler.",
        "code": "pip install tf-nightly tb-nightly tbp-nightly"
      },
      {
        "description": "Example of using start_trace() and stop_trace() to capture a profiler trace.",
        "code": "import jax\n\njax.profiler.start_trace(\"/tmp/tensorboard\")\n# Run the operations to be profiled\nkey = jax.random.key(0)\nx = jax.random.normal(key, (5000, 5000))\ny = x @ x\ny.block_until_ready()\njax.profiler.stop_trace()"
      },
      {
        "description": "Example of using the jax.profiler.trace() context manager to capture a profiler trace.",
        "code": "import jax\n\nwith jax.profiler.trace(\"/tmp/tensorboard\"):\n    key = jax.random.key(0)\n    x = jax.random.normal(key, (5000, 5000))\n    y = x @ x\n    y.block_until_ready()"
      },
      {
        "description": "Command to start TensorBoard.",
        "code": "$ tensorboard --logdir=/tmp/tensorboard"
      },
      {
        "description": "Starting the profiler server in a Python program.",
        "code": "import jax.profiler\n\njax.profiler.start_server(9999)"
      },
      {
        "description": "Stopping the profiler server.",
        "code": "jax.profiler.stop_server()"
      }
    ]
  },
  {
    "title": "Troubleshooting GPU Profiling",
    "concepts": [
      "If GPU traces are missing, check program logs for error messages.",
      "Add the path to libcupti.so to the environment variable LD_LIBRARY_PATH.",
      "If the error is CUPTI_ERROR_INSUFFICIENT_PRIVILEGES, run specific commands and reboot.",
      "Run the JAX program on a remote machine and use SSH local port forwarding to access TensorBoard web UI."
    ],
    "code_examples": [
      {
        "description": "Setting the LD_LIBRARY_PATH environment variable.",
        "code": "export LD_LIBRARY_PATH=/usr/local/cuda-10.1/extras/CUPTI/lib64/:$LD_LIBRARY_PATH"
      },
      {
        "description": "Commands to resolve CUPTI_ERROR_INSUFFICIENT_PRIVILEGES.",
        "code": "echo 'options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"' | sudo tee -a /etc/modprobe.d/nvidia-kernel-common.conf\nsudo update-initramfs -u\nsudo reboot now"
      },
      {
        "description": "SSH command to forward the default TensorBoard port.",
        "code": "ssh -L 6006:localhost:6006 <remote server address>"
      },
      {
        "description": "Gcloud command to forward the default TensorBoard port.",
        "code": "$ gcloud compute ssh <machine-name> -- -L 6006:localhost:6006"
      }
    ]
  },
  {
    "title": "Resolving Duplicate Plugins Error",
    "concepts": [
      "Duplicate plugins error in TensorBoard can occur due to multiple versions of TensorBoard and/or TensorFlow installed.",
      "Uninstalling and reinstalling a single version of TensorFlow can resolve this."
    ],
    "code_examples": [
      {
        "description": "Commands to uninstall and reinstall TensorFlow to resolve duplicate plugins error.",
        "code": "pip uninstall tensorflow tf-nightly tensorboard tb-nightly\npip install tensorflow"
      }
    ]
  },
  {
    "title": "Nsight Profiling",
    "concepts": [
      "NVIDIA\u2019s Nsight tools can be used to trace and profile JAX code on GPU."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Device Memory Profiler",
    "concepts": [
      "JAX device memory profiler helps understand GPU/TPU memory usage.",
      "It can be used to identify memory usage by arrays and executables.",
      "It can be used to track down memory leaks.",
      "Tensorboard profiling is recommended for device memory analysis.",
      "The JAX device memory profiler emits output that can be interpreted using pprof."
    ],
    "code_examples": []
  },
  {
    "title": "Installing and Using pprof",
    "concepts": [
      "pprof from google/pprof is required.",
      "Install pprof using Go (version 1.16+).",
      "The gperftools version of pprof is incompatible with JAX.",
      "Use jax.profiler.save_device_memory_profile() to capture a device memory profile to disk.",
      "Use `pprof --web memory.prof` to visualize the profile in a web browser."
    ],
    "code_examples": [
      {
        "description": "Installing pprof using go",
        "code": "go\ninstall\ngithub.com/google/pprof@latest"
      }
    ]
  },
  {
    "title": "Capturing a Device Memory Profile",
    "concepts": [
      "jax.profiler.save_device_memory_profile() saves the memory profile.",
      "block_until_ready() ensures operations complete before profiling.",
      "The callgraph visualization shows the Python stack at the point of each live buffer allocation.",
      "Functions compiled with jax.jit() are opaque to the profiler; memory is attributed to the whole function."
    ],
    "code_examples": [
      {
        "description": "Example of capturing a device memory profile using jax.profiler.save_device_memory_profile().",
        "code": "import jax\nimport jax.numpy as jnp\nimport jax.profiler\n\ndef func1(x):\n    return jnp.tile(x, 10) * 0.5\n\ndef func2(x):\n    y = func1(x)\n    return y, jnp.tile(x, 10) + 1\n\nx = jax.random.normal(jax.random.key(42), (1000, 1000))\ny, z = func2(x)\nz.block_until_ready()\njax.profiler.save_device_memory_profile(\"memory.prof\")\n"
      }
    ]
  },
  {
    "title": "Tracking Down Memory Leaks",
    "concepts": [
      "pprof can visualize the change in memory usage between two profiles.",
      "The --diff_base option in pprof helps identify memory growth across iterations.",
      "Visualizing the final memory profile might not reveal the gradual memory accumulation."
    ],
    "code_examples": [
      {
        "description": "Example of capturing multiple device memory profiles to track down memory leaks using pprof's --diff_base.",
        "code": "import jax\nimport jax.numpy as jnp\nimport jax.profiler\n\ndef afunction():\n    return jax.random.normal(jax.random.key(77), (1000000,))\n\nz = afunction()\n\ndef anotherfunc():\n    arrays = []\n    for i in range(1, 10):\n        x = jax.random.normal(jax.random.key(42), (i, 10000))\n        arrays.append(x)\n        x.block_until_ready()\n        jax.profiler.save_device_memory_profile(f\"memory{i}.prof\")\n\nanotherfunc()\n"
      }
    ]
  },
  {
    "title": "Interactive Inspection with jax.debug",
    "concepts": [
      "Use `jax.debug.print()` to print values to stdout in JIT, pmap, and pjit decorated functions.",
      "Use `jax.debug.breakpoint()` to pause execution and inspect values in the call stack."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using `jax.debug.print()` and `jax.debug.breakpoint()` within a JIT-compiled function to print intermediate values and pause execution for inspection.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef f(x):\n    jax.debug.print(\"\ud83e\udd2f {x} \ud83e\udd2f\", x=x)\n    y = jnp.sin(x)\n    jax.debug.breakpoint()\n    jax.debug.print(\"\ud83e\udd2f {y} \ud83e\udd2f\", y=y)\n    return y\n\nf(2.)\n# Prints:\n# \ud83e\udd2f 2.0 \ud83e\udd2f\n# Enters breakpoint to inspect values!\n# \ud83e\udd2f 0.9092974662780762 \ud83e\udd2f"
      },
      {
        "description": "Demonstrates using `jax.debug.print()` and `jax.debug.breakpoint()` within a JIT-compiled function to print intermediate values and pause execution for inspection.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef f(x):\n    jax.debug.print(\"\ud83e\udd2f {x} \ud83e\udd2f\", x=x)\n    y = jnp.sin(x)\n    jax.debug.breakpoint()\n    jax.debug.print(\"\ud83e\udd2f {y} \ud83e\udd2f\", y=y)\n    return y\n\nf(2.)\n# Prints:\n# \ud83e\udd2f 2.0 \ud83e\udd2f\n# Enters breakpoint to inspect values!\n# \ud83e\udd2f 0.9092974662780762 \ud83e\udd2f"
      }
    ]
  },
  {
    "title": "Functional Error Checks with jax.experimental.checkify",
    "concepts": [
      "Checkify allows adding JIT-able runtime error checking to JAX code.",
      "Use `checkify.checkify` transformation with `checkify.check` function to add runtime checks.",
      "Checkify can automatically add common checks like index and float checks."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using `checkify.check` to add a runtime check for non-negative index.",
        "code": "from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, i):\n    checkify.check(i >= 0, \"index needs to be non-negative!\")\n    y = x[i]\n    z = jnp.sin(y)\n    return z\n\njittable_f = checkify.checkify(f)\nerr, z = jax.jit(jittable_f)(jnp.ones((5,)), -1)\nprint(err.get())\n# >> index needs to be non-negative! (check failed at <...>:6 (f))"
      },
      {
        "description": "Demonstrates using `checkify.check` to add a runtime check for non-negative index.",
        "code": "from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, i):\n    checkify.check(i >= 0, \"index needs to be non-negative!\")\n    y = x[i]\n    z = jnp.sin(y)\n    return z\n\njittable_f = checkify.checkify(f)\nerr, z = jax.jit(jittable_f)(jnp.ones((5,)), -1)\nprint(err.get())\n# >> index needs to be non-negative! (check failed at <...>:6 (f))"
      },
      {
        "description": "Demonstrates using `checkify` to automatically add common checks like index and float checks.",
        "code": "errors = checkify.user_checks | checkify.index_checks | checkify.float_checks\nchecked_f = checkify.checkify(f, errors=errors)\n\nerr, z = checked_f(jnp.ones((5,)), 100)\nerr.throw()\n# ValueError: out-of-bounds indexing at <..>:7 (f)\n\nerr, z = checked_f(jnp.ones((5,)), -1)\nerr.throw()\n# ValueError: index needs to be non-negative! (check failed at <\u2026>:6 (f))\n\nerr, z = checked_f(jnp.array([jnp.inf, 1]), 0)\nerr.throw()\n# ValueError: nan generated by primitive sin at <...>:8 (f)"
      },
      {
        "description": "Demonstrates using `checkify` to automatically add common checks like index and float checks.",
        "code": "errors = checkify.user_checks | checkify.index_checks | checkify.float_checks\nchecked_f = checkify.checkify(f, errors=errors)\n\nerr, z = checked_f(jnp.ones((5,)), 100)\nerr.throw()\n# ValueError: out-of-bounds indexing at <..>:7 (f)\n\nerr, z = checked_f(jnp.ones((5,)), -1)\nerr.throw()\n# ValueError: index needs to be non-negative! (check failed at <\u2026>:6 (f))\n\nerr, z = checked_f(jnp.array([jnp.inf, 1]), 0)\nerr.throw()\n# ValueError: nan generated by primitive sin at <...>:8 (f)"
      }
    ]
  },
  {
    "title": "Throwing Python Errors with JAX\u2019s Debug Flags",
    "concepts": [
      "Enable the `jax_debug_nans` flag to detect NaNs in JIT-compiled code.",
      "Enable the `jax_disable_jit` flag to disable JIT-compilation and use traditional Python debugging tools."
    ],
    "code_examples": [
      {
        "description": "Demonstrates enabling the `jax_debug_nans` flag to raise a `FloatingPointError` when a NaN is produced.",
        "code": "import jax\n\njax.config.update(\"jax_debug_nans\", True)\n\ndef f(x, y):\n    return x / y\n\njax.jit(f)(0., 0.)\n# ==> raises FloatingPointError exception!"
      },
      {
        "description": "Demonstrates enabling the `jax_debug_nans` flag to raise a `FloatingPointError` when a NaN is produced.",
        "code": "import jax\n\njax.config.update(\"jax_debug_nans\", True)\n\ndef f(x, y):\n    return x / y\n\njax.jit(f)(0., 0.)\n# ==> raises FloatingPointError exception!"
      }
    ]
  },
  {
    "title": "Introduction to jax.debug and jax.debug.print",
    "concepts": [
      "The jax.debug package provides tools for inspecting values inside compiled functions.",
      "jax.debug.print() prints traced array values to stdout in compiled functions.",
      "Python's builtin print function doesn't work with jax.jit or jax.pmap because those transformations delay numerical evaluation.",
      "jax.debug.print should be used for dynamic array values within JAX transformations.",
      "For printing static values, a normal Python print statement can be used."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using jax.debug.print to print the values of x and y within a jitted function.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef f(x):\n  jax.debug.print(\"\ud83e\udd2f {x} \ud83e\udd2f\", x=x)\n  y = jnp.sin(x)\n  jax.debug.print(\"\ud83e\udd2f {y} \ud83e\udd2f\", y=y)\n  return y\n\nf(2.)\n# Prints:\n# \ud83e\udd2f 2.0 \ud83e\udd2f\n# \ud83e\udd2f 0.9092974662780762 \ud83e\udd2f"
      },
      {
        "description": "A definition of a function equivalent to jax.debug.print",
        "code": "def debug.print(fmt: str, *args: PyTree[Array], **kwargs: PyTree[Array]) -> None:\n  print(fmt.format(*args, **kwargs))"
      }
    ]
  },
  {
    "title": "Evaluation Order and JAX Semantics with jax.debug.print",
    "concepts": [
      "jax.debug.print can reveal information about how computations are evaluated.",
      "The output of jax.debug.print doesn't respect JAX's usual semantics guarantees.",
      "Evaluation order details are useful for debugging.",
      "jax.debug.print should be used for debugging, not when semantics guarantees are important."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how jax.vmap and jax.lax.map can result in different print orders when using jax.debug.print.",
        "code": "xs = jnp.arange(3.)\n\ndef f(x):\n  jax.debug.print(\"x: {}\", x)\n  y = jnp.sin(x)\n  jax.debug.print(\"y: {}\", y)\n  return y\n\njax.vmap(f)(xs)\n# Prints: x: 0.0\n#         x: 1.0\n#         x: 2.0\n#         y: 0.0\n#         y: 0.841471\n#         y: 0.9092974\n\njax.lax.map(f, xs)\n# Prints: x: 0.0\n#         y: 0.0\n#         x: 1.0\n#         y: 0.841471\n#         x: 2.0\n#         y: 0.9092974"
      }
    ]
  },
  {
    "title": "Additional Considerations When Using jax.debug.print",
    "concepts": [
      "When jax.pmap-ed, jax.debug.prints might be reordered.",
      "Under a jax.grad, jax.debug.prints will only print on the forward pass.",
      "jax.debug.print also works in other transformations like pjit.",
      "jax.debug.print is a thin convenience wrapper around jax.debug.callback.",
      "jax.debug.print can interfere with compiler optimizations and potentially affect the memory profile.",
      "jax.debug.print inherently incurs communication between an accelerator and its host."
    ],
    "code_examples": [
      {
        "description": "Demonstrates that jax.debug.print can be reordered with pmap.",
        "code": "xs = jnp.arange(2.)\n\ndef f(x):\n  jax.debug.print(\"x: {}\", x)\n  return x\n\njax.pmap(f)(xs)\n# Prints: x: 1.0\n#         x: 0.0\n# OR\n# Prints: x: 1.0\n#         x: 0.0"
      },
      {
        "description": "Demonstrates that jax.debug.print will only print on the forward pass under a jax.grad.",
        "code": "def f(x):\n  jax.debug.print(\"x: {}\", x)\n  return x * 2.\n\njax.grad(f)(1.)\n# Prints: x: 1.0"
      },
      {
        "description": "Demonstrates how to print on the backward pass using jax.custom_vjp.",
        "code": "@jax.custom_vjp\ndef print_grad(x):\n  return x\n\ndef print_grad_fwd(x):\n  return x, None\n\ndef print_grad_bwd(_, x_grad):\n  jax.debug.print(\"x_grad: {}\", x_grad)\n  return (x_grad,)\n\nprint_grad.defvjp(print_grad_fwd, print_grad_bwd)\n\ndef f(x):\n  x = print_grad(x)\n  return x * 2.\n\njax.grad(f)(1.)\n# Prints: x_grad: 2.0"
      },
      {
        "description": "Definition of a function equivalent to jax.debug.callback.",
        "code": "def callback(fun: Callable, *args: PyTree[Array], **kwargs: PyTree[Array]) -> None:\n  fun(*args, **kwargs)\n  return None"
      },
      {
        "description": "Demonstrates how jax.debug.print can be reordered when staged out by jax.jit.",
        "code": "@jax.jit\ndef f(x, y):\n  jax.debug.print(\"x: {}\", x)\n  jax.debug.print(\"y: {}\", y)\n  return x + y\n\nf(2., 3.)\n# Prints: x: 2.0\n#         y: 3.0\n# OR\n# Prints: y: 3.0\n#         x: 2.0"
      },
      {
        "description": "Demonstrates how to block on jax.debug.print statements with jax.effects_barrier.",
        "code": "@jax.jit\ndef f(x):\n  jax.debug.print(\"x: {}\", x)\n  return x\n\nf(2.).block_until_ready()\njax.effects_barrier()\n# Prints: x: 2.\n# <do something else>"
      },
      {
        "description": "Example of how jax.debug.print can force intermediates to be materialized, potentially slowing down the program and increasing memory usage.",
        "code": "def f(w, b, x):\n  logits = w.dot(x) + b\n  jax.debug.print(\"logits: {}\", logits)\n  return jax.nn.relu(logits)"
      }
    ]
  },
  {
    "title": "Summary of jax.debug.print",
    "concepts": [
      "Print debugging is simple and intuitive",
      "jax.debug.callback can be used for other innocuous side-effects",
      "Adding print statements is a manual process",
      "Can have performance impacts"
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.debug.breakpoint",
    "concepts": [
      "jax.debug.breakpoint() pauses the execution of your JAX program to inspect values.",
      "jax.debug.breakpoint() is an application of jax.debug.callback(...) that captures call stack information.",
      "Calling jax.debug.breakpoint() in a compiled function pauses the program and presents a pdb-like prompt.",
      "Debugger commands include help, p, pp, u(p), d(own), w(here)/bt, l(ist), c(ont(inue)), q(uit)/exit.",
      "jax.debug.breakpoint can be combined with jax.lax.cond to detect nans or infs."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using jax.debug.breakpoint to pause execution and inspect values.",
        "code": "@jax.jit\ndef f(x):\n  y, z = jnp.sin(x), jnp.cos(x)\n  jax.debug.breakpoint()\n  return y * z\n\nf(2.)\n# ==> Pauses during execution!"
      },
      {
        "description": "Demonstrates using jax.debug.breakpoint with jax.lax.cond to break when non-finite values are encountered.",
        "code": "def breakpoint_if_nonfinite(x):\n  is_finite = jnp.isfinite(x).all()\n  def true_fn(x):\n    pass\n  def false_fn(x):\n    jax.debug.breakpoint()\n  lax.cond(is_finite, true_fn, false_fn, x)\n\n@jax.jit\ndef f(x, y):\n  z = x / y\n  breakpoint_if_nonfinite(z)\n  return z\n\nf(2., 0.)\n# ==> Pauses during execution!"
      }
    ]
  },
  {
    "title": "Limitations of jax.debug.breakpoint",
    "concepts": [
      "jax.debug.breakpoint has the same transformation behaviors as jax.debug.print",
      "jax.debug.breakpoint materializes more intermediates than jax.debug.print.",
      "jax.debug.breakpoint has more runtime overhead than jax.debug.print.",
      "Need to potentially use many breakpoints to pinpoint the source of an error",
      "Materializes many intermediates"
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Checkify",
    "concepts": [
      "Checkify adds JIT-able runtime error checking to JAX code.",
      "It uses the `checkify.checkify` transformation and `checkify.check` function.",
      "The `checkify.check` function allows you to specify conditions that must be met at runtime.",
      "The `checkify.checkify` transformation functionalizes error checking, making it compatible with JAX transformations."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using `checkify.check` to check for non-negative indices.",
        "code": "from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, i):\n    checkify.check(i >= 0, \"index needs to be non-negative, got {i}\", i=i)\n    y = x[i]\n    z = jnp.sin(y)\n    return z\n\njittable_f = checkify.checkify(f)\nerr, z = jax.jit(jittable_f)(jnp.ones((5,)), -2)\nprint(err.get())\n# >> index needs to be non-negative, got -2! (check failed at <...>:6 (f))"
      }
    ]
  },
  {
    "title": "Automatic Error Checking",
    "concepts": [
      "Checkify can automatically add checks for common errors like out-of-bounds indexing and NaN generation.",
      "This is done using predefined sets of checks, such as `checkify.index_checks` and `checkify.float_checks`.",
      "The `errors` argument in `checkify.checkify` is used to enable these automatic checks."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating how to automatically add index and float checks to a function.",
        "code": "from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, i):\n    y = x[i]\n    z = jnp.sin(y)\n    return z\n\nerrors = checkify.user_checks | checkify.index_checks | checkify.float_checks\nchecked_f = checkify.checkify(f, errors=errors)\n\nerr, z = checked_f(jnp.ones((5,)), 100)\nerr.throw()\n# ValueError: out-of-bounds indexing at <..>:7 (f)\n\nerr, z = checked_f(jnp.array([jnp.inf, 1]), 0)\nerr.throw()\n# ValueError: nan generated by primitive sin at <...>:8 (f)"
      }
    ]
  },
  {
    "title": "Functional Purity and JAX Transformations",
    "concepts": [
      "The assert-like check API is not functionally pure and can raise exceptions.",
      "Ordinary assertions don't work inside JIT, PMAP, PJIT, or SCAN because numeric computations are staged out.",
      "The checkify transformation functionalizes checks, returning an error value as output and maintaining functional purity.",
      "Functionalized checks can be composed with other JAX transformations."
    ],
    "code_examples": [
      {
        "description": "Illustrates the issue of using regular assertions within JIT.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x):\n  assert x > 0., \"must be positive!\"\n  return jnp.log(x)\n\njax.jit(f)(0.)\n# ConcretizationTypeError: \"Abstract tracer value encountered ...\""
      },
      {
        "description": "Demonstrates manual error handling for JIT compatibility.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f_checked(x):\n  error = x <= 0.\n  result = jnp.log(x)\n  return error, result\n\nerr, y = jax.jit(f_checked)(0.)\nif err:\n  raise ValueError(\"must be positive!\")\n# ValueError: \"must be positive!\""
      },
      {
        "description": "Illustrates how checkify automates the error handling process for JIT compatibility.",
        "code": "from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x):\n  checkify.check(x > 0., \"{} must be positive!\", x)\n  return jnp.log(x)\n\nf_checked = checkify.checkify(f)\nerr, x = jax.jit(f_checked)(-1.)\nerr.throw()\n# ValueError: -1. must be positive! (check failed at <...>:2 (f))"
      }
    ]
  },
  {
    "title": "Checkify and JAX Transformations",
    "concepts": [
      "Checkified functions are functionally pure and compose with JAX transformations.",
      "Checkify can be applied before or after JIT.",
      "Vmap and pmap work with checkified functions.",
      "Pjit requires specifying out_axis_resources for the error value.",
      "Checkify can be applied to the gradient computation using checkify-of-grad.",
      "Use custom_vjp for applying checks on gradient values."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using `checkify` with `jax.jit`.",
        "code": "from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, i):\n    return x[i]\n\ncheckify_of_jit = checkify.checkify(jax.jit(f))\njit_of_checkify = jax.jit(checkify.checkify(f))\n\nerr, _ = checkify_of_jit(jnp.ones((5,)), 100)\nerr.get()\n# out-of-bounds indexing at <..>:2 (f)\n\nerr, _ = jit_of_checkify(jnp.ones((5,)), 100)\n# out-of-bounds indexing at <..>:2 (f)"
      },
      {
        "description": "Demonstrates using `checkify` with `jax.vmap` to obtain a mapped error.",
        "code": "from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x, i):\n    checkify.check(i >= 0, \"index needs to be non-negative!\")\n    return x[i]\n\nchecked_f = checkify.checkify(f, errors=checkify.all_checks)\n\nerrs, out = jax.vmap(checked_f)(jnp.ones((3, 5)), jnp.array([-1, 2, 100]))\nerrs.throw()\n\"\"\"\nValueError:\nat mapped index 0: index needs to be non-negative! (check failed at <...>:2 (f))\nat mapped index 2: out-of-bounds indexing at <...>:3 (f)\n\"\"\""
      },
      {
        "description": "Demonstrates using `checkify` with `jax.vmap` where checkify is applied after vmap to obtain a single (unmapped) error.",
        "code": "from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\n@jax.vmap\ndef f(x, i):\n    checkify.check(i >= 0, \"index needs to be non-negative!\")\n    return x[i]\n\nchecked_f = checkify.checkify(f, errors=checkify.all_checks)\n\nerr, out = checked_f(jnp.ones((3, 5)), jnp.array([-1, 2, 100]))\nerr.throw()\n# ValueError: index needs to be non-negative! (check failed at <...>:2 (f))"
      },
      {
        "description": "Example using checkify with gradient computation",
        "code": "from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\ndef f(x):\n  return x / (1 + jnp.sqrt(x))\n\ngrad_f = jax.grad(f)\nerr, _ = checkify.checkify(grad_f, errors=checkify.nan_checks)(0.)\nprint(err.get())\n# >> nan generated by primitive mul at <...>:3 (f)"
      },
      {
        "description": "Example using checkify with custom_vjp to check gradient values.",
        "code": "from jax.experimental import checkify\nimport jax\nimport jax.numpy as jnp\n\n@jax.custom_vjp\ndef assert_gradient_negative(x):\n  return x\n\ndef fwd(x):\n  return assert_gradient_negative(x), None\n\ndef bwd(_, grad):\n  checkify.check(grad < 0, \"gradient needs to be negative!\")\n  return (grad,)\n\nassert_gradient_negative.defvjp(fwd, bwd)\n\njax.grad(assert_gradient_negative)(-1.)\n# ValueError: gradient needs to be negative!"
      }
    ]
  },
  {
    "title": "Benefits and Drawbacks",
    "concepts": [
      "Checkify can be used everywhere since errors are treated as values.",
      "It allows for automatic instrumentation without local code modifications.",
      "Adding a lot of runtime checks can be expensive.",
      "Requires threading error values and manually throwing errors.",
      "Throwing errors materializes error values on the host, which can block JAX's async run-ahead."
    ],
    "code_examples": []
  },
  {
    "title": "Debugging with jax_debug_nans",
    "concepts": [
      "The jax_debug_nans flag raises an error when a NaN is detected in JIT-compiled code.",
      "When a NaN is detected, the JIT-ted function is re-run eagerly to pinpoint the NaN-producing primitive.",
      "The jax_debug_nans flag can be enabled via environment variable, jax.config.update, or command-line flag.",
      "It is not compatible with jax.pmap or jax.pjit.",
      "It can error on false positives (e.g. intentionally created NaNs)."
    ],
    "code_examples": [
      {
        "description": "Enables the jax_debug_nans flag and demonstrates how it raises an error when a NaN is produced during the division by zero.",
        "code": "import jax\n\njax.config.update(\"jax_debug_nans\", True)\n\ndef f(x, y):\n  return x / y\n\njax.jit(f)(0., 0.)\n# ==> raises FloatingPointError exception!"
      },
      {
        "description": "Enables the jax_debug_nans flag and demonstrates how it raises an error when a NaN is produced during the division by zero. (duplicate example)",
        "code": "import jax\n\njax.config.update(\"jax_debug_nans\", True)\n\ndef f(x, y):\n  return x / y\n\njax.jit(f)(0., 0.)\n# ==> raises FloatingPointError exception!"
      }
    ]
  },
  {
    "title": "Debugging with jax_disable_jit",
    "concepts": [
      "The jax_disable_jit flag disables JIT-compilation throughout JAX.",
      "Disabling JIT-compilation enables the use of traditional Python debugging tools like print and pdb.",
      "The jax_disable_jit flag can be enabled via environment variable, jax.config.update, or command-line flag.",
      "It is not compatible with jax.pmap or jax.pjit.",
      "Running functions without JIT-compilation can be slow."
    ],
    "code_examples": [
      {
        "description": "Enables the jax_disable_jit flag and demonstrates how it allows the use of breakpoint() for debugging.",
        "code": "import jax\nimport jax.numpy as jnp\n\njax.config.update(\"jax_disable_jit\", True)\n\ndef f(x):\n  y = jnp.log(x)\n  if jnp.isnan(y):\n    breakpoint()\n  return y\n\njax.jit(f)(-2.)\n# ==> Enters PDB breakpoint!"
      },
      {
        "description": "Enables the jax_disable_jit flag and demonstrates how it allows the use of breakpoint() for debugging. (duplicate example)",
        "code": "import jax\nimport jax.numpy as jnp\n\njax.config.update(\"jax_disable_jit\", True)\n\ndef f(x):\n  y = jnp.log(x)\n  if jnp.isnan(y):\n    breakpoint()\n  return y\n\njax.jit(f)(-2.)\n# ==> Enters PDB breakpoint!"
      }
    ]
  },
  {
    "title": "Bfloat16 Precision",
    "concepts": [
      "Using bfloat16 precision can improve performance on recent GPUs.",
      "Flax's Dense layers can be instantiated with bfloat16 precision using flax.linen.Dense(..., dtype=jax.numpy.bfloat16).",
      "Dense modules in Flax LM1B example are instantiated with a configurable dtype defaulting to bfloat16.",
      "DenseGeneral modules in MaxText are also instantiated with a configurable dtype defaulting to bfloat16."
    ],
    "code_examples": []
  },
  {
    "title": "XLA Flags for NVIDIA GPUs",
    "concepts": [
      "Setting XLA flags can improve performance with jaxlib >= 0.4.18.",
      "Some XLA flags relate to communication between GPUs and are relevant for multi-device computations.",
      "XLA flags can be set via the XLA_FLAGS shell environment variable.",
      "--xla_gpu_triton_gemm_any enables the Triton-based GEMM emitter for supported GEMMs.",
      "--xla_gpu_enable_latency_hiding_scheduler enables latency hiding schedulers to overlap communication with computation.",
      "--xla_gpu_memory_limit_slop_factor adjusts the memory limit for the Latency Hiding Scheduler.",
      "--xla_gpu_enable_pipelined_collectives enables overlapping weight AllGather/Reduce/ReduceScatter with computation in pipeline parallelism.",
      "--xla_gpu_collective_permute_decomposer_threshold decomposes CollectivePermutes into ReceiveDone/SendDone pairs for pipelining.",
      "--xla_gpu_all_gather_combine_threshold_bytes, --xla_gpu_reduce_scatter_combine_threshold_bytes, --xla_gpu_all_reduce_combine_threshold_bytes tune combining small collectives to reduce communication overhead."
    ],
    "code_examples": [
      {
        "description": "Example of setting XLA flags using environment variables.",
        "code": "import os\nos.environ['XLA_FLAGS'] = ('--xla_gpu_triton_gemm_any=True '--xla_gpu_enable_latency_hiding_scheduler=true ')"
      },
      {
        "description": "Example of setting XLA flags using environment variables.",
        "code": "import os\nos.environ['XLA_FLAGS'] = ('--xla_gpu_triton_gemm_any=True '--xla_gpu_enable_latency_hiding_scheduler=true ')"
      }
    ]
  },
  {
    "title": "Nvidia NCCL Flags",
    "concepts": [
      "Nvidia NCCL flags can improve single-host multi-device communication speed on Nvidia GPUs.",
      "NCCL_LL128_BUFFSIZE, NCCL_LL_BUFFSIZE, and NCCL_PROTO flags are useful for single-host communication."
    ],
    "code_examples": [
      {
        "description": "Setting NCCL flags using environment variables.",
        "code": "import os\nos.environ.update({\n    \"NCCL_LL128_BUFFSIZE\": \"-2\",\n    \"NCCL_LL_BUFFSIZE\": \"-2\",\n    \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n})"
      },
      {
        "description": "Setting NCCL flags using environment variables.",
        "code": "import os\nos.environ.update({\n    \"NCCL_LL128_BUFFSIZE\": \"-2\",\n    \"NCCL_LL_BUFFSIZE\": \"-2\",\n    \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n})"
      }
    ]
  },
  {
    "title": "Processes Per GPU vs. Per Node",
    "concepts": [
      "Using one process per GPU can speed up jitted computation.",
      "jax.distributed.initialize() API automatically understands this configuration when run under SLURM.",
      "It may be useful to test both one process per GPU and one process per node."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Compilation Cache",
    "concepts": [
      "JAX has an optional disk cache for compiled programs.",
      "The cache saves recompilation time when running similar tasks repeatedly.",
      "If the compilation cache is not on a local filesystem, etils needs to be installed."
    ],
    "code_examples": [
      {
        "description": "Installs etils using pip.",
        "code": "pip\ninstall\netils"
      },
      {
        "description": "Configures and uses the JAX compilation cache.",
        "code": "import jax\nimport jax.numpy as jnp\n\njax.config.update(\n    \"jax_compilation_cache_dir\",\n    \"/tmp/jax_cache\"\n)\njax.config.update(\n    \"jax_persistent_cache_min_entry_size_bytes\",\n    -1\n)\njax.config.update(\n    \"jax_persistent_cache_min_compile_time_secs\",\n    0\n)\njax.config.update(\n    \"jax_persistent_cache_enable_xla_caches\",\n    \"xla_gpu_per_fusion_autotune_cache_dir\"\n)\n\n@jax.jit\ndef f(x):\n    return x + 1\n\nx = jnp.zeros((2, 2))\nf(x)"
      },
      {
        "description": "Configures and uses the JAX compilation cache.",
        "code": "import jax\nimport jax.numpy as jnp\n\njax.config.update(\n    \"jax_compilation_cache_dir\",\n    \"/tmp/jax_cache\"\n)\njax.config.update(\n    \"jax_persistent_cache_min_entry_size_bytes\",\n    -1\n)\njax.config.update(\n    \"jax_persistent_cache_min_compile_time_secs\",\n    0\n)\njax.config.update(\n    \"jax_persistent_cache_enable_xla_caches\",\n    \"xla_gpu_per_fusion_autotune_cache_dir\"\n)\n\n@jax.jit\ndef f(x):\n    return x + 1\n\nx = jnp.zeros((2, 2))\nf(x)"
      }
    ]
  },
  {
    "title": "Enabling the Compilation Cache",
    "concepts": [
      "The compilation cache is enabled when the cache location is set.",
      "The cache location should be set before the first compilation.",
      "The cache location can be set using an environment variable.",
      "The cache location can be set using jax.config.update().",
      "The cache location can be set using set_cache_dir()."
    ],
    "code_examples": [
      {
        "description": "Sets the JAX compilation cache directory using an environment variable in the shell.",
        "code": "export JAX_COMPILATION_CACHE_DIR = \"/tmp/jax_cache\""
      },
      {
        "description": "Sets the JAX compilation cache directory using an environment variable in Python.",
        "code": "import os\nos.environ[\"JAX_COMPILATION_CACHE_DIR\"] = \"/tmp/jax_cache\""
      },
      {
        "description": "Sets the JAX compilation cache directory using jax.config.update().",
        "code": "jax.config.update(\n    \"jax_compilation_cache_dir\",\n    \"/tmp/jax_cache\"\n)"
      },
      {
        "description": "Sets the JAX compilation cache directory using set_cache_dir().",
        "code": "from jax.experimental.compilation_cache import compilation_cache as cc\ncc.set_cache_dir(\"/tmp/jax_cache\")"
      },
      {
        "description": "Sets the JAX compilation cache directory using set_cache_dir().",
        "code": "from jax.experimental.compilation_cache import compilation_cache as cc\ncc.set_cache_dir(\"/tmp/jax_cache\")"
      }
    ]
  },
  {
    "title": "Cache Configuration Options",
    "concepts": [
      "jax_persistent_cache_min_compile_time_secs: Minimum compilation time (in seconds) for a computation to be written to the cache. Defaults to 1.0 second.",
      "jax_persistent_cache_min_entry_size_bytes: Minimum size (in bytes) of an entry to be cached.",
      "-1 disables the size restriction and prevents overrides.",
      "0 allows for overrides.",
      "> 0 sets the actual minimum size desired.",
      "Both criteria (time and size) need to be satisfied for caching.",
      "XLA supports additional caching mechanisms.",
      "jax_persistent_cache_enable_xla_caches: Enables or disables XLA caching features.",
      "Possible values: all, none, xla_gpu_kernel_cache_file, xla_gpu_per_fusion_autotune_cache_dir (default)."
    ],
    "code_examples": []
  },
  {
    "title": "Using Google Cloud Storage (GCS) for Compilation Cache",
    "concepts": [
      "The compilation cache can be placed on a Google Cloud Storage (GCS) bucket when running on Google Cloud.",
      "The bucket should be in the same region and project as the workload.",
      "Permissions should be set for the VM(s) to write to the bucket.",
      "Replication is not necessary for smaller workloads but could benefit larger ones.",
      "Use 'Standard' storage class for the bucket.",
      "Set the soft delete policy to 7 days.",
      "Set object lifecycle to the expected duration of the workload run to avoid unlimited cache growth.",
      "All encryption policies are supported.",
      "The cache location can be set to the GCS bucket path."
    ],
    "code_examples": [
      {
        "description": "Sets the JAX compilation cache directory to a GCS bucket.",
        "code": "jax.config.update(\n    \"jax_compilation_cache_dir\",\n    \"gs://jax-cache\"\n)"
      },
      {
        "description": "Sets the JAX compilation cache directory to a GCS bucket.",
        "code": "jax.config.update(\n    \"jax_compilation_cache_dir\",\n    \"gs://jax-cache\"\n)"
      }
    ]
  },
  {
    "title": "Cache Key Details",
    "concepts": [
      "The cache key is the signature for a compiled function.",
      "It contains the computation performed by the function (non-optimized HLO).",
      "It contains the jaxlib version.",
      "It contains relevant XLA compilation flags.",
      "It contains device configuration (number of devices and topology).",
      "For GPUs, the topology contains the GPU name.",
      "It contains the compression algorithm used.",
      "It contains a string produced by jax._src.cache_key.custom_hook().",
      "The custom_hook() can be reassigned by the user to alter the resulting string."
    ],
    "code_examples": []
  },
  {
    "title": "Multi-Process and Multi-Node Considerations",
    "concepts": [
      "The process with rank 0 writes to the persistent cache on the first run.",
      "All processes attempt to read from the cache on subsequent runs.",
      "The cache must be in a shared file system (e.g., NFS) or remote storage (e.g., GCS).",
      "If the cache is local to rank 0, other processes will recompile on subsequent runs."
    ],
    "code_examples": []
  },
  {
    "title": "Mocking Multi-Node Environments for Cache Population",
    "concepts": [
      "JAX can populate the cache with compiled programs for multiple nodes on a single node.",
      "This decreases compilation time on a cluster.",
      "Users can create fake remote devices using the jax_mock_gpu_topology configuration option.",
      "The process running the mocked program must have the same amount of GPUs and GPU model as the target nodes.",
      "Results of communications with other nodes are undefined in mocked environments."
    ],
    "code_examples": [
      {
        "description": "Instructs JAX to mock a cluster with four nodes, each running eight processes with one GPU each.",
        "code": "jax.config.update(\n    \"jax_mock_gpu_topology\",\n    \"4x8x1\"\n)"
      },
      {
        "description": "Instructs JAX to mock a cluster with four nodes, each running eight processes with one GPU each.",
        "code": "jax.config.update(\n    \"jax_mock_gpu_topology\",\n    \"4x8x1\"\n)"
      }
    ]
  },
  {
    "title": "Debugging the Compilation Cache",
    "concepts": [
      "Enable logging of related source files using the JAX_DEBUG_LOG_MODULES environment variable.",
      "Change the global JAX logging level using the JAX_LOGGING_LEVEL environment variable.",
      "Enable logging of cache misses with explanations using the jax_explain_cache_misses configuration flag."
    ],
    "code_examples": [
      {
        "description": "Enables logging of compiler and LRU cache modules.",
        "code": "import os\nos.environ[\"JAX_DEBUG_LOG_MODULES\"] = \"jax._src.compiler,jax._src.lru_cache\""
      },
      {
        "description": "Sets the global JAX logging level to DEBUG using an environment variable.",
        "code": "import os\nos.environ[\"JAX_LOGGING_LEVEL\"] = \"DEBUG\""
      },
      {
        "description": "Sets the JAX logging level to DEBUG using jax.config.update().",
        "code": "jax.config.update(\n    \"jax_logging_level\",\n    \"DEBUG\"\n)"
      },
      {
        "description": "Sets the JAX logging level to DEBUG using jax.config.update().",
        "code": "jax.config.update(\n    \"jax_logging_level\",\n    \"DEBUG\"\n)"
      },
      {
        "description": "Enables logging of cache misses with explanations.",
        "code": "jax.config.update(\n    \"jax_explain_cache_misses\",\n    True\n)"
      },
      {
        "description": "Enables logging of cache misses with explanations.",
        "code": "jax.config.update(\n    \"jax_explain_cache_misses\",\n    True\n)"
      }
    ]
  },
  {
    "title": "Known Pitfalls and Workarounds",
    "concepts": [
      "The persistent cache doesn't work with functions that have host callbacks.",
      "The persistent cache doesn't work with functions using primitives that implement custom_partitioning.",
      "Using shard_map can circumvent custom_partitioning for those primitives and enable the compilation cache."
    ],
    "code_examples": [
      {
        "description": "A function F that implements a layernorm followed by a matrix multiplication using a primitive LayerNorm that implements custom_partitioning.",
        "code": "import jax\ndef F(x1, x2, gamma, beta):\n    ln_out = LayerNorm(x1, gamma, beta)\n    return ln_out @ x2"
      },
      {
        "description": "A function F that implements a layernorm followed by a matrix multiplication using a primitive LayerNorm that implements custom_partitioning.",
        "code": "import jax\ndef F(x1, x2, gamma, beta):\n    ln_out = LayerNorm(x1, gamma, beta)\n    return ln_out @ x2"
      },
      {
        "description": "Compile function F without shard_map.",
        "code": "layernorm_matmul_without_shard_map = jax.jit(\n    F,\n    in_shardings=(...),\n    out_sharding=(...)\n)(x1, x2, gamma, beta)"
      },
      {
        "description": "Compile function F without shard_map.",
        "code": "layernorm_matmul_without_shard_map = jax.jit(\n    F,\n    in_shardings=(...),\n    out_sharding=(...)\n)(x1, x2, gamma, beta)"
      },
      {
        "description": "Wrap the layernorm primitive in shard_map and define a function G that performs the same computation",
        "code": "import jax\nfrom jax.experimental.shard_map import shard_map\n\ndef G(x1, x2, gamma, beta, mesh, ispecs, ospecs):\n    ln_out = shard_map(\n        LayerNorm,\n        mesh,\n        in_specs=ispecs,\n        out_specs=ospecs,\n        check_rep=False\n    )(x1, x2, gamma, beta)\n    return ln_out @ x2\n\nispecs = jax.sharding.PartitionSpec(...)\nospecs = jax.sharding.PartitionSpec(...)\nmesh = jax.sharding.Mesh(...)\n\nlayernorm_matmul_with_shard_map = jax.jit(\n    G,\n    static_argnames=['mesh', 'ispecs', 'ospecs']\n)(x1, x2, gamma, beta, mesh, ispecs, ospecs)"
      },
      {
        "description": "Wrap the layernorm primitive in shard_map and define a function G that performs the same computation",
        "code": "import jax\nfrom jax.experimental.shard_map import shard_map\n\ndef G(x1, x2, gamma, beta, mesh, ispecs, ospecs):\n    ln_out = shard_map(\n        LayerNorm,\n        mesh,\n        in_specs=ispecs,\n        out_specs=ospecs,\n        check_rep=False\n    )(x1, x2, gamma, beta)\n    return ln_out @ x2\n\nispecs = jax.sharding.PartitionSpec(...)\nospecs = jax.sharding.PartitionSpec(...)\nmesh = jax.sharding.Mesh(...)\n\nlayernorm_matmul_with_shard_map = jax.jit(\n    G,\n    static_argnames=['mesh', 'ispecs', 'ospecs']\n)(x1, x2, gamma, beta, mesh, ispecs, ospecs)"
      }
    ]
  },
  {
    "title": "Introduction to PyTrees in JAX",
    "concepts": [
      "Pytrees are tree-like structures built from container-like Python objects.",
      "Lists, tuples, and dicts are container-like by default in JAX.",
      "Objects not in the pytree container registry are considered leaf pytrees.",
      "JAX can convert any tree of registered container objects into tuples.",
      "Many JAX functions operate over pytrees of arrays.",
      "Function transformations can be applied to functions accepting and producing pytrees of arrays.",
      "Optional parameters for JAX function transformations can also be pytrees, matching the structure of argument pytrees.",
      "Parameter pytrees can be prefixes of argument pytrees.",
      "tree_structure can be used to view the pytree definition of an object."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use `tree_structure` to view the pytree definition of an object.",
        "code": "from jax.tree_util import tree_structure\nprint(tree_structure(object))"
      },
      {
        "description": "Demonstrates how to use `tree_structure` to view the pytree definition of an object.",
        "code": "from jax.tree_util import tree_structure\nprint(tree_structure(object))"
      }
    ]
  },
  {
    "title": "Flattening and Unflattening PyTrees",
    "concepts": [
      "JAX flattens pytrees into lists of leaves at the api.py boundary and in control flow primitives.",
      "Flattening simplifies JAX internals by allowing transformations to operate on functions with flat array inputs and outputs.",
      "Flattening produces a list of leaves and a treedef object encoding the original structure.",
      "The treedef can be used to reconstruct a matching structured value after transforming the leaves.",
      "Pytrees are tree-like, assuming referential transparency and no reference cycles."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to flatten a structured value, transform the flat value, and then reconstruct the structured output.",
        "code": "from jax.tree_util import tree_flatten, tree_unflatten\nimport jax.numpy as jnp\n\n# The structured value to be transformed\nvalue_structured = [1., (2., 3.)]\n\n# The leaves in value_flat correspond to the `*` markers in value_tree\nvalue_flat, value_tree = tree_flatten(value_structured)\nprint(f\"{value_flat=}\\n{value_tree=}\")\n\n# Transform the flat value list using an element-wise numeric transformer\ntransformed_flat = list(map(lambda v: v * 2., value_flat))\nprint(f\"{transformed_flat=}\")\n\n# Reconstruct the structured output, using the original\ntransformed_structured = tree_unflatten(value_tree, transformed_flat)\nprint(f\"{transformed_structured=}\")"
      },
      {
        "description": "Demonstrates how to flatten a structured value, transform the flat value, and then reconstruct the structured output.",
        "code": "from jax.tree_util import tree_flatten, tree_unflatten\nimport jax.numpy as jnp\n\n# The structured value to be transformed\nvalue_structured = [1., (2., 3.)]\n\n# The leaves in value_flat correspond to the `*` markers in value_tree\nvalue_flat, value_tree = tree_flatten(value_structured)\nprint(f\"{value_flat=}\\n{value_tree=}\")\n\n# Transform the flat value list using an element-wise numeric transformer\ntransformed_flat = list(map(lambda v: v * 2., value_flat))\nprint(f\"{transformed_flat=}\")\n\n# Reconstruct the structured output, using the original\ntransformed_structured = tree_unflatten(value_tree, transformed_flat)\nprint(f\"{transformed_structured=}\")"
      }
    ]
  },
  {
    "title": "Default Pytree Containers and Leaves",
    "concepts": [
      "By default, lists, tuples, dicts, namedtuple, None, and OrderedDict are pytree containers.",
      "Other types, including numeric and ndarray values, are treated as leaves.",
      "The examples demonstrate how different structured values are flattened and unflattened."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how different container types and leaves are flattened and unflattened using `tree_flatten` and `tree_unflatten`.",
        "code": "from collections import namedtuple\nimport jax.numpy as jnp\nfrom jax.tree_util import tree_flatten, tree_unflatten\n\nPoint = namedtuple('Point', ['x', 'y'])\n\nexample_containers = [\n    (1., [2., 3.]),\n    (1., {'b': 2., 'a': 3.}),\n    1.,\n    None,\n    jnp.zeros(2),\n    Point(1., 2.)\n]\n\ndef show_example(structured):\n    flat, tree = tree_flatten(structured)\n    unflattened = tree_unflatten(tree, flat)\n    print(f\"{structured=}\\n  {flat=}\\n  {tree=}\\n  {unflattened=}\")\n\nfor structured in example_containers:\n    show_example(structured)"
      },
      {
        "description": "Demonstrates how different container types and leaves are flattened and unflattened using `tree_flatten` and `tree_unflatten`.",
        "code": "from collections import namedtuple\nimport jax.numpy as jnp\nfrom jax.tree_util import tree_flatten, tree_unflatten\n\nPoint = namedtuple('Point', ['x', 'y'])\n\nexample_containers = [\n    (1., [2., 3.]),\n    (1., {'b': 2., 'a': 3.}),\n    1.,\n    None,\n    jnp.zeros(2),\n    Point(1., 2.)\n]\n\ndef show_example(structured):\n    flat, tree = tree_flatten(structured)\n    unflattened = tree_unflatten(tree, flat)\n    print(f\"{structured=}\\n  {flat=}\\n  {tree=}\\n  {unflattened=}\")\n\nfor structured in example_containers:\n    show_example(structured)"
      }
    ]
  },
  {
    "title": "Custom Classes as Pytree Leaves",
    "concepts": [
      "By default, objects of user-defined classes are treated as pytree leaves."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how a custom class is treated as a leaf in a pytree.",
        "code": "from jax.tree_util import tree_flatten, tree_unflatten\n\nclass Special(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def __repr__(self):\n        return \"Special(x={}, y={})\".format(self.x, self.y)\n\ndef show_example(structured):\n    flat, tree = tree_flatten(structured)\n    unflattened = tree_unflatten(tree, flat)\n    print(f\"{structured=}\\n  {flat=}\\n  {tree=}\\n  {unflattened=}\")\n\n\nshow_example(Special(1., 2.))"
      },
      {
        "description": "Demonstrates how a custom class is treated as a leaf in a pytree.",
        "code": "from jax.tree_util import tree_flatten, tree_unflatten\n\nclass Special(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def __repr__(self):\n        return \"Special(x={}, y={})\".format(self.x, self.y)\n\ndef show_example(structured):\n    flat, tree = tree_flatten(structured)\n    unflattened = tree_unflatten(tree, flat)\n    print(f\"{structured=}\\n  {flat=}\\n  {tree=}\\n  {unflattened=}\")\n\n\nshow_example(Special(1., 2.))"
      }
    ]
  },
  {
    "title": "Extending PyTrees with register_pytree_node",
    "concepts": [
      "The set of Python types considered internal pytree nodes is extensible.",
      "Types can be registered with `register_pytree_node` to be traversed recursively.",
      "A flattening recipe specifies how to convert an instance to (children, metadata).",
      "An unflattening recipe specifies how to convert (metadata, children) back to an instance."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to register a custom class as a pytree node using `register_pytree_node`.",
        "code": "from jax.tree_util import register_pytree_node, tree_flatten, tree_unflatten\n\nclass Special(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def __repr__(self):\n        return \"Special(x={}, y={})\".format(self.x, self.y)\n\nclass RegisteredSpecial(Special):\n    def __repr__(self):\n        return \"RegisteredSpecial(x={}, y={})\".format(self.x, self.y)\n\ndef special_flatten(v):\n    \"\"\"Specifies a flattening recipe.\n    Params:\n    v: the value of registered type to flatten.\n    Returns:\n    a pair of an iterable with the children to be flattened recursively,\n    and some opaque auxiliary data to pass back to the unflattening recipe.\n    The auxiliary data is stored in the treedef for use during unflattening.\n    The auxiliary data could be used, e.g., for dictionary keys.\n    \"\"\"\n    children = (v.x, v.y)\n    aux_data = None\n    return (children, aux_data)\n\ndef special_unflatten(aux_data, children):\n    \"\"\"Specifies an unflattening recipe.\n    Params:\n    aux_data: the opaque data that was specified during flattening of the\n    current treedef.\n    children: the unflattened children\n    Returns:\n    a re-constructed object of the registered type, using the specified\n    children and auxiliary data.\n    \"\"\"\n    return RegisteredSpecial(*children)\n\n# Global registration\nregister_pytree_node(\n    RegisteredSpecial,\n    special_flatten,  # tell JAX what are the children nodes\n    special_unflatten # tell JAX how to pack back into a RegisteredSpecial\n)\n\ndef show_example(structured):\n    flat, tree = tree_flatten(structured)\n    unflattened = tree_unflatten(tree, flat)\n    print(f\"{structured=}\\n  {flat=}\\n  {tree=}\\n  {unflattened=}\")\n\nshow_example(RegisteredSpecial(1., 2.))"
      },
      {
        "description": "Demonstrates how to register a custom class as a pytree node using `register_pytree_node`.",
        "code": "from jax.tree_util import register_pytree_node, tree_flatten, tree_unflatten\n\nclass Special(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def __repr__(self):\n        return \"Special(x={}, y={})\".format(self.x, self.y)\n\nclass RegisteredSpecial(Special):\n    def __repr__(self):\n        return \"RegisteredSpecial(x={}, y={})\".format(self.x, self.y)\n\ndef special_flatten(v):\n    \"\"\"Specifies a flattening recipe.\n    Params:\n    v: the value of registered type to flatten.\n    Returns:\n    a pair of an iterable with the children to be flattened recursively,\n    and some opaque auxiliary data to pass back to the unflattening recipe.\n    The auxiliary data is stored in the treedef for use during unflattening.\n    The auxiliary data could be used, e.g., for dictionary keys.\n    \"\"\"\n    children = (v.x, v.y)\n    aux_data = None\n    return (children, aux_data)\n\ndef special_unflatten(aux_data, children):\n    \"\"\"Specifies an unflattening recipe.\n    Params:\n    aux_data: the opaque data that was specified during flattening of the\n    current treedef.\n    children: the unflattened children\n    Returns:\n    a re-constructed object of the registered type, using the specified\n    children and auxiliary data.\n    \"\"\"\n    return RegisteredSpecial(*children)\n\n# Global registration\nregister_pytree_node(\n    RegisteredSpecial,\n    special_flatten,  # tell JAX what are the children nodes\n    special_unflatten # tell JAX how to pack back into a RegisteredSpecial\n)\n\ndef show_example(structured):\n    flat, tree = tree_flatten(structured)\n    unflattened = tree_unflatten(tree, flat)\n    print(f\"{structured=}\\n  {flat=}\\n  {tree=}\\n  {unflattened=}\")\n\nshow_example(RegisteredSpecial(1., 2.))"
      }
    ]
  },
  {
    "title": "Extending PyTrees with register_pytree_node_class",
    "concepts": [
      "Alternatively, you can define `tree_flatten` and `tree_unflatten` methods on your class.",
      "Decorating the class with `register_pytree_node_class` automatically registers it as a pytree node."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to register a custom class as a pytree node using `register_pytree_node_class`.",
        "code": "from jax.tree_util import register_pytree_node_class, tree_flatten, tree_unflatten\n\nclass Special(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def __repr__(self):\n        return \"Special(x={}, y={})\".format(self.x, self.y)\n\n@register_pytree_node_class\nclass RegisteredSpecial2(Special):\n    def __repr__(self):\n        return \"RegisteredSpecial2(x={}, y={})\".format(self.x, self.y)\n\n    def tree_flatten(self):\n        children = (self.x, self.y)\n        aux_data = None\n        return (children, aux_data)\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n\ndef show_example(structured):\n    flat, tree = tree_flatten(structured)\n    unflattened = tree_unflatten(tree, flat)\n    print(f\"{structured=}\\n  {flat=}\\n  {tree=}\\n  {unflattened=}\")\n\nshow_example(RegisteredSpecial2(1., 2.))"
      },
      {
        "description": "Demonstrates how to register a custom class as a pytree node using `register_pytree_node_class`.",
        "code": "from jax.tree_util import register_pytree_node_class, tree_flatten, tree_unflatten\n\nclass Special(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def __repr__(self):\n        return \"Special(x={}, y={})\".format(self.x, self.y)\n\n@register_pytree_node_class\nclass RegisteredSpecial2(Special):\n    def __repr__(self):\n        return \"RegisteredSpecial2(x={}, y={})\".format(self.x, self.y)\n\n    def tree_flatten(self):\n        children = (self.x, self.y)\n        aux_data = None\n        return (children, aux_data)\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n\ndef show_example(structured):\n    flat, tree = tree_flatten(structured)\n    unflattened = tree_unflatten(tree, flat)\n    print(f\"{structured=}\\n  {flat=}\\n  {tree=}\\n  {unflattened=}\")\n\nshow_example(RegisteredSpecial2(1., 2.))"
      }
    ]
  },
  {
    "title": "Auxiliary Data and Dynamic/Static Elements",
    "concepts": [
      "Children should contain all the dynamic elements of the data structure.",
      "Auxiliary data should contain all the static elements that will be rolled into the treedef structure.",
      "Care must be taken to ensure that auxiliary data supports meaningful hashing and equality comparisons for JIT cache."
    ],
    "code_examples": []
  },
  {
    "title": "Gotchas with User-Defined PyTree Objects",
    "concepts": [
      "JAX transformations may initialize user-defined PyTree objects with unexpected values.",
      "Input validation in `__init__` may fail due to unexpected values like `object()`.",
      "`jacobian` can pass the class itself to the init",
      "Avoid array conversion or input validation in `__init__`, or anticipate and handle special cases.",
      "Alternatively, structure the `tree_unflatten` function to avoid calling `__init__`."
    ],
    "code_examples": [
      {
        "description": "Example showing an error due to input validation in the constructor.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import register_pytree_node\n\nclass MyTree:\n    def __init__(self, a):\n        self.a = jnp.asarray(a)\n\nregister_pytree_node(\n    MyTree,\n    lambda tree: ((tree.a,), None),\n    lambda _, args: MyTree(*args)\n)\n\ntree = MyTree(jnp.arange(5.0))\n\njax.vmap(lambda x: x)(tree) # Error because object() is passed to MyTree.\njax.jacobian(lambda x: x)(tree) # Error because MyTree(...) is passed to MyTree"
      },
      {
        "description": "Example showing an error due to input validation in the constructor.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import register_pytree_node\n\nclass MyTree:\n    def __init__(self, a):\n        self.a = jnp.asarray(a)\n\nregister_pytree_node(\n    MyTree,\n    lambda tree: ((tree.a,), None),\n    lambda _, args: MyTree(*args)\n)\n\ntree = MyTree(jnp.arange(5.0))\n\njax.vmap(lambda x: x)(tree) # Error because object() is passed to MyTree.\njax.jacobian(lambda x: x)(tree) # Error because MyTree(...) is passed to MyTree"
      },
      {
        "description": "Example showing how to avoid the error by adding checks in the constructor.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import register_pytree_node\n\nclass MyTree:\n    def __init__(self, a):\n        if not (type(a) is object or a is None or isinstance(a, MyTree)):\n            a = jnp.asarray(a)\n        self.a = a"
      },
      {
        "description": "Example showing how to avoid the error by adding checks in the constructor.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import register_pytree_node\n\nclass MyTree:\n    def __init__(self, a):\n        if not (type(a) is object or a is None or isinstance(a, MyTree)):\n            a = jnp.asarray(a)\n        self.a = a"
      },
      {
        "description": "Example showing how to avoid the error by skipping the constructor in `tree_unflatten`.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import register_pytree_node\n\nclass MyTree:\n    def __init__(self, a):\n        self.a = jnp.asarray(a)\n\nregister_pytree_node(\n    MyTree,\n    lambda tree: ((tree.a,), None),\n    lambda aux_data, children:\n      MyTree.tree_unflatten(aux_data, children)\n)\n\nclass MyTree:\n    def __init__(self, a):\n      self.a = a\n\n    @staticmethod\n    def tree_unflatten(aux_data, children):\n      del aux_data # unused in this class\n      obj = object.__new__(MyTree)\n      obj.a = children[0]\n      return obj"
      },
      {
        "description": "Example showing how to avoid the error by skipping the constructor in `tree_unflatten`.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import register_pytree_node\n\nclass MyTree:\n    def __init__(self, a):\n        self.a = jnp.asarray(a)\n\nregister_pytree_node(\n    MyTree,\n    lambda tree: ((tree.a,), None),\n    lambda aux_data, children:\n      MyTree.tree_unflatten(aux_data, children)\n)\n\nclass MyTree:\n    def __init__(self, a):\n      self.a = a\n\n    @staticmethod\n    def tree_unflatten(aux_data, children):\n      del aux_data # unused in this class\n      obj = object.__new__(MyTree)\n      obj.a = children[0]\n      return obj"
      }
    ]
  },
  {
    "title": "ConcretizationTypeError",
    "concepts": [
      "This error occurs when a JAX Tracer object is used in a context where a concrete value is required.",
      "A common cause is using a traced value where a static value is required.",
      "Marking problematic arguments as static can fix the error.",
      "The error may also arise when a shape in your JIT-compiled computation depends on the values within a traced quantity.",
      "This is an operation incompatible with JAX\u2019s JIT compilation model, which requires array sizes to be known at compile-time.",
      "It is often possible to work around this by modifying the logic used in the function."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the error when a traced value is used as an axis argument to jnp.min().",
        "code": "from functools import partial\nfrom jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef func(x, axis):\n  return x.min(axis)\n\nfunc(jnp.arange(4), 0)"
      },
      {
        "description": "Fixes the error by marking the axis argument as static using partial.",
        "code": "from functools import partial\nfrom jax import jit\nimport jax.numpy as jnp\n\n@partial(jit, static_argnums=1)\ndef func(x, axis):\n  return x.min(axis)\n\nfunc(jnp.arange(4), 0)"
      },
      {
        "description": "Shows the error when a shape in a JIT-compiled computation depends on the values within a traced quantity.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef func(x):\n  return jnp.where(x < 0)\n\nfunc(jnp.arange(4))"
      },
      {
        "description": "Demonstrates the error when using boolean indexing within a JIT context.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef func(x):\n  indices = jnp.where(x > 1)\n  return x[indices].sum()\n\nfunc(jnp.arange(4))"
      },
      {
        "description": "Illustrates how to avoid creation of a dynamically-sized index array using jnp.where.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef func(x):\n  return jnp.where(x > 1, x, 0).sum()\n\nfunc(jnp.arange(4))"
      }
    ]
  },
  {
    "title": "KeyReuseError",
    "concepts": [
      "This error occurs when a PRNG key is reused in an unsafe manner.",
      "Key reuse is checked only when jax_debug_key_reuse is set to True.",
      "JAX PRNG is stateless, and keys must be manually split."
    ],
    "code_examples": [
      {
        "description": "Illustrates the error when a PRNG key is reused without splitting.",
        "code": "import jax\nimport jax.random\n\nwith jax.debug_key_reuse(True):\n  key = jax.random.key(0)\n  value = jax.random.uniform(key)\n  new_value = jax.random.uniform(key)"
      }
    ]
  },
  {
    "title": "NonConcreteBooleanIndexError",
    "concepts": [
      "This error occurs when a program attempts to use non-concrete boolean indices in a traced indexing operation.",
      "JAX arrays must have static shapes under JIT compilation, requiring careful use of boolean masks.",
      "Some logic implemented via boolean masking is not possible in a jax.jit() function.",
      "The logic can sometimes be re-expressed in a JIT-compatible way using the three-argument version of where().",
      "It commonly arises when attempting to create an array via a boolean mask within a JIT context.",
      "It also arises when using boolean indices with .at[...].set(...)."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the error when attempting to create an array via a boolean mask within a JIT context.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef positive_values(x):\n  return x[x > 0]\n\npositive_values(jnp.arange(-5, 5))"
      },
      {
        "description": "Shows another function that fails under JIT due to boolean masking.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef sum_of_positive(x):\n  return x[x > 0].sum()\n\nsum_of_positive(jnp.arange(-5, 5))"
      },
      {
        "description": "Illustrates how to express the same logic using the three-argument version of jax.numpy.where().",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef sum_of_positive(x):\n  return jnp.where(x > 0, x, 0).sum()\n\nsum_of_positive(jnp.arange(-5, 5))"
      },
      {
        "description": "Demonstrates the error when using boolean indices with .at[...].set(...).",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef manual_clip(x):\n  return x.at[x < 0].set(0)\n\nmanual_clip(jnp.arange(-2, 2))"
      },
      {
        "description": "Shows how to address the issue by re-expressing the logic in terms of where().",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef manual_clip(x):\n  return jnp.where(x < 0, 0, x)\n\nmanual_clip(jnp.arange(-2, 2))"
      }
    ]
  },
  {
    "title": "TracerArrayConversionError",
    "concepts": [
      "This error occurs when a program attempts to convert a JAX Tracer object into a standard NumPy array.",
      "It can occur if you attempt to use a non-JAX library like numpy or scipy inside a JAX transformation.",
      "If the error involves array indexing, the array being indexed might be a standard numpy.ndarray while the indices are traced JAX arrays."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the error when using numpy.sin() inside a JAX JIT-compiled function.",
        "code": "from jax import jit\nimport numpy as np\n\n@jit\ndef func(x):\n  return np.sin(x)\n\nfunc(np.arange(4))"
      },
      {
        "description": "Fixes the error by using jax.numpy.sin() instead of numpy.sin().",
        "code": "import jax.numpy as jnp\nfrom jax import jit\n\n@jit\ndef func(x):\n  return jnp.sin(x)\n\nfunc(jnp.arange(4))"
      },
      {
        "description": "Illustrates the error when indexing a NumPy array with a JAX Tracer.",
        "code": "import numpy as np\nfrom jax import jit\n\nx = np.arange(10)\n\n@jit\ndef func(i):\n  return x[i]\n\nfunc(0)"
      },
      {
        "description": "Fixes the error by converting the NumPy array to a JAX array.",
        "code": "import jax.numpy as jnp\nfrom jax import jit\nimport numpy as np\n\nx = np.arange(10)\n\n@jit\ndef func(i):\n  return jnp.asarray(x)[i]\n\nfunc(0)"
      },
      {
        "description": "An alternative fix by declaring the index as a static argument.",
        "code": "from functools import partial\nfrom jax import jit\nimport numpy as np\n\nx = np.arange(10)\n\n@partial(jit, static_argnums=(0,))\ndef func(i):\n  return x[i]\n\nfunc(0)"
      }
    ]
  },
  {
    "title": "TracerBoolConversionError",
    "concepts": [
      "This error occurs when a traced value in JAX is used in a context where a boolean value is expected.",
      "The boolean cast may be explicit (e.g. bool(x)) or implicit through control flow or boolean operators.",
      "The problem can sometimes be fixed by marking traced values as static.",
      "The error indicates operations not directly supported by JAX's JIT compilation model.",
      "One case is when a traced value is used in Python control flow.",
      "Another common cause is inadvertently tracing over a boolean flag.",
      "Using non-JAX aware functions within JAX code can also cause the error."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the error when a traced value is used in Python control flow.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef func(x, y):\n  return x if x.sum() < y.sum() else y\n\nfunc(jnp.ones(4), jnp.zeros(4))"
      },
      {
        "description": "Shows how to re-express the if statement using jax.numpy.where().",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef func(x, y):\n  return jnp.where(x.sum() < y.sum(), x, y)\n\nfunc(jnp.ones(4), jnp.zeros(4))"
      },
      {
        "description": "Illustrates the error when tracing over a boolean flag used in Python control flow.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef func(x, normalize=True):\n  if normalize:\n    return x / x.sum()\n  return x\n\nfunc(jnp.arange(5), True)"
      },
      {
        "description": "Fixes the error by marking the boolean flag as static.",
        "code": "from functools import partial\nfrom jax import jit\nimport jax.numpy as jnp\n\n@partial(jit, static_argnames=['normalize'])\ndef func(x, normalize=True):\n  if normalize:\n    return x / x.sum()\n  return x\n\nfunc(jnp.arange(5), True)"
      },
      {
        "description": "Demonstrates the error when using Python's built-in min function within JAX code.",
        "code": "from jax import jit\n\n@jit\ndef func(x):\n  return min(x, 0)\n\nfunc(2)"
      },
      {
        "description": "Fixes the error by replacing Python's min function with jnp.minimum.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef func(x):\n  return jnp.minimum(x, 0)\n\nprint(func(2))"
      }
    ]
  },
  {
    "title": "TracerIntegerConversionError",
    "concepts": [
      "This error occurs when a JAX Tracer object is used in a context where a Python integer is expected.",
      "It can occur if you attempt to pass a traced value to a function that requires a static integer argument.",
      "When this happens, the solution is often to mark the problematic argument as static.",
      "The error can occur if you attempt to index a Python list with a traced quantity.",
      "The error can be fixed by converting the list to a JAX array or by declaring the index as a static argument."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the error when passing a traced value to a function that requires a static integer argument (np.split).",
        "code": "from jax import jit\nimport numpy as np\n\n@jit\ndef func(x, axis):\n  return np.split(x, 2, axis)\n\nfunc(np.arange(4), 0)"
      },
      {
        "description": "Fixes the error by marking the problematic argument (axis) as static.",
        "code": "from functools import partial\nfrom jax import jit\nimport numpy as np\n\n@partial(jit, static_argnums=1)\ndef func(x, axis):\n  return np.split(x, 2, axis)\n\nfunc(np.arange(10), 0)"
      },
      {
        "description": "An alternative is to apply the transformation to a closure that encapsulates the arguments to be protected using a lambda function.",
        "code": "from jax import jit\nimport numpy as np\n\njit(lambda arr: np.split(arr, 2, 0))(np.arange(4))"
      },
      {
        "description": "Demonstrates the error when indexing a Python list with a traced quantity.",
        "code": "import jax.numpy as jnp\nfrom jax import jit\n\nL = [1, 2, 3]\n\n@jit\ndef func(i):\n  return L[i]\n\nfunc(0)"
      },
      {
        "description": "Fixes the error by converting the list to a JAX array.",
        "code": "import jax.numpy as jnp\nfrom jax import jit\n\nL = [1, 2, 3]\n\n@jit\ndef func(i):\n  return jnp.array(L)[i]\n\nfunc(0)"
      },
      {
        "description": "Fixes the error by declaring the index as a static argument.",
        "code": "from functools import partial\nfrom jax import jit\n\nL = [1, 2, 3]\n\n@partial(jit, static_argnums=0)\ndef func(i):\n  return L[i]\n\nfunc(0)"
      }
    ]
  },
  {
    "title": "UnexpectedTracerError",
    "concepts": [
      "This error occurs when you use a JAX value that has leaked out of a function.",
      "Leaking a value means storing, outside of a transformed function f, a reference to an intermediate value computed in f.",
      "JAX detects leaks when you use the leaked value in another operation later on.",
      "To fix the error, avoid side effects and return the value explicitly from the transformed function.",
      "The error message tries to point to the location of the transformed function, the stack trace of where the leaked Tracer was created, and the line of code that created the leaked Tracer.",
      "The leak checker can be used to raise an error as soon as a Tracer is leaked, but it's experimental and has a negative effect on performance."
    ],
    "code_examples": [
      {
        "description": "Demonstrates a transformed function that leaks a Traced value to an outer scope.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\nouts = []\n\n@jit  # 1\ndef side_effecting(x):\n  y = x + 1  # 3\n  outs.append(y)  # 4\n\nx = 1\nside_effecting(x)  # 2\nouts[0] + 1  # 5"
      },
      {
        "description": "Fixes the error by returning the value out of the transformed function.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\nouts = []\n\n@jit\ndef not_side_effecting(x):\n  y = x + 1\n  return y\n\nx = 1\ny = not_side_effecting(x)\nouts.append(y)\nouts[0] + 1  # all good! no longer a leaked value."
      },
      {
        "description": "Shows example usage of the leak checker.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\nouts = []\n\n@jit\ndef side_effecting(x):\n  y = x + 1\n  outs.append(y)\n\nx = 1\n\nwith jax.checking_leaks():\n  y = side_effecting(x)"
      }
    ]
  },
  {
    "title": "Introduction to Ahead-of-Time (AOT) Compilation in JAX",
    "concepts": [
      "JAX's jax.jit performs just-in-time (JIT) compilation.",
      "Ahead-of-time (AOT) compilation allows full compilation prior to execution.",
      "AOT provides control over the compilation process.",
      "AOT is useful when full compilation is required before execution."
    ],
    "code_examples": []
  },
  {
    "title": "Stages of Compilation in JAX",
    "concepts": [
      "Stage out a specialized version of the Python callable to an internal representation.",
      "Specialize the function to input types inferred from the arguments.",
      "Tracing specializes the function to a jaxpr, which is a function in the Jaxpr intermediate language.",
      "Lower the specialized computation to StableHLO, the XLA compiler's input language.",
      "Compile the lowered HLO program to produce an optimized executable.",
      "Execute the compiled executable with the arrays as arguments."
    ],
    "code_examples": []
  },
  {
    "title": "JAX's AOT API: Tracing, Lowering, and Compiling",
    "concepts": [
      "JAX's AOT API gives direct control over compilation steps.",
      "The `.trace()` method traces a jitted function with example inputs.",
      "The `.jaxpr` attribute of the traced object reveals the intermediate Jaxpr representation.",
      "The `.lower()` method lowers the jaxpr to StableHLO.",
      "The `.as_text()` method prints the lowered HLO program.",
      "The `.compile()` method compiles the lowered HLO program to create an executable.",
      "The `.cost_analysis()` method queries cost analysis, e.g. FLOP count.",
      "The compiled function can be executed with the same arguments it was traced with."
    ],
    "code_examples": [
      {
        "description": "Demonstrates tracing, lowering, compiling, and executing a simple JAX function.",
        "code": "import jax\n\ndef f(x, y):\n    return 2 * x + y\n\nx, y = 3, 4\n\ntraced = jax.jit(f).trace(x, y)\n\n# Print the specialized, staged-out representation (as Jaxpr IR)\nprint(traced.jaxpr)\n\nlowered = traced.lower()\n\n# Print lowered HLO\nprint(lowered.as_text())\n\ncompiled = lowered.compile()\n\n# Query for cost analysis, print FLOP estimate\nprint(compiled.cost_analysis()['flops'])\n\n# Execute the compiled function!\nprint(compiled(x, y))"
      },
      {
        "description": "Demonstrates tracing, lowering, compiling, and executing a simple JAX function. Repeated.",
        "code": "import jax\n\ndef f(x, y):\n    return 2 * x + y\n\nx, y = 3, 4\n\ntraced = jax.jit(f).trace(x, y)\n\n# Print the specialized, staged-out representation (as Jaxpr IR)\nprint(traced.jaxpr)\n\nlowered = traced.lower()\n\n# Print lowered HLO\nprint(lowered.as_text())\n\ncompiled = lowered.compile()\n\n# Query for cost analysis, print FLOP estimate\nprint(compiled.cost_analysis()['flops'])\n\n# Execute the compiled function!\nprint(compiled(x, y))"
      }
    ]
  },
  {
    "title": "Considerations and Limitations of AOT Compilation",
    "concepts": [
      "Lowered objects are only valid within the same process.",
      "AOT respects optional arguments to jit like `static_argnums`.",
      "trace arguments can be `ShapeDtypeStruct` instances specifying shape and dtype.",
      "Incompatible argument types at execution time raise a TypeError.",
      "AOT-compiled functions cannot be transformed by JAX transformations (e.g., jax.jit, jax.grad, jax.vmap)."
    ],
    "code_examples": [
      {
        "description": "Demonstrates tracing with ShapeDtypeStruct objects.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return 2 * x + y\n\nx, y = 3, 4\n\ni32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n\nprint(jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x, y))"
      },
      {
        "description": "Demonstrates tracing with ShapeDtypeStruct objects. Repeated.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return 2 * x + y\n\nx, y = 3, 4\n\ni32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n\nprint(jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x, y))"
      },
      {
        "description": "Demonstrates an error when providing incompatible shapes to the compiled function.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return 2 * x + y\n\nx, y = 3, 4\n\ni32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n\nx_1d = y_1d = jnp.arange(3)\n\n# This will raise a TypeError at runtime due to shape mismatch.\n# jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x_1d, y_1d)"
      },
      {
        "description": "Demonstrates an error when providing incompatible data types to the compiled function.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return 2 * x + y\n\nx, y = 3, 4\n\ni32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n\nx_f = y_f = jnp.float32(72.)\n\n# This will raise a TypeError at runtime due to data type mismatch.\n# jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x_f, y_f)"
      },
      {
        "description": "Demonstrates an error when providing incompatible shapes to the compiled function. Repeated.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return 2 * x + y\n\nx, y = 3, 4\n\ni32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n\nx_1d = y_1d = jnp.arange(3)\n\n# This will raise a TypeError at runtime due to shape mismatch.\n# jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x_1d, y_1d)"
      },
      {
        "description": "Demonstrates an error when providing incompatible data types to the compiled function. Repeated.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return 2 * x + y\n\nx, y = 3, 4\n\ni32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n\nx_f = y_f = jnp.float32(72.)\n\n# This will raise a TypeError at runtime due to data type mismatch.\n# jax.jit(f).trace(i32_scalar, i32_scalar).lower().compile()(x_f, y_f)"
      }
    ]
  },
  {
    "title": "Static Arguments in AOT Compilation",
    "concepts": [
      "When tracing with static arguments, `trace` still receives arguments corresponding to the static arguments.",
      "The static argument's value is baked into the lowered computation during lowering.",
      "The compiled function only takes the non-static arguments as input.",
      "The static argument must be a concrete value during tracing, not a shape/dtype structure."
    ],
    "code_examples": [
      {
        "description": "Demonstrates AOT compilation with a static argument, showing how the static argument's value affects the lowered HLO.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return 2 * x + y\n\n\nlowered_with_x = jax.jit(f, static_argnums=0).trace(7, 8).lower()\n\n# Lowered HLO, specialized to the *value* of the first argument (7)\nprint(lowered_with_x.as_text())\n\nprint(lowered_with_x.compile()(5))"
      },
      {
        "description": "Demonstrates AOT compilation with a static argument, showing how the static argument's value affects the lowered HLO. Repeated.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return 2 * x + y\n\n\nlowered_with_x = jax.jit(f, static_argnums=0).trace(7, 8).lower()\n\n# Lowered HLO, specialized to the *value* of the first argument (7)\nprint(lowered_with_x.as_text())\n\nprint(lowered_with_x.compile()(5))"
      },
      {
        "description": "Demonstrates tracing with a static argument using ShapeDtypeStruct for the non-static argument.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return 2 * x + y\n\ni32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n\nprint(jax.jit(f, static_argnums=0).trace(10, i32_scalar).lower().compile()(5))"
      },
      {
        "description": "Demonstrates tracing with a static argument using ShapeDtypeStruct for the non-static argument. Repeated.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return 2 * x + y\n\ni32_scalar = jax.ShapeDtypeStruct((), jnp.dtype('int32'))\n\nprint(jax.jit(f, static_argnums=0).trace(10, i32_scalar).lower().compile()(5))"
      }
    ]
  },
  {
    "title": "Incompatibility with JAX Transformations",
    "concepts": [
      "Compiled functions are specialized to particular argument types.",
      "Transformations like jax.vmap alter the type signature of functions.",
      "JAX disallows compiled functions to be involved in transformations.",
      "Attempts to use jax.vmap with AOT-compiled functions will raise a TypeError."
    ],
    "code_examples": [
      {
        "description": "Illustrates that you cannot apply JAX transformations to a function lowered and compiled for a particular signature.  Specifically, it demonstrates the error when attempting to vmap an AOT compiled function.",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef g(x):\n    assert x.shape == (3, 2)\n    return x @ jnp.ones(2)\n\ndef make_z(*shape):\n    return jnp.arange(np.prod(shape)).reshape(shape)\n\nz, zs = make_z(3, 2), make_z(4, 3, 2)\n\ng_jit = jax.jit(g)\ng_aot = jax.jit(g).trace(z).lower().compile()\n\nprint(jax.vmap(g_jit)(zs))\n# This will raise a TypeError\n# jax.vmap(g_aot)(zs)"
      },
      {
        "description": "Illustrates that you cannot apply JAX transformations to a function lowered and compiled for a particular signature.  Specifically, it demonstrates the error when attempting to vmap an AOT compiled function. Repeated.",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef g(x):\n    assert x.shape == (3, 2)\n    return x @ jnp.ones(2)\n\ndef make_z(*shape):\n    return jnp.arange(np.prod(shape)).reshape(shape)\n\nz, zs = make_z(3, 2), make_z(4, 3, 2)\n\ng_jit = jax.jit(g)\ng_aot = jax.jit(g).trace(z).lower().compile()\n\nprint(jax.vmap(g_jit)(zs))\n# This will raise a TypeError\n# jax.vmap(g_aot)(zs)"
      }
    ]
  },
  {
    "title": "Debugging and Compiler Feedback with AOT Stages",
    "concepts": [
      "JAX's AOT stages offer features for debugging and compiler feedback.",
      "Lowered and compiled functions provide a text representation via `as_text()`.",
      "Compiled functions offer cost and memory analyses from the compiler via `cost_analysis()`.",
      "The `debug_info` parameter to `lowered.as_text()` provides more debugging information.",
      "These methods are meant for manual inspection and are not a reliable programmable API.",
      "Availability and output of these methods vary by compiler, platform, and runtime.",
      "There are very limited guarantees on the consistency of the return values across configurations, backends, versions, or invocations."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Export API",
    "concepts": [
      "The JAX export API allows serializing lowered JAX functions for compilation and execution in separate processes.",
      "This enables compiling and executing functions in environments without access to the JAX program or the target accelerator during tracing.",
      "It facilitates archiving JAX function snapshots for reproducibility.",
      "The jax.export API allows you to serialize a lowered JAX function for compilation and execution in a separate process or machine, without requiring access to the JAX program or repeating the staging-out and lowering."
    ],
    "code_examples": [
      {
        "description": "Demonstrates exporting, inspecting, serializing, deserializing, and calling a JAX function.",
        "code": "import re\nimport numpy as np\nimport jax\nfrom jax import export\n\ndef f(x):\n    return 2 * x * x\n\nexported: export.Exported = export.export(jax.jit(f))(jax.ShapeDtypeStruct((), np.float32))\n\n# You can inspect the Exported object\nexported.fun_name\nexported.in_avals\nprint(re.search(r\".*@main.*\", exported.mlir_module()).group(0))\n\n# And you can serialize the Exported to a bytearray.\nserialized: bytearray = exported.serialize()\n\n# The serialized function can later be rehydrated and called from\n# another JAX computation, possibly in another process.\nrehydrated_exp: export.Exported = export.deserialize(serialized)\nrehydrated_exp.in_avals\n\ndef callee(y):\n    return 3. * rehydrated_exp.call(y * 4.)\n\ncallee(1.)"
      }
    ]
  },
  {
    "title": "Serialization Process",
    "concepts": [
      "Serialization involves exporting to a jax.export.Exported object containing StableHLO and metadata.",
      "Actual serialization to a byte array uses the flatbuffers format.",
      "Alternative serialization to a TensorFlow graph is possible for TensorFlow interoperation."
    ],
    "code_examples": []
  },
  {
    "title": "Higher-Order Reverse-Mode AD",
    "concepts": [
      "Serialization can optionally support higher-order reverse-mode automatic differentiation (AD) via jax.vjp().",
      "The user can specify the number of VJP levels to serialize (default is 0, disabling differentiation).",
      "VJP functions are computed lazily during serialization, respecting features like jax.custom_vjp() and jax.remat().",
      "The rehydrated function does not support forward-mode AD (jvp) or jax.vmap()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates serializing a function with multiple levels of VJP and computing gradients of the rehydrated function.",
        "code": "import jax\nfrom jax import export\nfrom typing import Callable\n\ndef f(x):\n    return 7 * x * x * x\n\n# Serialize 3 levels of VJP along with the primal function\nblob: bytearray = export.export(jax.jit(f))(1.).serialize(vjp_order=3)\n\nrehydrated_f: Callable = export.deserialize(blob).call\nrehydrated_f(0.1)\n\njax.grad(rehydrated_f)(0.1)\n\njax.grad(jax.grad(rehydrated_f))(0.1)\n\njax.grad(jax.grad(jax.grad(rehydrated_f)))(0.1)\n\n# The following line will throw an error because only 3 levels of VJP are serialized.\njax.grad(jax.grad(jax.grad(jax.grad(rehydrated_f))))(0.1)"
      }
    ]
  },
  {
    "title": "Compatibility Guarantees",
    "concepts": [
      "Using raw StableHLO from lowering is discouraged due to potential compiler version differences and custom call dependencies.",
      "JAX export provides compatibility guarantees for compiled and executed artifacts.",
      "JAX exported artifacts can be compiled and executed by compilers and JAX runtime systems that are up to 6 months newer (backward compatibility) or 3 weeks older (forward compatibility) than the JAX version used for exporting.",
      "Compatibility depends on the build time of exporting and consuming components, not the execution time.",
      "Internal JAX users should rebuild/redeploy consumer systems frequently; external users should use the same jaxlib version for exporting and consuming, or export for archival with the latest jaxlib.",
      "Compatibility guarantees do not apply if bypassing jax.export APIs.",
      "JAX refrains from using new custom call targets for 3 weeks to ensure forward compatibility, controllable with the --jax_export_ignore_forward_compatibility flag or JAX_EXPORT_IGNORE_FORWARD_COMPATIBILITY environment variable.",
      "Only a subset of custom calls have compatibility guarantees.",
      "The section clarifies that JAX export provides compatibility guarantees for compiled and executed artifacts, ensuring they can be used with compiler and runtime versions within a specific timeframe relative to the JAX version used for exporting."
    ],
    "code_examples": []
  },
  {
    "title": "Disabling Safety Checks for Custom Calls",
    "concepts": [
      "Custom calls without compatibility guarantees can cause export errors.",
      "The safety check for custom calls can be disabled using export.DisabledSafetyCheck.custom_call(target).",
      "The section describes how to disable safety checks for specific custom calls during the JAX export process, allowing the serialization of code that invokes custom call targets without compatibility guarantees."
    ],
    "code_examples": [
      {
        "description": "Defines a new JAX primitive backed by a custom call and demonstrates how to disable the safety check during export.",
        "code": "import jax\nfrom jax import export\nfrom jax import lax\nfrom jax._src import core\nfrom jax._src.interpreters import mlir\n\n# Define a new primitive backed by a custom call\nnew_prim = core.Primitive(\"new_prim\")\n_ = new_prim.def_abstract_eval(lambda x: x)\n_ = mlir.register_lowering(new_prim, lambda ctx, o: mlir.custom_call(\"my_new_prim\", operands=[o], result_types=[o.type]).results)\n\nprint(jax.jit(new_prim.bind).lower(1.).compiler_ir())\n\n# If we try to export, we get an error\n# export.export(jax.jit(new_prim.bind))(1.)  # This line would raise an error\n\n# We can avoid the error if we pass a `DisabledSafetyCheck.custom_call`\nexp = export.export(\n    jax.jit(new_prim.bind),\n    disabled_checks=[export.DisabledSafetyCheck.custom_call(\"my_new_prim\")]\n)(1.)"
      }
    ]
  },
  {
    "title": "Platform Specific Lowering and Export",
    "concepts": [
      "JAX lowering is platform-specific for some primitives.",
      "Code is lowered and exported for the accelerator present on the exporting machine by default.",
      "There is a safety check to prevent compiling an Exported object on a machine lacking the target accelerator.",
      "You can explicitly specify target platforms during export (e.g., 'tpu', 'cpu', 'cuda', 'rocm'), even if the exporting machine doesn't have that accelerator.",
      "Multi-platform export allows creating an Exported object compilable and executable on multiple platforms.",
      "The section discusses how JAX lowering is platform-specific for some primitives and explains how to specify the target platforms during export, enabling cross-platform compilation and execution."
    ],
    "code_examples": [
      {
        "description": "Shows how to export for a specific platform and how to disable the platform safety check.",
        "code": "import jax\nfrom jax import export\nfrom jax import lax\n\n# You can specify the export platform, e.g., `tpu`, `cpu`, `cuda`, `rocm`\n# even if the current machine does not have that accelerator.\nexp = export.export(jax.jit(lax.cos), platforms=['tpu'])(1.)\n\n# But you will get an error if you try to compile `exp`\n# on a machine that does not have TPUs.\n# exp.call(1.)  # This will raise a ValueError if run on a non-TPU machine.\n\n# We can avoid the error if we pass a `DisabledSafetyCheck.platform`\n# parameter to `export`, e.g., because you have reasons to believe\n# that the code lowered will run adequately on the current\n# compilation platform (which is the case for `cos` in this\n# example):\nexp_unsafe = export.export(\n    jax.jit(lax.cos),\n    platforms=['tpu'],\n    disabled_checks=[export.DisabledSafetyCheck.platform()])(1.)\n\nexp_unsafe.call(1.)\n\n# and similarly with multi-platform lowering\nexp_multi = export.export(\n    jax.jit(lax.cos),\n    platforms=['tpu', 'cpu', 'cuda'])(1.)\n\nexp_multi.call(1.)"
      },
      {
        "description": "Demonstrates that multi-platform export only marginally increases the size of the serialized module.",
        "code": "import jax\nfrom jax import export\nfrom jax import lax\nimport jax.numpy as jnp\n\n# A largish function\ndef f(x):\n    for i in range(1000):\n        x = jnp.cos(x)\n    return x\n\nexp_single = export.export(jax.jit(f))(1.)\nlen(exp_single.mlir_module_serialized)\n\nexp_multi = export.export(\n    jax.jit(f),\n    platforms=[\"cpu\", \"tpu\", \"cuda\"])(1.)\nlen(exp_multi.mlir_module_serialized)"
      }
    ]
  },
  {
    "title": "Shape Polymorphism",
    "concepts": [
      "Exporting supports dimension variables for input dimensions to enable usage with multiple input shapes.",
      "This leverages the Shape polymorphism documentation for achieving flexibility in handling different input sizes."
    ],
    "code_examples": []
  },
  {
    "title": "Device Polymorphism and Sharding",
    "concepts": [
      "Exported artifacts may contain sharding annotations for inputs, outputs, and intermediates, referring to logical devices rather than physical ones.",
      "This enables compiling and running exported artifacts on different physical devices than used for exporting.",
      "The best way to achieve device-polymorphic export is using shardings constructed with jax.sharding.AbstractMesh.",
      "You can use shardings constructed for a mesh with concrete devices, as the actual devices are ignored for tracing and lowering.",
      "When calling an Exported, you must use a concrete set of devices.",
      "Invoking an exported artifact with a different number of devices than it was exported for results in an error.",
      "Helper functions are available to shard inputs for calling exported artifacts using a new mesh constructed at the call site.",
      "Functions exported for 1 device without sharding annotations can be invoked on arguments sharded on multiple devices."
    ],
    "code_examples": [
      {
        "description": "Shows how to use an AbstractMesh and a concrete Mesh for exporting with sharding, and how to call the exported function with a new mesh.",
        "code": "import jax\nfrom jax import export\nfrom jax.sharding import AbstractMesh, Mesh, NamedSharding\nfrom jax.sharding import PartitionSpec as P\nimport jax.numpy as jnp\nimport numpy as np\n\n# Use an AbstractMesh for exporting\nexport_mesh = AbstractMesh(((\"a\", 4),))\n\ndef f(x):\n    return x.T\n\nexp = export.export(jax.jit(f))(\n    jax.ShapeDtypeStruct((32,), dtype=np.int32, sharding=NamedSharding(export_mesh, P(\"a\"))))\n\n# `exp` knows for how many devices it was exported.\nexp.nr_devices\n\n# and it knows the shardings for the inputs. These will be applied\n# when the exported is called.\nexp.in_shardings_hlo\n\n# You can also use a concrete set of devices for exporting\nconcrete_devices = jax.local_devices()[:4]\nconcrete_mesh = Mesh(concrete_devices, (\"a\",))\nexp2 = export.export(jax.jit(f))(\n    jax.ShapeDtypeStruct((32,), dtype=np.int32, sharding=NamedSharding(concrete_mesh, P(\"a\"))))\n\n# You can expect the same results\nassert exp.in_shardings_hlo == exp2.in_shardings_hlo\n\n# When you call an Exported, you must use a concrete set of devices\narg = jnp.arange(8 * 4)\nres1 = exp.call(jax.device_put(arg, NamedSharding(concrete_mesh, P(\"a\"))))\n\n# Check out the first 2 shards of the result\n[f\"device={s.device} index={s.index}\" for s in res1.addressable_shards[:2]]\n\n# We can call `exp` with some other 4 devices and another\n# mesh with a different shape, as long as the number of devices is\n# the same.\nother_mesh = Mesh(np.array(jax.local_devices()[2:6]).reshape((2, 2)), (\"b\", \"c\"))\nres2 = exp.call(jax.device_put(arg, NamedSharding(other_mesh, P(\"b\"))))\n\n# Check out the first 2 shards of the result. Notice that the output is\n# sharded similarly; this means that the input was resharded according to the\n# exp.in_shardings.\n[f\"device={s.device} index={s.index}\" for s in res2.addressable_shards[:2]]"
      },
      {
        "description": "Demonstrates how to use the helper functions for calling an exported artifact with a new mesh.",
        "code": "import jax\nfrom jax import export\nfrom jax.sharding import Mesh, NamedSharding\nfrom jax.sharding import PartitionSpec as P\nimport jax.numpy as jnp\nimport numpy as np\n\nexport_devices = jax.local_devices()\nexport_mesh = Mesh(np.array(export_devices), (\"a\",))\n\ndef f(x):\n    return x.T\n\nexp = export.export(jax.jit(f))(\n    jax.ShapeDtypeStruct((4 * len(export_devices),), dtype=np.int32, sharding=NamedSharding(export_mesh, P(\"a\"))))\n\n# Prepare the mesh for calling `exp`.\ncalling_mesh = Mesh(np.array(export_devices[::-1]), (\"b\",))\n\n# Shard the arg according to what `exp` expects.\narg = jnp.arange(4 * len(export_devices))\nsharded_arg = jax.device_put(arg, exp.in_shardings_jax(calling_mesh)[0])\nres = exp.call(sharded_arg)"
      },
      {
        "description": "Demonstrates the special case where a function exported for 1 device without sharding annotations can be invoked on arguments sharded on multiple devices.",
        "code": "import jax\nfrom jax import export\nfrom jax.sharding import Mesh, NamedSharding\nfrom jax.sharding import PartitionSpec as P\nimport jax.numpy as jnp\nimport numpy as np\n\ndef f(x):\n    return jnp.cos(x)\n\narg = jnp.arange(4)\nexp = export.export(jax.jit(f))(arg)\n\nexp.in_avals\nexp.nr_devices\n\n# Prepare the mesh for calling `exp`.\ncalling_mesh = Mesh(jax.local_devices()[:4], (\"b\",))\n\n# Shard the arg according to what `exp` expects.\nsharded_arg = jax.device_put(\n    arg,\n    NamedSharding(calling_mesh, P(\"b\")))\nres = exp.call(sharded_arg)"
      }
    ]
  },
  {
    "title": "Calling Convention Versions",
    "concepts": [
      "The JAX export support has evolved, and a calling convention version is maintained for each Exported object.",
      "Functions are exported with the latest calling convention version.",
      "The calling convention version can be controlled using the --jax_export_calling_convention_version flag or the JAX_EXPORT_CALLING_CONVENTION_VERSION environment variable.",
      "Support for older calling convention versions may be removed after 6 months."
    ],
    "code_examples": [
      {
        "description": "Shows how to inspect the calling convention version of an exported function and how to control it using configuration.",
        "code": "import jax.numpy as jnp\nfrom jax import export\nfrom jax._src import config\n\nexp: export.Exported = export.export(jnp.cos)(1.)\nexp.calling_convention_version\n\n(export.minimum_supported_calling_convention_version, export.maximum_supported_calling_convention_version)\n\nwith config.jax_export_calling_convention_version(9):\n    exp = export.export(jnp.cos)(1.)\n    exp.calling_convention_version"
      }
    ]
  },
  {
    "title": "MLIR Module Calling Convention",
    "concepts": [
      "The Exported.mlir_module has a main function with a specific calling convention.",
      "The main function takes an optional platform index, followed by token arguments for ordered effects, and kept array arguments.",
      "Inner functions have a different calling convention, including an optional platform index, dimension variable arguments, token arguments, and regular array arguments.",
      "The dimension arguments correspond to dimension variables appearing in the args_avals."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Shape Polymorphism in JAX Export",
    "concepts": [
      "JAX in JIT mode traces, lowers, and compiles functions for each unique input type and shape combination.",
      "Exporting functions allows for later deserialization and execution without Python sources.",
      "Shape polymorphism enables exported functions to handle a variety of input shapes, traced and lowered only once during export.",
      "Exported objects contain the necessary information to compile and execute the function on various concrete input shapes.",
      "Symbolic shapes, containing dimension variables, are specified during export to achieve shape polymorphism."
    ],
    "code_examples": [
      {
        "description": "Demonstrates exporting a JAX function `f` with symbolic shapes and calling it with concrete shapes.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as jnp\nimport numpy as np\n\ndef f(x):\n    # f: f32[a, b]\n    return jnp.concatenate([x, x], axis=1)\n\n# We construct symbolic dimension variables.\na, b = export.symbolic_shape(\"a, b\")\n\n# We can use the symbolic dimensions to construct shapes.\nx_shape = (a, b)\n\n# Then we export with symbolic shapes:\nexp: export.Exported = export.export(jax.jit(f))(jax.ShapeDtypeStruct(x_shape, jnp.int32))\n\n# We can later call with concrete shapes (with a=3 and b=4), without re-tracing `f`.\nres = exp.call(np.ones((3, 4), dtype=np.int32))"
      }
    ]
  },
  {
    "title": "Using jax.export.symbolic_args_specs()",
    "concepts": [
      "jax.export.symbolic_args_specs() constructs pytrees of jax.ShapeDtypeStruct objects based on a polymorphic shape specification.",
      "A polymorphic shape specification string defines the symbolic shapes for the arguments of the function.",
      "The \"...\" placeholder in the specification represents 0 or more dimensions, filled from the concrete shapes of the arguments.",
      "The \"_\" placeholder stands for one dimension.",
      "The polymorphic shapes specification can be a pytree prefix.",
      "The function constructs a pytree of argument specifications (jax.ShapeDtypeStruct) matching the structure of the arguments passed to it."
    ],
    "code_examples": [
      {
        "description": "Illustrates the use of `jax.export.symbolic_args_specs()` with a function `f1` and a polymorphic shape specification.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as jnp\nimport numpy as np\n\ndef f1(x, y):\n    # x: f32[a, 1], y : f32[a, 4]\n    return x + y\n\n# Assuming you have some actual args with concrete shapes\nx = np.ones((3, 1), dtype=np.int32)\ny = np.ones((3, 4), dtype=np.int32)\n\nargs_specs = export.symbolic_args_specs((x, y), \"a, ...\")\nexp = export.export(jax.jit(f1))(*args_specs)"
      }
    ]
  },
  {
    "title": "Correctness and Limitations of Shape Polymorphism",
    "concepts": [
      "The exported program should produce the same results as the original JAX program for any applicable concrete shapes.",
      "JAX functions can re-invoke the tracing machinery for each distinct concrete shape.",
      "The execution of exp.call(arg) cannot use JAX tracing.",
      "Ensuring correctness is challenging, and exporting may fail in difficult cases."
    ],
    "code_examples": []
  },
  {
    "title": "Computing with Dimension Variables",
    "concepts": [
      "JAX tracks the shapes of all intermediate results, computing them as symbolic dimension expressions when shapes depend on dimension variables.",
      "Dimension variables stand for integer values greater or equal to 1.",
      "Symbolic expressions can represent arithmetic operations on dimension expressions and integers.",
      "Symbolic dimensions can be used in shape-parameters of JAX primitives and APIs.",
      "Dimension expressions can be explicitly converted to JAX arrays using jnp.array().",
      "When a symbolic dimension is used in arithmetic operations with non-integers, it is automatically converted to a JAX array using jnp.array()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates flattening a 2D array using symbolic dimensions.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as jnp\nimport numpy as np\n\nf = lambda x: jnp.reshape(x, (x.shape[0] * x.shape[1],))\narg_spec = jax.ShapeDtypeStruct(export.symbolic_shape(\"b, 4\"), jnp.int32)\nexp = export.export(jax.jit(f))(arg_spec)"
      },
      {
        "description": "Illustrates converting a dimension expression to a JAX array.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as jnp\nimport numpy as np\n\nexp = export.export(jax.jit(lambda x: jnp.array(x.shape[0]) + x))(jax.ShapeDtypeStruct(export.symbolic_shape(\"b\"), np.int32))\nres = exp.call(jnp.arange(3, dtype=np.int32))"
      },
      {
        "description": "Example of a symbolic dimension used in arithmetic operations with non-integers, showing automatic conversion to a JAX array.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as jnp\nimport numpy as np\n\nexp = export.export(jax.jit(\n    lambda x: (\n        5. + x.shape[0],\n        x.shape[0] - np.arange(5, dtype=jnp.int32),\n        x + x.shape[0] + jnp.sin(x.shape[0]))))\n    (jax.ShapeDtypeStruct(export.symbolic_shape(\"b\"), jnp.int32))\n\nres = exp.call(jnp.ones((3,), jnp.int32))"
      },
      {
        "description": "Example illustrating automatic conversion of symbolic dimension to JAX array when computing averages.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as jnp\nimport numpy as np\n\nexp = export.export(jax.jit(\n    lambda x: jnp.sum(x, axis=0) / x.shape[0]))(\n    jax.ShapeDtypeStruct(export.symbolic_shape(\"b, c\"), jnp.int32))\n\nres = exp.call(jnp.arange(12, dtype=jnp.int32).reshape((3, 4)))"
      }
    ]
  },
  {
    "title": "Handling Shape Mismatches and Comparisons",
    "concepts": [
      "JAX's shape checking can lead to errors when using symbolic shapes if shapes are incompatible.",
      "Equality comparisons between symbolic dimensions are supported, but may be unsound.",
      "Inequality comparisons are partially supported, considering that dimension variables range over strictly positive integers.",
      "InconclusiveDimensionOperation is raised when a comparison cannot be resolved to a boolean.",
      "core.max_dim and core.min_dim can delay inequality comparisons to compilation time.",
      "Symbolic constraints can be specified to add implicit constraints for dimension sizes.",
      "Explicit symbolic constraints can be defined using constraints argument in jax.export.symbolic_shape().",
      "Symbolic constraints are stored in a jax.export.SymbolicScope object.",
      "Equality constraints are treated as rewrite rules."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating shape mismatch during broadcasting.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as jnp\nimport numpy as np\n\nv, = export.symbolic_shape(\"v,\")\n# This will cause a traceback TypeError\n#export.export(jax.jit(lambda x, y: x + y))(jax.ShapeDtypeStruct((v,), dtype=np.int32), jax.ShapeDtypeStruct((4,), dtype=np.int32))"
      },
      {
        "description": "Illustrates an error due to incompatible shapes in matrix multiplication.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as jnp\nimport numpy as np\n\nv, = export.symbolic_shape(\"v,\")\n# This will cause a traceback TypeError\n#export.export(jax.jit(lambda x: jnp.matmul(x, x)))(jax.ShapeDtypeStruct((v, 4), dtype=np.int32))"
      },
      {
        "description": "Example of inconclusive dimension operation.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as np\n\n# This will cause a traceback InconclusiveDimensionOperation\n#export.export(jax.jit(lambda x: 0 if x.shape[0] + 1 >= x.shape[1] else 1))(jax.ShapeDtypeStruct(export.symbolic_shape(\"a, b\"), dtype=np.int32))"
      },
      {
        "description": "Demonstrates using implicit symbolic constraints by using `b + 15` as shape to ensure the slice will not exceed dimensions size.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as np\n\n_ = export.export(jax.jit(lambda x: x[0:16]))(jax.ShapeDtypeStruct(export.symbolic_shape(\"b + 15\"), dtype=np.int32))"
      },
      {
        "description": "Shows how to use explicit symbolic constraints.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as np\n\n# Introduce dimension variable with constraints.\na, b = export.symbolic_shape(\"a, b\", constraints=(\"a >= b\", \"b >= 16\"))\n_ = export.export(jax.jit(lambda x: x[:x.shape[1], :16]))(jax.ShapeDtypeStruct((a, b), dtype=np.int32))"
      },
      {
        "description": "Illustrates the use of equality constraints in symbolic shapes.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as np\n\n# Introduce dimension variable with equality constraints.\na, b, c, d = export.symbolic_shape(\"a, b, c, d\", constraints=(\"a * b == c + d\",))\n"
      },
      {
        "description": "Example demonstrating a symbolic constraint used to prove the size of the slice is lower than dimension b.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as np\nfrom jax import lax\n\nb, = export.symbolic_shape(\"b\", constraints=[\"b >= mod(b, 3)\"])\nf = lambda x: lax.slice_in_dim(x, 0, x.shape[0] % 3)\n_ = export.export(jax.jit(f))(jax.ShapeDtypeStruct((b,), dtype=np.int32))"
      },
      {
        "description": "Example demonstrating that you cannot mix expressions of dimension variables which originate from different invocations of `jax.export.symbolic_shape`.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as np\n\n# This will raise ValueError: Invalid mixing of symbolic scopes for linear combination.\n#a1, = export.symbolic_shape(\"a,\")\n#a2, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\n#a1 + a2"
      },
      {
        "description": "Demonstrates that you can re-use scopes.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as np\n\na, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\nb, = export.symbolic_shape(\"b,\", scope=a.scope)  # Reuse the scope of `a`\na + b  # Allowed"
      },
      {
        "description": "Demonstrates the explicit creation of a scope.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as np\n\nmy_scope = export.SymbolicScope()\nc, = export.symbolic_shape(\"c\", scope=my_scope)\nd, = export.symbolic_shape(\"d\", scope=my_scope)\nc + d  # Allowed"
      }
    ]
  },
  {
    "title": "Passing Dimension Variable Values",
    "concepts": [
      "Dimension variable values are passed indirectly through the shapes of array arguments.",
      "Functions parameterized by an integer value determining shapes can be problematic.",
      "An UnexpectedDimVar error occurs if a dimension variable doesn't appear in the shapes of the function arguments.",
      "A workaround is to replace the function parameter with an array of shape (0, k), deriving k from its shape."
    ],
    "code_examples": [
      {
        "description": "Illustrates exporting my_top_k with static k.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as np\nfrom jax import lax\n\ndef my_top_k(k, x):\n    # x: i32[4, 10], k <= 10\n    return lax.top_k(x, k)[0]  # : i32[4, 3]\n\nx = np.arange(40, dtype=np.int32).reshape((4, 10))\n\n# Export with static `k=3`. Since `k` appears in shapes it must be in `static_argnums`.\nexp_static_k = export.export(jax.jit(my_top_k, static_argnums=0))(3, x)\n# When calling the exported function we pass only the non-static arguments\nres = exp_static_k.call(x)"
      },
      {
        "description": "Illustrates the workaround of exporting my_top_k with a symbolic dimension.",
        "code": "import jax\nfrom jax import export\nfrom jax import numpy as np\nfrom jax import lax\n\ndef my_top_k(k, x):\n    # x: i32[4, 10], k <= 10\n    return lax.top_k(x, k)[0]  # : i32[4, 3]\n\nx = np.arange(40, dtype=np.int32).reshape((4, 10))\n\nk, = export.symbolic_shape(\"k\", constraints=[\"k <= 10\"])\n\ndef my_top_k_with_dimensions(dimensions, x):\n    # dimensions: i32[0, k], x: i32[4, 10]\n    return my_top_k(dimensions.shape[1], x)\n\nexp = export.export(jax.jit(my_top_k_with_dimensions))(\n    jax.ShapeDtypeStruct((0, k), dtype=np.int32),\n    x)\n\n# When we invoke `exp` we must construct and pass an array of shape (0, k)\nres = exp.call(np.zeros((0, 3), dtype=np.int32), x)"
      }
    ]
  },
  {
    "title": "JAX2TF Overview",
    "concepts": [
      "The document refers to JAX2TF documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Type Promotion",
    "concepts": [
      "The document describes JAX's type promotion rules using jax.numpy.promote_types().",
      "JAX's type promotion behavior is determined via a type promotion lattice.",
      "Promotion between any two types is given by their join on this lattice.",
      "JAX's type promotion rules differ from NumPy's in certain cases."
    ],
    "code_examples": []
  },
  {
    "title": "Key Differences from NumPy Type Promotion",
    "concepts": [
      "When promoting a weakly typed value against a typed JAX value, JAX prefers the precision of the JAX value.",
      "When promoting an integer or boolean type against a floating-point or complex type, JAX prefers the floating-point or complex type.",
      "JAX supports bfloat16, which promotes to float32 with float16.",
      "JAX's promotion rules are more suited to modern accelerator devices and less aggressive about promoting floating-point types.",
      "Python operators dispatch based on the Python type, leading to potentially confusing promotion semantics when mixing NumPy and JAX types."
    ],
    "code_examples": [
      {
        "description": "Demonstrates JAX preferring its own type's precision over implicit promotion to int64 when adding a Python scalar to a JAX int16 array.",
        "code": "import jax.numpy as jnp\n\njnp.int16(1) + 1"
      },
      {
        "description": "Shows that NumPy arrays added to JAX arrays follow the lattice promotion rules, resulting in int64 promotion.",
        "code": "import jax.numpy as jnp\nimport numpy as np\n\njnp.int16(1) + np.array(1)"
      }
    ]
  },
  {
    "title": "Weakly-typed Values in JAX",
    "concepts": [
      "Weakly-typed values in JAX have promotion behavior equivalent to Python scalars.",
      "JAX's weak type framework prevents unwanted type promotion within binary operations between JAX values and Python scalar literals.",
      "DeviceArray objects carry a weak_type flag.",
      "Specifying the dtype explicitly results in a strongly-typed array value."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how a Python scalar interacts with a JAX array, preserving the array's original dtype.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(5, dtype='int8')\n2 * x"
      },
      {
        "description": "Shows how without weak typing, multiplication of a JAX array with a jnp.int32 scalar would lead to promotion to int32.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(5, dtype='int8')\njnp.int32(2) * x"
      },
      {
        "description": "Illustrates how DeviceArray objects carry a weak_type flag.",
        "code": "import jax.numpy as jnp\n\njnp.asarray(2)"
      },
      {
        "description": "Shows that specifying the dtype explicitly results in a strongly-typed array.",
        "code": "import jax.numpy as jnp\n\njnp.asarray(2, dtype='int32')"
      }
    ]
  },
  {
    "title": "Strict Dtype Promotion",
    "concepts": [
      "Implicit type promotion can be disabled using the jax_numpy_dtype_promotion flag.",
      "Setting the flag to 'strict' requires explicit type casting.",
      "Strict promotion mode allows safe weakly-typed promotions.",
      "The configuration can be set locally using a context manager or globally using jax.config.update()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the TypePromotionError when strict mode is enabled and incompatible types are used without explicit casting.",
        "code": "import jax.numpy as jnp\nimport jax\n\nx = jnp.float32(1)\ny = jnp.int32(1)\nwith jax.numpy_dtype_promotion('strict'):\n    z = x + y"
      },
      {
        "description": "Shows that strict mode allows safe weakly-typed promotions between JAX arrays and Python scalars.",
        "code": "import jax.numpy as jnp\nimport jax\n\nx = jnp.float32(1)\nwith jax.numpy_dtype_promotion('strict'):\n    z = x + 1\nprint(z)"
      },
      {
        "description": "Demonstrates how to set the jax_numpy_dtype_promotion configuration globally to 'strict'.",
        "code": "import jax\n\njax.config.update('jax_numpy_dtype_promotion', 'strict')"
      },
      {
        "description": "Demonstrates how to restore the default standard type promotion by setting the configuration to 'standard'.",
        "code": "import jax\n\njax.config.update('jax_numpy_dtype_promotion', 'standard')"
      }
    ]
  },
  {
    "title": "Redundant content",
    "concepts": [],
    "code_examples": []
  },
  {
    "title": "Introduction to Pallas",
    "concepts": [
      "Pallas is an extension to JAX for writing custom GPU and TPU kernels.",
      "Pallas provides fine-grained control over code generation.",
      "Pallas offers high-level ergonomics of JAX tracing and the jax.numpy API.",
      "Pallas is experimental and under active development."
    ],
    "code_examples": []
  },
  {
    "title": "Resources and Warnings",
    "concepts": [
      "This section contains tutorials, guides, and examples for using Pallas.",
      "Refer to the jax.experimental.pallas module API documentation.",
      "Pallas is experimental and subject to frequent changes.",
      "Users should be aware of potential errors and unimplemented features."
    ],
    "code_examples": []
  },
  {
    "title": "Further Guides",
    "concepts": [
      "Platform Features",
      "Design Notes",
      "Other"
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Pallas",
    "concepts": [
      "Pallas is a JAX extension for writing custom GPU/TPU kernels.",
      "Pallas operates at a lower level of abstraction, requiring memory access and computation distribution considerations.",
      "Pallas lowers to Triton on GPUs and Mosaic on TPUs.",
      "Pallas is an experimental API."
    ],
    "code_examples": [
      {
        "description": "Import necessary libraries: functools, jax, pallas, jax.numpy, numpy.",
        "code": "from\nfunctools\nimport\npartial\nimport\njax\nfrom\njax.experimental\nimport\npallas\nas\npl\nimport\njax.numpy\nas\njnp\nimport\nnumpy\nas\nnp"
      }
    ]
  },
  {
    "title": "Hello World: Vector Addition Kernel",
    "concepts": [
      "Pallas kernels take Ref objects as inputs, representing mutable memory buffers.",
      "Kernels operate as atomic units of execution on an accelerator.",
      "Reading from a Ref returns a jax.Array.",
      "Writing to a Ref mutates its underlying buffer.",
      "The pallas_call function lifts a Pallas kernel into a JAX program operation.",
      "The out_shape argument of pallas_call determines the shape and dtype of the output Ref.",
      "On GPUs, x_ref corresponds to a value in HBM, and x_ref[...] copies the value to SRAM.",
      "On TPUs, x_ref corresponds to a value in SRAM, and x_ref[...] copies the value to a register."
    ],
    "code_examples": [
      {
        "description": "Define a kernel to add two vectors using Ref objects.",
        "code": "def\nadd_vectors_kernel\n(\n x_ref\n,\n y_ref\n,\n o_ref\n):\n x\n,\n y\n=\n x_ref\n[\n...\n],\n y_ref\n[\n...\n]\n o_ref\n[\n...\n]\n=\n x\n+\n y"
      },
      {
        "description": "Use pallas_call to invoke the add_vectors_kernel within a JAX function.",
        "code": "@jax\n.jit\ndef\nadd_vectors\n(\n x\n:\njax\n.Array\n,\n y\n:\njax\n.Array\n)\n->\njax\n.Array\n:\n return\n pl\n.pallas_call\n(\n add_vectors_kernel\n,\n out_shape\n=\njax\n.ShapeDtypeStruct\n(\n x\n.shape\n,\n x\n.dtype\n)\n)(\n x\n,\n y\n)\nadd_vectors\n(\n jnp\n.arange\n(\n8\n),\n jnp\n.arange\n(\n8\n))"
      }
    ]
  },
  {
    "title": "Grid and program_id",
    "concepts": [
      "For large arrays, computations operate on blocks that fit in SRAM.",
      "A grid specifies an iteration space for the kernel, enabling SPMD programming.",
      "The kernel function is executed once for each element in the grid.",
      "program_id(axis=...) provides the index of the current program in the grid.",
      "On GPUs, each program is executed in parallel.",
      "On TPUs, the grid can be specified in parallel and sequential dimensions.",
      "TPUs operate like a very wide SIMD machine and can be treated as a single-threaded processor."
    ],
    "code_examples": [
      {
        "description": "Define a kernel that sets the output value to the program ID.",
        "code": "def\niota_kernel\n(\n o_ref\n):\n i\n=\npl\n.program_id\n(\n0\n)\n o_ref\n[\n i\n]\n=\n i"
      },
      {
        "description": "Call the iota_kernel using pallas_call with a grid argument for GPUs.",
        "code": "# GPU version\ndef\niota\n(\n size\n:\nint\n):\n return\n pl\n.pallas_call\n(\n iota_kernel\n,\n out_shape\n=\njax\n.ShapeDtypeStruct\n((\nsize\n,),\n jnp\n.int32\n),\n grid\n=\n(\nsize\n,))\n()\niota\n(\n8\n)"
      },
      {
        "description": "Call the iota_kernel using pallas_call with a grid and BlockSpec for TPUs.",
        "code": "# TPU version\nfrom\njax.experimental.pallas\nimport\ntpu\nas\npltpu\ndef\niota\n(\n size\n:\nint\n):\n return\n pl\n.pallas_call\n(\n iota_kernel\n,\n out_specs\n=\npl\n.BlockSpec\n(\n memory_space\n=\npltpu\n.TPUMemorySpace\n.SMEM\n),\n out_shape\n=\njax\n.ShapeDtypeStruct\n((\nsize\n,),\n jnp\n.int32\n),\n grid\n=\n(\nsize\n,))\n()\niota\n(\n8\n)"
      }
    ]
  },
  {
    "title": "BlockSpec and Matrix Multiplication",
    "concepts": [
      "BlockSpec specifies a block shape for each input/output and an index map function.",
      "BlockSpec s are used to automatically carve up inputs and outputs into Refs for each block.",
      "The in_specs and out_specs arguments of pallas_call accept BlockSpec objects."
    ],
    "code_examples": [
      {
        "description": "Define a matrix multiplication kernel using BlockSpec to split the matrices.",
        "code": "def\nmatmul_kernel\n(\n x_ref\n,\n y_ref\n,\n z_ref\n):\n z_ref\n[\n...\n]\n=\n x_ref\n[\n...\n]\n@\n y_ref\n[\n...\n]\ndef\nmatmul\n(\n x\n:\njax\n.Array\n,\n y\n:\njax\n.Array\n):\n return\n pl\n.pallas_call\n(\n matmul_kernel\n,\n out_shape\n=\njax\n.ShapeDtypeStruct\n((\n x\n.shape\n[\n0\n],\n y\n.shape\n[\n1\n]),\n x\n.dtype\n),\n grid\n=\n(\n2\n,\n2\n),\n in_specs\n=\n[\n pl\n.BlockSpec\n((\n x\n.shape\n[\n0\n]\n//\n2\n,\n x\n.shape\n[\n1\n]),\nlambda\ni\n,\nj\n:\n(\ni\n,\n0\n)),\n pl\n.BlockSpec\n((\n y\n.shape\n[\n0\n],\n y\n.shape\n[\n1\n]\n//\n2\n),\nlambda\ni\n,\nj\n:\n(\n0\n,\nj\n))\n ],\n out_specs\n=\npl\n.BlockSpec\n(\n(\n x\n.shape\n[\n0\n]\n//\n2\n,\n y\n.shape\n[\n1\n]\n//\n2\n),\nlambda\ni\n,\nj\n:\n(\ni\n,\nj\n),\n)\n)(\n x\n,\n y\n)\nk1\n,\nk2\n=\njax\n.random\n.split\n(\njax\n.random\n.key\n(\n0\n))\nx\n=\njax\n.random\n.normal\n(\nk1\n,\n(\n1024\n,\n1024\n))\ny\n=\njax\n.random\n.normal\n(\nk2\n,\n(\n1024\n,\n1024\n))\nz\n=\nmatmul\n(\nx\n,\ny\n)\nnp\n.testing\n.assert_allclose\n(\nz\n,\nx\n@\ny\n)"
      }
    ]
  },
  {
    "title": "Fused Activation and vmap Composition",
    "concepts": [
      "Higher-order functions can be passed into the kernel for operations like fused activation.",
      "Pallas kernels compose with jax.vmap to create batched versions."
    ],
    "code_examples": [
      {
        "description": "Add a fused activation function to the matrix multiplication kernel.",
        "code": "def\nmatmul_kernel\n(\n x_ref\n,\n y_ref\n,\n z_ref\n,\n*\n,\nactivation\n):\n z_ref\n[\n...\n]\n=\nactivation\n(\n x_ref\n[\n...\n]\n@\n y_ref\n[\n...\n])\ndef\nmatmul\n(\n x\n:\njax\n.Array\n,\n y\n:\njax\n.Array\n,\n*\n,\nactivation\n):\n return\n pl\n.pallas_call\n(\npartial\n(\n matmul_kernel\n,\nactivation\n=\nactivation\n),\n out_shape\n=\njax\n.ShapeDtypeStruct\n((\n x\n.shape\n[\n0\n],\n y\n.shape\n[\n1\n]),\n x\n.dtype\n),\n grid\n=\n(\n2\n,\n2\n),\n in_specs\n=\n[\n pl\n.BlockSpec\n((\n x\n.shape\n[\n0\n]\n//\n2\n,\n x\n.shape\n[\n1\n]),\nlambda\ni\n,\nj\n:\n(\ni\n,\n0\n)),\n pl\n.BlockSpec\n((\n y\n.shape\n[\n0\n],\n y\n.shape\n[\n1\n]\n//\n2\n),\nlambda\ni\n,\nj\n:\n(\n0\n,\nj\n))\n ],\n out_specs\n=\npl\n.BlockSpec\n(\n(\n x\n.shape\n[\n0\n]\n//\n2\n,\n y\n.shape\n[\n1\n]\n//\n2\n),\nlambda\ni\n,\nj\n:\n(\ni\n,\nj\n)\n),\n)(\n x\n,\n y\n)\nk1\n,\nk2\n=\njax\n.random\n.split\n(\njax\n.random\n.key\n(\n0\n))\nx\n=\njax\n.random\n.normal\n(\nk1\n,\n(\n1024\n,\n1024\n))\ny\n=\njax\n.random\n.normal\n(\nk2\n,\n(\n1024\n,\n1024\n))\nz\n=\nmatmul\n(\nx\n,\ny\n,\nactivation\n=\njax\n.nn\n.relu\n)\nnp\n.testing\n.assert_allclose\n(\nz\n,\njax\n.nn\n.relu\n(\nx\n@\ny\n))"
      },
      {
        "description": "Create a batched version of the matrix multiplication using jax.vmap.",
        "code": "k1\n,\nk2\n=\njax\n.random\n.split\n(\njax\n.random\n.key\n(\n0\n))\nx\n=\njax\n.random\n.normal\n(\nk1\n,\n(\n4\n,\n1024\n,\n1024\n))\ny\n=\njax\n.random\n.normal\n(\nk2\n,\n(\n4\n,\n1024\n,\n1024\n))\nz\n=\njax\n.vmap\n(\npartial\n(\n matmul\n,\nactivation\n=\njax\n.nn\n.relu\n))\n(\nx\n,\ny\n)\nnp\n.testing\n.assert_allclose\n(\nz\n,\njax\n.nn\n.relu\n(\njax\n.vmap\n(\njnp\n.matmul\n)\n(\nx\n,\ny\n)))"
      }
    ]
  },
  {
    "title": "Grids and Program IDs",
    "concepts": [
      "pallas_call executes a kernel function multiple times based on the grid argument.",
      "Grids can be multi-dimensional, corresponding to nested loops.",
      "The kernel is executed as many times as the product of the grid dimensions.",
      "program_id() provides access to the current program's index within the grid.",
      "num_programs() returns the grid size for a given axis."
    ],
    "code_examples": []
  },
  {
    "title": "BlockSpec Semantics with Blocked Indexing",
    "concepts": [
      "BlockSpec defines how the input is sliced for each kernel invocation.",
      "BlockSpec is provided to pallas_call via in_specs and out_specs.",
      "The index_map of the BlockSpec takes invocation indices and returns block indices.",
      "Block indices are multiplied by block_shape to get the actual element index.",
      "Certain block shapes are required on TPU and GPU.",
      "If block_shape does not divide evenly the overall shape, the last iteration on each axis will receive references to padded blocks on input and discarded on output."
    ],
    "code_examples": [
      {
        "description": "Calculates the slices for a given invocation based on the block specification, grid, and invocation indices.",
        "code": "import jax\nfrom jax.experimental import pallas as pl\n\ndef slices_for_invocation(\n    x_shape: tuple[int, ...],\n    x_spec: pl.BlockSpec,\n    grid: tuple[int, ...],\n    invocation_indices: tuple[int, ...]) -> tuple[slice, ...]:\n  assert len(invocation_indices) == len(grid)\n  assert all(0 <= i < grid_size for i, grid_size in zip(invocation_indices, grid))\n  block_indices = x_spec.index_map(*invocation_indices)\n  assert len(x_shape) == len(x_spec.block_shape) == len(block_indices)\n  elem_indices = []\n  for x_size, block_size, block_idx in zip(x_shape, x_spec.block_shape, block_indices):\n    start_idx = block_idx * block_size\n    # At least one element of the block must be within bounds\n    assert start_idx < x_size\n    elem_indices.append(slice(start_idx, start_idx + block_size))\n  return elem_indices"
      },
      {
        "description": "Calculates the slices for a given invocation based on the block specification, grid, and invocation indices.",
        "code": "import jax\nfrom jax.experimental import pallas as pl\n\ndef slices_for_invocation(\n    x_shape: tuple[int, ...],\n    x_spec: pl.BlockSpec,\n    grid: tuple[int, ...],\n    invocation_indices: tuple[int, ...]) -> tuple[slice, ...]:\n  assert len(invocation_indices) == len(grid)\n  assert all(0 <= i < grid_size for i, grid_size in zip(invocation_indices, grid))\n  block_indices = x_spec.index_map(*invocation_indices)\n  assert len(x_shape) == len(x_spec.block_shape) == len(block_indices)\n  elem_indices = []\n  for x_size, block_size, block_idx in zip(x_shape, x_spec.block_shape, block_indices):\n    start_idx = block_idx * block_size\n    # At least one element of the block must be within bounds\n    assert start_idx < x_size\n    elem_indices.append(slice(start_idx, start_idx + block_size))\n  return elem_indices"
      }
    ]
  },
  {
    "title": "Examples of BlockSpec Slicing",
    "concepts": [
      "Demonstrates how slices are calculated using slices_for_invocation function with various block shapes, grids, and invocation indices.",
      "Illustrates scenarios with partial out-of-bounds accesses.",
      "Shows how iterating over the same block multiple times affects the indices.",
      "Demonstrates the BlockSpec usage with the `Blocked` indexing mode."
    ],
    "code_examples": [
      {
        "description": "Calculates the slices for a given invocation based on the block specification, grid, and invocation indices.",
        "code": "slices_for_invocation(\n    x_shape=(100, 100),\n    x_spec=pl.BlockSpec((10, 20), lambda i, j: (i, j)),\n    grid=(10, 5),\n    invocation_indices=(2, 4))\n"
      },
      {
        "description": "Calculates the slices for a given invocation based on the block specification, grid, and invocation indices.",
        "code": "slices_for_invocation(\n    x_shape=(100, 100),\n    x_spec=pl.BlockSpec((10, 20), lambda i, j, k: (i, j)),\n    grid=(10, 5, 4),\n    invocation_indices=(2, 4, 0))\n"
      },
      {
        "description": "Calculates the slices for a given invocation based on the block specification, grid, and invocation indices.",
        "code": "slices_for_invocation(\n    x_shape=(100, 90),\n    x_spec=pl.BlockSpec((10, 20), lambda i, j: (i, j)),\n    grid=(10, 5),\n    invocation_indices=(2, 4))\n"
      },
      {
        "description": "Calculates the slices for a given invocation based on the block specification, grid, and invocation indices.",
        "code": "slices_for_invocation(\n    x_shape=(100, 100),\n    x_spec=pl.BlockSpec((10, 20), lambda i, j: (i, j)),\n    grid=(10, 5),\n    invocation_indices=(2, 4))\n"
      },
      {
        "description": "Calculates the slices for a given invocation based on the block specification, grid, and invocation indices.",
        "code": "slices_for_invocation(\n    x_shape=(100, 100),\n    x_spec=pl.BlockSpec((10, 20), lambda i, j, k: (i, j)),\n    grid=(10, 5, 4),\n    invocation_indices=(2, 4, 0))\n"
      },
      {
        "description": "Calculates the slices for a given invocation based on the block specification, grid, and invocation indices.",
        "code": "slices_for_invocation(\n    x_shape=(100, 90),\n    x_spec=pl.BlockSpec((10, 20), lambda i, j: (i, j)),\n    grid=(10, 5),\n    invocation_indices=(2, 4))\n"
      }
    ]
  },
  {
    "title": "Showing Program IDs with Pallas",
    "concepts": [
      "Demonstrates how to use Pallas to visualize program IDs.",
      "The show_program_ids function fills each output block with a decimal number representing the invocation indices.",
      "Uses pallas_call to execute the program_ids_kernel and display the results.",
      "Illustrates the BlockSpec usage with the `Blocked` indexing mode."
    ],
    "code_examples": [
      {
        "description": "Fills each output block with a decimal number where the first digit represents the invocation index over the first axis, and the second the invocation index over the second axis",
        "code": "def show_program_ids(\n    x_shape,\n    block_shape,\n    grid,\n    index_map=lambda i, j: (i, j),\n    indexing_mode=pl.Blocked()):\n\n  def program_ids_kernel(o_ref):\n    # Fill the output block with 10*program_id(1) + program_id(0)\n    axes = 0\n    for axis in range(len(grid)):\n      axes += pl.program_id(axis) * 10**(len(grid) - 1 - axis)\n    o_ref[...] = jnp.full(o_ref.shape, axes)\n\n  res = pl.pallas_call(\n      program_ids_kernel,\n      out_shape=jax.ShapeDtypeStruct(x_shape, dtype=np.int32),\n      grid=grid,\n      in_specs=[],\n      out_specs=pl.BlockSpec(\n          block_shape, index_map, indexing_mode=indexing_mode),\n      interpret=True)()\n  print(res)"
      },
      {
        "description": "Fills each output block with a decimal number where the first digit represents the invocation index over the first axis, and the second the invocation index over the second axis",
        "code": "def show_program_ids(\n    x_shape,\n    block_shape,\n    grid,\n    index_map=lambda i, j: (i, j),\n    indexing_mode=pl.Blocked()):\n\n  def program_ids_kernel(o_ref):\n    # Fill the output block with 10*program_id(1) + program_id(0)\n    axes = 0\n    for axis in range(len(grid)):\n      axes += pl.program_id(axis) * 10**(len(grid) - 1 - axis)\n    o_ref[...] = jnp.full(o_ref.shape, axes)\n\n  res = pl.pallas_call(\n      program_ids_kernel,\n      out_shape=jax.ShapeDtypeStruct(x_shape, dtype=np.int32),\n      grid=grid,\n      in_specs=[],\n      out_specs=pl.BlockSpec(\n          block_shape, index_map, indexing_mode=indexing_mode),\n      interpret=True)()\n  print(res)"
      }
    ]
  },
  {
    "title": "Program ID Examples and Edge Cases",
    "concepts": [
      "Illustrates the show_program_ids function with various shapes, block shapes, and grids.",
      "Demonstrates scenarios with out-of-bounds accesses.",
      "Shows how it handles cases where the shape is smaller than the block shape.",
      "Shows that when multiple invocations write to the same element, the result is platform dependent"
    ],
    "code_examples": [
      {
        "description": "Illustrates the show_program_ids function with various shapes, block shapes, and grids.",
        "code": "show_program_ids(\n    x_shape=(8, 6),\n    block_shape=(2, 3),\n    grid=(4, 2),\n    index_map=lambda i, j: (i, j))\n"
      },
      {
        "description": "Illustrates the show_program_ids function with various shapes, block shapes, and grids.",
        "code": "show_program_ids(\n    x_shape=(7, 5),\n    block_shape=(2, 3),\n    grid=(4, 2),\n    index_map=lambda i, j: (i, j))\n"
      },
      {
        "description": "Illustrates the show_program_ids function with various shapes, block shapes, and grids.",
        "code": "show_program_ids(\n    x_shape=(1, 2),\n    block_shape=(2, 3),\n    grid=(1, 1),\n    index_map=lambda i, j: (i, j))\n"
      },
      {
        "description": "Illustrates the show_program_ids function with various shapes, block shapes, and grids.",
        "code": "show_program_ids(\n    x_shape=(8, 6),\n    block_shape=(2, 3),\n    grid=(4, 2),\n    index_map=lambda i, j: (i, j))\n"
      },
      {
        "description": "Illustrates the show_program_ids function with various shapes, block shapes, and grids.",
        "code": "show_program_ids(\n    x_shape=(7, 5),\n    block_shape=(2, 3),\n    grid=(4, 2),\n    index_map=lambda i, j: (i, j))\n"
      },
      {
        "description": "Illustrates the show_program_ids function with various shapes, block shapes, and grids.",
        "code": "show_program_ids(\n    x_shape=(1, 2),\n    block_shape=(2, 3),\n    grid=(1, 1),\n    index_map=lambda i, j: (i, j))\n"
      },
      {
        "description": "Demonstrates overlapping blocks",
        "code": "show_program_ids(\n    x_shape=(8, 6),\n    block_shape=(2, 3),\n    grid=(4, 2, 10),\n    index_map=lambda i, j, k: (i, j))\n"
      },
      {
        "description": "Demonstrates overlapping blocks",
        "code": "show_program_ids(\n    x_shape=(8, 6),\n    block_shape=(2, 3),\n    grid=(4, 2, 10),\n    index_map=lambda i, j, k: (i, j))\n"
      }
    ]
  },
  {
    "title": "None Values in BlockSpec",
    "concepts": [
      "A None value as a dimension in block_shape behaves as 1, and the corresponding block axis is squeezed.",
      "A None value for block_shape defaults to the shape of the overall array.",
      "A None value for index_map defaults to a function that returns a tuple of zeros."
    ],
    "code_examples": [
      {
        "description": "Demonstrates usage of None value in block_shape",
        "code": "def kernel(o_ref):\n  assert o_ref.shape == (2,)\n  o_ref[...] = jnp.full((2,), 10 * pl.program_id(1) + pl.program_id(0))\n\npl.pallas_call(\n    kernel,\n    jax.ShapeDtypeStruct((3, 4), dtype=np.int32),\n    out_specs=pl.BlockSpec((None, 2), lambda i, j: (i, j)),\n    grid=(3, 2),\n    interpret=True)()\n"
      },
      {
        "description": "Demonstrates usage of None value in block_shape",
        "code": "def kernel(o_ref):\n  assert o_ref.shape == (2,)\n  o_ref[...] = jnp.full((2,), 10 * pl.program_id(1) + pl.program_id(0))\n\npl.pallas_call(\n    kernel,\n    jax.ShapeDtypeStruct((3, 4), dtype=np.int32),\n    out_specs=pl.BlockSpec((None, 2), lambda i, j: (i, j)),\n    grid=(3, 2),\n    interpret=True)()\n"
      },
      {
        "description": "Demonstrates usage of None value in BlockSpec",
        "code": "show_program_ids(\n    x_shape=(4, 4),\n    block_shape=None,\n    grid=(2, 3),\n    index_map=None)\n"
      },
      {
        "description": "Demonstrates usage of None value in BlockSpec",
        "code": "show_program_ids(\n    x_shape=(4, 4),\n    block_shape=(4,4),\n    grid=(2, 3),\n    index_map=None)\n"
      },
      {
        "description": "Demonstrates usage of None value in BlockSpec",
        "code": "show_program_ids(\n    x_shape=(4, 4),\n    block_shape=None,\n    grid=(2, 3),\n    index_map=None)\n"
      },
      {
        "description": "Demonstrates usage of None value in BlockSpec",
        "code": "show_program_ids(\n    x_shape=(4, 4),\n    block_shape=(4,4),\n    grid=(2, 3),\n    index_map=None)\n"
      }
    ]
  },
  {
    "title": "Unblocked Indexing Mode",
    "concepts": [
      "The values returned by the index_map function are used directly as array indices, without scaling by the block size.",
      "Virtual padding of the array can be specified as a tuple of low-high paddings for each dimension.",
      "The unblocked mode is currently supported only on TPUs."
    ],
    "code_examples": [
      {
        "description": "Demonstrates usage of Unblocked indexing mode.",
        "code": "show_program_ids(\n    x_shape=(8, 6),\n    block_shape=(2, 3),\n    grid=(4, 2),\n    index_map=lambda i, j: (2 * i, 3 * j),\n    indexing_mode=pl.Unblocked())\n"
      },
      {
        "description": "Demonstrates usage of Unblocked indexing mode with padding.",
        "code": "show_program_ids(\n    x_shape=(7, 7),\n    block_shape=(2, 3),\n    grid=(4, 3),\n    index_map=lambda i, j: (2 * i, 3 * j),\n    indexing_mode=pl.Unblocked(((1, 0), (2, 0))))\n"
      },
      {
        "description": "Demonstrates usage of Unblocked indexing mode.",
        "code": "show_program_ids(\n    x_shape=(8, 6),\n    block_shape=(2, 3),\n    grid=(4, 2),\n    index_map=lambda i, j: (2 * i, 3 * j),\n    indexing_mode=pl.Unblocked())\n"
      },
      {
        "description": "Demonstrates usage of Unblocked indexing mode with padding.",
        "code": "show_program_ids(\n    x_shape=(7, 7),\n    block_shape=(2, 3),\n    grid=(4, 3),\n    index_map=lambda i, j: (2 * i, 3 * j),\n    indexing_mode=pl.Unblocked(((1, 0), (2, 0))))\n"
      }
    ]
  },
  {
    "title": "TPU Specific Documentation Introduction",
    "concepts": [
      "The document provides TPU specific documentation.",
      "The document includes guides."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Pallas Kernels on Google TPUs",
    "concepts": [
      "The TPU backend is still experimental and only a subset of JAX NumPy is accepted.",
      "Writing performant code for TPUs requires understanding the hardware's capabilities.",
      "TPUs are sequential machines with a wide vector register, allowing asynchronous scheduling of operations.",
      "Understanding the TPU architecture helps in writing performant kernels.",
      "HBM memory accesses are prefetched to lower memory levels by DMA subunits.",
      "Matrix multiplies are supported by the MXU unit, and matrix transpositions/permutes are supported by the XLU unit."
    ],
    "code_examples": []
  },
  {
    "title": "BlockSpecs and Memory Spaces",
    "concepts": [
      "BlockSpecs behave as expected in Pallas; every kernel invocation gets access to input slices and initializes an output slice.",
      "Inputs to pallas_call often reside in HBM, while references passed to the kernel body point to buffers in VMEM or SMEM.",
      "The compiler handles communication with HBM and overlaps it with compute.",
      "TPUs process the grid sequentially, unlocking capabilities like skipping HBM transfers for consecutive grid indices using the same input slice.",
      "Multiple kernel invocations can write to the same output slice consecutively without race conditions.",
      "The \"consecutive\" restriction on output usually implies that a prefix of grid dimensions varies the output slice.",
      "VMEM is fairly large (16MB+), allowing for large window sizes which often improves hardware utilization."
    ],
    "code_examples": []
  },
  {
    "title": "Dimension Ordering and Vectorization",
    "concepts": [
      "Dimension ordering of arrays is meaningful in Pallas.",
      "TPUs perform computation on 2D vector registers (typically 8x128 for 32-bit values).",
      "Pallas maps the last two dimensions of intermediate arrays to the 8x128 vector register dimensions.",
      "Singleton dimensions in the last two axes are wasteful.",
      "Vector computation is padded up to the tile size.",
      "The last two axes of an array are treated differently than other axes; reductions, reshapes, and transposes involving them are generally more expensive."
    ],
    "code_examples": []
  },
  {
    "title": "Multicore TPU Configurations",
    "concepts": [
      "To leverage multiple cores, Pallas can parallelize one of the grid axes over cores.",
      "pallas_call requires a dimension_semantics parameter to specify which dimensions can be partitioned.",
      "Parallel dimensions are those where the output window varies.",
      "Partitioning a kernel over a 2-core TPU device often leads to a speedup, but can be limited by varying costs across instances.",
      "Pallas TPU favors partitioning axes that are a multiple of the number of TPU cores and prefers to partition leading grid axes."
    ],
    "code_examples": []
  },
  {
    "title": "Scalar Memory (SMEM)",
    "concepts": [
      "TPUs have a separate scalar unit and scalar memory (SMEM) for scalar operations and control-flow.",
      "Data used for control-flow decisions should be placed in SMEM.",
      "SMEM supports random access but only allows reading/writing 32-bit values.",
      "SMEM is useful for kernels with irregular access patterns, such as block-sparse kernels.",
      "Using PrefetchScalarGridSpec with pallas_call places the first n arguments in SMEM."
    ],
    "code_examples": []
  },
  {
    "title": "Data Types, Matrix Multiplication and Transposition",
    "concepts": [
      "Pallas TPU supports jnp.float32, jnp.bfloat16, jnp.int*, jnp.uint*, and jnp.bool_ data types.",
      "Scalar (0D) arrays are stored in scalar registers and operated on by the scalar core; other arrays are processed by the vector core.",
      "Matrix multiplication always produces results in float32; use lax.dot with preferred_element_type for other types.",
      "Fusing transpositions of the last two dimensions of matrix multiplication operands can improve performance.",
      "Pallas TPU lowering is aware of jax.default_matmul_precision; use bfloat16 for best performance.",
      "Arbitrary transpositions of all but the last two axes are free if the value has at least 4 dimensions; otherwise, only the transposition of the last two axes is implemented."
    ],
    "code_examples": []
  },
  {
    "title": "Slicing and Memory Access",
    "concepts": [
      "Arbitrary slices of references can be read or updated, subject to implementation constraints.",
      "Reads and writes aligned to multiples of 8 and 128 in the last two dimensions are always supported for 32-bit values.",
      "Best performance is achieved when memory access has indices divisible by the tiling, and the read region size is a multiple of the tile size.",
      "Reads and writes to vector memory happen on tiles of shape (8, 128)."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Operations",
    "concepts": [
      "The hardware generally supports elementwise computation using 32-bit types.",
      "Lower-precision types should generally be upcast to 32-bit before applying elementwise ops.",
      "Elementwise operations vary significantly in cost (cheap, medium, expensive)."
    ],
    "code_examples": []
  },
  {
    "title": "Array Constructors, Reductions and Broadcasting",
    "concepts": [
      "All constant array constructors (jnp.ones, jnp.zeros, jnp.full) are supported.",
      "sum, max, min (for floating point values) reductions are supported, as well as any and all for boolean values. Integer reductions are not supported.",
      "Reductions over the last array dimension are generally the slowest.",
      "The performance characteristics of broadcasting are very similar to those of reductions.",
      "Broadcasting along all but the two trailing dimensions is always supported and free.",
      "Broadcasting along the second to last dimension is slower, while broadcasting along the last dimension is the slowest."
    ],
    "code_examples": []
  },
  {
    "title": "Reshaping, Random Number Generation and Control Flow",
    "concepts": [
      "Reshapes in all dimensions but the last two dimensions are supported and free.",
      "Supported reshape cases involving the last two dimensions are flattening leading dimensions onto the second to last dimension, or adding a dimension removed by a reduction.",
      "Pallas supports commonly used functions from the jax.random module (uniform, normal, bernoulli) with threefry2x32 keys.",
      "The TPU backend features limited support for control flow (cond, fori_loop, for_loop).",
      "Loop primitives get fully unrolled during compilation; keep the loop trip count reasonably small.",
      "Overusing control flow can lead to significant regressions in low-level code generation."
    ],
    "code_examples": []
  },
  {
    "title": "Imports",
    "concepts": [
      "Import jax.",
      "Import pallas as pl from jax.experimental.",
      "Import tpu as pltpu from jax.experimental.pallas.",
      "Import jax.numpy as jnp.",
      "Import numpy as np."
    ],
    "code_examples": [
      {
        "description": "Import statements for JAX, Pallas, and NumPy.",
        "code": "import jax\nfrom jax.experimental import pallas as pl\nfrom jax.experimental.pallas import tpu as pltpu\nimport jax.numpy as jnp\nimport numpy as np"
      }
    ]
  },
  {
    "title": "TPU Memory Spaces, Registers, and Compute Units",
    "concepts": [
      "TPUs have high-bandwidth memory (HBM) for device memory.",
      "TPUs have vector memory (VMEM) for storing vector and array values.",
      "TPUs have scalar memory (SMEM) for storing scalar values.",
      "TensorCores have vector registers (VREGs) for array values and scalar registers (SREGs) for scalar values.",
      "TensorCores have scalar unit, vector unit (VPU) and matrix unit (MXU) for numerical computation.",
      "Vectorized computation requires copying data between HBM, VMEM, and VREGs."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Matrix Addition with Pallas",
    "concepts": [
      "Pallas kernels operate on memory references (Refs) in VMEM.",
      "Loading from a VMEM Ref produces a value in VREGs.",
      "VREGs behave like jax.Arrays, allowing jax.numpy operations.",
      "pallas_call copies data between HBM and VMEM.",
      "pallas_call allocates VMEM buffers for kernel operations."
    ],
    "code_examples": [
      {
        "description": "Pallas kernel to add two matrices. Operates on memory references (Refs) in VMEM.",
        "code": "def add_matrices_kernel(\n    x_vmem_ref,\n    y_vmem_ref,\n    z_vmem_ref\n):\n  # Load x and y from VMEM into VREGs\n  x_vregs = x_vmem_ref[:, :]\n  y_vregs = y_vmem_ref[:, :]\n  # Execute a vectorized add\n  z_vregs = x_vregs + y_vregs\n  # Store the output values in VREGs back into VMEM\n  z_vmem_ref[:, :] = z_vregs"
      },
      {
        "description": "Function to add two JAX arrays using the Pallas kernel.  Handles data transfer between HBM and VMEM.",
        "code": "def add_matrices(\n    x: jax.Array,\n    y: jax.Array\n) -> jax.Array:\n  # pallas_call will first allocate scratch buffers for `x` and `y` in VMEM.\n  # It will then copy `x` and `y` from HBM into VMEM.\n  z = pl.pallas_call(\n      add_matrices_kernel,\n      out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)\n  )(x, y)\n  # pallas_call will also copy the output from VMEM back into HBM.\n  return z\n\nx, y = jnp.ones((512, 512)), jnp.ones((512, 512))\nadd_matrices(x, y)"
      }
    ]
  },
  {
    "title": "Pipelining for Memory Capacity and Bandwidth",
    "concepts": [
      "VMEM and SMEM have limited capacity.",
      "Copying data between HBM and VMEM is slow.",
      "Pipelining overlaps memory I/O with compute to improve performance.",
      "Arrays can be chunked into subcomputations for pipelining.",
      "Arithmetic intensity (FLOPs/memory usage) determines if computation is compute-bound or memory-bound."
    ],
    "code_examples": []
  },
  {
    "title": "Pallas API for Pipelining: Grids and BlockSpecs",
    "concepts": [
      "Pallas provides an API for pipelining using grids and BlockSpecs.",
      "jax.experimental.pallas.pallas_call() with the grid argument executes a kernel multiple times.",
      "jax.experimental.pallas.BlockSpec specifies how to construct the input of each kernel invocation.",
      "BlockSpec.block_shape defines the shape of the blocks.",
      "index_map defines how to select blocks from the input arrays.",
      "in_specs and out_specs specify the BlockSpecs for input and output arguments, respectively."
    ],
    "code_examples": [
      {
        "description": "Defines an index map for accessing the first part of the array",
        "code": "def x_index_map(i):\n  return (i, 0)"
      },
      {
        "description": "Constructs a BlockSpec with a block shape of (256, 512) and the defined index map",
        "code": "block_spec = pl.BlockSpec((256, 512), x_index_map)"
      },
      {
        "description": "Pipelined matrix addition using BlockSpec and grid.",
        "code": "def add_matrices_pipelined(\n    x: jax.Array,\n    y: jax.Array\n) -> jax.Array:\n  block_spec = pl.BlockSpec((256, 512), lambda i: (i, 0))\n  return pl.pallas_call(\n      add_matrices_kernel,\n      out_shape=x,\n      in_specs=[block_spec, block_spec],\n      out_specs=block_spec,\n      grid=(2,)\n  )(x, y)\n\nadd_matrices_pipelined(x, y)"
      }
    ]
  },
  {
    "title": "Generalized Pipelined Kernel with Parameterized Block Shapes",
    "concepts": [
      "Block shapes can be parameterized to tune performance.",
      "Inputs and outputs can be carved up along multiple dimensions.",
      "Smaller blocks add more iterations to the pipelined loop.",
      "Block sizes are a crucial parameter for Pallas kernel optimization."
    ],
    "code_examples": [
      {
        "description": "Generalized pipelined matrix addition with parameterized block shapes along both dimensions.",
        "code": "def add_matrices_pipelined_2d(\n    x: jax.Array,\n    y: jax.Array,\n    *,\n    bm: int = 256,\n    bn: int = 256\n) -> jax.Array:\n  m, n = x.shape\n  block_spec = pl.BlockSpec((bm, bn), lambda i, j: (i, j))\n  return pl.pallas_call(\n      add_matrices_kernel,\n      out_shape=x,\n      in_specs=[block_spec, block_spec],\n      out_specs=block_spec,\n      grid=(m // bm, n // bn),\n  )(x, y)\n\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_2d(x, y, bm=256, bn=256),\n    x + y\n)\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_2d(x, y, bm=128, bn=128),\n    x + y\n)\nnp.testing.assert_array_equal(\n    add_matrices_pipelined_2d(x, y, bm=512, bn=512),\n    x + y\n)"
      }
    ]
  },
  {
    "title": "Implementing Reduction (jnp.sum) with Pallas",
    "concepts": [
      "Pallas can be used to implement reductions like jnp.sum.",
      "A grid can be used to iterate over the reduction dimension.",
      "BlockSpec defines how to load data into VMEM for each iteration.",
      "When performing a reduction, the output Ref needs to be initialized to avoid accumulating into garbage.",
      "pl.when conditionally executes code based on the iteration number.",
      "pl.program_id queries the current iteration in a grid axis.",
      "Reductions must be done in the minormost (rightmost) dimensions of the grid."
    ],
    "code_examples": [
      {
        "description": "Incorrect naive implementation of sum using Pallas.",
        "code": "def naive_sum_kernel(\n    x_ref,\n    o_ref\n):\n  o_ref[...] += x_ref[...]\n\ndef naive_sum(\n    x: jax.Array\n) -> jax.Array:\n  grid, *out_shape = x.shape\n  return pl.pallas_call(\n      naive_sum_kernel,\n      grid=grid,\n      # None in `block_shape` means we pick a size of 1 and squeeze it away\n      in_specs=[\n          pl.BlockSpec(\n              (None, *out_shape),\n              lambda i: (i, 0, 0))\n      ],\n      out_specs=pl.BlockSpec(\n          out_shape,\n          lambda i: (0, 0)),\n      out_shape=jax.ShapeDtypeStruct(\n          out_shape,\n          x.dtype\n      ),\n  )(x)\n\nnaive_sum(x)"
      },
      {
        "description": "Correct implementation of sum using Pallas, with initialization of the output Ref.",
        "code": "def sum_kernel(\n    x_ref,\n    o_ref\n):\n  @pl.when(pl.program_id(axis=0) == 0)\n  def _():\n    o_ref[...] = jnp.zeros_like(o_ref)\n  o_ref[...] += x_ref[...]\n\ndef sum(\n    x: jax.Array\n) -> jax.Array:\n  grid, *out_shape = x.shape\n  return pl.pallas_call(\n      sum_kernel,\n      grid=grid,\n      # None in `block_shape` means we pick a size of 1 and squeeze it away\n      in_specs=[\n          pl.BlockSpec(\n              (None, *out_shape),\n              lambda i: (i, 0, 0))\n      ],\n      out_specs=pl.BlockSpec(\n          out_shape,\n          lambda i: (0, 0)),\n      out_shape=jax.ShapeDtypeStruct(\n          out_shape,\n          x.dtype\n      )\n  )(x)\n\nsum(x)"
      }
    ]
  },
  {
    "title": "Utilizing Megacore TPUs",
    "concepts": [
      "Some TPUs have two TensorCores (Megacore) that share HBM but have separate VMEM, VREGs, SMEM, SREGs, and compute units.",
      "Megacore TPUs behave like simple GPUs with two threads.",
      "Embarrassingly parallel dimensions can be split across TensorCores.",
      "dimension_semantics annotation in pallas_call indicates parallelizable dimensions.",
      "dimension_semantics should be a tuple of 'parallel' or 'arbitrary' strings, with the same length as the grid.",
      "'parallel' indicates that iterations can be executed independently.",
      "'arbitrary' indicates that no assumptions can be made about the grid dimension.",
      "Megacore is available on TPU v4 and TPU v5p."
    ],
    "code_examples": [
      {
        "description": "Pipelined matrix addition utilizing Megacore TPUs.",
        "code": "def add_matrices_pipelined_megacore(\n    x: jax.Array,\n    y: jax.Array\n) -> jax.Array:\n  block_spec = pl.BlockSpec((256, 512), lambda i: (i, 0))\n  return pl.pallas_call(\n      add_matrices_kernel,\n      out_shape=x,\n      in_specs=[block_spec, block_spec],\n      out_specs=block_spec,\n      grid=(2,),\n      compiler_params=pltpu.TPUCompilerParams(\n          dimension_semantics=(\"parallel\",))\n  )(x, y)\n\nx, y = jnp.ones((512, 512)), jnp.ones((512, 512))\nadd_matrices_pipelined_megacore(x, y)"
      }
    ]
  },
  {
    "title": "Introduction to Block-Sparse Computing in Pallas",
    "concepts": [
      "Sparse computation can be implemented using Pallas kernels.",
      "XLA has limitations expressing programs with a dynamic amount of computation due to static array shapes.",
      "Scalar prefetch allows passing small amounts of data into SMEM.",
      "Prefetched data is available for data-dependent indexing calculations in the index_map."
    ],
    "code_examples": [
      {
        "description": "Import necessary libraries",
        "code": "import functools\nimport timeit\nimport numpy as np\nimport jax\nfrom jax import numpy as jnp\nfrom jax import lax\nfrom jax.experimental import checkify\nfrom jax.experimental import pallas as pl\nfrom jax.experimental.pallas import tpu as pltpu\nassert \"TPU\" in jax.devices()[0].device_kind, \"Please run this notebook with TPU devices.\"\nprint(\"Running on\", jax.devices()[0].device_kind)"
      }
    ]
  },
  {
    "title": "Scalar Prefetch with PrefetchScalarGridSpec",
    "concepts": [
      "pltpu.PrefetchScalarGridSpec is used for scalar prefetch.",
      "The num_scalar_prefetch parameter specifies the number of scalar prefetch values.",
      "Prefetch Refs are allocated in SMEM and are not partitioned into blocks.",
      "Index maps expect prefetch Refs after grid indices.",
      "Kernels expect prefetch Refs before input Refs and scratch Refs after output Refs.",
      "When calling a kernel, scalar prefetch arguments come before input arguments."
    ],
    "code_examples": [
      {
        "description": "Definition of PrefetchScalarGridSpec class",
        "code": "class PrefetchScalarGridSpec:\n  def __init__(self, num_scalar_prefetch: int, grid: tuple[int, ...],\n               in_specs: PyTree[BlockSpec], out_specs: PyTree[BlockSpec],\n               scratch_shapes: tuple[MemorySpace, ...]):\n    ..."
      },
      {
        "description": "Index map signature with prefetch Refs",
        "code": "def index_map(*grid_indices, *prefetch_refs):\n  ..."
      },
      {
        "description": "Kernel signature with prefetch Refs",
        "code": "def kernel(*prefetch_refs, *input_refs, *output_refs, *scratch_refs):\n  ..."
      },
      {
        "description": "Calling a kernel with prefetch arguments",
        "code": "kernel = pl.pallas_call(...)\nresult = kernel(*prefetch_args, *input_args)"
      }
    ]
  },
  {
    "title": "Block-Aligned Dynamic Slice Kernel Example",
    "concepts": [
      "Implementing a block-aligned dynamic slice kernel to extract a block from a larger array.",
      "Computing the block index outside the kernel and passing it as a scalar prefetch argument.",
      "Using the block index in the index map to select the corresponding block.",
      "Limitations: Slice sizes must fit inside a kernel block, and starts must be size-aligned"
    ],
    "code_examples": [
      {
        "description": "Dynamic slice kernel definition and block_dynamic_slice function",
        "code": "def dynamic_slice_kernel(indices, x_ref, o_ref):\n  del indices\n  o_ref[...] = x_ref[...]\n\n@checkify.checkify\n@functools.partial(jax.jit, static_argnums=(2,))\ndef block_dynamic_slice(x, starts, sizes):\n  grid_spec = pltpu.PrefetchScalarGridSpec(\n      num_scalar_prefetch=1,\n      grid=(1, 1),\n      in_specs=[pl.BlockSpec(sizes, lambda i, j, block_idx: (block_idx[0], block_idx[1]))],\n      out_specs=pl.BlockSpec(sizes, lambda *_: (0, 0)),\n  )\n  kernel = pl.pallas_call(\n      dynamic_slice_kernel,\n      grid_spec=grid_spec,\n      out_shape=jax.ShapeDtypeStruct(shape=sizes, dtype=x.dtype),\n  )\n  # Checkify inserts a runtime assert that starts are divisible by block size.\n  checkify.check(starts[0] % sizes[0] == 0, \"Starts must be divisible by size.\")\n  checkify.check(starts[1] % sizes[1] == 0, \"Starts must be divisible by size.\")\n  block_idx = jnp.array([starts[0] // sizes[0], starts[1] // sizes[1]])\n  return kernel(block_idx, x)\n\nshape = (512, 512)\nx = jnp.reshape(jnp.arange(np.prod(shape), dtype=jnp.int32), shape)\nerr, result = block_dynamic_slice(x, starts=(128, 256), sizes=(128, 128))\nerr.throw()\nref = lax.dynamic_slice(x, start_indices=(128, 256), slice_sizes=(128, 128))\ndiff = jnp.max(jnp.abs(result - ref))\nprint(\"Error |result - lax.dynamic_slice| =\", diff)"
      }
    ]
  },
  {
    "title": "Sparse Matrix Representation (Block-COO Format)",
    "concepts": [
      "Sparse matrices are represented in a blocked variant of the coordinate-list format (COO).",
      "The matrix is stored as a list of (block_index, block_data) pairs.",
      "Blocks not explicitly stored are assumed to be zero.",
      "This format saves memory if there are many zero blocks in the matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Helper Function: generate_block_sparse_mat",
    "concepts": [
      "The generate_block_sparse_mat function samples a block-sparse matrix.",
      "It returns a dense matrix (for checking), block data, and block indices.",
      "The function takes as input: RNG key, matrix dimensions (M, N), block dimensions (blk_M, blk_N), probability of non-zero block (p), and dtype.",
      "The mask determines which blocks are non-zero."
    ],
    "code_examples": [
      {
        "description": "Generate block sparse matrix and its block sparse representation.",
        "code": "def generate_block_sparse_mat(\n    key, M, N, blk_M, blk_N, p=0.2, dtype=jnp.float32):\n  \"\"\"Returns a sampled matrix and its block-sparse representation.\n  Args:\n    key: RNG Key.\n    M: Major array dimension.\n    N: Minor array dimension.\n    blk_M: Block size along M dimension.\n    blk_N: Block size along N dimension.\n    p: Probability that a block will be non-zero.\n    dtype: dtype of the sampled matrix.\n  Returns:\n    dense_mat: A (M, N) dense sampled array.\n    block_data: A (num_blocks, blk_M, blk_N) array of data blocks representing\n      the non-zero blocks of the matrix.\n    indices_i: A (num_blocks,) array of block indices for the first axis.\n    indices_j: A (num_blocks,) array of block indices for the second axis.\n  \"\"\"\n  mask_key, blocks_key = jax.random.split(key)\n  num_blocks = (M // blk_M, N // blk_N)\n  # We first sample a block mask, denoting which blocks are nonzero.\n  block_mask = jax.random.bernoulli(mask_key, p=p, shape=num_blocks)\n  num_blocks = jnp.sum(block_mask)\n  indices = jnp.where(block_mask)\n  # For each non-zero block, we sample a block of random values.\n  block_data = jax.random.uniform(\n      blocks_key, shape=(num_blocks, blk_M, blk_N), dtype=dtype)\n  # For checking purposes, create the dense version of the sparse matrix.\n  dense_mat = jnp.zeros((M, N), dtype=dtype)\n  for blk in range(num_blocks):\n    idx_i = indices[0][blk]\n    idx_j = indices[1][blk]\n    slice_i = slice(idx_i * blk_M, (idx_i + 1) * blk_M)\n    slice_j = slice(idx_j * blk_N, (idx_j + 1) * blk_N)\n    dense_mat = dense_mat.at[slice_i, slice_j].set(block_data[blk])\n  return dense_mat, block_data, indices[0], indices[1]"
      }
    ]
  },
  {
    "title": "Sparse LHS x Dense RHS Matrix Multiplication",
    "concepts": [
      "Multiplying a sparse LHS matrix with a dense RHS matrix to produce a dense output.",
      "Structuring the kernel grid with an outer loop over RHS/output columns and an inner loop over LHS sparse blocks.",
      "Loading one block from the LHS and looking up the corresponding block in the RHS.",
      "Multiplying the blocks and accumulating the result in the correct output block.",
      "Grouping block indices by row before passing them to the kernel to zero-out the accumulator in the output ref and to enable pipelining.",
      "Pallas pipeline emitter will realize that we are loading the same output block and keep it in VMEM if we are accessing output blocks consecutively."
    ],
    "code_examples": [
      {
        "description": "Dense = Sparse @ Dense (DSD) matmul kernel definition.",
        "code": "M = N = K = 16384\nblk_M = blk_N = blk_K = 512\n\ndef dsd_kernel(\n    idxs_i_ref,\n    idxs_k_ref,\n    # Scalar prefetch inputs.\n    x_ref,\n    y_ref,\n    _,\n    o_ref,\n    # Kernel inputs.\n    accum_scratch,\n):\n  \"\"\"A DSD (Dense = Sparse @ Dense) matmul kernel.\"\"\"\n  del idxs_k_ref\n  blk_idx = pl.program_id(0)\n  is_start = blk_idx == 0\n  changed_blocks = (idxs_i_ref[blk_idx] != idxs_i_ref[jnp.maximum(blk_idx - 1, 0)])\n\n  @pl.when(is_start | changed_blocks)\n  def _():\n    accum_scratch[...] = jnp.zeros_like(accum_scratch)\n    accum_scratch[...] += jnp.dot(\n        x_ref[0, :, :], y_ref[...], preferred_element_type=jnp.float32\n    )\n\n  next_block_change = (idxs_i_ref[blk_idx] != idxs_i_ref[jnp.minimum(blk_idx + 1, num_blocks)])\n  is_end = blk_idx == (num_blocks - 1)\n\n  @pl.when(is_end | next_block_change)\n  def _():\n    o_ref[...] = accum_scratch[...].astype(o_ref.dtype)\n\n\ndef x_map(blk_idx, j, blk_idxs_i, blk_idxs_k):\n  del j, blk_idxs_i, blk_idxs_k\n  return (blk_idx, 0, 0)\n\n\ndef y_map(blk_idx, j, blk_idxs_i, blk_idxs_k):\n  del blk_idxs_i\n  return (blk_idxs_k[blk_idx], j)\n\n\ndef o_map(blk_idx, j, blk_idxs_i, blk_idxs_k):\n  del blk_idxs_k\n  return (blk_idxs_i[blk_idx], j)\n\n\n(X_dense, X_blocks, indices_i, indices_k) = generate_block_sparse_mat(\n    jax.random.key(0), M, K, blk_M, blk_K, p=0.1, dtype=jnp.bfloat16\n)\nnum_blocks = X_blocks.shape[0]\nY = jax.random.uniform(jax.random.key(1), shape=(K, N), dtype=jnp.bfloat16)\nzeros = jnp.zeros((M, N), dtype=jnp.bfloat16)\nout_shape = jax.ShapeDtypeStruct((M, N), dtype=jnp.bfloat16)\ngrid_spec = pltpu.PrefetchScalarGridSpec(\n    num_scalar_prefetch=2,\n    # Note that while num_blocks is static here, Pallas does support\n    # dynamic grid sizes.\n    grid=(num_blocks, N // blk_N),\n    in_specs=[\n        pl.BlockSpec((1, blk_M, blk_K), x_map),\n        pl.BlockSpec((blk_K, blk_N), y_map),\n        # Placeholder for a zeros-array used by input_output_aliases.\n        pl.BlockSpec((blk_M, blk_N), o_map),\n    ],\n    out_specs=pl.BlockSpec((blk_M, blk_N), o_map),\n    scratch_shapes=[pltpu.VMEM((blk_M, blk_N), dtype=jnp.float32)],\n)\nkernel = pl.pallas_call(\n    dsd_kernel,\n    grid_spec=grid_spec,\n    out_shape=out_shape,\n    # We use input-output aliases to zero-out o_ref for blocks that we never\n    # visit. By passing in an array of zeros we avoid having o_ref start with\n    # uninitialized values.\n    input_output_aliases={4: 0},  # Map zeros to o_ref.\n)\nargs = (indices_i, indices_k, X_blocks, Y, zeros)\nresult = kernel(*args)\nref = X_dense @ Y\ndiff = jnp.abs(ref - result)\nprint('mean |result - ref|:', jnp.mean(diff))"
      }
    ]
  },
  {
    "title": "Performance Tips",
    "concepts": [
      "Using dtype=jnp.bfloat16 is critical for performance as it reduces memory bandwidth.",
      "Larger block sizes help as matrix multiply is an O(N^3) compute and O(N^2) memory operation.",
      "Smaller block sizes enable data to be more sparse."
    ],
    "code_examples": []
  },
  {
    "title": "Benchmarking Sparse Pallas Kernel",
    "concepts": [
      "Benchmarking the sparse Pallas kernel against a dense JAX matmul implementation.",
      "Using timeit to measure the execution time of both implementations.",
      "Comparing performance to evaluate the benefits of sparse computation."
    ],
    "code_examples": [
      {
        "description": "Benchmark function definition",
        "code": "def benchmark(f, ntrials: int = 100):\n  def run(*args, **kwargs):\n    # Compile function first\n    jax.block_until_ready(f(*args, **kwargs))\n    # Time function\n    result = timeit.timeit(\n        lambda: jax.block_until_ready(f(*args, **kwargs)), number=ntrials)\n    time = result / ntrials\n    return time\n\n  return run\n\nn_trials = 100\npallas_impl = lambda *args: kernel(*args)\ntime = benchmark(pallas_impl, n_trials)(indices_i, indices_k, X_blocks, Y, zeros)\nprint(\"Sparse Kernel: %.3f ms (avg over %d trials)\" % (time * 1000, n_trials))\n\nref_impl = jax.jit(lambda x, y: x @ y)\ntime = benchmark(ref_impl, n_trials)(X_dense, Y)\nprint(\"Reference: %.3f ms (avg over %d trials)\" % (time * 1000, n_trials))"
      }
    ]
  },
  {
    "title": "Sparse Computation over Dense Data",
    "concepts": [
      "Implementing sparse computation over dense data.",
      "Using a dense kernel grid and skipping blocks based on a block-sparse mask.",
      "This pattern is common in machine learning applications with masks.",
      "The main performance consideration is the interaction with pipelining.",
      "Construct prefetch maps to tell the pipeline which block to fetch next.",
      "The Pallas pipeline emitter will attempt to prefetch the next block of data by calling the index_map."
    ],
    "code_examples": []
  },
  {
    "title": "Prefetch Maps",
    "concepts": [
      "Sparse access patterns require prefetch maps for pipelining.",
      "A prefetch map contains indices to the next non-skipped block of data for each kernel input.",
      "The prefetch map is passed as a scalar prefetch argument.",
      "It is queried in the index_map function of the BlockSpec."
    ],
    "code_examples": [
      {
        "description": "Mask index map example",
        "code": "def mask_index_map(\n    prefetch_map, i, j, ...):\n  next_nonzero_block = prefetch_map[i, j]\n  return (next_nonzero_block, 0, 0)"
      }
    ]
  },
  {
    "title": "Dense Matrix Multiplication Fused with Sparse Output Mask",
    "concepts": [
      "Covering dense matrix multiplication fused with a sparse output mask.",
      "Using prefetch maps to improve pipelining performance.",
      "Selectively skipping computing output blocks that are zeroed-out.",
      "The function sparsify_mask computes the block_mask, prefetch_mask, prefetch_i, prefetch_j and mask_data."
    ],
    "code_examples": [
      {
        "description": "Sparsify Mask definition",
        "code": "def sparsify_mask(\n    mask: jax.Array,\n    block_shape: tuple[int, int]):\n  \"\"\"Preprocesses a mask into a sparse reprentation.\n  Args:\n    mask: A boolean array of shape [M, N]\n    block_shape: The size of a single block.\n  Returns:\n    block_mask: A block_shape array of booleans indicating whether a block\n      is all-zeros (0) or contains non-zero elements (1).\n    prefetch_mask: A block_shape array of integers indicating the index of the\n      next non-zero block.\n    mask_data: A (num_blocks, block_shape) array containing\n      the data for non-zero blocks of the mask.\n  \"\"\"\n  M, N = mask.shape\n  bm, bn = block_shape\n  block_mask = jnp.zeros((M // bm, N // bn), dtype=mask.dtype)\n  mask_types_finder = []\n  mask_data = []\n  mask_type_idxs = []\n  next_mask_type_idx = 0\n  prefetch_mask = jnp.zeros_like(block_mask)\n  next_i = (M // bm) - 1\n  next_j = (N // bn) - 1\n  prefetch_i = jnp.zeros_like(block_mask)\n  prefetch_j = jnp.zeros_like(block_mask)\n  for i in range(M // bm, -1, -1):\n    for j in range(N // bn, -1, -1):\n      mask_block = mask[i * bm:(i + 1) * bm, j * bn:(j + 1) * bn]\n      is_nonzero = jnp.any(mask_block)\n      if is_nonzero:\n        try:\n          type_index = mask_types_finder.index(str(mask_block))\n        except ValueError:\n          type_index = len(mask_types_finder)\n          mask_types_finder.append(str(mask_block))\n          mask_data.append(mask_block)\n          next_mask_type_idx = type_index\n        next_i = i\n        next_j = j\n      else:\n        type_index = -1\n      mask_type_idxs.append(type_index)\n      block_mask = block_mask.at[i, j].set(is_nonzero)\n      prefetch_mask = prefetch_mask.at[i, j].set(next_mask_type_idx)\n      prefetch_i = prefetch_i.at[i, j].set(next_i)\n      prefetch_j = prefetch_j.at[i, j].set(next_j)\n  return block_mask, prefetch_mask, prefetch_i, prefetch_j, jnp.stack(mask_data)"
      }
    ]
  },
  {
    "title": "Sparse Mask Matmul Kernel Structure",
    "concepts": [
      "Using the same grid pattern as the standard matrix multiplication kernel.",
      "Looping over N, M, and K dimensions.",
      "Checking the block_mask to see if the mask for the current output block was all zeros.",
      "Skipping computation if the mask is all zeros."
    ],
    "code_examples": [
      {
        "description": "Sparse mask matmul function definition",
        "code": "M = N = K = 16384\nblk_M = blk_N = 512\nblk_K = 1024\n\ndef sparse_mask_matmul(\n    block_mask_ref,\n    prefetch_mask,\n    prefetch_i,\n    prefetch_j,\n    # Scalar prefetch inputs.\n    x_ref,\n    y_ref,\n    mask_ref,\n    o_ref,\n    # Kernel inputs.\n    accum_scratch):\n  del prefetch_mask, prefetch_i, prefetch_j\n  i, j, k = pl.program_id(0), pl.program_id(1), pl.program_id(2)\n  should_compute = block_mask_ref[i, j] != 0\n\n  @pl.when(k == 0)\n  def _():\n    o_ref[...] = jnp.zeros_like(o_ref)\n    accum_scratch[...] = jnp.zeros_like(accum_scratch[...])\n\n  # We only compute the output for blocks with non-zero masks.\n  # Otherwise we skip the computation entirely.\n  @pl.when(should_compute)\n  def _():\n    result = jnp.dot(x_ref[...], y_ref[...], preferred_element_type=jnp.float32)\n    accum_scratch[...] += result\n\n    o_ref[...] = accum_scratch[...] * mask_ref[...]\n"
      }
    ]
  },
  {
    "title": "Introduction to Distributed Computing with Pallas on TPUs",
    "concepts": [
      "Distributed computing on TPUs using Pallas is introduced.",
      "TPU topologies, remote DMA, and shard_map are key components.",
      "Advanced kernel writing techniques like double-buffering and pipelining will be covered.",
      "Implementation of collective primitives such as lax.ppermute, lax.all_gather, lax.psum, and lax.psum_scatter is demonstrated."
    ],
    "code_examples": [
      {
        "description": "Import necessary libraries and verify the number and type of devices available.",
        "code": "import jax\nfrom jax import lax\nfrom jax import numpy as jnp\nfrom jax.experimental import pallas as pl\nfrom jax.experimental import shard_map\nfrom jax.experimental.pallas import tpu as pltpu\n\nP = jax.sharding.PartitionSpec\nnum_devices = jax.local_device_count()\nassert num_devices > 1, \"Please run this notebook with more than one device.\"\nassert \"TPU\" in jax.devices()[0].device_kind, \"Please run this notebook with TPU devices.\"\nprint(f\"Running with {num_devices} {jax.devices()[0].device_kind} devices.\")"
      }
    ]
  },
  {
    "title": "TPU Pods and Interchip Interconnect (ICI)",
    "concepts": [
      "TPUs are deployed in pods with high-bandwidth interchip interconnect (ICI) for fast communication.",
      "ICI bandwidth is significantly faster than typical network connections.",
      "ICI facilitates fast and performant distributed kernels.",
      "TPU pods are arranged in an ND torus topology.",
      "Rings are formed when taking slices along an axis of the pod, simplifying communication patterns."
    ],
    "code_examples": []
  },
  {
    "title": "Remote Direct Memory Access (RDMA)",
    "concepts": [
      "TPUs communicate via a push-only model known as remote direct memory access (RDMA).",
      "A TPU can push data from a local buffer to any buffer on another device within the same pod asynchronously.",
      "A TPU can only read data that is stored locally.",
      "pltpu.make_async_remote_copy function is used to create a remote DMA descriptor object.",
      "The descriptor object parameterizes both a send and a receive operation."
    ],
    "code_examples": [
      {
        "description": "Signature of the `make_async_remote_copy` function.",
        "code": "def make_async_remote_copy(\n    src_ref: Ref,\n    dst_ref: Ref,\n    send_sem: Ref[SemaphoreType],\n    recv_sem: Ref[SemaphoreType],\n    device_id: int | tuple[int, ...],\n    device_id_type: DeviceIdType\n) -> AsyncCopyDescriptor:"
      },
      {
        "description": "Using the DMA descriptor object to initiate and wait for the DMA operation.",
        "code": "dma_descriptor = make_async_remote_copy(\n    src_ref,\n    dst_ref,\n    send_sem,\n    recv_sem,\n    device_id\n)\ndma_descriptor.start()  # Initiate the DMA (non-blocking).\n# ... do other work\ndma_descriptor.wait_send()  # Block until all data has been sent.\ndma_descriptor.wait_recv()  # Block until all data has been received."
      },
      {
        "description": "Example kernel demonstrating asymmetric communication using DMA and pl.when based on device ID.",
        "code": "def example_kernel(input_ref, output_ref, send_sem, recv_sem):\n    device_id = lax.axis_index('x')\n\n    copy_0_to_1 = pltpu.make_async_remote_copy(\n        src_ref=input_ref,\n        dst_ref=output_ref,\n        send_sem=send_sem,\n        recv_sem=recv_sem,\n        device_id=1,\n    )\n    copy_2_to_3 = pltpu.make_async_remote_copy(\n        src_ref=input_ref,\n        dst_ref=output_ref,\n        send_sem=send_sem,\n        recv_sem=recv_sem,\n        device_id=3,\n    )\n    copy_3_to_2 = pltpu.make_async_remote_copy(\n        src_ref=input_ref,\n        dst_ref=output_ref,\n        send_sem=send_sem,\n        recv_sem=recv_sem,\n        device_id=2,\n    )\n\n    @pl.when(device_id == 0)\n    def _():\n        copy_0_to_1.start()\n        copy_0_to_1.wait_send()\n\n    @pl.when(device_id == 1)\n    def _():\n        copy_0_to_1.wait_recv()\n\n    @pl.when(device_id == 2)\n    def _():\n        copy_2_to_3.start()\n        copy_2_to_3.wait_send()\n        copy_3_to_2.wait_recv()\n\n    @pl.when(device_id == 3)\n    def _():\n        copy_3_to_2.start()\n        copy_3_to_2.wait_send()\n        copy_2_to_3.wait_recv()"
      }
    ]
  },
  {
    "title": "DMA Semaphores and Potential Failure Modes",
    "concepts": [
      "send_sem and recv_sem are DMA semaphores used to synchronize data transfers.",
      "DMA semaphores are allocated with tpu.SemaphoreType.DMA.",
      "DMA semaphores act as integer-valued progress trackers, incrementing during send and decrementing during wait.",
      "Incorrect DMA usage can lead to crashes, hanging, or silent data corruption.",
      "Common causes include insufficient bytes received, over-signaled semaphores, and race conditions."
    ],
    "code_examples": []
  },
  {
    "title": "Right Permutation Kernel Implementation",
    "concepts": [
      "Implementation of a right permutation kernel to send data to the right neighbor.",
      "shard_map is used to call the kernel in distributed mode.",
      "lax.axis_index is used to obtain the device_id for computing target devices.",
      "Comparison with lax.ppermute to verify correctness."
    ],
    "code_examples": [
      {
        "description": "Right permutation kernel implementation using pltpu.make_async_remote_copy.",
        "code": "partition = P(None, 'x')\nmesh = jax.make_mesh((num_devices,), ('x',))\nsharding = jax.sharding.NamedSharding(mesh, partition)\n\n# Create an input array that shards the last dimension across\n# all devices.\ninput_arr = jax.random.uniform(jax.random.key(0), (8, 128 * num_devices))\ninput_arr = jax.device_put(input_arr, sharding)\n\ndef right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\n    my_id = lax.axis_index('x')\n    right_neighbor = lax.rem(my_id + 1, num_devices)\n\n    remote_copy_op = pltpu.make_async_remote_copy(\n        src_ref=input_ref,\n        dst_ref=output_ref,\n        send_sem=send_sem,\n        recv_sem=recv_sem,\n        device_id=(right_neighbor,),\n        device_id_type=pltpu.DeviceIdType.MESH,\n    )\n    remote_copy_op.start()\n    remote_copy_op.wait()\n\nout_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\ngrid_spec = pltpu.PrefetchScalarGridSpec(\n    num_scalar_prefetch=0,\n    # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\n    in_specs=[\n        pl.BlockSpec(\n            memory_space=pltpu.TPUMemorySpace.ANY),\n    ],\n    out_specs=pl.BlockSpec(\n        memory_space=pltpu.TPUMemorySpace.ANY),\n    scratch_shapes=(\n        # We allocate DMA semaphores in scratch memory.\n        [pltpu.SemaphoreType.DMA] * 2),\n)\n\nright_permute = pl.pallas_call(\n    right_permute_kernel,\n    out_shape=out_shape,\n    grid_spec=grid_spec,\n)\n\n# Wrap the kernel within a shard_map to call.\npallas_result = jax.jit(shard_map.shard_map(\n    right_permute,\n    mesh=mesh,\n    in_specs=partition,\n    out_specs=partition,\n    check_rep=False,\n))(\n    input_arr\n)\n\n# Compare Pallas result to XLA shard_map result.\nperm = tuple(((src, (src + 1) % num_devices) for src in range(num_devices)))\nxla_result = jax.jit(shard_map.shard_map(\n    lambda x: lax.ppermute(x, 'x', perm),\n    mesh=mesh,\n    in_specs=partition,\n    out_specs=partition\n))(\n    input_arr\n)\n\nprint('Input = ', input_arr[0, ::128])\nprint('Pallas Result = ', pallas_result[0, ::128])\nprint('lax.ppermute Result = ', xla_result[0, ::128])\nprint(\n    'Difference |Pallas - lax.ppermute| = ',\n    jnp.mean(jnp.abs(pallas_result - xla_result)),\n)"
      }
    ]
  },
  {
    "title": "All-Gather Kernel Implementation",
    "concepts": [
      "Implementation of all-gather collective operation using a looped kernel.",
      "Leverages a ring topology for communication.",
      "Each device receives a slice from its left neighbor and copies the previous slice to its right neighbor.",
      "Pallas's grid argument is re-purposed to implement the loop, using pl.program_id to get loop iteration.",
      "Use of multiple receive semaphores is emphasized to avoid race conditions."
    ],
    "code_examples": [
      {
        "description": "All-gather kernel implementation using a looped kernel and pltpu.make_async_remote_copy.",
        "code": "partition = P('x', None)\nmesh = jax.make_mesh((num_devices,), ('x',))\nsharding = jax.sharding.NamedSharding(mesh, partition)\n\n# Create an input array that shards the first dimension across\n# all devices.\ninput_arr = jax.random.uniform(jax.random.key(0), (8 * num_devices, 128))\ninput_arr = jax.device_put(input_arr, sharding)\n\ndef all_gather_kernel(input_ref, output_ref, local_copy_sem, send_sem, recv_sems):\n    outer_step = pl.program_id(0)\n    my_id = lax.axis_index('x')\n    right_neighbor = lax.rem(my_id + 1, num_devices)\n\n    copy_slot = my_id - outer_step\n    copy_slot = lax.rem(copy_slot + num_devices, num_devices)\n\n    @pl.when(outer_step == 0)\n    def _():\n        local_copy_op = pltpu.make_async_copy(\n            src_ref=input_ref,\n            dst_ref=output_ref.at[my_id],\n            sem=local_copy_sem,\n        )\n        local_copy_op.start()\n        local_copy_op.wait()\n\n    # Copy to our right neighbor.\n    # Note that we will also be receiving data from our left neighbor,\n    # but at `copy_slot-1` rather than `copy_slot`! This makes use of the fact\n    # that the indices do not need to be symmetric between remote DMAs.\n    remote_copy_op = pltpu.make_async_remote_copy(\n        src_ref=output_ref.at[copy_slot],\n        dst_ref=output_ref.at[copy_slot],\n        send_sem=send_sem,\n        recv_sem=recv_sems.at[outer_step],\n        device_id=(right_neighbor,),\n        device_id_type=pltpu.DeviceIdType.MESH,\n    )\n    remote_copy_op.start()\n    remote_copy_op.wait()\n\nout_shape = jax.ShapeDtypeStruct((num_devices, 8, 128), jnp.float32)\ngrid_spec = pltpu.PrefetchScalarGridSpec(\n    num_scalar_prefetch=0,\n    in_specs=[\n        # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\n        pl.BlockSpec(\n            memory_space=pltpu.TPUMemorySpace.ANY),\n    ],\n    out_specs=pl.BlockSpec(\n        memory_space=pltpu.TPUMemorySpace.ANY),\n    scratch_shapes=(\n        # DMA semaphores are allocated in scratch memory.\n        # We allocated one semaphore for a local HBM-VMEM copy,\n        # and one for the remote send semaphore.\n        [pltpu.SemaphoreType.DMA] * 2\n        # We additionally allocate one receive semaphore per device.\n        # This is to avoid situations where we have multiple\n        # DMAs in flight, as we do not want to share a receive\n        # semaphore between the DMAs.\n        + [pltpu.SemaphoreType.DMA((num_devices - 1,))]),\n    grid=(num_devices - 1,)\n)\n\nall_gather = pl.pallas_call(\n    all_gather_kernel,\n    out_shape=out_shape,\n    grid_spec=grid_spec,\n)\n\n# Wrap the kernel within a shard_map to call.\npallas_result = jax.jit(shard_map.shard_map(\n    all_gather,\n    mesh=mesh,\n    in_specs=partition,\n    out_specs=partition,\n    check_rep=False\n))(\n    input_arr\n)\n\n# Compare Pallas result to XLA shard_map result.\nxla_result = jax.jit(shard_map.shard_map(\n    lambda x: lax.all_gather(x, 'x'),\n    mesh=mesh,\n    in_specs=partition,\n    out_specs=partition\n))(\n    input_arr\n)\n\nprint('Input: ', input_arr.shape, input_arr[::8, 0])\nprint('Pallas Result: ', pallas_result.shape, pallas_result[:, 0, 0])\nprint('lax.all_gather Result: ', xla_result.shape, xla_result[:, 0, 0])\nprint(\n    'Difference |Pallas - lax.all_gather| = ',\n    jnp.mean(jnp.abs(pallas_result - xla_result)))\n"
      }
    ]
  },
  {
    "title": "Advanced Synchronization Techniques",
    "concepts": [
      "More complex communication patterns require additional synchronization primitives.",
      "Pallas provides regular and barrier semaphores for this purpose.",
      "Regular semaphores are standard tools to synchronize across multiple devices.",
      "They can be incremented, waited upon, and read.",
      "Semaphores must be zero at the end of a Pallas program for successful completion.",
      "Over-signaling results in crashes, while over-waiting leads to hanging."
    ],
    "code_examples": [
      {
        "description": "Semaphore Operations",
        "code": "def semaphore_signal(\n    sem: Ref[SemaphoreType],\n    inc: int,\n    device_id: int | tuple[int, ...],\n    device_id_type: DeviceIdType\n) -> None:\n    ...\n    # Increments the semaphore `sem` on the target device `device_id` by `inc`.\n\ndef semaphore_wait(\n    semaphore: Ref[SemaphoreType],\n    value: int,\n) -> None:\n    ...\n    # Blocks until the locally allocated copy of `sem` reaches `value`, then decrement by `value` and proceed.\n\ndef semaphore_read(\n    sem: Ref[SemaphoreType],\n) -> jax.Array:\n    ...\n    # Returns the current value of `sem` as an `int32[]`."
      }
    ]
  },
  {
    "title": "Design",
    "concepts": [],
    "code_examples": []
  },
  {
    "title": "Introduction to Pallas",
    "concepts": [
      "JAX is successful due to XLA, but users need an escape hatch for custom kernels.",
      "CustomCall in XLA requires C++ and CUDA knowledge, which is a barrier for many users.",
      "Triton offers an array-based programming model for GPUs and is used in PyTorch 2.0.",
      "Pallas is an extension to JAX that enables kernel programming for GPUs and TPUs using a Triton-like model.",
      "Pallas aims to provide a higher-level front end that abstracts away hardware details and offers a tile-based programming model for portability.",
      "Embedding the kernel programming language in JAX allows re-use of JAX's tracing infrastructure and transformations.",
      "JAX transformations can be leveraged to transform user-written kernels.",
      "Pallas extends JAX with reference types (Refs), Pallas-specific primitives, and a `pallas_call` higher-order function.",
      "These APIs are experimental and subject to change."
    ],
    "code_examples": []
  },
  {
    "title": "Pallas Kernel Example: Vector Addition",
    "concepts": [
      "Pallas kernels use Refs instead of immutable array arguments.",
      "Refs can be read from and updated in-place using NumPy-like syntax.",
      "Refs represent stateful computations and are used for mutable memory operations in kernels.",
      "Kernels receive Refs for inputs and outputs.",
      "Reading from a Ref loads an array into the lowest level of memory hierarchy (L1-cache or vector registers).",
      "Writing to a Ref corresponds to writing an Array into the memory.",
      "Refs are special types that must be read from before being used with JAX primitives."
    ],
    "code_examples": [
      {
        "description": "Example Pallas program for adding two vectors.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax.experimental\nimport\npallas\nas\npl\ndef\nadd_kernel\n(\n    x_ref,\n    y_ref,\n    o_ref,\n):\n    # In this code, `x_ref`, `y_ref` and `o_ref` are (8,)-shaped `Ref`s\n    x = x_ref[:]\n    y = y_ref[:]\n    o_ref[:] = x + y\n\nx, y = jnp.arange(8), jnp.arange(8, 16)\nadd = pl.pallas_call(add_kernel, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\nadd(x, y)"
      },
      {
        "description": "Example Pallas program for adding two vectors. (Duplicated in original document)",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax.experimental\nimport\npallas\nas\npl\ndef\nadd_kernel\n(\n    x_ref,\n    y_ref,\n    o_ref,\n):\n    # In this code, `x_ref`, `y_ref` and `o_ref` are (8,)-shaped `Ref`s\n    x = x_ref[:]\n    y = y_ref[:]\n    o_ref[:] = x + y\n\nx, y = jnp.arange(8), jnp.arange(8, 16)\nadd = pl.pallas_call(add_kernel, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\nadd(x, y)"
      }
    ]
  },
  {
    "title": "Indexing and Slicing with Refs",
    "concepts": [
      "Writing to Refs can be done via `__setitem__` style indexing.",
      "Vanilla Python indexing and NumPy advanced int indexing are supported.",
      "Other forms of indexing (e.g., dynamic slicing) can be done via `pallas.load` and `pallas.store`.",
      "NumPy advanced int indexing requires broadcasting the indices."
    ],
    "code_examples": [
      {
        "description": "Demonstrates Python indexing and Numpy advanced int indexing.",
        "code": "def f(x_ref, o_ref):\n    # Using vanilla Python indexing\n    x = x_ref[0, 2:5, :]\n    # Or via Numpy advanced int indexing\n    o_ref[jnp.arange(3), :] = x\n# Note that in order to use NumPy advanced int indexing, you need to broadcast the indices against each other into the desired multidimensional shape:"
      },
      {
        "description": "Demonstrates how to read out a (2, 3) slice from an (8, 4) Ref using advanced indexing.",
        "code": "def f(x_ref):\n    # Assume x_ref is (8, 4) and we want to read out a (2, 3) slice\n    x = x_ref[jnp.arange(2)[..., None], jnp.arange(3)[None, ...]]"
      },
      {
        "description": "Demonstrates Python indexing and Numpy advanced int indexing. (Duplicated in original document)",
        "code": "def f(x_ref, o_ref):\n    # Using vanilla Python indexing\n    x = x_ref[0, 2:5, :]\n    # Or via Numpy advanced int indexing\n    o_ref[jnp.arange(3), :] = x\n# Note that in order to use NumPy advanced int indexing, you need to broadcast the indices against each other into the desired multidimensional shape:"
      },
      {
        "description": "Demonstrates how to read out a (2, 3) slice from an (8, 4) Ref using advanced indexing. (Duplicated in original document)",
        "code": "def f(x_ref):\n    # Assume x_ref is (8, 4) and we want to read out a (2, 3) slice\n    x = x_ref[jnp.arange(2)[..., None], jnp.arange(3)[None, ...]]"
      }
    ]
  },
  {
    "title": "Pallas Primitives: load and store",
    "concepts": [
      "Pallas supplements JAX primitives with new ones specific to the target compiler (Triton or Mosaic).",
      "`pallas.load` and `pallas.store` allow loading from and storing into memory, providing more flexibility than `__getitem__` and `__setitem__`.",
      "`pallas.dynamic_slice` (`pallas.ds`) provides dynamic slicing functionality."
    ],
    "code_examples": [
      {
        "description": "Demonstrates reading from memory using `pallas.load` and storing into memory using `pallas.store` with `pallas.dynamic_slice`.",
        "code": "def f(x_ref, o_ref):\n    # Reading from memory via pallas.load\n    x = pl.load(x_ref, (0, slice(2, 5), slice(None)))\n    # Using integer indexing automatically broadcasts\n    x = pl.load(x_ref, (0, 2 + jnp.arange(3), slice(None)))\n    # You can also use `pl.dynamic_slice` (`pl.ds` for short) objects as well\n    pl.store(o_ref, (0, pl.ds(start=2, size=3), slice(None)), x)"
      },
      {
        "description": "Demonstrates reading from memory using `pallas.load` and storing into memory using `pallas.store` with `pallas.dynamic_slice`. (Duplicated in original document)",
        "code": "def f(x_ref, o_ref):\n    # Reading from memory via pallas.load\n    x = pl.load(x_ref, (0, slice(2, 5), slice(None)))\n    # Using integer indexing automatically broadcasts\n    x = pl.load(x_ref, (0, 2 + jnp.arange(3), slice(None)))\n    # You can also use `pl.dynamic_slice` (`pl.ds` for short) objects as well\n    pl.store(o_ref, (0, pl.ds(start=2, size=3), slice(None)), x)"
      }
    ]
  },
  {
    "title": "Masking with pallas.load and pallas.store",
    "concepts": [
      "`pallas.load` and `pallas.store` support masking via the `mask` argument.",
      "Masking is important for handling out-of-bounds loads/stores.",
      "The operational semantics of masking can be compiler-determined."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using masking with `pallas.load`.",
        "code": "def f(x_ref, o_ref):\n    # Reading from memory via pallas.load\n    idx = jnp.arange(8)\n    mask = idx < 5\n    x = pl.load(x_ref, (idx,), mask=mask, other=float('-inf'))"
      },
      {
        "description": "Demonstrates using masking with `pallas.load`. (Duplicated in original document)",
        "code": "def f(x_ref, o_ref):\n    # Reading from memory via pallas.load\n    idx = jnp.arange(8)\n    mask = idx < 5\n    x = pl.load(x_ref, (idx,), mask=mask, other=float('-inf'))"
      }
    ]
  },
  {
    "title": "Pallas Primitives: program_id and num_programs",
    "concepts": [
      "`pallas.program_id` returns the index of the current kernel execution within a multidimensional grid, analogous to threadId in CUDA.",
      "`pallas.num_programs` returns the grid size for a given axis.",
      "These primitives indicate the position of the kernel's execution within the overall computation.",
      "The 'program' terminology is borrowed from Triton and may change in the future."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using `pallas.program_id` to get the execution index.",
        "code": "def f(x_ref, o_ref):\n    i = pl.program_id(axis=0)  # execution index in the first axis of the grid\n    o_ref[i] = jnp.exp(x_ref[i])"
      },
      {
        "description": "Demonstrates using `pallas.program_id` to get the execution index. (Duplicated in original document)",
        "code": "def f(x_ref, o_ref):\n    i = pl.program_id(axis=0)  # execution index in the first axis of the grid\n    o_ref[i] = jnp.exp(x_ref[i])"
      }
    ]
  },
  {
    "title": "Supported and Unsupported JAX Primitives in Pallas Kernels",
    "concepts": [
      "Pallas kernels can support elementwise operations, simple dot products, and JAX control flow.",
      "Some JAX primitives may not be efficiently representable or useful in Pallas kernels.",
      "Examples of unsupported primitives include `conv_general` and `gather/scatter`."
    ],
    "code_examples": []
  },
  {
    "title": "Executing Pallas Kernels with pallas_call",
    "concepts": [
      "`pallas_call` is a higher-order function (like `jax.jit` and `jax.pmap`) that executes Pallas kernels.",
      "`pallas_call` takes the kernel function, output shape, input specifications (`in_specs`), output specifications (`out_specs`), and grid size as arguments.",
      "`out_shape` tells the kernel what the outputs look like.",
      "`in_specs`, `out_specs`, and `grid` specify how the kernel is scheduled on the accelerator.",
      "`pallas_call` iterates over the grid, transforming inputs and outputs based on `in_specs` and `out_specs`.",
      "The loop over the iteration space can be executed in parallel, e.g., on a GPU.",
      "`pallas_call` provides no guarantees on the order of loop iterations.",
      "Compilers like Triton and Mosaic have specific operational semantics associated with the grid."
    ],
    "code_examples": [
      {
        "description": "Signature of the pallas_call function.",
        "code": "def pallas_call(\n    kernel: Callable,\n    out_shape: Sequence[jax.ShapeDtypeStruct],\n    *,\n    in_specs: Sequence[Spec],\n    out_specs: Sequence[Spec],\n    grid: Optional[Tuple[int, ...]] = None\n) -> Callable:\n    ..."
      },
      {
        "description": "Signature of the pallas_call function. (Duplicated in original document)",
        "code": "def pallas_call(\n    kernel: Callable,\n    out_shape: Sequence[jax.ShapeDtypeStruct],\n    *,\n    in_specs: Sequence[Spec],\n    out_specs: Sequence[Spec],\n    grid: Optional[Tuple[int, ...]] = None\n) -> Callable:\n    ..."
      },
      {
        "description": "Semantics for pallas_call.",
        "code": "def pallas_call(kernel, out_shape, *, in_specs, out_specs, grid):\n    def execute(*args):\n        outputs = map(empty_ref, out_shape)\n        grid_indices = map(range, grid)\n        for indices in itertools.product(*grid_indices):\n            # Could run in parallel!\n            local_inputs = [in_spec.transform(arg, indices) for arg, in_spec in zip(args, in_specs)]\n            local_outputs = [out_spec.transform(arg, indices) for arg, out_spec in zip(outputs, out_specs)]\n            kernel(*local_inputs, *local_outputs)  # writes to outputs\n        return execute"
      },
      {
        "description": "Semantics for pallas_call. (Duplicated in original document)",
        "code": "def pallas_call(kernel, out_shape, *, in_specs, out_specs, grid):\n    def execute(*args):\n        outputs = map(empty_ref, out_shape)\n        grid_indices = map(range, grid)\n        for indices in itertools.product(*grid_indices):\n            # Could run in parallel!\n            local_inputs = [in_spec.transform(arg, indices) for arg, in_spec in zip(args, in_specs)]\n            local_outputs = [out_spec.transform(arg, indices) for arg, out_spec in zip(outputs, out_specs)]\n            kernel(*local_inputs, *local_outputs)  # writes to outputs\n        return execute"
      }
    ]
  },
  {
    "title": "Input and Output Specifications (Specs) for pallas_call",
    "concepts": [
      "`in_specs` and `out_specs` allow transformation of inputs and outputs.",
      "The supported options are an identity transformation and `BlockSpec`s.",
      "`BlockSpec` takes an `index_map` function and a `block_shape`.",
      "`index_map` maps loop indices to block indices.",
      "`block_shape` defines the size of the blocks.",
      "Specifying `None` in `block_shape` corresponds to mapping over that dimension and removing it from the block within the kernel."
    ],
    "code_examples": [
      {
        "description": "Definition of the BlockSpec class with transform method.",
        "code": "class BlockSpec:\n    index_map: Callable[[Tuple[Int, ...]], Tuple[Int, ...]]\n    block_shape: Tuple[Optional[int], ...]\n    def transform(self, ref, *loop_indices):\n        block_indices = self.transform_function(loop_indices)\n        # Returns a view of `ref` starting at `block_indices` of shape self.block_shape\n        ..."
      },
      {
        "description": "Definition of the BlockSpec class with transform method. (Duplicated in original document)",
        "code": "class BlockSpec:\n    index_map: Callable[[Tuple[Int, ...]], Tuple[Int, ...]]\n    block_shape: Tuple[Optional[int], ...]\n    def transform(self, ref, *loop_indices):\n        block_indices = self.transform_function(loop_indices)\n        # Returns a view of `ref` starting at `block_indices` of shape self.block_shape\n        ..."
      }
    ]
  },
  {
    "title": "Benefits of a JAX Front-End for Kernel Writing",
    "concepts": [
      "JAX users are familiar with the benefits and limitations of programming with JAX and its tracing-based transformations.",
      "Users can use closures and other Python constructs when writing Pallas kernels.",
      "Pallas is more amenable to templating than Triton.",
      "Kernels can be represented as programs with JAX primitives and new Pallas primitives.",
      "Pallas programs can be lowered to StableHLO and compiled/executed with XLA.",
      "`pallas_call` can be implemented as a `lax.scan` over the grid.",
      "This enables developing GPU or TPU kernels on any XLA-supported platform (including CPU) and debugging with JAX/XLA tools.",
      "XLA numerics can be used to verify the correctness of the Triton and Mosaic compilers.",
      "Perturbing the scan ordering can simulate parallel reads and writes on GPU."
    ],
    "code_examples": [
      {
        "description": "Example of using higher-order functions in Python to template a kernel.",
        "code": "def make_kernel(eltwise_kernel):\n    def add(x_ref, y_ref, o_ref):\n        x = pl.load(x_ref, ())\n        y = pl.load(y_ref, ())\n        pl.store(o_ref, (), eltwise_kernel(x + y))\n    return add\n\nkernel1 = make_kernel(lambda x: x * 2)\nkernel2 = make_kernel(jnp.exp)\n\npl.pallas_call(kernel1, out_shape=x, grid=1)(1., 1.)\npl.pallas_call(kernel2, out_shape=x, grid=1)(1., 1.)"
      },
      {
        "description": "Example of using higher-order functions in Python to template a kernel. (Duplicated in original document)",
        "code": "def make_kernel(eltwise_kernel):\n    def add(x_ref, y_ref, o_ref):\n        x = pl.load(x_ref, ())\n        y = pl.load(y_ref, ())\n        pl.store(o_ref, (), eltwise_kernel(x + y))\n    return add\n\nkernel1 = make_kernel(lambda x: x * 2)\nkernel2 = make_kernel(jnp.exp)\n\npl.pallas_call(kernel1, out_shape=x, grid=1)(1., 1.)\npl.pallas_call(kernel2, out_shape=x, grid=1)(1., 1.)"
      }
    ]
  },
  {
    "title": "Example: Vector Addition with BlockSpec",
    "concepts": [
      "Demonstrates vector addition operating over (2,)-sized blocks using BlockSpecs.",
      "GPU only. Requires tweaks to the block sizes to work on TPUs."
    ],
    "code_examples": [
      {
        "description": "Modified add_kernel example to operate over (2,)-sized blocks using BlockSpec s.",
        "code": "def add_kernel(x_ref, y_ref, o_ref):\n    # In this code, `x_ref`, `y_ref` and `o_ref` are (2,)-shaped `Ref`s\n    x = x_ref[:]\n    y = y_ref[:]\n    o_ref[:] = x + y\n\nx, y = jnp.arange(8), jnp.arange(8, 16)\nadd = pl.pallas_call(\n    add_kernel,\n    out_shape=jax.ShapeDtypeStruct((8,), jnp.int32),\n    in_specs=[\n        pl.BlockSpec((2,), lambda i: i),\n        pl.BlockSpec((2,), lambda i: i)\n    ],\n    out_specs=pl.BlockSpec((2,), lambda i: i),\n    grid=(4,)\n)\nadd(x, y)"
      },
      {
        "description": "Modified add_kernel example to operate over (2,)-sized blocks using BlockSpec s. (Duplicated in original document)",
        "code": "def add_kernel(x_ref, y_ref, o_ref):\n    # In this code, `x_ref`, `y_ref` and `o_ref` are (2,)-shaped `Ref`s\n    x = x_ref[:]\n    y = y_ref[:]\n    o_ref[:] = x + y\n\nx, y = jnp.arange(8), jnp.arange(8, 16)\nadd = pl.pallas_call(\n    add_kernel,\n    out_shape=jax.ShapeDtypeStruct((8,), jnp.int32),\n    in_specs=[\n        pl.BlockSpec((2,), lambda i: i),\n        pl.BlockSpec((2,), lambda i: i)\n    ],\n    out_specs=pl.BlockSpec((2,), lambda i: i),\n    grid=(4,)\n)\nadd(x, y)"
      }
    ]
  },
  {
    "title": "Example: Matrix Multiplication with BlockSpec and Fused Activation",
    "concepts": [
      "Demonstrates computing tiles of the output by doing an unrolled accumulation over blocks of rows and columns from the input arrays.",
      "Inlines an activation function into the body of the kernel using a higher-order function for a fused kernel.",
      "GPU only. Requires tweaks to the block sizes to work on TPUs."
    ],
    "code_examples": [
      {
        "description": "Example of a matrix multiplication kernel using BlockSpec and a fused GELU activation function.",
        "code": "def matmul_kernel(x_ref, y_ref, o_ref, *, activation, block_k):\n    acc = jnp.zeros((x_ref.shape[0], y_ref.shape[1]), jnp.float32)\n    for k in range(x_ref.shape[1] // block_k):\n        x = x_ref[:, k*block_k:(k+1)*block_k]\n        y = y_ref[k*block_k:(k+1)*block_k, :]\n        acc += x @ y\n    o_ref[:, :] = activation(acc).astype(o_ref.dtype)\n\nx, y = jnp.ones((512, 256)), jnp.ones((256, 1024))\nblock_shape = 128, 256, 128\n\n@partial(jax.jit, static_argnames=[\"block_shape\", \"activation\"])\ndef matmul(x, y, *, block_shape, activation):\n    block_m, block_n, block_k = block_shape\n    fused_matmul = pl.pallas_call(\n        partial(matmul_kernel, block_k=block_k, activation=activation),\n        out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1],), jnp.float32),\n        in_specs=[\n            pl.BlockSpec((block_m, x.shape[1]), lambda i, j: (i, 0)),\n            pl.BlockSpec((y.shape[0], block_n), lambda i, j: (0, j))\n        ],\n        out_specs=pl.BlockSpec((block_m, block_n), lambda i, j: (i, j)),\n        grid=(4, 4),\n    )\n    return fused_matmul(x, y)\n\nz = matmul(x, y, block_shape=block_shape, activation=jax.nn.gelu)"
      },
      {
        "description": "Example of a matrix multiplication kernel using BlockSpec and a fused GELU activation function. (Duplicated in original document)",
        "code": "def matmul_kernel(x_ref, y_ref, o_ref, *, activation, block_k):\n    acc = jnp.zeros((x_ref.shape[0], y_ref.shape[1]), jnp.float32)\n    for k in range(x_ref.shape[1] // block_k):\n        x = x_ref[:, k*block_k:(k+1)*block_k]\n        y = y_ref[k*block_k:(k+1)*block_k, :]\n        acc += x @ y\n    o_ref[:, :] = activation(acc).astype(o_ref.dtype)\n\nx, y = jnp.ones((512, 256)), jnp.ones((256, 1024))\nblock_shape = 128, 256, 128\n\n@partial(jax.jit, static_argnames=[\"block_shape\", \"activation\"])\ndef matmul(x, y, *, block_shape, activation):\n    block_m, block_n, block_k = block_shape\n    fused_matmul = pl.pallas_call(\n        partial(matmul_kernel, block_k=block_k, activation=activation),\n        out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1],), jnp.float32),\n        in_specs=[\n            pl.BlockSpec((block_m, x.shape[1]), lambda i, j: (i, 0)),\n            pl.BlockSpec((y.shape[0], block_n), lambda i, j: (0, j))\n        ],\n        out_specs=pl.BlockSpec((block_m, block_n), lambda i, j: (i, j)),\n        grid=(4, 4),\n    )\n    return fused_matmul(x, y)\n\nz = matmul(x, y, block_shape=block_shape, activation=jax.nn.gelu)"
      }
    ]
  },
  {
    "title": "Lowering Pallas Kernels to Triton and Mosaic",
    "concepts": [
      "Pallas kernels are lowered to different representations depending on the target backend.",
      "On GPUs, Pallas is lowered to Triton IR, and on TPUs, it's lowered to Mosaic.",
      "Lowering to Triton is straightforward as Pallas was designed with Triton in mind.",
      "Triton uses pointers for memory loads and stores, while Pallas uses indices.",
      "Slices in Pallas are lowered to arrays of pointers in Triton.",
      "JAX dot products are lowered to Triton dot products, and JAX unary primitives are lowered to their Triton equivalents.",
      "Triton's atomic operations are lowered via new Pallas atomic primitives.",
      "Pallas can be lowered to Mosaic by translating JAX primitives to MLIR.",
      "BlockSpecs can be converted into pipeline schedules in Mosaic."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Transformations and Pallas Kernels",
    "concepts": [
      "JAX transformations can be applied both inside and outside Pallas kernels.",
      "Transformations inside Pallas kernels should generally work if the transformed code can be lowered.",
      "Transformations of Pallas kernels from outer JAX programs are more interesting, e.g., `vmap(pallas_call)` and `grad(pallas_call)`.",
      "A reasonable default `vmap` rule can be offered for `pallas_call` while allowing customization with `jax.custom_vmap`.",
      "When `pallas_call` is `vmap`-ed, the `pallas_call` is augmented with an extra grid dimension, and the `BlockSpec`s are transformed accordingly.",
      "`grad(pallas_call)` enables automatic differentiation of kernels.",
      "Automatic differentiation can lead to a performance hit due to memory access transposition.",
      "A potential direction for efficient automatic differentiation is to explore a different representation, like Dex.",
      "`jax.custom_vjp` is an escape hatch to express Pallas kernels that work with `jax.grad`.",
      "Other JAX transformations, like `checkify` and `custom_partitioning`, could be integrated with Pallas kernels."
    ],
    "code_examples": []
  },
  {
    "title": "Motivation and XLA Optimization",
    "concepts": [
      "Pallas aims to expose APIs for overlapping computation and communication across multiple kernels.",
      "JAX pseudocode demonstrates a scenario where ppermute and x + 1 can be performed concurrently.",
      "XLA automatically optimizes this by decomposing ppermute into ppermute_start and ppermute_done, connected via a future.",
      "XLA schedules independent operations between ppermute_start and ppermute_done to achieve concurrency."
    ],
    "code_examples": [
      {
        "description": "JAX pseudocode illustrating potential for overlapping computation and communication.",
        "code": "def f(x):\n  y = ppermute(x)\n  z = x + 1\n  return y, z"
      },
      {
        "description": "XLA optimizes the JAX code by decomposing ppermute and scheduling x + 1 in between.",
        "code": "def f(x):\n  fut = ppermute_start(x)\n  z = x + 1  # happens at the same time as ppermute\n  y = ppermute_done(fut)\n  return y, z"
      }
    ]
  },
  {
    "title": "Custom Pallas ppermute and Explicit Fusion",
    "concepts": [
      "The document considers a custom Pallas ppermute implementation.",
      "Currently, Pallas cannot decompose ppermute into start/done pairs like XLA.",
      "The alternative is to explicitly fuse the x + 1 computation into the ppermute kernel.",
      "This approach uses pltpu.make_async_remote_copy for communication and pltpu.emit_pipeline to schedule add_one within the ppermute kernel."
    ],
    "code_examples": [
      {
        "description": "Custom Pallas ppermute kernel using make_async_remote_copy.",
        "code": "def ppermute_kernel(x_ref, y_ref, send_sem, recv_sem):\n  right_neighbor = ...\n  descriptor = pltpu.make_async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=right_neighbor)\n  descriptor.start()\n  descriptor.wait_send()\n  descriptor.wait_recv()\n\ndef ppermute(x):\n  return pl.pallas_call(ppermute_kernel, out_shape=x, ...)(x)"
      },
      {
        "description": "Kernel that adds one to x, used in the fused ppermute_add_one_kernel.",
        "code": "def add_one(x_ref, z_ref):\n  z_ref[...] = x_ref[...] + 1"
      },
      {
        "description": "Fused ppermute and add_one kernel using make_async_remote_copy and emit_pipeline.",
        "code": "def ppermute_add_one_kernel(x_ref, y_ref, z_ref, send_sem, recv_sem):\n  right_neighbor = ...\n  descriptor = pltpu.make_async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=right_neighbor)\n  descriptor.start()\n  # Explicitly schedule inner kernel between start/wait\n  pltpu.emit_pipeline(add_one)(x_ref, z_ref)\n  descriptor.wait_send()\n  descriptor.wait_recv()\n\ndef ppermute_and_add_one(x):\n  return pl.pallas_call(ppermute_add_one_kernel, out_shape=(x, x), ...)(x)"
      }
    ]
  },
  {
    "title": "Decomposed Async Operations in Pallas",
    "concepts": [
      "The goal is to decompose ppermute into start and done kernels for better code readability and maintainability.",
      "The key is to determine the content of the 'future' passed between the start and done kernels.",
      "The 'descriptor' used in the original ppermute kernel is needed for both start and wait operations.",
      "The underlying TPU hardware tracks async op progress via send_sem and recv_sem.",
      "Pallas is extended to support returning semaphores from kernels."
    ],
    "code_examples": []
  },
  {
    "title": "Implementing ppermute_start Kernel",
    "concepts": [
      "The ppermute_start_kernel initiates the remote copy operation.",
      "It uses pltpu.make_async_remote_copy and .start() to start the asynchronous transfer.",
      "It returns send_sem, recv_sem, and the output buffer.",
      "Pallas treats returned semaphores as \"reserved\", preventing their allocation by other kernels.",
      "The output buffer is returned even while the data is being copied into it."
    ],
    "code_examples": [
      {
        "description": "ppermute_start_kernel that initializes and starts the remote copy and returns the semaphores.",
        "code": "def ppermute_start_kernel(in_ref, send_sem, recv_sem, out_ref, *, axis_name):\n  axis_size = jax.lax.psum(1, axis_name)\n  left_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size)\n  right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n  barrier_sem = pltpu.get_barrier_semaphore()\n  pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n  pltpu.semaphore_wait(barrier_sem, 1)\n  pltpu.make_async_remote_copy(in_ref, out_ref, send_sem, recv_sem, device_id=right_neighbor).start()\n\ndef ppermute_start(x, *, axis_name) -> tuple[Semaphore, Semaphore, Array]:\n  send_sem, recv_sem, out = pl.pallas_call(\n      functools.partial(ppermute_start_kernel, axis_name=axis_name),\n      out_shape=(\n          pltpu.SemaphoreType.DMA(()),\n          pltpu.SemaphoreType.DMA(()),\n          jax.ShapeDtypeStruct(x.shape, dtype=x.dtype),\n      ),\n      in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)],\n      out_specs=(\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.ANY),\n      ),\n  )(x)\n  return send_sem, recv_sem, out"
      }
    ]
  },
  {
    "title": "Implementing ppermute_done Kernel",
    "concepts": [
      "The ppermute_done_kernel performs the blocking operation, waiting for the remote copy to complete.",
      "It uses pltpu.make_async_copy(...).wait() to block on the send_sem and recv_sem.",
      "The output buffer 'out' is passed into the kernel to compute the shape needed to block on the semaphore.",
      "The output buffer is I/O aliased to guarantee that consumers are downstream of ppermute_done."
    ],
    "code_examples": [
      {
        "description": "ppermute_done_kernel that waits on the semaphores to complete the remote copy.",
        "code": "def ppermute_done_kernel(ref, send_sem, recv_sem, _):\n  pltpu.make_async_copy(ref, ref, send_sem).wait()\n  pltpu.make_async_copy(ref, ref, recv_sem).wait()\n\ndef ppermute_done(send_sem, recv_sem, out) -> Array:\n  out = pl.pallas_call(\n      ppermute_done_kernel,\n      out_shape=(jax.ShapeDtypeStruct(out.shape, dtype=out.dtype),),\n      in_specs=[\n          pl.BlockSpec(memory_space=pltpu.ANY),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n      ],\n      out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n      input_output_aliases={0: 0},\n  )(out, send_sem, recv_sem)\n  return out"
      }
    ]
  },
  {
    "title": "Decomposed Collective Permute",
    "concepts": [
      "The document combines the ppermute_start and ppermute_done kernels to implement the decomposed collective permute.",
      "Scheduling, lifetimes and defensive copies are the three main remaining issues."
    ],
    "code_examples": [
      {
        "description": "The decomposed collective permute using ppermute_start and ppermute_done.",
        "code": "def f(x):\n  fut = ppermute_start(x)\n  z = x + 1  # happens at the same time as ppermute\n  y = ppermute_done(fut)\n  return y, z"
      }
    ]
  },
  {
    "title": "Scheduling with optimization_barrier",
    "concepts": [
      "XLA may reorder operations, potentially executing x + 1 before ppermute_start or after ppermute_done.",
      "optimization_barrier introduces an explicit data dependency to force op ordering.",
      "It acts as an identity function but prevents XLA from moving ops around it.",
      "By making x dependent on the future returned by ppermute_start, x + 1 is forced to happen after ppermute_start.",
      "A dependency is also introduced to force output y to depend on z."
    ],
    "code_examples": [
      {
        "description": "Using optimization_barrier to enforce scheduling constraints.",
        "code": "def f(x):\n  fut = ppermute_start(x)\n  x, fut = optimization_barrier((x, fut))  # x now depends on fut\n  z = x + 1\n  z, fut = optimization_barrier((z, fut))  # fut now depends on z\n  y = ppermute_done(fut)\n  return y, z"
      }
    ]
  },
  {
    "title": "Extending Value Lifetimes",
    "concepts": [
      "XLA can free a buffer after its last use, potentially leading to issues if an asynchronous copy is still in progress.",
      "If XLA frees 'x' before ppermute completes, it results in reading from garbage memory.",
      "To extend the lifetime of x, ppermute_start is rewritten to return x, aliasing it through the kernel.",
      "ppermute_done is updated to take x as input, although it does not use it, ensuring that x's lifetime extends to ppermute_done."
    ],
    "code_examples": [
      {
        "description": "Rewriting ppermute_start to return x, aliasing it through the kernel.",
        "code": "def ppermute_start_kernel(in_ref, send_sem, recv_sem, out_ref, _, *, axis_name):\n  axis_size = jax.lax.psum(1, axis_name)\n  left_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size)\n  right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n  barrier_sem = pltpu.get_barrier_semaphore()\n  pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n  pltpu.semaphore_wait(barrier_sem, 1)\n  pltpu.make_async_remote_copy(in_ref, out_ref, send_sem, recv_sem, device_id=right_neighbor).start()\n\ndef ppermute_start(x, *, axis_name) -> tuple[Semaphore, Semaphore, Array, Array]:\n  send_sem, recv_sem, x, out = pl.pallas_call(\n      functools.partial(ppermute_start_kernel, axis_name=axis_name),\n      out_shape=(\n          pltpu.SemaphoreType.DMA(()),\n          pltpu.SemaphoreType.DMA(()),\n          jax.ShapeDtypeStruct(x.shape, dtype=x.dtype),\n          jax.ShapeDtypeStruct(x.shape, dtype=x.dtype),\n      ),\n      in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)],\n      out_specs=(\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.ANY),\n          pl.BlockSpec(memory_space=pltpu.ANY),\n      ),\n      input_output_aliases={0: 2},\n  )(x)\n  return send_sem, recv_sem, x, out"
      },
      {
        "description": "Updating ppermute_done to take x as input to extend its lifetime.",
        "code": "def ppermute_done_kernel(_, ref, send_sem, recv_sem, _):\n  pltpu.make_async_copy(ref, ref, send_sem).wait()\n  pltpu.make_async_copy(ref, ref, recv_sem).wait()\n\ndef ppermute_done(send_sem, recv_sem, x, out) -> Array:\n  out = pl.pallas_call(\n      ppermute_done_kernel,\n      out_shape=(jax.ShapeDtypeStruct(out.shape, dtype=out.dtype),),\n      in_specs=[\n          pl.BlockSpec(memory_space=pltpu.ANY),\n          pl.BlockSpec(memory_space=pltpu.ANY),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n          pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n      ],\n      out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n      input_output_aliases={1: 0},\n  )(x, out, send_sem, recv_sem)\n  return out"
      },
      {
        "description": "Example usage with updated ppermute_start and ppermute_done, showing x being passed through.",
        "code": "def f(x):\n  *sems, x, out = ppermute_start(x)\n  z = x + 1\n  y = ppermute_done(*sems, x, out)\n  return y, z"
      }
    ]
  },
  {
    "title": "Handling Defensive Copies",
    "concepts": [
      "XLA inserts copies to ensure correctness when operations perform in-place updates and there are multiple consumers of the input buffer.",
      "XLA might insert an unnecessary copy between ppermute_start and z = x + 1 because it doesn't know that ppermute_start does not change 'x'.",
      "To avoid this unnecessary copy, the code can be rewritten to explicitly use x2 (the aliased output of ppermute_start) instead of x for the z = x + 1 operation.",
      "This approach couples the lifetime problem to the copying problem."
    ],
    "code_examples": [
      {
        "description": "Example of how a copy might be introduced by XLA.",
        "code": "def f(x):\n  x2 = copy(x)\n  *sems, x2, y = ppermute_start(x2)\n  z = x + 1\n  y = ppermute_done((*sems, x2, y))\n  return y, z"
      },
      {
        "description": "Rewriting the code to avoid the defensive copy.",
        "code": "def f(x):\n  *sems, x2, y = ppermute_start(x)\n  z = x2 + 1\n  y = ppermute_done((*sems, x2, y))\n  return y, z"
      }
    ]
  },
  {
    "title": "Advanced Example: while_loop with ppermute",
    "concepts": [
      "An example is presented where values are sent around a ring using a while_loop with ppermute.",
      "fori_loop automatically aliases input and output buffers.",
      "Alias analysis shows that all buffers might be colored the same, potentially leading to incorrect results due to buffer re-use.",
      "XLA might insert a defensive copy to address the buffer re-use issue, but this can be expensive.",
      "The solution is to unroll the loop (using fori_loop with unroll >= 2) to introduce double buffering and avoid expensive copies."
    ],
    "code_examples": [
      {
        "description": "Original code with while_loop and ppermute.",
        "code": "def f(x):\n  def body(i, x):\n    fut = ppermute_start(x)\n    y = ppermute_done(fut)\n    return y\n  return fori_loop(0, 8, body, x)"
      },
      {
        "description": "Code after manually unrolling the loop.",
        "code": "def f(x):\n  def body(i, x):\n    *sems, x, x2 = ppermute_start(x)\n    x2 = ppermute_done((*sems, x, x2))\n    *sems, x2, y = ppermute_start(x2)\n    y = ppermute_done((*sems, x2, y))\n    return y\n  return fori_loop(0, 4, body, x)"
      },
      {
        "description": "Using fori_loop with unroll >= 2 to avoid copies.",
        "code": "def f(x):\n  def body(i, x):\n    fut = ppermute_start(x)\n    y = ppermute_done(fut)\n    return y\n  return fori_loop(0, 8, body, x, unroll=2)"
      }
    ]
  },
  {
    "title": "Staggered Loop Example",
    "concepts": [
      "An example is presented that staggers the loop by starting the ppermute in a prologue and waiting on it at the beginning of the loop.",
      "The future value, including the semaphores, input buffer and target output buffer, is threaded as a loop carry.",
      "Same aliasing issues with x and out as before will arise.",
      "Unrolling the loop with unroll >= 2 avoids the aliasing issues."
    ],
    "code_examples": [
      {
        "description": "Code example demonstrating a staggered loop using ppermute.",
        "code": "def f(x):\n  fut = ppermute_start(x)\n  def body(i, fut):\n    x = ppermute_done(fut)\n    fut = ppermute_start(x)\n    return fut\n  fut = fori_loop(0, 7, body, fut)\n  return ppermute_done(fut)"
      },
      {
        "description": "Example with loop unrolling.",
        "code": "def f(x):\n  fut = ppermute_start(x)\n  def body(i, fut):\n    x = ppermute_done(fut)\n    fut = ppermute_start(x)\n    return fut\n  fut = fori_loop(0, 7, body, x, unroll=2)\n  return ppermute_done(fut)"
      }
    ]
  },
  {
    "title": "Combined Example and Rules of Thumb",
    "concepts": [
      "A comprehensive example demonstrates ppermute operations in a loop with accumulation.",
      "Rules of thumb are summarized:",
      "- If operations depend on the input value to ppermute, unpack the future to use the aliased value instead of the original value.",
      "- Use unroll >= 2 when doing ppermutes in a loop body.",
      "optimization_barrier is not needed because the loop boundary acts as a scheduling barrier."
    ],
    "code_examples": [
      {
        "description": "A combined function doing ppermute in a loop and accumulating the result.",
        "code": "def f(x):\n  out = jnp.zeros_like(x)\n  fut = (*sems, x, out) = ppermute_start(x)\n  out = out + x\n  def body(i, carry):\n    out, fut = carry\n    x = ppermute_done(fut)\n    fut = (*sems, x, out) = ppermute_start(x)\n    out = out + x\n    return out, fut\n  out, fut = fori_loop(0, 7, body, (out, fut), unroll=2)\n  return out, ppermute_done(fut)"
      }
    ]
  },
  {
    "title": "New Functionality: Debug Printing",
    "concepts": [
      "Added vector support for jax.experimental.pallas.debug_print() on TPU."
    ],
    "code_examples": []
  },
  {
    "title": "New Functionality: Dot Product Lowering",
    "concepts": [
      "Added support for DotAlgorithmPreset precision arguments for dot lowering on Triton backend."
    ],
    "code_examples": []
  },
  {
    "title": "Removals: Deprecated Aliases",
    "concepts": [
      "Removed deprecated aliases jax.experimental.pallas.tpu.CostEstimate and jax.experimental.tpu.run_scoped().",
      "Both aliases are now available in jax.experimental.pallas."
    ],
    "code_examples": []
  },
  {
    "title": "New Functionality: Cost Estimation Tool",
    "concepts": [
      "Added a cost estimate tool pl.estimate_cost() for automatically constructing a kernel cost estimate from a JAX reference function."
    ],
    "code_examples": []
  },
  {
    "title": "Changes: Debug Printing Arguments",
    "concepts": [
      "jax.experimental.pallas.debug_print() no longer requires all arguments to be scalars.",
      "Restrictions on arguments are now backend-specific.",
      "Non-scalar arguments are currently only supported on GPU, when using Triton."
    ],
    "code_examples": []
  },
  {
    "title": "Changes: BlockSpec Argument Order",
    "concepts": [
      "jax.experimental.pallas.BlockSpec no longer supports the deprecated argument order, where index_map comes before block_shape."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecations: GPU Submodule",
    "concepts": [
      "The jax.experimental.pallas.gpu submodule is deprecated.",
      "Use jax.experimental.pallas.triton for the Triton backend."
    ],
    "code_examples": []
  },
  {
    "title": "New Functionality: pallas_call Scratch Shapes",
    "concepts": [
      "jax.experimental.pallas.pallas_call() now accepts scratch_shapes.",
      "scratch_shapes is a PyTree specifying backend-specific temporary objects.",
      "checkify.check() can now be used to insert runtime asserts when pallas_call is called with the pltpu.enable_runtime_assert(True) context manager."
    ],
    "code_examples": []
  },
  {
    "title": "Changes: Kernel Function Constants",
    "concepts": [
      "Kernel functions are not allowed to close over constants.",
      "All needed arrays must be passed as inputs with proper block specs."
    ],
    "code_examples": []
  },
  {
    "title": "New Functionality: Improved Error Messages",
    "concepts": [
      "Improved error messages for mistakes in the signature of index map functions.",
      "Error messages now include the name and source location of the index map."
    ],
    "code_examples": []
  },
  {
    "title": "Changes: BlockSpec Argument Order (Revisited)",
    "concepts": [
      "jax.experimental.pallas.BlockSpec now expects block_shape to be passed before index_map.",
      "The old argument order is deprecated."
    ],
    "code_examples": []
  },
  {
    "title": "Changes: GridSpec Fields",
    "concepts": [
      "jax.experimental.pallas.GridSpec does not have the in_specs_tree and out_specs_tree fields anymore.",
      "The in_specs and out_specs trees now store the values as pytrees of BlockSpec.",
      "Previously, in_specs and out_specs were flattened."
    ],
    "code_examples": []
  },
  {
    "title": "Changes: Removed Methods",
    "concepts": [
      "The method compute_index of jax.experimental.pallas.GridSpec has been removed because it is private.",
      "The get_grid_mapping and unzip_dynamic_bounds methods have been removed from BlockSpec."
    ],
    "code_examples": []
  },
  {
    "title": "Changes: Interpret Mode and Padding",
    "concepts": [
      "Fixed the interpret mode to work with BlockSpec that involve padding.",
      "Padding in interpret mode will be with NaN, to help debug out-of-bounds errors.",
      "This behavior is not present when running in custom kernel mode."
    ],
    "code_examples": []
  },
  {
    "title": "Changes: Private API Access",
    "concepts": [
      "It is no longer possible to import many APIs that are meant to be private as jax.experimental.pallas.pallas."
    ],
    "code_examples": []
  },
  {
    "title": "New Functionality: Documentation and Error Messages",
    "concepts": [
      "Added documentation for BlockSpec: Grids and BlockSpecs.",
      "Improved error messages for the jax.experimental.pallas.pallas_call() API.",
      "Added lowering rules for TPU for lax.shift_right_arithmetic and lax.erf_inv.",
      "Added initial support for shape polymorphism for the Pallas TPU custom kernels.",
      "Added TPU support for checkify.",
      "Added clearer error messages when the block sizes do not match the TPU requirements."
    ],
    "code_examples": []
  },
  {
    "title": "New Functionality: TPU Block Size Requirements",
    "concepts": [
      "Added support for TPU lowering with 1D blocks.",
      "Relaxed the requirements for block sizes with at least 2 dimensions.",
      "The last 2 dimensions must be divisible by 8 and 128 respectively, unless they span the entire corresponding array dimension."
    ],
    "code_examples": []
  },
  {
    "title": "New Functionality: checkify and PRNG Keys",
    "concepts": [
      "Added checkify support for jax.experimental.pallas.pallas_call() in interpret mode.",
      "Improved support for PRNG keys for TPU kernels."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX FFI",
    "concepts": [
      "JAX FFI allows calling external compiled libraries from JAX.",
      "FFI is useful for leveraging optimized C or CUDA libraries or optimizing runtime/memory performance.",
      "FFI should be considered a last resort option.",
      "JAX doesn't automatically differentiate through foreign functions; differentiation rules must be provided.",
      "JAX FFI consists of a C++ header-only library from XLA and a Python front end in jax.ffi.",
      "The tutorial demonstrates FFI usage on CPU with generalizations to GPU or multi-device environments.",
      "The end-to-end code for the example is available in the JAX FFI examples project on GitHub."
    ],
    "code_examples": [
      {
        "description": "Sets up the environment to be treated by JAX as having multiple CPUs.",
        "code": "import os\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=4\"\nimport os\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=4\""
      }
    ]
  },
  {
    "title": "RMS Normalization Implementation in JAX",
    "concepts": [
      "Demonstrates FFI using a simple root-mean-square (RMS) normalization function.",
      "RMS normalization is implemented in JAX for reference.",
      "The reference implementation is used to test the FFI version."
    ],
    "code_examples": [
      {
        "description": "Reference implementation of RMS normalization in JAX.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef rms_norm_ref(x, eps=1e-5):\n  scale = jnp.sqrt(jnp.mean(jnp.square(x), axis=-1, keepdims=True) + eps)\n  return x / scale\nimport jax\nimport jax.numpy as jnp\n\ndef rms_norm_ref(x, eps=1e-5):\n  scale = jnp.sqrt(jnp.mean(jnp.square(x), axis=-1, keepdims=True) + eps)\n  return x / scale"
      }
    ]
  },
  {
    "title": "RMS Normalization Implementation in C++",
    "concepts": [
      "Provides a simple implementation of RMS normalization in C++.",
      "This C++ function will be exposed to JAX using the FFI."
    ],
    "code_examples": [
      {
        "description": "Simple C++ implementation of RMS normalization.",
        "code": "#include <cmath>\n#include <cstdint>\n\nfloat ComputeRmsNorm(float eps, int64_t size, const float *x, float *y) {\n  float sm = 0.0f;\n  for (int64_t n = 0; n < size; ++n) {\n    sm += x[n] * x[n];\n  }\n  float scale = 1.0f / std::sqrt(sm / float(size) + eps);\n  for (int64_t n = 0; n < size; ++n) {\n    y[n] = x[n] * scale;\n  }\n  return scale;\n}\n#include <cmath>\n#include <cstdint>\n\nfloat ComputeRmsNorm(float eps, int64_t size, const float *x, float *y) {\n  float sm = 0.0f;\n  for (int64_t n = 0; n < size; ++n) {\n    sm += x[n] * x[n];\n  }\n  float scale = 1.0f / std::sqrt(sm / float(size) + eps);\n  for (int64_t n = 0; n < size; ++n) {\n    y[n] = x[n] * scale;\n  }\n  return scale;\n}"
      }
    ]
  },
  {
    "title": "FFI Wrapper for the C++ RMS Normalization",
    "concepts": [
      "Exposes the C++ RMS normalization function to JAX and XLA.",
      "Uses the header-only library from XLA's xla/ffi/api directory.",
      "The wrapper handles buffer shape and pointers to the underlying data.",
      "Batching is handled in the left-most dimensions of the input arguments.",
      "The GetDims helper function provides support for batching behavior."
    ],
    "code_examples": [
      {
        "description": "C++ wrapper for the RMS normalization function using XLA's FFI API.",
        "code": "#include <functional>\n#include <numeric>\n#include <utility>\n\n#include \"xla/ffi/api/c_api.h\"\n#include \"xla/ffi/api/ffi.h\"\n\nnamespace ffi = xla::ffi;\n\n// A helper function for extracting the relevant dimensions from `ffi::Buffer`s.\n// In this example, we treat all leading dimensions as batch dimensions, so this\n// function returns the total number of elements in the buffer, and the size of\n// the last dimension.\ntemplate <ffi::DataType T>\nstd::pair<int64_t, int64_t> GetDims(const ffi::Buffer<T> &buffer) {\n  auto dims = buffer.dimensions();\n  if (dims.size() == 0) {\n    return std::make_pair(0, 0);\n  }\n  return std::make_pair(buffer.element_count(), dims.back());\n}\n\n// A wrapper function providing the interface between the XLA FFI call and our\n// library function `ComputeRmsNorm` above. This function handles the batch\n// dimensions by calling `ComputeRmsNorm` within a loop.\nffi::Error RmsNormImpl(\n    float eps, ffi::Buffer<ffi::F32> x, ffi::ResultBuffer<ffi::F32> y) {\n  auto [totalSize, lastDim] = GetDims(x);\n  if (lastDim == 0) {\n    return ffi::Error::InvalidArgument(\"RmsNorm input must be an array\");\n  }\n  for (int64_t n = 0; n < totalSize; n += lastDim) {\n    ComputeRmsNorm(eps, lastDim, &(x.typed_data()[n]), &(y->typed_data()[n]));\n  }\n  return ffi::Error::Success();\n}\n\n// Wrap `RmsNormImpl` and specify the interface to XLA. If you need to declare\n// this handler in a header, you can use the `XLA_FFI_DECLARE_HANDLER_SYMBOL`\n// macro: `XLA_FFI_DECLARE_HANDLER_SYMBOL(RmsNorm)`.\nXLA_FFI_DEFINE_HANDLER_SYMBOL(\n    RmsNorm, RmsNormImpl,\n    ffi::Ffi::Bind()\n        .Attr<float>(\"eps\")\n        .Arg<ffi::Buffer<ffi::F32>>()  // x\n        .Ret<ffi::Buffer<ffi::F32>>()  // y\n);"
      }
    ]
  },
  {
    "title": "Compiling and Exposing the FFI Wrapper to Python",
    "concepts": [
      "Compiles the C++ wrapper into a shared library.",
      "Registers the handler with XLA using register_ffi_target().",
      "The handler is wrapped in a PyCapsule using the pycapsule() helper function.",
      "Alternative patterns for exposing handlers include using nanobind or pybind11."
    ],
    "code_examples": [
      {
        "description": "Commands used to compile the shared library using CMake.",
        "code": "!cmake -DCMAKE_BUILD_TYPE=Release -B ffi/_build ffi\n!cmake --build ffi/_build\n!cmake --install ffi/_build\n!cmake -DCMAKE_BUILD_TYPE=Release -B ffi/_build ffi\n!cmake --build ffi/_build\n!cmake --install ffi/_build"
      },
      {
        "description": "Registers the FFI target using the ctypes library.",
        "code": "import ctypes\nfrom pathlib import Path\n\npath = next(Path(\"ffi\").glob(\"librms_norm*\"))\nrms_norm_lib = ctypes.cdll.LoadLibrary(path)\n\njax.ffi.register_ffi_target(\n    \"rms_norm\", jax.ffi.pycapsule(rms_norm_lib.RmsNorm), platform=\"cpu\")\nimport ctypes\nfrom pathlib import Path\n\npath = next(Path(\"ffi\").glob(\"librms_norm*\"))\nrms_norm_lib = ctypes.cdll.LoadLibrary(path)\n\njax.ffi.register_ffi_target(\n    \"rms_norm\", jax.ffi.pycapsule(rms_norm_lib.RmsNorm), platform=\"cpu\")"
      },
      {
        "description": "Nanobind code for exposing the handler to Python.",
        "code": "#include <type_traits>\n\n#include \"nanobind/nanobind.h\"\n#include \"xla/ffi/api/c_api.h\"\n\nnamespace nb = nanobind;\n\ntemplate <typename T>\nnb::capsule EncapsulateFfiCall(T *fn) {\n  // This check is optional, but it can be helpful for avoiding invalid handlers.\n  static_assert(\n      std::is_invocable_r_v<XLA_FFI_Error *, T, XLA_FFI_CallFrame *>,\n      \"Encapsulated function must be and XLA FFI handler\");\n  return nb::capsule(reinterpret_cast<void *>(fn));\n}\n\nNB_MODULE(rms_norm, m) {\n  m.def(\"rms_norm\", []() { return EncapsulateFfiCall(RmsNorm); });\n}\n#include <type_traits>\n\n#include \"nanobind/nanobind.h\"\n#include \"xla/ffi/api/c_api.h\"\n\nnamespace nb = nanobind;\n\ntemplate <typename T>\nnb::capsule EncapsulateFfiCall(T *fn) {\n  // This check is optional, but it can be helpful for avoiding invalid handlers.\n  static_assert(\n      std::is_invocable_r_v<XLA_FFI_Error *, T, XLA_FFI_CallFrame *>,\n      \"Encapsulated function must be and XLA FFI handler\");\n  return nb::capsule(reinterpret_cast<void *>(fn));\n}\n\nNB_MODULE(rms_norm, m) {\n  m.def(\"rms_norm\", []() { return EncapsulateFfiCall(RmsNorm); });\n}"
      },
      {
        "description": "Registers the handler using the nanobind extension.",
        "code": "# Assuming that we compiled a nanobind extension called `rms_norm`:\nimport rms_norm as rms_norm_lib\n\njax.ffi.register_ffi_target(\n    \"rms_norm\", rms_norm_lib.rms_norm(), platform=\"cpu\")\n# Assuming that we compiled a nanobind extension called `rms_norm`:\nimport rms_norm as rms_norm_lib\n\njax.ffi.register_ffi_target(\n    \"rms_norm\", rms_norm_lib.rms_norm(), platform=\"cpu\")"
      }
    ]
  },
  {
    "title": "Calling the FFI Function from JAX",
    "concepts": [
      "Calls the C++ library from JAX using the ffi_call() function.",
      "The target name in ffi_call() must match the target name used in register_custom_call_target.",
      "Attributes defined in the C++ wrapper are passed as keyword arguments to ffi_call().",
      "The vmap_method argument to ffi_call() defines how the FFI call interacts with vmap().",
      "The shape and dtype of the output array must be specified for the call.",
      "Dimension information is included with the Buffer objects, so dimensions no longer need to be computed in Python when lowering."
    ],
    "code_examples": [
      {
        "description": "Defines a function `rms_norm` that uses `jax.ffi.ffi_call` to invoke the C++ RMS normalization function.",
        "code": "import numpy as np\n\ndef rms_norm(x, eps=1e-5):\n  # We only implemented the `float32` version of this function, so we start by\n  # checking the dtype. This check isn't strictly necessary because type\n  # checking is also performed by the FFI when decoding input and output\n  # buffers, but it can be useful to check types in Python to raise more\n  # informative errors.\n  if x.dtype != jnp.float32:\n    raise ValueError(\"Only the float32 dtype is implemented by rms_norm\")\n\n  call = jax.ffi.ffi_call(\n      # The target name must be the same string as we used to register the target\n      # above in `register_custom_call_target`\n      \"rms_norm\",\n      # In this case, the output of our FFI function is just a single array with\n      # the same shape and dtype as the input. We discuss a case with a more\n      # interesting output type below.\n      jax.ShapeDtypeStruct(x.shape, x.dtype),\n      # The `vmap_method` parameter controls this function's behavior under `vmap`\n      # as discussed below.\n      vmap_method=\"broadcast_all\",\n  )\n\n  # Note that here we're use `numpy` (not `jax.numpy`) to specify a dtype for\n  # the attribute `eps`. Our FFI function expects this to have the C++ `float`\n  # type (which corresponds to numpy's `float32` type), and it must be a\n  # static parameter (i.e. not a JAX array).\n  return call(x, eps=np.float32(eps))\n\n\n# Test that this gives the same result as our reference implementation\nx = jnp.linspace(-0.5, 0.5, 32).reshape((8, 4))\nnp.testing.assert_allclose(rms_norm(x), rms_norm_ref(x), rtol=1e-5)\nimport numpy as np\n\ndef rms_norm(x, eps=1e-5):\n  # We only implemented the `float32` version of this function, so we start by\n  # checking the dtype. This check isn't strictly necessary because type\n  # checking is also performed by the FFI when decoding input and output\n  # buffers, but it can be useful to check types in Python to raise more\n  # informative errors.\n  if x.dtype != jnp.float32:\n    raise ValueError(\"Only the float32 dtype is implemented by rms_norm\")\n\n  call = jax.ffi.ffi_call(\n      # The target name must be the same string as we used to register the target\n      # above in `register_custom_call_target`\n      \"rms_norm\",\n      # In this case, the output of our FFI function is just a single array with\n      # the same shape and dtype as the input. We discuss a case with a more\n      # interesting output type below.\n      jax.ShapeDtypeStruct(x.shape, x.dtype),\n      # The `vmap_method` parameter controls this function's behavior under `vmap`\n      # as discussed below.\n      vmap_method=\"broadcast_all\",\n  )\n\n  # Note that here we're use `numpy` (not `jax.numpy`) to specify a dtype for\n  # the attribute `eps`. Our FFI function expects this to have the C++ `float`\n  # type (which corresponds to numpy's `float32` type), and it must be a\n  # static parameter (i.e. not a JAX array).\n  return call(x, eps=np.float32(eps))"
      }
    ]
  },
  {
    "title": "FFI and vmap()",
    "concepts": [
      "ffi_call() supports simple vmap() semantics using the vmap_method parameter.",
      "vmap_method='sequential' rewrites ffi_call as a scan() when vmapped.",
      "vmap_method='expand_dims' or 'broadcast_all' can be used when the foreign function handles batch dimensions.",
      "broadcast_all guarantees all inputs will be broadcasted to have the same batch dimensions.",
      "The implementation of rms_norm supports vmap with vmap_method='broadcast_all' out of the box."
    ],
    "code_examples": [
      {
        "description": "Demonstrates vmapping the RMS normalization function.",
        "code": "np.testing.assert_allclose(\n    jax.vmap(rms_norm)(x), jax.vmap(rms_norm_ref)(x), rtol=1e-5)\nnp.testing.assert_allclose(\n    jax.vmap(rms_norm)(x), jax.vmap(rms_norm_ref)(x), rtol=1e-5)"
      },
      {
        "description": "Inspects the jaxpr of the vmap() of rms_norm to confirm that it isn\u2019t being rewritten using scan().",
        "code": "jax.make_jaxpr(jax.vmap(rms_norm))(x)\njax.make_jaxpr(jax.vmap(rms_norm))(x)"
      }
    ]
  },
  {
    "title": "Introduction",
    "concepts": [
      "Training a simple neural network is demonstrated.",
      "A simple MLP is trained on MNIST using JAX.",
      "Tensorflow datasets data loading API is used to load images and labels.",
      "JAX can be used with any API that is compatible with NumPy."
    ],
    "code_examples": [
      {
        "description": "Import necessary JAX libraries.",
        "code": "import\njax.numpy\nas\njnp\nfrom\njax\nimport\ngrad\n,\njit\n,\nvmap\nfrom\njax\nimport\nrandom\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\ngrad\n,\njit\n,\nvmap\nfrom\njax\nimport\nrandom"
      }
    ]
  },
  {
    "title": "Neural Network Initialization",
    "concepts": [
      "A helper function randomly initializes weights and biases for a dense neural network layer.",
      "The network parameters are initialized for a fully-connected neural network with specified sizes.",
      "Layer sizes, step size, number of epochs, batch size, and number of targets are defined.",
      "The `random_layer_params` function initializes weights and biases.",
      "The `init_network_params` function initializes all layers of the network."
    ],
    "code_examples": [
      {
        "description": "Helper function to randomly initialize weights and biases for a dense neural network layer.",
        "code": "def\nrandom_layer_params\n(\n    m\n    ,\n    n\n    ,\n    key\n    ,\n    scale\n    =\n    1e-2\n):\n    w_key\n    ,\n    b_key\n    =\n    random\n    .\n    split\n    (\n        key\n    )\n    return\n    scale\n    *\n    random\n    .\n    normal\n    (\n        w_key\n        ,\n        (\n            n\n            ,\n            m\n        )\n    )\n    ,\n    scale\n    *\n    random\n    .\n    normal\n    (\n        b_key\n        ,\n        (\n            n\n            ,\n        )\n    )"
      },
      {
        "description": "Initialize all layers for a fully-connected neural network with sizes \"sizes\".",
        "code": "def\ninit_network_params\n(\n    sizes\n    ,\n    key\n):\n    keys\n    =\n    random\n    .\n    split\n    (\n        key\n        ,\n        len\n        (\n            sizes\n        )\n    )\n    return\n    [\n        random_layer_params\n        (\n            m\n            ,\n            n\n            ,\n            k\n        )\n        for\n        m\n        ,\n        n\n        ,\n        k\n        in\n        zip\n        (\n            sizes\n            [:\n            -\n            1\n            ]\n            ,\n            sizes\n            [\n                1\n            :\n            ]\n            ,\n            keys\n        )\n    ]"
      },
      {
        "description": "Defining layer sizes, step size, and initializing network parameters.",
        "code": "layer_sizes\n=\n[\n    784\n    ,\n    512\n    ,\n    512\n    ,\n    10\n]\nstep_size\n=\n0.01\nnum_epochs\n=\n10\nbatch_size\n=\n128\nn_targets\n=\n10\nparams\n=\ninit_network_params\n(\n    layer_sizes\n    ,\n    random\n    .\n    key\n    (\n        0\n    )\n)"
      }
    ]
  },
  {
    "title": "Prediction Function",
    "concepts": [
      "The ReLU activation function is defined.",
      "The `predict` function defines the neural network prediction for a single image example.",
      "JAX's vmap function is used to handle mini-batches automatically.",
      "The `logsumexp` function is used for numerical stability."
    ],
    "code_examples": [
      {
        "description": "Import logsumexp from jax.scipy.special.",
        "code": "from\njax.scipy.special\nimport\nlogsumexp"
      },
      {
        "description": "Define the ReLU activation function.",
        "code": "def\nrelu\n(\n    x\n):\n    return\n    jnp\n    .\n    maximum\n    (\n        0\n        ,\n        x\n    )"
      },
      {
        "description": "Define the predict function for a single image example.",
        "code": "def\npredict\n(\n    params\n    ,\n    image\n):\n    # per-example predictions\n    activations\n    =\n    image\n    for\n    w\n    ,\n    b\n    in\n    params\n    [:\n    -\n    1\n    ]:\n        outputs\n        =\n        jnp\n        .\n        dot\n        (\n            w\n            ,\n            activations\n        )\n        +\n        b\n        activations\n        =\n        relu\n        (\n            outputs\n        )\n    final_w\n    ,\n    final_b\n    =\n    params\n    [\n        -\n        1\n    ]\n    logits\n    =\n    jnp\n    .\n    dot\n    (\n        final_w\n        ,\n        activations\n    )\n    +\n    final_b\n    return\n    logits\n    -\n    logsumexp\n    (\n        logits\n    )"
      }
    ]
  },
  {
    "title": "Batch Prediction with vmap",
    "concepts": [
      "The `predict` function is tested to work on single images.",
      "The `predict` function is shown not to work with batches directly.",
      "The `vmap` function is used to create a batched version of the `predict` function.",
      "`vmap` automatically handles mini-batches with no performance penalty."
    ],
    "code_examples": [
      {
        "description": "Test the predict function on a single example.",
        "code": "random_flattened_image\n=\nrandom\n.\nnormal\n(\n    random\n    .\n    key\n    (\n        1\n    )\n    ,\n    (\n        28\n        *\n        28\n        ,\n    )\n)\npreds\n=\npredict\n(\n    params\n    ,\n    random_flattened_image\n)\nprint\n(\n    preds\n    .\n    shape\n)"
      },
      {
        "description": "Demonstrates the predict function does not work on a batch of images without using vmap.",
        "code": "random_flattened_images\n=\nrandom\n.\nnormal\n(\n    random\n    .\n    key\n    (\n        1\n    )\n    ,\n    (\n        10\n        ,\n        28\n        *\n        28\n    )\n)\ntry:\n    preds\n    =\n    predict\n    (\n        params\n        ,\n        random_flattened_images\n    )\nexcept\nTypeError:\n    print\n    (\n        'Invalid shapes!'\n    )"
      },
      {
        "description": "Creates a batched version of the predict function using vmap.",
        "code": "batched_predict\n=\nvmap\n(\n    predict\n    ,\n    in_axes\n    =\n    (\n        None\n        ,\n        0\n    )\n)\nbatched_preds\n=\nbatched_predict\n(\n    params\n    ,\n    random_flattened_images\n)\nprint\n(\n    batched_preds\n    .\n    shape\n)"
      }
    ]
  },
  {
    "title": "Loss Function and Training",
    "concepts": [
      "The `one_hot` function creates a one-hot encoding of labels.",
      "The `accuracy` function calculates the accuracy of predictions.",
      "The `loss` function calculates the loss value.",
      "The `update` function calculates the gradients and updates the network parameters.",
      "JIT is used to speed up the training process."
    ],
    "code_examples": [
      {
        "description": "Create a one-hot encoding of x of size k.",
        "code": "def\none_hot\n(\n    x\n    ,\n    k\n    ,\n    dtype\n    =\n    jnp\n    .\n    float32\n):\n    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n    return\n    jnp\n    .\n    array\n    (\n        x\n        [:\n        ,\n        None\n        ]\n        ==\n        jnp\n        .\n        arange\n        (\n            k\n        )\n        ,\n        dtype\n    )"
      },
      {
        "description": "Calculate the accuracy of the model.",
        "code": "def\naccuracy\n(\n    params\n    ,\n    images\n    ,\n    targets\n):\n    target_class\n    =\n    jnp\n    .\n    argmax\n    (\n        targets\n        ,\n        axis\n        =\n        1\n    )\n    predicted_class\n    =\n    jnp\n    .\n    argmax\n    (\n        batched_predict\n        (\n            params\n            ,\n            images\n        )\n        ,\n        axis\n        =\n        1\n    )\n    return\n    jnp\n    .\n    mean\n    (\n        predicted_class\n        ==\n        target_class\n    )"
      },
      {
        "description": "Define the loss function.",
        "code": "def\nloss\n(\n    params\n    ,\n    images\n    ,\n    targets\n):\n    preds\n    =\n    batched_predict\n    (\n        params\n        ,\n        images\n    )\n    return\n    -\n    jnp\n    .\n    mean\n    (\n        preds\n        *\n        targets\n    )"
      },
      {
        "description": "Define the update step to update network parameters based on gradients.",
        "code": "@jit\ndef\nupdate\n(\n    params\n    ,\n    x\n    ,\n    y\n):\n    grads\n    =\n    grad\n    (\n        loss\n    )\n    (\n        params\n        ,\n        x\n        ,\n        y\n    )\n    return\n    [\n        (\n            w\n            -\n            step_size\n            *\n            dw\n            ,\n            b\n            -\n            step_size\n            *\n            db\n        )\n        for\n        (\n            w\n            ,\n            b\n        )\n        ,\n        (\n            dw\n            ,\n            db\n        )\n        in\n        zip\n        (\n            params\n            ,\n            grads\n        )\n    ]"
      }
    ]
  },
  {
    "title": "Data Loading with TensorFlow Datasets",
    "concepts": [
      "TensorFlow Datasets (tfds) is used for data loading.",
      "GPU visibility is configured for TensorFlow.",
      "The MNIST dataset is loaded using tfds.",
      "The dataset is converted to NumPy arrays.",
      "Training and testing data are extracted and reshaped.",
      "Labels are converted to one-hot encoding."
    ],
    "code_examples": [
      {
        "description": "Import TensorFlow and TensorFlow Datasets.",
        "code": "import\ntensorflow\nas\ntf\n# Ensure TF does not see GPU and grab all GPU memory.\ntf\n.\nconfig\n.\nset_visible_devices\n(\n    [\n    ]\n    ,\n    device_type\n    =\n    'GPU'\n)\nimport\ntensorflow_datasets\nas\ntfds"
      },
      {
        "description": "Load the MNIST dataset using TensorFlow Datasets and convert it to numpy arrays.",
        "code": "data_dir\n=\n'/tmp/tfds'\n# Fetch full datasets for evaluation\n# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\nmnist_data\n,\ninfo\n=\ntfds\n.\nload\n(\n    name\n    =\n    \"mnist\"\n    ,\n    batch_size\n    =\n    -\n    1\n    ,\n    data_dir\n    =\n    data_dir\n    ,\n    with_info\n    =\n    True\n)\nmnist_data\n=\ntfds\n.\nas_numpy\n(\n    mnist_data\n)\ntrain_data\n,\ntest_data\n=\nmnist_data\n[\n    'train'\n],\nmnist_data\n[\n    'test'\n]\nnum_labels\n=\ninfo\n.\nfeatures\n[\n    'label'\n]\n.\nnum_classes\nh\n,\nw\n,\nc\n=\ninfo\n.\nfeatures\n[\n    'image'\n]\n.\nshape\nnum_pixels\n=\nh\n*\nw\n*\nc\n# Full train set\ntrain_images\n,\ntrain_labels\n=\ntrain_data\n[\n    'image'\n],\ntrain_data\n[\n    'label'\n]\ntrain_images\n=\njnp\n.\nreshape\n(\n    train_images\n    ,\n    (\n        len\n        (\n            train_images\n        )\n        ,\n        num_pixels\n    )\n)\ntrain_labels\n=\none_hot\n(\n    train_labels\n    ,\n    num_labels\n)\n# Full test set\ntest_images\n,\ntest_labels\n=\ntest_data\n[\n    'image'\n],\ntest_data\n[\n    'label'\n]\ntest_images\n=\njnp\n.\nreshape\n(\n    test_images\n    ,\n    (\n        len\n        (\n            test_images\n        )\n        ,\n        num_pixels\n    )\n)\ntest_labels\n=\none_hot\n(\n    test_labels\n    ,\n    num_labels\n)"
      }
    ]
  },
  {
    "title": "Training Loop",
    "concepts": [
      "The training loop iterates through epochs and batches.",
      "The `get_train_batches` function retrieves training batches from TensorFlow Datasets.",
      "Images are reshaped and labels are converted to one-hot encoding within the loop.",
      "The `update` function is called to update network parameters.",
      "Training and test accuracy are calculated and printed after each epoch.",
      "The time taken for each epoch is also tracked."
    ],
    "code_examples": [
      {
        "description": "Function to get training batches using TensorFlow Datasets.",
        "code": "import\ntime\ndef\nget_train_batches\n():\n    # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n    ds\n    =\n    tfds\n    .\n    load\n    (\n        name\n        =\n        'mnist'\n        ,\n        split\n        =\n        'train'\n        ,\n        as_supervised\n        =\n        True\n        ,\n        data_dir\n        =\n        data_dir\n    )\n    # You can build up an arbitrary tf.data input pipeline\n    ds\n    =\n    ds\n    .\n    batch\n    (\n        batch_size\n    )\n    .\n    prefetch\n    (\n        1\n    )\n    # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n    return\n    tfds\n    .\n    as_numpy\n    (\n        ds\n    )"
      },
      {
        "description": "Main training loop.",
        "code": "for\nepoch\nin\nrange\n(\n    num_epochs\n):\n    start_time\n    =\n    time\n    .\n    time\n    ()\n    for\n    x\n    ,\n    y\n    in\n    get_train_batches\n    ():\n        x\n        =\n        jnp\n        .\n        reshape\n        (\n            x\n            ,\n            (\n                len\n                (\n                    x\n                )\n                ,\n                num_pixels\n            )\n        )\n        y\n        =\n        one_hot\n        (\n            y\n            ,\n            num_labels\n        )\n        params\n        =\n        update\n        (\n            params\n            ,\n            x\n            ,\n            y\n        )\n    epoch_time\n    =\n    time\n    .\n    time\n    ()\n    -\n    start_time\n    train_acc\n    =\n    accuracy\n    (\n        params\n        ,\n        train_images\n        ,\n        train_labels\n    )\n    test_acc\n    =\n    accuracy\n    (\n        params\n        ,\n        test_images\n        ,\n        test_labels\n    )\n    print\n    (\n        \"Epoch\n        {}\n        in\n        {:0.2f}\n        sec\"\n        .\n        format\n        (\n            epoch\n            ,\n            epoch_time\n        )\n    )\n    print\n    (\n        \"Training set accuracy\n        {}\n        \"\n        .\n        format\n        (\n            train_acc\n        )\n    )\n    print\n    (\n        \"Test set accuracy\n        {}\n        \"\n        .\n        format\n        (\n            test_acc\n        )\n    )"
      }
    ]
  },
  {
    "title": "Conclusion",
    "concepts": [
      "JAX API is used for derivatives, speedups, and auto-vectorization.",
      "NumPy is used to specify the computation.",
      "TensorFlow Datasets is used for data loading.",
      "The entire process runs on the GPU."
    ],
    "code_examples": []
  },
  {
    "title": "Import Statements",
    "concepts": [
      "Importing JAX's NumPy implementation as jnp.",
      "Importing grad, jit, and vmap from the jax library.",
      "Importing the random module from the jax library."
    ],
    "code_examples": [
      {
        "description": "Importing necessary JAX modules",
        "code": "import\njax.numpy\nas\njnp\nfrom\njax\nimport\ngrad\n,\njit\n,\nvmap\nfrom\njax\nimport\nrandom"
      }
    ]
  },
  {
    "title": "Network Initialization",
    "concepts": [
      "Defining a function to randomly initialize weights and biases for a dense neural network layer.",
      "Defining a function to initialize all layers for a fully-connected neural network with specified sizes.",
      "Defining network parameters like layer sizes, step size, number of epochs, and batch size.",
      "Initializing network parameters."
    ],
    "code_examples": [
      {
        "description": "Function to randomly initialize weights and biases for a dense neural network layer",
        "code": "def\nrandom_layer_params\n(\nm\n,\nn\n,\nkey\n,\nscale\n=\n1e-2\n):\nw_key\n,\nb_key\n=\nrandom\n.\nsplit\n(\nkey\n)\nreturn\nscale\n*\nrandom\n.\nnormal\n(\nw_key\n,\n(\nn\n,\nm\n))\n,\nscale\n*\nrandom\n.\nnormal\n(\nb_key\n,\n(\nn\n,))"
      },
      {
        "description": "Function to initialize all layers for a fully-connected neural network",
        "code": "def\ninit_network_params\n(\nsizes\n,\nkey\n):\nkeys\n=\nrandom\n.\nsplit\n(\nkey\n,\nlen\n(\nsizes\n))\nreturn\n[\nrandom_layer_params\n(\nm\n,\nn\n,\nk\n)\nfor\nm\n,\nn\n,\nk\nin\nzip\n(\nsizes\n[:\n-\n1\n],\nsizes\n[\n1\n:],\nkeys\n)]"
      },
      {
        "description": "Initializing network parameters",
        "code": "layer_sizes\n=\n[\n784\n,\n512\n,\n512\n,\n10\n]\nstep_size\n=\n0.01\nnum_epochs\n=\n8\nbatch_size\n=\n128\nn_targets\n=\n10\nparams\n=\ninit_network_params\n(\nlayer_sizes\n,\nrandom\n.\nkey\n(\n0\n))"
      }
    ]
  },
  {
    "title": "Prediction Function",
    "concepts": [
      "Defining the ReLU activation function.",
      "Defining the prediction function for a single image example.",
      "The prediction function calculates activations, applies ReLU, and computes logits."
    ],
    "code_examples": [
      {
        "description": "ReLU activation function",
        "code": "def\nrelu\n(\nx\n):\nreturn\njnp\n.\nmaximum\n(\n0\n,\nx\n)"
      },
      {
        "description": "Prediction function for a single image example",
        "code": "from\njax.scipy.special\nimport\nlogsumexp\ndef\npredict\n(\nparams\n,\nimage\n):\n# per-example predictions\nactivations\n=\nimage\nfor\nw\n,\nb\nin\nparams\n[:\n-\n1\n]:\noutputs\n=\njnp\n.\ndot\n(\nw\n,\nactivations\n)\n+\nb\nactivations\n=\nrelu\n(\noutputs\n)\nfinal_w\n,\nfinal_b\n=\nparams\n[\n-\n1\n]\nlogits\n=\njnp\n.\ndot\n(\nfinal_w\n,\nactivations\n)\n+\nfinal_b\nreturn\nlogits\n-\nlogsumexp\n(\nlogits\n)"
      }
    ]
  },
  {
    "title": "Batching with vmap",
    "concepts": [
      "Demonstrating the prediction function works on single images.",
      "Showing that the prediction function does not work with batches without modification.",
      "Upgrading the prediction function to handle batches using `vmap`.",
      "The `vmap` function automatically handles mini-batches."
    ],
    "code_examples": [
      {
        "description": "Prediction on single example",
        "code": "random_flattened_image\n=\nrandom\n.\nnormal\n(\nrandom\n.\nkey\n(\n1\n),\n(\n28\n*\n28\n,))\npreds\n=\npredict\n(\nparams\n,\nrandom_flattened_image\n)\nprint\n(\npreds\n.\nshape\n)"
      },
      {
        "description": "Attempting prediction on a batch without vmap",
        "code": "random_flattened_images\n=\nrandom\n.\nnormal\n(\nrandom\n.\nkey\n(\n1\n),\n(\n10\n,\n28\n*\n28\n))\ntry\n:\npreds\n=\npredict\n(\nparams\n,\nrandom_flattened_images\n)\nexcept\nTypeError\n:\nprint\n(\n'Invalid shapes!'\n)"
      },
      {
        "description": "Using vmap to create a batched prediction function",
        "code": "# Let's upgrade it to handle batches using `vmap`\n# Make a batched version of the `predict` function\nbatched_predict\n=\nvmap\n(\npredict\n,\nin_axes\n=\n(\nNone\n,\n0\n))\n# `batched_predict` has the same call signature as `predict`\nbatched_preds\n=\nbatched_predict\n(\nparams\n,\nrandom_flattened_images\n)\nprint\n(\nbatched_preds\n.\nshape\n)"
      }
    ]
  },
  {
    "title": "Loss, Accuracy, and Update Functions",
    "concepts": [
      "Defining a one-hot encoding function.",
      "Defining an accuracy calculation function.",
      "Defining a loss function using the batched prediction function.",
      "Defining an update function that computes gradients and updates parameters using JAX's jit."
    ],
    "code_examples": [
      {
        "description": "One-hot encoding function",
        "code": "def\none_hot\n(\nx\n,\nk\n,\ndtype\n=\njnp\n.\nfloat32\n):\n\"\"\"Create a one-hot encoding of x of size k.\"\"\"\nreturn\njnp\n.\narray\n(\nx[:,\nNone\n]\n==\njnp\n.\narange\n(\nk\n),\ndtype\n)"
      },
      {
        "description": "Accuracy calculation function",
        "code": "def\naccuracy\n(\nparams\n,\nimages\n,\ntargets\n):\ntarget_class\n=\njnp\n.\nargmax\n(\ntargets\n,\naxis\n=\n1\n)\npredicted_class\n=\njnp\n.\nargmax\n(\nbatched_predict\n(\nparams\n,\nimages\n),\naxis\n=\n1\n)\nreturn\njnp\n.\nmean\n(\npredicted_class\n==\ntarget_class\n)"
      },
      {
        "description": "Loss function",
        "code": "def\nloss\n(\nparams\n,\nimages\n,\ntargets\n):\npreds\n=\nbatched_predict\n(\nparams\n,\nimages\n)\nreturn\n-\njnp\n.\nmean\n(\npreds\n*\ntargets\n)"
      },
      {
        "description": "Update function using grad and jit",
        "code": "@jit\ndef\nupdate\n(\nparams\n,\nx\n,\ny\n):\ngrads\n=\ngrad\n(\nloss\n)(\nparams\n,\nx\n,\ny\n)\nreturn\n[(\nw\n-\nstep_size\n*\ndw\n,\nb\n-\nstep_size\n*\ndb\n)\nfor\n(\nw\n,\nb\n),\n(\ndw\n,\ndb\n)\nin\nzip\n(\nparams\n,\ngrads\n)]"
      }
    ]
  },
  {
    "title": "Data Loading with PyTorch",
    "concepts": [
      "Installing PyTorch and TorchVision.",
      "Creating a custom data loader using PyTorch's data loading API to load images and labels.",
      "Defining a collate function to convert PyTorch batches to NumPy arrays.",
      "Creating a custom DataLoader class (NumpyLoader) based on PyTorch's DataLoader, which outputs numpy arrays.",
      "Defining a transformation to flatten and cast images to jnp.float32.",
      "Downloading the MNIST dataset using torchvision.",
      "Creating a NumPy-based training data loader (training_generator).",
      "Getting the full training and testing datasets as NumPy arrays."
    ],
    "code_examples": [
      {
        "description": "Installing PyTorch and TorchVision",
        "code": "!\npip\ninstall\ntorch\ntorchvision"
      },
      {
        "description": "Defining a custom collate function",
        "code": "import\nnumpy\nas\nnp\nfrom\njax.tree_util\nimport\ntree_map\nfrom\ntorch.utils\nimport\ndata\nfrom\ntorchvision.datasets\nimport\nMNIST\ndef\nnumpy_collate\n(\nbatch\n):\nreturn\ntree_map\n(\nnp\n.\nasarray\n,\ndata\n.\ndefault_collate\n(\nbatch\n))"
      },
      {
        "description": "Defining a custom data loader (NumpyLoader)",
        "code": "class\nNumpyLoader\n(\ndata\n.\nDataLoader\n):\ndef\n__init__\n(\nself\n,\ndataset\n,\nbatch_size\n=\n1\n,\nshuffle\n=\nFalse\n,\nsampler\n=\nNone\n,\nbatch_sampler\n=\nNone\n,\nnum_workers\n=\n0\n,\npin_memory\n=\nFalse\n,\ndrop_last\n=\nFalse\n,\ntimeout\n=\n0\n,\nworker_init_fn\n=\nNone\n):\nsuper\n(\nself\n.\n__class__\n,\nself\n)\n.\n__init__\n(\ndataset\n,\nbatch_size\n=\nbatch_size\n,\nshuffle\n=\nshuffle\n,\nsampler\n=\nsampler\n,\nbatch_sampler\n=\nbatch_sampler\n,\nnum_workers\n=\nnum_workers\n,\ncollate_fn\n=\nnumpy_collate\n,\npin_memory\n=\npin_memory\n,\ndrop_last\n=\ndrop_last\n,\ntimeout\n=\ntimeout\n,\nworker_init_fn\n=\nworker_init_fn\n)"
      },
      {
        "description": "Defining a transformation to flatten and cast images",
        "code": "class\nFlattenAndCast\n(\nobject\n):\ndef\n__call__\n(\nself\n,\npic\n):\nreturn\nnp\n.\nravel\n(\nnp\n.\narray\n(\npic\n,\ndtype\n=\njnp\n.\nfloat32\n))\n"
      },
      {
        "description": "Loading the MNIST dataset using torchvision and the custom DataLoader",
        "code": "# Define our dataset, using torch datasets\nmnist_dataset\n=\nMNIST\n(\n'/tmp/mnist/'\n,\ndownload\n=\nTrue\n,\ntransform\n=\nFlattenAndCast\n())\ntraining_generator\n=\nNumpyLoader\n(\nmnist_dataset\n,\nbatch_size\n=\nbatch_size\n,\nnum_workers\n=\n0\n)"
      },
      {
        "description": "Getting the full training and testing datasets as NumPy arrays",
        "code": "# Get the full train dataset (for checking accuracy while training)\ntrain_images\n=\nnp\n.\narray\n(\nmnist_dataset\n.\ntrain_data\n)\n.\nreshape\n(\nlen\n(\nmnist_dataset\n.\ntrain_data\n),\n-\n1\n)\ntrain_labels\n=\none_hot\n(\nnp\n.\narray\n(\nmnist_dataset\n.\ntrain_labels\n),\nn_targets\n)\n# Get full test dataset\nmnist_dataset_test\n=\nMNIST\n(\n'/tmp/mnist/'\n,\ndownload\n=\nTrue\n,\ntrain\n=\nFalse\n)\ntest_images\n=\njnp\n.\narray\n(\nmnist_dataset_test\n.\ntest_data\n.\nnumpy\n()\n.\nreshape\n(\nlen\n(\nmnist_dataset_test\n.\ntest_data\n),\n-\n1\n),\ndtype\n=\njnp\n.\nfloat32\n)\ntest_labels\n=\none_hot\n(\nnp\n.\narray\n(\nmnist_dataset_test\n.\ntest_labels\n),\nn_targets\n)"
      }
    ]
  },
  {
    "title": "Training Loop",
    "concepts": [
      "Iterating through epochs and batches.",
      "Updating the network parameters using the update function.",
      "Calculating and printing the training and test accuracy after each epoch.",
      "Measuring the time taken for each epoch."
    ],
    "code_examples": [
      {
        "description": "Training loop",
        "code": "import\ntime\nfor\nepoch\nin\nrange\n(\nnum_epochs\n):\nstart_time\n=\ntime\n.\ntime\n()\nfor\nx\n,\ny\nin\ntraining_generator\n:\ny\n=\none_hot\n(\ny\n,\nn_targets\n)\nparams\n=\nupdate\n(\nparams\n,\nx\n,\ny\n)\nepoch_time\n=\ntime\n.\ntime\n()\n-\nstart_time\ntrain_acc\n=\naccuracy\n(\nparams\n,\ntrain_images\n,\ntrain_labels\n)\ntest_acc\n=\naccuracy\n(\nparams\n,\ntest_images\n,\ntest_labels\n)\nprint\n(\n\"Epoch\n{}\nin\n{:0.2f}\nsec\"\n.\nformat\n(\nepoch\n,\nepoch_time\n))\nprint\n(\n\"Training set accuracy\n{}\n\"\n.\nformat\n(\ntrain_acc\n))\nprint\n(\n\"Test set accuracy\n{}\n\"\n.\nformat\n(\ntest_acc\n))"
      }
    ]
  },
  {
    "title": "Summary",
    "concepts": [
      "The example used JAX's grad, jit, and vmap for training.",
      "NumPy was used to specify computation.",
      "PyTorch's data loaders were used for data loading.",
      "The training was run on the GPU."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction and Setup",
    "concepts": [
      "Demonstrates Bayesian inference with autobatching.",
      "Uses JAX for numerical computation and automatic differentiation.",
      "Defines the number of features and data points.",
      "Generates synthetic data for logistic regression."
    ],
    "code_examples": [
      {
        "description": "Import necessary libraries and set a random seed for reproducibility.",
        "code": "import matplotlib.pyplot as plt\nimport jax\nimport jax.numpy as jnp\nimport jax.scipy as jsp\nfrom jax import random\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport jax\nimport jax.numpy as jnp\nimport jax.scipy as jsp\nfrom jax import random\nimport numpy as np\nimport scipy as sp\nnp.random.seed(10009)"
      },
      {
        "description": "Define the number of features, data points, and true beta values. Also, create input data 'all_x' and labels 'y' using a logistic model.",
        "code": "num_features = 10\nnum_points = 100\ntrue_beta = np.random.randn(num_features).astype(jnp.float32)\nall_x = np.random.randn(num_points, num_features).astype(jnp.float32)\ny = (np.random.rand(num_points) < sp.special.expit(all_x.dot(true_beta))).astype(jnp.int32)\nnp.random.seed(10009)\nnum_features = 10\nnum_points = 100\ntrue_beta = np.random.randn(num_features).astype(jnp.float32)\nall_x = np.random.randn(num_points, num_features).astype(jnp.float32)\ny = (np.random.rand(num_points) < sp.special.expit(all_x.dot(true_beta))).astype(jnp.int32)"
      }
    ]
  },
  {
    "title": "Non-Batched Log Joint Probability",
    "concepts": [
      "Defines the log joint probability function for a single sample.",
      "Calculates the log probability of the beta prior using a normal distribution.",
      "Calculates the log probability of the data likelihood using a logistic model.",
      "Does not handle batched inputs."
    ],
    "code_examples": [
      {
        "description": "Defines the log joint probability function.",
        "code": "def log_joint(beta):\n    result = 0.\n    # Note that no `axis` parameter is provided to `jnp.sum`.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=1.))\n    result = result + jnp.sum(- jnp.log(1 + jnp.exp(- (2 * y - 1) * jnp.dot(all_x, beta))))\n    return result\ndef log_joint(beta):\n    result = 0.\n    # Note that no `axis` parameter is provided to `jnp.sum`.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=1.))\n    result = result + jnp.sum(- jnp.log(1 + jnp.exp(- (2 * y - 1) * jnp.dot(all_x, beta))))\n    return result"
      },
      {
        "description": "Demonstrates the usage of the log_joint function with a single sample.",
        "code": "log_joint(np.random.randn(num_features))\nlog_joint(np.random.randn(num_features))"
      },
      {
        "description": "Attempts to use the non-batched log_joint function with batched inputs, which results in a ValueError.",
        "code": "try:\n    batch_size = 10\n    batched_test_beta = np.random.randn(batch_size, num_features)\n    log_joint(np.random.randn(batch_size, num_features))\nexcept ValueError as e:\n    print(\"Caught expected exception \" + str(e))\ntry:\n    batch_size = 10\n    batched_test_beta = np.random.randn(batch_size, num_features)\n    log_joint(np.random.randn(batch_size, num_features))\nexcept ValueError as e:\n    print(\"Caught expected exception \" + str(e))"
      }
    ]
  },
  {
    "title": "Manually Batched Log Joint Probability",
    "concepts": [
      "Defines a batched log joint probability function.",
      "Requires explicit handling of axes and transposes for batched inputs.",
      "Can be error-prone due to manual batching implementation."
    ],
    "code_examples": [
      {
        "description": "Defines the batched log joint probability function.",
        "code": "def batched_log_joint(beta):\n    result = 0.\n    # Here (and below) `sum` needs an `axis` parameter. At best, forgetting to set axis\n    # or setting it incorrectly yields an error; at worst, it silently changes the\n    # semantics of the model.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=1.), axis=-1)\n    # Note the multiple transposes. Getting this right is not rocket science,\n    # but it's also not totally mindless. (I didn't get it right on the first\n    # try.)\n    result = result + jnp.sum(- jnp.log(1 + jnp.exp(- (2 * y - 1) * jnp.dot(all_x, beta.T).T)), axis=-1)\n    return result\ndef batched_log_joint(beta):\n    result = 0.\n    # Here (and below) `sum` needs an `axis` parameter. At best, forgetting to set axis\n    # or setting it incorrectly yields an error; at worst, it silently changes the\n    # semantics of the model.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=1.), axis=-1)\n    # Note the multiple transposes. Getting this right is not rocket science,\n    # but it's also not totally mindless. (I didn't get it right on the first\n    # try.)\n    result = result + jnp.sum(- jnp.log(1 + jnp.exp(- (2 * y - 1) * jnp.dot(all_x, beta.T).T)), axis=-1)\n    return result"
      },
      {
        "description": "Demonstrates the usage of the batched_log_joint function with a batch of inputs.",
        "code": "batch_size = 10\nbatched_test_beta = np.random.randn(batch_size, num_features)\nbatched_log_joint(batched_test_beta)\nbatch_size = 10\nbatched_test_beta = np.random.randn(batch_size, num_features)\nbatched_log_joint(batched_test_beta)"
      }
    ]
  },
  {
    "title": "Autobatching with `jax.vmap`",
    "concepts": [
      "Uses `jax.vmap` for automatic vectorization of the log joint probability function.",
      "`jax.vmap` applies a function over an axis of the input array.",
      "Simplifies the code and reduces the risk of errors compared to manual batching."
    ],
    "code_examples": [
      {
        "description": "Applies vmap to the non-batched log_joint to create a batched version.",
        "code": "vmap_batched_log_joint = jax.vmap(log_joint)\nvmap_batched_log_joint(batched_test_beta)\nvmap_batched_log_joint = jax.vmap(log_joint)\nvmap_batched_log_joint(batched_test_beta)"
      }
    ]
  },
  {
    "title": "Variational Inference with Autobatching and JIT",
    "concepts": [
      "Defines an ELBO (Evidence Lower Bound) objective for variational inference.",
      "Uses `jax.jit` to compile the log joint probability function and ELBO for faster execution.",
      "Uses a reparameterization trick for gradient estimation.",
      "Demonstrates a basic training loop for variational parameters."
    ],
    "code_examples": [
      {
        "description": "Defines the log_joint function again, this time using jit and a different prior scale. Then, creates the batched log joint using vmap and jit.",
        "code": "@jax.jit\ndef log_joint(beta):\n    result = 0.\n    # Note that no `axis` parameter is provided to `jnp.sum`.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=10.))\n    result = result + jnp.sum(- jnp.log(1 + jnp.exp(- (2 * y - 1) * jnp.dot(all_x, beta))))\n    return result\nbatched_log_joint = jax.jit(jax.vmap(log_joint))\n@jax.jit\ndef log_joint(beta):\n    result = 0.\n    # Note that no `axis` parameter is provided to `jnp.sum`.\n    result = result + jnp.sum(jsp.stats.norm.logpdf(beta, loc=0., scale=10.))\n    result = result + jnp.sum(- jnp.log(1 + jnp.exp(- (2 * y - 1) * jnp.dot(all_x, beta))))\n    return result\nbatched_log_joint = jax.jit(jax.vmap(log_joint))"
      },
      {
        "description": "Defines the ELBO function for variational inference.",
        "code": "def elbo(beta_loc, beta_log_scale, epsilon):\n    beta_sample = beta_loc + jnp.exp(beta_log_scale) * epsilon\n    return jnp.mean(batched_log_joint(beta_sample), 0) + jnp.sum(beta_log_scale - 0.5 * np.log(2 * np.pi))\nelbo = jax.jit(elbo)\nelbo_val_and_grad = jax.jit(jax.value_and_grad(elbo, argnums=(0, 1)))\ndef elbo(beta_loc, beta_log_scale, epsilon):\n    beta_sample = beta_loc + jnp.exp(beta_log_scale) * epsilon\n    return jnp.mean(batched_log_joint(beta_sample), 0) + jnp.sum(beta_log_scale - 0.5 * np.log(2 * np.pi))\nelbo = jax.jit(elbo)\nelbo_val_and_grad = jax.jit(jax.value_and_grad(elbo, argnums=(0, 1)))"
      },
      {
        "description": "Defines a function for sampling from a normal distribution using JAX's random number generation.",
        "code": "def normal_sample(key, shape):\n    \"\"\"Convenience function for quasi-stateful RNG.\"\"\"\n    new_key, sub_key = random.split(key)\n    return new_key, random.normal(sub_key, shape)\nnormal_sample = jax.jit(normal_sample, static_argnums=(1,))"
      },
      {
        "description": "Sets up and runs a simple training loop to optimize the variational parameters.",
        "code": "key = random.key(10003)\nbeta_loc = jnp.zeros(num_features, jnp.float32)\nbeta_log_scale = jnp.zeros(num_features, jnp.float32)\nstep_size = 0.01\nbatch_size = 128\nepsilon_shape = (batch_size, num_features)\nfor i in range(1000):\n    key, epsilon = normal_sample(key, epsilon_shape)\n    elbo_val, (beta_loc_grad, beta_log_scale_grad) = elbo_val_and_grad(beta_loc, beta_log_scale, epsilon)\n    beta_loc += step_size * beta_loc_grad\n    beta_log_scale += step_size * beta_log_scale_grad\n    if i % 10 == 0:\n        print('{}\t{}'.format(i, elbo_val))\nkey = random.key(10003)\nbeta_loc = jnp.zeros(num_features, jnp.float32)\nbeta_log_scale = jnp.zeros(num_features, jnp.float32)\nstep_size = 0.01\nbatch_size = 128\nepsilon_shape = (batch_size, num_features)\nfor i in range(1000):\n    key, epsilon = normal_sample(key, epsilon_shape)\n    elbo_val, (beta_loc_grad, beta_log_scale_grad) = elbo_val_and_grad(beta_loc, beta_log_scale, epsilon)\n    beta_loc += step_size * beta_loc_grad\n    beta_log_scale += step_size * beta_log_scale_grad\n    if i % 10 == 0:\n        print('{}\t{}'.format(i, elbo_val))"
      }
    ]
  },
  {
    "title": "Visualization of Results",
    "concepts": [
      "Plots the true beta values against the approximated posterior means.",
      "Visualizes the uncertainty in the approximated posterior using error bars (2 standard deviations).",
      "Compares the estimated beta to the true beta.",
      "Evaluates the coverage of the variational inference method."
    ],
    "code_examples": [
      {
        "description": "Plots the true beta against the estimated beta with error bars.",
        "code": "plt.figure(figsize=(7, 7))\nplt.plot(true_beta, beta_loc, '.', label='Approximated Posterior Means')\nplt.plot(true_beta, beta_loc + 2 * jnp.exp(beta_log_scale), 'r.', label=r'Approximated Posterior $2\\sigma$ Error Bars')\nplt.plot(true_beta, beta_loc - 2 * jnp.exp(beta_log_scale), 'r.')\nplot_scale = 3\nplt.plot([-plot_scale, plot_scale], [-plot_scale, plot_scale], 'k')\nplt.xlabel('True beta')\nplt.ylabel('Estimated beta')\nplt.legend(loc='best')\nplt.figure(figsize=(7, 7))\nplt.plot(true_beta, beta_loc, '.', label='Approximated Posterior Means')\nplt.plot(true_beta, beta_loc + 2 * jnp.exp(beta_log_scale), 'r.', label=r'Approximated Posterior $2\\sigma$ Error Bars')\nplt.plot(true_beta, beta_loc - 2 * jnp.exp(beta_log_scale), 'r.')\nplot_scale = 3\nplt.plot([-plot_scale, plot_scale], [-plot_scale, plot_scale], 'k')\nplt.xlabel('True beta')\nplt.ylabel('Estimated beta')\nplt.legend(loc='best')"
      }
    ]
  },
  {
    "title": "Introduction to Advanced Topics",
    "concepts": [
      "The section covers advanced topics.",
      "Topics include multi-core computation, automatic differentiation, and custom operations."
    ],
    "code_examples": []
  },
  {
    "title": "List of topics",
    "concepts": [
      "Parallel computation is an advanced topic.",
      "Automatic differentiation is an advanced topic.",
      "Deep dives are part of the advanced topics."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.Array and Parallelism",
    "concepts": [
      "jax.Array is a unified datatype for representing arrays, even with physical storage spanning multiple devices.",
      "Using jax.Array with jax.jit can provide automatic compiler-based parallelization."
    ],
    "code_examples": [
      {
        "description": "Create a sharded jax.Array across multiple devices, apply a computation, and visualize the sharding of the result.",
        "code": "from jax.sharding import PartitionSpec as P, NamedSharding\n# Create a Sharding object to distribute a value across devices:\nmesh = jax.make_mesh(((4, 2), ('x', 'y')))\n# Create an array of random values:\nx = jax.random.normal(jax.random.key(0), (8192, 8192))\n# and use jax.device_put to distribute it across devices:\ny = jax.device_put(x, NamedSharding(mesh, P('x', 'y')))\njax.debug.visualize_array_sharding(y)\n\nz = jnp.sin(y)\njax.debug.visualize_array_sharding(z)"
      },
      {
        "description": "Compare the execution time of jnp.sin on a single device vs. a sharded array.",
        "code": "# `x` is present on a single device\n%timeit -n 5 -r 5 jnp.sin(x).block_until_ready()\n\n# `y` is sharded across 8 devices.\n%timeit -n 5 -r 5 jnp.sin(y).block_until_ready()"
      }
    ]
  },
  {
    "title": "Sharding with jax.device_put and Sharding Objects",
    "concepts": [
      "Sharding objects describe distributed memory layouts in JAX.",
      "jax.device_put is used with Sharding objects to produce a value with distributed layout.",
      "jax.make_mesh creates a numpy.ndarray of Devices using hardware topology.",
      "NamedSharding and PartitionSpec are used to define how an array is partitioned across a mesh.",
      "Trailing None values in PartitionSpec can be omitted."
    ],
    "code_examples": [
      {
        "description": "Visualize the sharding of an array stored on a single device.",
        "code": "import jax\nx = jax.random.normal(jax.random.key(0), (8192, 8192))\njax.debug.visualize_array_sharding(x)"
      },
      {
        "description": "Shard an array across multiple devices using jax.device_put and NamedSharding.",
        "code": "from jax.sharding import Mesh, PartitionSpec, NamedSharding\nP = PartitionSpec\nmesh = jax.make_mesh(((4, 2), ('a', 'b')))\ny = jax.device_put(x, NamedSharding(mesh, P('a', 'b')))\njax.debug.visualize_array_sharding(y)"
      },
      {
        "description": "Helper function for creating NamedSharding.",
        "code": "default_mesh = jax.make_mesh(((4, 2), ('a', 'b')))\ndef mesh_sharding(pspec: PartitionSpec, mesh: Optional[Mesh] = None,) -> NamedSharding:\n    if mesh is None:\n        mesh = default_mesh\n    return NamedSharding(mesh, pspec)\n\ny = jax.device_put(x, mesh_sharding(P('a', 'b')))\njax.debug.visualize_array_sharding(y)"
      },
      {
        "description": "Demonstrate different sharding configurations using PartitionSpec.",
        "code": "y = jax.device_put(x, mesh_sharding(P('b', 'a')))\njax.debug.visualize_array_sharding(y)\n\n# This `None` means that `x` is not sharded on its second dimension,\n# and since the Mesh axis name 'b' is not mentioned, shards are\n# replicated across it.\ny = jax.device_put(x, mesh_sharding(P('a', None)))\njax.debug.visualize_array_sharding(y)\n\ny = jax.device_put(x, mesh_sharding(P(None, 'b')))\njax.debug.visualize_array_sharding(y)\n\ny = jax.device_put(x, mesh_sharding(P(( 'a', 'b'), None)))\njax.debug.visualize_array_sharding(y)"
      }
    ]
  },
  {
    "title": "Parallel Computation with Sharded Inputs and jax.jit",
    "concepts": [
      "Functions decorated with jax.jit can operate over sharded arrays without copying data onto a single device.",
      "The compiler decides shardings for intermediates and output values based on the input data sharding.",
      "The compiler automatically parallelizes computation and inserts communication operations as necessary.",
      "The compiler chooses output sharding to maximize parallelization."
    ],
    "code_examples": [
      {
        "description": "Demonstrate elementwise operation with sharded input and output.",
        "code": "mesh = jax.make_mesh(((4, 2), ('a', 'b')))\nx = jax.device_put(x, NamedSharding(mesh, P('a', 'b')))\nprint('input sharding:')\njax.debug.visualize_array_sharding(x)\ny = jnp.sin(x)\nprint('output sharding:')\njax.debug.visualize_array_sharding(y)"
      },
      {
        "description": "Matrix multiplication with sharded inputs and automatic output sharding.",
        "code": "y = jax.device_put(x, NamedSharding(mesh, P('a', None)))\nz = jax.device_put(x, NamedSharding(mesh, P(None, 'b')))\nprint('lhs sharding:')\njax.debug.visualize_array_sharding(y)\nprint('rhs sharding:')\njax.debug.visualize_array_sharding(z)\nw = jnp.dot(y, z)\nprint('out sharding:')\njax.debug.visualize_array_sharding(w)"
      },
      {
        "description": "Timing comparison of matrix multiplication with single-device vs. sharded inputs.",
        "code": "x_single = jax.device_put(x, jax.devices()[0])\njax.debug.visualize_array_sharding(x_single)\nnp.allclose(jnp.dot(x_single, x_single), jnp.dot(y, z))\n\n%timeit -n 5 -r 5 jnp.dot(x_single, x_single).block_until_ready()\n%timeit -n 5 -r 5 jnp.dot(y, z).block_until_ready()"
      },
      {
        "description": "Copying a sharded Array.",
        "code": "w_copy = jnp.copy(w)\njax.debug.visualize_array_sharding(w_copy)"
      }
    ]
  },
  {
    "title": "Explicit Device Placement and Uncommitted Arrays",
    "concepts": [
      "Arrays that have been explicitly placed or sharded with jax.device_put are committed to their device(s).",
      "Uncommitted arrays are placed on the default device and can be moved and resharded automatically.",
      "Outputs of jnp.zeros, jnp.arange, and jnp.array are uncommitted."
    ],
    "code_examples": [
      {
        "description": "Demonstrate incompatible device placement leading to an error.",
        "code": "import textwrap\nfrom termcolor import colored\ndef print_exception(e):\n    name = colored(f'{type(e).__name__}', 'red', force_color=True)\n    print(textwrap.fill(f'{name}: {str(e)}'))\n    \nsharding1 = NamedSharding(Mesh(jax.devices()[:4], 'x'), P('x'))\nsharding2 = NamedSharding(Mesh(jax.devices()[4:], 'x'), P('x'))\ny = jax.device_put(x, sharding1)\nz = jax.device_put(x, sharding2)\ntry:\n    y + z\nexcept ValueError as e:\n    print_exception(e)\n\ndevices = jax.devices()\npermuted_devices = [devices[i] for i in [0, 1, 2, 3, 6, 7, 4, 5]]\nsharding1 = NamedSharding(Mesh(devices, 'x'), P('x'))\nsharding2 = NamedSharding(Mesh(permuted_devices, 'x'), P('x'))\ny = jax.device_put(x, sharding1)\nz = jax.device_put(x, sharding2)\ntry:\n    y + z\nexcept ValueError as e:\n    print_exception(e)"
      },
      {
        "description": "Demonstrate uncommitted arrays being used with committed arrays without error.",
        "code": "y = jax.device_put(x, sharding1)\ny + jnp.ones_like(y)\ny + jnp.arange(y.size).reshape(y.shape)\nprint('no error!')"
      }
    ]
  },
  {
    "title": "Using jax.lax.with_sharding_constraint",
    "concepts": [
      "jax.lax.with_sharding_constraint is used inside jit-decorated functions to give the compiler hints about sharding.",
      "It constrains the sharding of the output and helps the compiler decide shardings for other values.",
      "It's a good practice to annotate the outputs of computations based on how the values are consumed."
    ],
    "code_examples": [
      {
        "description": "Example of using jax.lax.with_sharding_constraint to constrain the sharding of an intermediate value.",
        "code": "mesh = jax.make_mesh(((4, 2), ('x', 'y')))\nx = jax.random.normal(jax.random.key(0), (8192, 8192))\nx = jax.device_put(x, NamedSharding(mesh, P('x', 'y')))\n\n@jax.jit\ndef f(x):\n    x = x + 1\n    y = jax.lax.with_sharding_constraint(x, NamedSharding(mesh, P('y', 'x')))\n    return y\n\njax.debug.visualize_array_sharding(x)\ny = f(x)\njax.debug.visualize_array_sharding(y)"
      },
      {
        "description": "Example of using jax.lax.with_sharding_constraint to replicate an array.",
        "code": "@jax.jit\ndef f(x):\n    x = x + 1\n    y = jax.lax.with_sharding_constraint(x, NamedSharding(mesh, P()))\n    return y\n\njax.debug.visualize_array_sharding(x)\ny = f(x)\njax.debug.visualize_array_sharding(y)"
      }
    ]
  },
  {
    "title": "Parallelizing Neural Networks with jax.device_put and jax.jit",
    "concepts": [
      "jax.device_put and jax.jit's computation-follows-sharding features can be used to parallelize computation in neural networks.",
      "The batch dimension of the input data can be sharded across devices.",
      "Model parameters can be replicated across devices or sharded based on the model structure."
    ],
    "code_examples": [
      {
        "description": "Define a basic neural network with predict and loss functions.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef predict(params, inputs):\n    for W, b in params:\n        outputs = jnp.dot(inputs, W) + b\n        inputs = jnp.maximum(outputs, 0)\n    return outputs\n\ndef loss(params, batch):\n    inputs, targets = batch\n    predictions = predict(params, inputs)\n    return jnp.mean(jnp.sum((predictions - targets)**2, axis=-1))\n\nloss_jit = jax.jit(loss)\ngradfun = jax.jit(jax.grad(loss))\n\ndef init_layer(key, n_in, n_out):\n    k1, k2 = jax.random.split(key)\n    W = jax.random.normal(k1, (n_in, n_out)) / jnp.sqrt(n_in)\n    b = jax.random.normal(k2, (n_out,))\n    return W, b\n\ndef init_model(key, layer_sizes, batch_size):\n    key, *keys = jax.random.split(key, len(layer_sizes))\n    params = list(map(init_layer, keys, layer_sizes[:-1], layer_sizes[1:]))\n    key, *keys = jax.random.split(key, 3)\n    inputs = jax.random.normal(keys[0], (batch_size, layer_sizes[0]))\n    targets = jax.random.normal(keys[1], (batch_size, layer_sizes[-1]))\n    return params, (inputs, targets)\n\nlayer_sizes = [784, 8192, 8192, 8192, 10]\nbatch_size = 8192\nparams, batch = init_model(jax.random.key(0), layer_sizes, batch_size)"
      },
      {
        "description": "Shard the batch dimension of the input data and replicate the model parameters.",
        "code": "mesh = jax.make_mesh(((8,), ('batch',)))\nsharding = NamedSharding(mesh, P('batch'))\nreplicated_sharding = NamedSharding(mesh, P())\n\nbatch = jax.device_put(batch, sharding)\nparams = jax.device_put(params, replicated_sharding)\nloss_jit(params, batch)"
      },
      {
        "description": "Train the neural network.",
        "code": "step_size = 1e-5\nfor _ in range(30):\n    grads = gradfun(params, batch)\n    params = [(W - step_size * dW, b - step_size * db) for (W, b), (dW, db) in zip(params, grads)]\n    print(loss_jit(params, batch))"
      },
      {
        "description": "Compare performance of gradient calculation on sharded and single device.",
        "code": "%timeit -n 5 -r 5 gradfun(params, batch)[0][0].block_until_ready()\n\nbatch_single = jax.device_put(batch, jax.devices()[0])\nparams_single = jax.device_put(params, jax.devices()[0])\n%timeit -n 5 -r 5 gradfun(params_single, batch_single)[0][0].block_until_ready()"
      },
      {
        "description": "Shard the batch dimension of the input data and shard some of the layers.",
        "code": "mesh = jax.make_mesh(((4, 2), ('batch', 'model')))\nbatch = jax.device_put(batch, NamedSharding(mesh, P('batch', None)))\njax.debug.visualize_array_sharding(batch[0])\njax.debug.visualize_array_sharding(batch[1])\nreplicated_sharding = NamedSharding(mesh, P())\n(W1, b1), (W2, b2), (W3, b3), (W4, b4) = params\nW1 = jax.device_put(W1, replicated_sharding)\nb1 = jax.device_put(b1, replicated_sharding)\nW2 = jax.device_put(W2, NamedSharding(mesh, P(None, 'model')))\nb2 = jax.device_put(b2, NamedSharding(mesh, P('model')))\nW3 = jax.device_put(W3, NamedSharding(mesh, P('model', None)))\nb3 = jax.device_put(b3, replicated_sharding)\nW4 = jax.device_put(W4, replicated_sharding)\nb4 = jax.device_put(b4, replicated_sharding)\nparams = (W1, b1), (W2, b2), (W3, b3), (W4, b4)\n\njax.debug.visualize_array_sharding(W2)\njax.debug.visualize_array_sharding(W3)"
      }
    ]
  },
  {
    "title": "Partitionable Random Number Generation",
    "concepts": [
      "JAX uses a functional, deterministic random number generator.",
      "Random number generation should be a pure map over counter values, and therefore trivially partitionable.",
      "The stable RNG implementation is not automatically partitionable for historical reasons.",
      "The jax_threefry_partitionable flag enables partitionable random number generation.",
      "Enabling jax_threefry_partitionable may result in different random values even with the same random key."
    ],
    "code_examples": [
      {
        "description": "Define a function that draws random uniform numbers and adds them to the input.",
        "code": "@jax.jit\ndef f(key, x):\n    numbers = jax.random.uniform(key, x.shape)\n    return x + numbers\n\nkey = jax.random.key(42)\nmesh = Mesh(jax.devices(), 'x')\nx_sharding = NamedSharding(mesh, P('x'))\nx = jax.device_put(jnp.arange(24), x_sharding)"
      },
      {
        "description": "Check if the compiled computation for f involves communication.",
        "code": "f_exe = f.lower(key, x).compile()\nprint('Communicating?', 'collective-permute' in f_exe.as_text())"
      },
      {
        "description": "Enable jax_threefry_partitionable and check if the compiled computation involves communication.",
        "code": "jax.config.update('jax_threefry_partitionable', True)\nf_exe = f.lower(key, x).compile()\nprint('Communicating?', 'collective-permute' in f_exe.as_text())"
      },
      {
        "description": "Demonstrate that enabling jax_threefry_partitionable may result in different random values.",
        "code": "jax.config.update('jax_threefry_partitionable', False)\nprint('Stable:')\nprint(f(key, x))\nprint()\njax.config.update('jax_threefry_partitionable', True)\nprint('Partitionable:')\nprint(f(key, x))"
      }
    ]
  },
  {
    "title": "Introduction to shard_map",
    "concepts": [
      "shard_map is an SPMD multi-device parallelism API.",
      "It maps a function over shards of data.",
      "Instances communicate via explicit collective communication operations.",
      "shard_map is complementary to and composable with jit.",
      "shard_map gives you manual control over device partitioning.",
      "It is an evolution of pmap, more expressive, performant, and composable.",
      "It works eagerly for easier debugging.",
      "The tutorial assumes an environment with eight devices."
    ],
    "code_examples": [
      {
        "description": "Setting the number of CPU devices to 8 using XLA flags.",
        "code": "import os\nos.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'  # Use 8 CPU devices"
      }
    ]
  },
  {
    "title": "Basic Matrix Multiplication Example with shard_map",
    "concepts": [
      "Demonstrates a matrix multiplication example using shard_map.",
      "The example uses a mesh with two dimensions ('x' and 'y').",
      "The input arrays are partitioned based on the mesh and PartitionSpec.",
      "The matmul_basic function performs local block matrix multiplications.",
      "A collective sum operation (jax.lax.psum) is used to combine partial results.",
      "The output is sharded along the rows.",
      "The example verifies that the result matches the standard matrix multiplication.",
      "The example highlights the eager evaluation and debugging capabilities of shard_map."
    ],
    "code_examples": [
      {
        "description": "Import necessary libraries and define the mesh.",
        "code": "from functools import partial\nimport jax\nimport jax.numpy as jnp\nfrom jax.sharding import Mesh, PartitionSpec as P\nfrom jax.experimental.shard_map import shard_map\n\nmesh = jax.make_mesh(((4, 2), ('x', 'y')))\na = jnp.arange(8 * 16.).reshape(8, 16)\nb = jnp.arange(16 * 4.).reshape(16, 4)"
      },
      {
        "description": "Define the matrix multiplication function using shard_map.",
        "code": "@partial(\n    shard_map,\n    mesh=mesh,\n    in_specs=(P('x', 'y'), P('y', None)),\n    out_specs=P('x', None))\ndef matmul_basic(a_block, b_block):\n  # a_block: f32[2, 8]\n  # b_block: f32[8, 4]\n  c_partialsum = jnp.dot(a_block, b_block)\n  c_block = jax.lax.psum(c_partialsum, 'y')\n  # c_block: f32[2, 4]\n  return c_block\n\nc = matmul_basic(a, b)\n# c: f32[8, 4]"
      },
      {
        "description": "Check if two arrays are close.",
        "code": "from jax.tree_util import tree_map, tree_all\ndef allclose(a, b):\n  return tree_all(tree_map(partial(jnp.allclose, atol=1e-2, rtol=1e-2), a, b))\n\nallclose(c, jnp.dot(a, b))"
      },
      {
        "description": "Visualize array sharding using jax.debug.visualize_array_sharding.",
        "code": "jax.debug.visualize_array_sharding(c)"
      }
    ]
  },
  {
    "title": "Comparison with jax.jit Automatic Parallelization",
    "concepts": [
      "The example demonstrates equivalent computation using jax.jit.",
      "NamedSharding is used to specify how the input arrays are sharded across devices.",
      "jax.lax.with_sharding_constraint ensures the output has the desired sharding.",
      "The example verifies that the result from the jitted function matches the standard matrix multiplication.",
      "shard_map performs device_put or with_sharding_constraint on its inputs based on mesh and in_specs.",
      "The example demonstrates and compares array sharding for shard_map and jitted version."
    ],
    "code_examples": [
      {
        "description": "Matrix multiplication with jax.jit automatic parallelization.",
        "code": "from jax.sharding import NamedSharding\na = jax.device_put(a, NamedSharding(mesh, P('x', 'y')))\nb = jax.device_put(b, NamedSharding(mesh, P('y', None)))\n\n@jax.jit\ndef matmul_reference(a, b):\n  c = jnp.dot(a, b)\n  return jax.lax.with_sharding_constraint(c, NamedSharding(mesh, P('x', None)))\n\nc_ref = matmul_reference(a, b)\nallclose(c_ref, jnp.dot(a, b))"
      },
      {
        "description": "Visualizing sharding of a, b and c",
        "code": "print('a blocks:'); jax.debug.visualize_array_sharding(a)\nprint('b blocks:'); jax.debug.visualize_array_sharding(b)\nprint('c blocks:'); jax.debug.visualize_array_sharding(c)"
      }
    ]
  },
  {
    "title": "Comparison with vmap and rank reducing behavior",
    "concepts": [
      "vmap and pmap are rank-reducing maps with unstacking/stacking of inputs/outputs.",
      "shard_map does not have the rank-reducing behavior.",
      "shard_map is a rank-preserving map with unconcatenating/concatenating of inputs/outputs.",
      "The number of logical applications of f in vmap/pmap is determined by the size of the input axis.",
      "The number of logical applications of f in shard_map is determined by the mesh size."
    ],
    "code_examples": [
      {
        "description": "Checking vmap semantics.",
        "code": "def check_vmap(f, xs):\n  ans = jax.vmap(f, in_axes=(0,), out_axes=0)(xs)\n  expected = jnp.stack([f(x) for x in xs])  # vmap reference semantics\n  print(allclose(ans, expected))\n\ncheck_vmap(lambda x: x @ x, jnp.arange(12).reshape(4, 3))"
      },
      {
        "description": "Checking shard_map semantics.",
        "code": "import numpy as np\ndevices = np.array(jax.devices()[:4])\nmesh = Mesh(devices, ('i',))\n# mesh.shape['i'] = 4\n\ndef check_shmap(f, y):\n  ans = shard_map(f, mesh, in_specs=P('i'), out_specs=P('i'))(y)\n  expected = jnp.concatenate(\n      [f(y_blk) for y_blk in jnp.split(y, mesh.shape['i'])])\n  print(allclose(ans, expected))\n\ncheck_shmap(lambda x: x.T @ x, jnp.arange(32).reshape(8, 4))"
      }
    ]
  },
  {
    "title": "Input and Output PartitionSpec in shard_map",
    "concepts": [
      "in_specs identifies input array axes with mesh axes using PartitionSpec.",
      "The identification determines shard sizes.",
      "When an input axis is identified with a mesh axis, the input is split along that axis.",
      "If an input's pspec does not mention a mesh axis name, there's no splitting over that mesh axis.",
      "When a mesh axis is not mentioned in an input pspec, a less efficient program using jnp.tile can be used.",
      "Physical data movement is possible on inputs.",
      "out_specs identifies output array axes with mesh axes.",
      "It represents how output blocks should be assembled.",
      "When a mesh axis name is not mentioned in an output pspec, it represents an un-tiling.",
      "Physical data movement is not possible on outputs; out_specs encode how to assemble block outputs into Arrays.",
      "shard_map has jnp.concatenate, untile, and block transpose built into its output.",
      "The shapes of arguments passed to f are computed based on the shapes of arguments to shard_map and PartitionSpec.",
      "Communication collectives can mention the axis names of mesh.",
      "in_specs and out_specs are PartitionSpec which can affinely mention axis names from mesh to express slicing/unconcatenation and concatenation of inputs and outputs, respectively"
    ],
    "code_examples": [
      {
        "description": "Example showing how input pspec affect input block shapes.",
        "code": "mesh = jax.make_mesh(((4, 2), ('i', 'j')))\n\n@partial(shard_map, mesh=mesh, in_specs=P('i', None), out_specs=P('i', 'j'))\ndef f1(x_block):\n  print(x_block.shape)  # prints (3, 12)\n  return x_block\n\nx1 = jnp.arange(12 * 12).reshape(12, 12)\ny = f1(x1)"
      },
      {
        "description": "Example showing how to tile input array when a mesh axis is not mentioned in the input pspec.",
        "code": "@partial(shard_map, mesh=mesh, in_specs=P('i', 'j'), out_specs=P('i', 'j'))\ndef f2(x_block):\n  print(x_block.shape)\n  return x_block\n\nx = jnp.arange(12 * 12).reshape(12, 12)\nx_ = jnp.tile(x, (1, mesh.shape['j']))  # x_ has shape (12, 24)\ny = f2(x_)\n# prints (3,12), and f1(x) == f2(x_)"
      },
      {
        "description": "Example showing un-tiling on the output side.",
        "code": "x = jnp.array([[3.]])\nz = shard_map(lambda: x, mesh=mesh, in_specs=(), out_specs=P('i', 'j'))()\nprint(z)\n# prints the same as jnp.tile(x, (4, 2))\n\nz = shard_map(lambda: x, mesh=mesh, in_specs=(), out_specs=P('i', None))()\nprint(z)\n# prints the same as jnp.tile(x, (4, 1)), or just jnp.tile(x, (4,))\n\nz = shard_map(lambda: x, mesh=mesh, in_specs=(), out_specs=P(None, None))()\nprint(z)\n# prints the same as jnp.tile(x, (1, 1)), or just x"
      },
      {
        "description": "Example combining a psum collective with an un-tiled output.",
        "code": "@partial(shard_map, mesh=mesh, in_specs=P('i', 'j'), out_specs=P('i', None))\ndef f3(x_block):\n  return jax.lax.psum(x_block, 'j')\n\nx = jnp.arange(12 * 12).reshape(12, 12)\ny3 = f3(x)\nprint(y3.shape)"
      },
      {
        "description": "Examples combining a psum collective with different output pspecs.",
        "code": "@partial(shard_map, mesh=mesh, in_specs=P('i', 'j'), out_specs=P(None, 'j'))\ndef f4(x_block):\n  return jax.lax.psum(x_block, 'i')\n\nx = jnp.arange(12 * 12).reshape(12, 12)\ny4 = f4(x)\nprint(y4.shape)\n# (3,12)\n\n@partial(shard_map, mesh=mesh, in_specs=P('i', 'j'), out_specs=P(None, None))\ndef f5(x_block):\n  return jax.lax.psum(x_block, ('i', 'j'))\n\ny5 = f5(x)\nprint(y5.shape)\n# (3,6)"
      }
    ]
  },
  {
    "title": "shard_map Function Signature and Semantics with Collectives",
    "concepts": [
      "shard_map function signature includes f (callable), mesh, in_specs, out_specs, auto, and check_rep.",
      "Collective communication allows function instances to communicate with each other.",
      "Reference semantics for shard_map are defined with array_split and concatenate when collectives are not involved.",
      "When f contains a collective, reference semantics involve collective_ref which depends on all z_blocks.",
      "jax.lax.psum is a collective that computes an all-reduce-sum along a device mesh axis."
    ],
    "code_examples": [
      {
        "description": "shard_map function signature.",
        "code": "from jax.sharding import Mesh\nSpecs = PyTree[PartitionSpec]\ndef shard_map(\n    f: Callable,\n    mesh: Mesh,\n    in_specs: Specs,\n    out_specs: Specs,\n    auto: collections.abc.Set[AxisName] = frozenset([]),\n    check_rep: bool = True,\n) -> Callable:\n  ..."
      },
      {
        "description": "Toy example of psum using shard_map.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import lax\nfrom jax.sharding import Mesh, NamedSharding, PartitionSpec as P\nfrom jax.experimental.shard_map import shard_map\n\nmesh1d = Mesh(jax.devices()[:4], ('i',))\n\n@partial(shard_map, mesh=mesh1d, in_specs=P('i'), out_specs=P(None))\ndef f1(x_block):\n  print('BEFORE: \\n', x_block)\n  y_block = jax.lax.psum(x_block, 'i')\n  print('AFTER: \\n', y_block)\n  return y_block\n\nx = jnp.array([\n    3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 1, 2\n])\ny = f1(x)\nprint('FINAL RESULT: \\n', y)"
      }
    ]
  },
  {
    "title": "Using psum with Multi-Dimensional Mesh",
    "concepts": [
      "psum can be applied over multiple axes at once.",
      "The example demonstrates psum over a 2D mesh.",
      "out_specs controls how the results are assembled based on the mesh axes.",
      "Un-tiling is achieved by not mentioning mesh axes in out_specs when the collective ensures equal values."
    ],
    "code_examples": [
      {
        "description": "Example of psum over a 2D mesh.",
        "code": "mesh2d = Mesh(np.array(jax.devices()[:4]).reshape(2, 2), ('i', 'j'))\n\n@partial(shard_map, mesh=mesh2d, in_specs=P('i', 'j'), out_specs=P(None, 'j'))\ndef f2(x_block):\n  print('BEFORE: \\n', x_block)\n  y_block = jax.lax.psum(x_block, 'i')\n  print('AFTER: \\n', y_block)\n  return y_block\n\ny = f2(jnp.arange(16).reshape(4, 4))\nprint('FINAL RESULT: \\n', y)"
      },
      {
        "description": "Example applying psum over both axes.",
        "code": "@partial(shard_map, mesh=mesh2d, in_specs=P('i', 'j'), out_specs=P(None, None))\ndef f3(x_block):\n  print('BEFORE: \\n', x_block)\n  y_block = jax.lax.psum(x_block, ('i', 'j'))\n  print('AFTER: \\n', y_block)\n  return y_block\n\ny = f3(jnp.arange(16).reshape(4, 4))\nprint('FINAL RESULT: \\n', y)"
      }
    ]
  },
  {
    "title": "Introduction to Multi-Process JAX",
    "concepts": [
      "JAX can be used in multi-process environments like GPU clusters and Cloud TPU pods.",
      "Multi-process environments require direct communication links between accelerators.",
      "This guide focuses on collective communication operations in multi-process settings.",
      "An important requirement is direct communication links between accelerators.",
      "Each JAX Python process runs independently in a Single Program, Multiple Data (SPMD) model.",
      "You must manually run your JAX program on each host."
    ],
    "code_examples": []
  },
  {
    "title": "Initializing the JAX Cluster",
    "concepts": [
      "The cluster should be initialized with jax.distributed.initialize() at the start of each process.",
      "jax.distributed.initialize() must be called before any JAX computations are executed.",
      "jax.distributed.initialize() takes arguments like coordinator_address, num_processes, and process_id.",
      "On Cloud TPU, Slurm and Open MPI environments, jax.distributed.initialize() can be called with no arguments.",
      "When running on GPUs with Slurm and Open MPI, it is assumed that one process is started per GPU.",
      "On TPU calling jax.distributed.initialize() is optional, but recommended."
    ],
    "code_examples": [
      {
        "description": "Example of initializing JAX in a distributed environment, specifying the coordinator address, number of processes, and process ID.",
        "code": "import jax\nimport jax.distributed\n\njax.distributed.initialize(\n    coordinator_address=\"192.168.0.1:1234\",\n    num_processes=2,\n    process_id=0\n)"
      },
      {
        "description": "Example of initializing JAX in a distributed environment, specifying the coordinator address, number of processes, and process ID.",
        "code": "import jax\nimport jax.distributed\n\njax.distributed.initialize(\n    coordinator_address=\"192.168.0.1:1234\",\n    num_processes=2,\n    process_id=0\n)"
      },
      {
        "description": "Example of initializing JAX in a distributed environment on Cloud TPU or Slurm, where arguments are automatically determined.",
        "code": "import jax\nimport jax.distributed\n\njax.distributed.initialize()"
      },
      {
        "description": "Example of initializing JAX in a distributed environment on Cloud TPU or Slurm, where arguments are automatically determined.",
        "code": "import jax\nimport jax.distributed\n\njax.distributed.initialize()"
      }
    ]
  },
  {
    "title": "Local vs. Global Devices",
    "concepts": [
      "Each process has a distinct set of local devices it can address.",
      "Global devices are the set of all devices across all processes.",
      "Local devices can be seen via jax.local_devices().",
      "Global devices can be seen via jax.devices().",
      "A process's local devices are always a subset of the global devices."
    ],
    "code_examples": []
  },
  {
    "title": "Running Computations Across Processes",
    "concepts": [
      "Use standard JAX parallelism APIs like shard_map() to run computations across processes.",
      "All processes must run the same cross-process computations in the same order.",
      "Processes passing differently-shaped inputs to the same parallel function can cause hangs or incorrect return values.",
      "\u201cLast batch\u201d issues can cause hangs.",
      "Conditions based on non-deterministic ordering of collections can cause code processes to hang."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the use of jax.pmap and jax.lax.psum in a multi-process environment to perform a parallel sum operation across devices. The code is intended to be run on each host in the cluster.",
        "code": "import jax\n\njax.distributed.initialize()\n\n# On GPU, see above for the necessary arguments.\n\njax.device_count()\n# total number of accelerator devices in the cluster\n\njax.local_device_count()\n# number of accelerator devices attached to this host\n\n# The psum is performed over all mapped devices across the pod slice\n\nxs = jax.numpy.ones(jax.local_device_count())\n\njax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(xs)"
      },
      {
        "description": "Example demonstrating the use of jax.pmap and jax.lax.psum in a multi-process environment to perform a parallel sum operation across devices. The code is intended to be run on each host in the cluster.",
        "code": "import jax\n\njax.distributed.initialize()\n\n# On GPU, see above for the necessary arguments.\n\njax.device_count()\n# total number of accelerator devices in the cluster\n\njax.local_device_count()\n# number of accelerator devices attached to this host\n\n# The psum is performed over all mapped devices across the pod slice\n\nxs = jax.numpy.ones(jax.local_device_count())\n\njax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(xs)"
      }
    ]
  },
  {
    "title": "Introduction to Distributed Data Loading in JAX",
    "concepts": [
      "Distributed data loading splits data across multiple processes in multi-host/multi-process JAX environments.",
      "Distributed data loading is generally more efficient but complex compared to loading the full dataset in a single process or all processes.",
      "Each device must have access to the input data shard it needs for the computation.",
      "Every jax.Array has an associated Sharding describing which shard of the global data is required by each global device.",
      "addressable_devices() provide a list of devices needed to load data for within the current process."
    ],
    "code_examples": []
  },
  {
    "title": "Sharding and Data Distribution",
    "concepts": [
      "A (64, 128) jax.Array can be sharded across 4 processes with 2 devices each (8 devices total).",
      "A 1D sharding can give each device a (64, 16) shard.",
      "Data shards should be loaded by the appropriate process.",
      "High-level methods for achieving this include: loading global data in each process, using a per-device data pipeline, using a consolidated per-process data pipeline, and loading data in some convenient way then resharding it inside the computation."
    ],
    "code_examples": []
  },
  {
    "title": "Data Loading Options",
    "concepts": [
      "Option 1: Load the global data in each process and transfer only the needed shards to the local devices. This is simple but inefficient.",
      "Option 2: Use a per-device data pipeline, where each device gets its own data loader for just the data shard it requires. This is efficient in terms of data loaded but can cause performance issues.",
      "Option 3: Use a consolidated per-process data pipeline, where each process loads data for all its local devices and then shards the local data. This is the most efficient but also the most complex.",
      "Option 4: Load data in a convenient sharding (e.g., per-column) and then reshard inside the computation using jax.lax.with_sharding_constraint().",
      "Option 4 offers flexibility but uses accelerator interconnect bandwidth for resharding and requires two Sharding definitions."
    ],
    "code_examples": []
  },
  {
    "title": "Replication",
    "concepts": [
      "Replication involves multiple devices having the same data shard.",
      "Full replication: all devices have a full copy of the data.",
      "Partial replication: there are multiple copies of the data, and each copy is sharded across multiple devices.",
      "There are many ways to perform partial replication for a given array value."
    ],
    "code_examples": []
  },
  {
    "title": "Data Parallelism",
    "concepts": [
      "In pure data parallelism, the model is replicated on each device, and each replica receives a different per-replica batch of data.",
      "The input data is represented as a single jax.Array containing the data across all replicas (global batch).",
      "A 1D sharding across all devices can be used.",
      "It doesn't matter which per-replica batch lands on which replica in data parallelism.",
      "Each device just needs an independent stream of per-replica batches.",
      "This simplifies data loading by creating an independent pipeline per process and chunking the resulting per-process batch into per-replica batches."
    ],
    "code_examples": [
      {
        "description": "Example of implementing data parallelism using tf.data and jax.make_array_from_process_local_data.",
        "code": "import jax\nimport tensorflow as tf\nimport numpy as np\n################################################################################\n# Step 1: setup the Dataset for pure data parallelism (do once)\n################################################################################\n# Fake example data (replace with your Dataset)\nds = tf.data.Dataset.from_tensor_slices([np.ones((16, 3)) * i for i in range(100)])\nds = ds.shard(num_shards=jax.process_count(), index=jax.process_index())\n################################################################################\n# Step 2: create a jax.Array of per-replica batches from the per-process batch\n# produced from the Dataset (repeat every step). This can be used with batches\n# produced by different data loaders as well!\n################################################################################\n# Grab just the first batch from the Dataset for this example\nper_process_batch = ds.as_numpy_iterator().next()\nmesh = jax.make_mesh((jax.device_count(),), ('batch',))\nsharding = jax.NamedSharding(mesh, jax.sharding.PartitionSpec('batch'))\nglobal_batch_array = jax.make_array_from_process_local_data(sharding, per_process_batch)"
      },
      {
        "description": "Example of implementing data parallelism using tf.data and jax.make_array_from_process_local_data.",
        "code": "import jax\nimport tensorflow as tf\nimport numpy as np\n################################################################################\n# Step 1: setup the Dataset for pure data parallelism (do once)\n################################################################################\n# Fake example data (replace with your Dataset)\nds = tf.data.Dataset.from_tensor_slices([np.ones((16, 3)) * i for i in range(100)])\nds = ds.shard(num_shards=jax.process_count(), index=jax.process_index())\n################################################################################\n# Step 2: create a jax.Array of per-replica batches from the per-process batch\n# produced from the Dataset (repeat every step). This can be used with batches\n# produced by different data loaders as well!\n################################################################################\n# Grab just the first batch from the Dataset for this example\nper_process_batch = ds.as_numpy_iterator().next()\nmesh = jax.make_mesh((jax.device_count(),), ('batch',))\nsharding = jax.NamedSharding(mesh, jax.sharding.PartitionSpec('batch'))\nglobal_batch_array = jax.make_array_from_process_local_data(sharding, per_process_batch)"
      }
    ]
  },
  {
    "title": "Model and Data Parallelism",
    "concepts": [
      "In pure model parallelism, there's just one model replica sharded across all devices, and the data is usually fully replicated.",
      "This guide considers using both data and model parallelism, where each of the multiple model replicas are sharded over multiple devices and the data is partially replicated.",
      "The simplest approach can be to shard each model replica within the local devices of a single process.",
      "Input data is represented as a single jax.Array with a 1D sharding, where each shard is a per-replica batch, and data is partially replicated.",
      "It\u2019s very important to replicate the per-replica batches to the correct devices!",
      "A single replica should only get a single batch."
    ],
    "code_examples": [
      {
        "description": "Example of implementing per-process model parallelism and data parallelism using tf.data.",
        "code": "import jax\nimport tensorflow as tf\nimport numpy as np\n################################################################################\n# Step 1: Set up the Dataset with a different data shard per-process (do once)\n#         (same as for pure data parallelism)\n################################################################################\n# Fake example data (replace with your Dataset)\nper_process_batches = [np.ones((16, 3)) * i for i in range(100)]\nds = tf.data.Dataset.from_tensor_slices(per_process_batches)\nds = ds.shard(num_shards=jax.process_count(), index=jax.process_index())\n################################################################################\n# Step 2: Create a jax.Array of per-replica batches from the per-process batch\n# produced from the Dataset (repeat every step)\n################################################################################\n# Grab just the first batch from the Dataset for this example\nper_process_batch = ds.as_numpy_iterator().next()\nnum_model_replicas_per_process = 2  # set according to your parallelism strategy\nnum_model_replicas_total = num_model_replicas_per_process * jax.process_count()\n# Create an example `Mesh` for per-process data parallelism. Make sure all devices\n# are grouped by process, and then resize so each row is a model replica.\nmesh_devices = np.array([jax.local_devices(process_idx) for process_idx in range(jax.process_count())])\nmesh_devices = mesh_devices.reshape(num_model_replicas_total, -1)\n# Double check that each replica's devices are on a single process.\nfor replica_devices in mesh_devices:\n  num_processes = len(set(d.process_index for d in replica_devices))\n  assert num_processes == 1\nmesh = jax.sharding.Mesh(mesh_devices, ['model_replicas', 'data_parallelism'])\n# Shard the data across model replicas. You don't shard across the\n# data_parallelism mesh axis, meaning each per-replica shard will be replicated\n# across that axis.\nsharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec('model_replicas'))\nglobal_batch_array = jax.make_array_from_process_local_data(sharding, per_process_batch)"
      },
      {
        "description": "Example of implementing per-process model parallelism and data parallelism using tf.data.",
        "code": "import jax\nimport tensorflow as tf\nimport numpy as np\n################################################################################\n# Step 1: Set up the Dataset with a different data shard per-process (do once)\n#         (same as for pure data parallelism)\n################################################################################\n# Fake example data (replace with your Dataset)\nper_process_batches = [np.ones((16, 3)) * i for i in range(100)]\nds = tf.data.Dataset.from_tensor_slices(per_process_batches)\nds = ds.shard(num_shards=jax.process_count(), index=jax.process_index())\n################################################################################\n# Step 2: Create a jax.Array of per-replica batches from the per-process batch\n# produced from the Dataset (repeat every step)\n################################################################################\n# Grab just the first batch from the Dataset for this example\nper_process_batch = ds.as_numpy_iterator().next()\nnum_model_replicas_per_process = 2  # set according to your parallelism strategy\nnum_model_replicas_total = num_model_replicas_per_process * jax.process_count()\n# Create an example `Mesh` for per-process data parallelism. Make sure all devices\n# are grouped by process, and then resize so each row is a model replica.\nmesh_devices = np.array([jax.local_devices(process_idx) for process_idx in range(jax.process_count())])\nmesh_devices = mesh_devices.reshape(num_model_replicas_total, -1)\n# Double check that each replica's devices are on a single process.\nfor replica_devices in mesh_devices:\n  num_processes = len(set(d.process_index for d in replica_devices))\n  assert num_processes == 1\nmesh = jax.sharding.Mesh(mesh_devices, ['model_replicas', 'data_parallelism'])\n# Shard the data across model replicas. You don't shard across the\n# data_parallelism mesh axis, meaning each per-replica shard will be replicated\n# across that axis.\nsharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec('model_replicas'))\nglobal_batch_array = jax.make_array_from_process_local_data(sharding, per_process_batch)"
      }
    ]
  },
  {
    "title": "Model Replicas Spread Across Processes",
    "concepts": [
      "Model replicas can be spread across processes if a single replica can't fit within a process or the device assignment is set up that way.",
      "Data loading becomes more complicated, requiring extra coordination across processes.",
      "Some processes must load the same data, while others load different data.",
      "It's crucial that each process doesn't mix up its per-replica batches, ensuring all devices in a replica get the same batch.",
      "As of August 2023, JAX cannot detect if jax.Array shards across processes are supposed to be replicated but aren\u2019t and will produce wrong results when the computation is run."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Automatic Differentiation with JAX",
    "concepts": [
      "JAX provides a general automatic differentiation system.",
      "The notebook explores various autodiff ideas."
    ],
    "code_examples": [
      {
        "description": "Import necessary JAX libraries.",
        "code": "import jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nkey = random.key(0)"
      },
      {
        "description": "Import necessary JAX libraries.",
        "code": "import jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\nkey = random.key(0)"
      }
    ]
  },
  {
    "title": "Basic Differentiation with `grad`",
    "concepts": [
      "`grad` takes a function and returns its gradient function.",
      "`grad(f)(x)` represents the value of the gradient of f at x.",
      "You can apply `grad` multiple times to compute higher-order derivatives."
    ],
    "code_examples": [
      {
        "description": "Differentiate the tanh function using `grad`.",
        "code": "grad_tanh = grad(jnp.tanh)\nprint(grad_tanh(2.0))"
      },
      {
        "description": "Differentiate the tanh function using `grad`.",
        "code": "grad_tanh = grad(jnp.tanh)\nprint(grad_tanh(2.0))"
      },
      {
        "description": "Compute higher-order derivatives by applying `grad` multiple times.",
        "code": "print(grad(grad(jnp.tanh))(2.0))\nprint(grad(grad(grad(jnp.tanh)))(2.0))\nprint(grad(grad(jnp.tanh))(2.0))\nprint(grad(grad(grad(jnp.tanh)))(2.0))"
      }
    ]
  },
  {
    "title": "Gradients in a Linear Logistic Regression Model",
    "concepts": [
      "Define a sigmoid function.",
      "Define a prediction function for logistic regression.",
      "Create a toy dataset for the model.",
      "Define a loss function (negative log-likelihood).",
      "Initialize random model coefficients.",
      "Compute gradients of the loss function with respect to model parameters.",
      "Use the argnums argument of the grad function to differentiate with respect to specific arguments.",
      "The grad API corresponds to mathematical notation for derivatives.",
      "Differentiating with respect to standard Python containers works."
    ],
    "code_examples": [
      {
        "description": "Define the sigmoid function, predict function, loss function, and initialize weights and bias for a logistic regression model.",
        "code": "def sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12, 0.77],\n                  [0.88, -1.08, 0.15],\n                  [0.52, 0.06, -1.30],\n                  [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n    preds = predict(W, b, inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\n# Initialize random model coefficients\nkey, W_key, b_key = random.split(key, 3)\nW = random.normal(W_key, (3,))\nb = random.normal(b_key, ())"
      },
      {
        "description": "Define the sigmoid function, predict function, loss function, and initialize weights and bias for a logistic regression model.",
        "code": "def sigmoid(x):\n    return 0.5 * (jnp.tanh(x / 2) + 1)\n\n# Outputs probability of a label being true.\ndef predict(W, b, inputs):\n    return sigmoid(jnp.dot(inputs, W) + b)\n\n# Build a toy dataset.\ninputs = jnp.array([[0.52, 1.12, 0.77],\n                  [0.88, -1.08, 0.15],\n                  [0.52, 0.06, -1.30],\n                  [0.74, -2.49, 1.39]])\ntargets = jnp.array([True, True, False, True])\n\n# Training loss is the negative log-likelihood of the training examples.\ndef loss(W, b):\n    preds = predict(W, b, inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\n# Initialize random model coefficients\nkey, W_key, b_key = random.split(key, 3)\nW = random.normal(W_key, (3,))\nb = random.normal(b_key, ())"
      },
      {
        "description": "Compute the gradient of the loss function with respect to W and b using `grad` with the `argnums` argument.",
        "code": "# Differentiate `loss` with respect to the first positional argument:\nW_grad = grad(loss, argnums=0)(W, b)\nprint('W_grad', W_grad)\n# Since argnums=0 is the default, this does the same thing:\nW_grad = grad(loss)(W, b)\nprint('W_grad', W_grad)\n# But we can choose different values too, and drop the keyword:\nb_grad = grad(loss, 1)(W, b)\nprint('b_grad', b_grad)\n# Including tuple values\nW_grad, b_grad = grad(loss, (0, 1))(W, b)\nprint('W_grad', W_grad)\nprint('b_grad', b_grad)"
      },
      {
        "description": "Compute the gradient of the loss function with respect to W and b using `grad` with the `argnums` argument.",
        "code": "# Differentiate `loss` with respect to the first positional argument:\nW_grad = grad(loss, argnums=0)(W, b)\nprint('W_grad', W_grad)\n# Since argnums=0 is the default, this does the same thing:\nW_grad = grad(loss)(W, b)\nprint('W_grad', W_grad)\n# But we can choose different values too, and drop the keyword:\nb_grad = grad(loss, 1)(W, b)\nprint('b_grad', b_grad)\n# Including tuple values\nW_grad, b_grad = grad(loss, (0, 1))(W, b)\nprint('W_grad', W_grad)\nprint('b_grad', b_grad)"
      },
      {
        "description": "Demonstrates differentiating a function with a dictionary argument.",
        "code": "def loss2(params_dict):\n    preds = predict(params_dict['W'], params_dict['b'], inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\nprint(grad(loss2)({'W': W, 'b': b}))"
      },
      {
        "description": "Demonstrates differentiating a function with a dictionary argument.",
        "code": "def loss2(params_dict):\n    preds = predict(params_dict['W'], params_dict['b'], inputs)\n    label_probs = preds * targets + (1 - preds) * (1 - targets)\n    return -jnp.sum(jnp.log(label_probs))\n\nprint(grad(loss2)({'W': W, 'b': b}))"
      }
    ]
  },
  {
    "title": "Value and Gradient Computation",
    "concepts": [
      "The `value_and_grad` function efficiently computes both a function's value and its gradient."
    ],
    "code_examples": [
      {
        "description": "Compute the loss value and gradients of W and b using value_and_grad.",
        "code": "from jax import value_and_grad\n\nloss_value, Wb_grad = value_and_grad(loss, (0, 1))(W, b)\nprint('loss value', loss_value)\nprint('loss value', loss(W, b))"
      },
      {
        "description": "Compute the loss value and gradients of W and b using value_and_grad.",
        "code": "from jax import value_and_grad\n\nloss_value, Wb_grad = value_and_grad(loss, (0, 1))(W, b)\nprint('loss value', loss_value)\nprint('loss value', loss(W, b))"
      }
    ]
  },
  {
    "title": "Checking Gradients with Finite Differences",
    "concepts": [
      "Gradients can be checked using finite differences approximation.",
      "Compare the automatically computed gradient with the numerical approximation.",
      "Checking is done by computing the gradient numerically and using automatic differentiation."
    ],
    "code_examples": [
      {
        "description": "Check the gradient of b using scalar finite differences.",
        "code": "# Set a step size for finite differences calculations\neps = 1e-4\n\n# Check b_grad with scalar finite differences\nb_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps\nprint('b_grad_numerical', b_grad_numerical)\nprint('b_grad_autodiff', grad(loss, 1)(W, b))"
      },
      {
        "description": "Check the gradient of W using finite differences in a random direction.",
        "code": "# Check W_grad with finite differences in a random direction\nkey, subkey = random.split(key)\nvec = random.normal(subkey, W.shape)\nunitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\nW_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps\nprint('W_dirderiv_numerical', W_grad_numerical)\nprint('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec))"
      },
      {
        "description": "Check the gradient of b using scalar finite differences.",
        "code": "# Set a step size for finite differences calculations\neps = 1e-4\n\n# Check b_grad with scalar finite differences\nb_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps\nprint('b_grad_numerical', b_grad_numerical)\nprint('b_grad_autodiff', grad(loss, 1)(W, b))"
      },
      {
        "description": "Check the gradient of W using finite differences in a random direction.",
        "code": "# Check W_grad with finite differences in a random direction\nkey, subkey = random.split(key)\nvec = random.normal(subkey, W.shape)\nunitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\nW_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps\nprint('W_dirderiv_numerical', W_grad_numerical)\nprint('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec))"
      }
    ]
  },
  {
    "title": "Gradient Checking Utility",
    "concepts": [
      "JAX provides a utility function `check_grads` for checking gradients to a specified order."
    ],
    "code_examples": [
      {
        "description": "Check gradients up to the second order using `check_grads`.",
        "code": "from jax.test_util import check_grads\n\ncheck_grads(loss, (W, b), order=2)  # check up to 2nd order derivatives"
      },
      {
        "description": "Check gradients up to the second order using `check_grads`.",
        "code": "from jax.test_util import check_grads\n\ncheck_grads(loss, (W, b), order=2)  # check up to 2nd order derivatives"
      }
    ]
  },
  {
    "title": "Hessian-Vector Product (HVP)",
    "concepts": [
      "A Hessian-vector product function can be useful for optimization and curvature analysis.",
      "The trick is not to instantiate the full Hessian matrix for efficiency.",
      "`grad` can be used to write an efficient Hessian-vector product function."
    ],
    "code_examples": [
      {
        "description": "Implement the Hessian-vector product function using reverse-mode autodiff.",
        "code": "def hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
      },
      {
        "description": "Implement the Hessian-vector product function using reverse-mode autodiff.",
        "code": "def hvp(f, x, v):\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
      }
    ]
  },
  {
    "title": "Jacobian Matrices",
    "concepts": [
      "Full Jacobian matrices can be computed using `jacfwd` and `jacrev`.",
      "`jacfwd` uses forward-mode differentiation, more efficient for tall matrices.",
      "`jacrev` uses reverse-mode differentiation, more efficient for wide matrices.",
      "`jacfwd` and `jacrev` can also be used with container types."
    ],
    "code_examples": [
      {
        "description": "Compute the Jacobian matrix using `jacfwd` and `jacrev`.",
        "code": "from jax import jacfwd, jacrev\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nJ = jacfwd(f)(W)\nprint(\"jacfwd result, with shape\", J.shape)\nprint(J)\n\nJ = jacrev(f)(W)\nprint(\"jacrev result, with shape\", J.shape)\nprint(J)"
      },
      {
        "description": "Compute the Jacobian matrix using `jacfwd` and `jacrev`.",
        "code": "from jax import jacfwd, jacrev\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nJ = jacfwd(f)(W)\nprint(\"jacfwd result, with shape\", J.shape)\nprint(J)\n\nJ = jacrev(f)(W)\nprint(\"jacrev result, with shape\", J.shape)\nprint(J)"
      },
      {
        "description": "Use jacrev with container types.",
        "code": "def predict_dict(params, inputs):\n    return predict(params['W'], params['b'], inputs)\n\nJ_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\n\nfor k, v in J_dict.items():\n    print(\"Jacobian from {} to logits is\".format(k))\n    print(v)"
      },
      {
        "description": "Use jacrev with container types.",
        "code": "def predict_dict(params, inputs):\n    return predict(params['W'], params['b'], inputs)\n\nJ_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\n\nfor k, v in J_dict.items():\n    print(\"Jacobian from {} to logits is\".format(k))\n    print(v)"
      }
    ]
  },
  {
    "title": "Hessian Matrices",
    "concepts": [
      "Dense Hessian matrices can be computed by composing `jacfwd` and `jacrev`.",
      "Forward-over-reverse (jacfwd(jacrev(f))) is typically the most efficient way to compute the Hessian."
    ],
    "code_examples": [
      {
        "description": "Compute the Hessian matrix using jacfwd and jacrev.",
        "code": "def hessian(f):\n    return jacfwd(jacrev(f))\n\nH = hessian(f)(W)\nprint(\"hessian, with shape\", H.shape)\nprint(H)"
      },
      {
        "description": "Compute the Hessian matrix using jacfwd and jacrev.",
        "code": "def hessian(f):\n    return jacfwd(jacrev(f))\n\nH = hessian(f)(W)\nprint(\"hessian, with shape\", H.shape)\nprint(H)"
      }
    ]
  },
  {
    "title": "Forward-Mode Automatic Differentiation: Jacobian-Vector Product (JVP)",
    "concepts": [
      "The Jacobian can be seen as a linear map called pushforward map.",
      "JAX's `jvp` function models the Jacobian-vector product.",
      "`jvp` transforms a function to evaluate both the function and its JVP.",
      "Memory cost is independent of the depth of the computation in `jvp`.",
      "The FLOP cost of jvp-transformed function is about 3x the cost of evaluating the function.",
      "JVP is efficient for functions with 'tall' Jacobians."
    ],
    "code_examples": [
      {
        "description": "Compute the Jacobian-vector product using `jvp`.",
        "code": "from jax import jvp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nkey, subkey = random.split(key)\nv = random.normal(subkey, W.shape)\n\n# Push forward the vector `v` along `f` evaluated at `W`\ny, u = jvp(f, (W,), (v,))"
      },
      {
        "description": "Compute the Jacobian-vector product using `jvp`.",
        "code": "from jax import jvp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\nkey, subkey = random.split(key)\nv = random.normal(subkey, W.shape)\n\n# Push forward the vector `v` along `f` evaluated at `W`\ny, u = jvp(f, (W,), (v,))"
      }
    ]
  },
  {
    "title": "Reverse-Mode Automatic Differentiation: Vector-Jacobian Product (VJP)",
    "concepts": [
      "Reverse-mode is a way to get back a function for evaluating vector-Jacobian products (VJPs).",
      "VJP is also known as Jacobian-transpose-vector product.",
      "The corresponding map on cotangent spaces is called the pullback.",
      "JAX's `vjp` function can take a Python function for evaluating f and give us back a Python function for evaluating the VJP.",
      "VJP is efficient for functions with 'wide' Jacobians.",
      "The implementation of VJP is traditionally more complex than that of forward-mode.",
      "Memory scales with the depth of the computation in reverse mode."
    ],
    "code_examples": [
      {
        "description": "Compute the vector-Jacobian product using `vjp`.",
        "code": "from jax import vjp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\ny, vjp_fun = vjp(f, W)\n\nkey, subkey = random.split(key)\nu = random.normal(subkey, y.shape)\n\n# Pull back the covector `u` along `f` evaluated at `W`\nv = vjp_fun(u)"
      },
      {
        "description": "Compute the vector-Jacobian product using `vjp`.",
        "code": "from jax import vjp\n\n# Isolate the function from the weight matrix to the predictions\nf = lambda W: predict(W, b, inputs)\n\ny, vjp_fun = vjp(f, W)\n\nkey, subkey = random.split(key)\nu = random.normal(subkey, y.shape)\n\n# Pull back the covector `u` along `f` evaluated at `W`\nv = vjp_fun(u)"
      },
      {
        "description": "Compute vector-valued gradients using `vjp`.",
        "code": "from jax import vjp\n\ndef vgrad(f, x):\n    y, vjp_fn = vjp(f, x)\n    return vjp_fn(jnp.ones(y.shape))[0]\n\nprint(vgrad(lambda x: 3 * x**2, jnp.ones((2, 2))))"
      },
      {
        "description": "Compute vector-valued gradients using `vjp`.",
        "code": "from jax import vjp\n\ndef vgrad(f, x):\n    y, vjp_fn = vjp(f, x)\n    return vjp_fn(jnp.ones(y.shape))[0]\n\nprint(vgrad(lambda x: 3 * x**2, jnp.ones((2, 2))))"
      }
    ]
  },
  {
    "title": "Efficient Hessian-Vector Product with Forward and Reverse Mode",
    "concepts": [
      "A more efficient Hessian-vector product function can be implemented using both forward and reverse modes.",
      "The recommended approach is forward-over-reverse.",
      "Avoid direct calls to `jnp.dot` for flexibility in array shapes and container types."
    ],
    "code_examples": [
      {
        "description": "Implement the Hessian-vector product function using forward-over-reverse mode.",
        "code": "from jax import jvp, grad\n\n# forward-over-reverse\ndef hvp(f, primals, tangents):\n    return jvp(grad(f), primals, tangents)[1]"
      },
      {
        "description": "Implement the Hessian-vector product function using forward-over-reverse mode.",
        "code": "from jax import jvp, grad\n\n# forward-over-reverse\ndef hvp(f, primals, tangents):\n    return jvp(grad(f), primals, tangents)[1]"
      },
      {
        "description": "Example usage of the Hessian-vector product function and comparing with direct computation",
        "code": "def f(X):\n    return jnp.sum(jnp.tanh(X)**2)\n\nkey, subkey1, subkey2 = random.split(key, 3)\nX = random.normal(subkey1, (30, 40))\nV = random.normal(subkey2, (30, 40))\n\nans1 = hvp(f, (X,), (V,))\nans2 = jnp.tensordot(hessian(f)(X), V, 2)\nprint(jnp.allclose(ans1, ans2, 1e-4, 1e-4))"
      },
      {
        "description": "Example usage of the Hessian-vector product function and comparing with direct computation",
        "code": "def f(X):\n    return jnp.sum(jnp.tanh(X)**2)\n\nkey, subkey1, subkey2 = random.split(key, 3)\nX = random.normal(subkey1, (30, 40))\nV = random.normal(subkey2, (30, 40))\n\nans1 = hvp(f, (X,), (V,))\nans2 = jnp.tensordot(hessian(f)(X), V, 2)\nprint(jnp.allclose(ans1, ans2, 1e-4, 1e-4))"
      },
      {
        "description": "Implement the Hessian-vector product using reverse over forward mode.",
        "code": "# reverse-over-forward\ndef hvp_revfwd(f, primals, tangents):\n    g = lambda primals: jvp(f, primals, tangents)[1]\n    return grad(g)(primals)"
      },
      {
        "description": "Implement the Hessian-vector product using reverse over forward mode.",
        "code": "# reverse-over-forward\ndef hvp_revfwd(f, primals, tangents):\n    g = lambda primals: jvp(f, primals, tangents)[1]\n    return grad(g)(primals)"
      },
      {
        "description": "Implement the Hessian-vector product using reverse over reverse mode.",
        "code": "# reverse-over-reverse, only works for single arguments\ndef hvp_revrev(f, primals, tangents):\n    x, = primals\n    v, = tangents\n    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
      }
    ]
  },
  {
    "title": "Introduction to Custom Differentiation Rules in JAX",
    "concepts": [
      "JAX provides ways to define custom differentiation rules.",
      "Two primary methods are jax.custom_jvp/jax.custom_vjp and defining new core.Primitive instances.",
      "This document focuses on using jax.custom_jvp and jax.custom_vjp.",
      "Familiarity with jax.jvp and jax.grad is assumed."
    ],
    "code_examples": []
  },
  {
    "title": "Example with jax.custom_jvp",
    "concepts": [
      "Demonstrates the use of jax.custom_jvp to define a custom JVP rule for a function.",
      "The @custom_jvp decorator marks a function for custom differentiation.",
      "The .defjvp method associates a JVP rule with the function.",
      "The JVP rule computes the primal and tangent outputs."
    ],
    "code_examples": [
      {
        "description": "Define a function 'f' and its custom JVP rule using jax.custom_jvp. The function is f(x, y) = sin(x) * y, and the JVP rule is defined as f_jvp.",
        "code": "import jax.numpy as jnp\nfrom jax import custom_jvp\n\n@custom_jvp\ndef f(x, y):\n    return jnp.sin(x) * y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n    x, y = primals\n    x_dot, y_dot = tangents\n    primal_out = f(x, y)\n    tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n    return primal_out, tangent_out"
      },
      {
        "description": "Test the defined function 'f' and its custom JVP rule using jvp and grad.",
        "code": "from jax import jvp, grad\n\nprint(f(2., 3.))\ny, y_dot = jvp(f, (2., 3.), (1., 0.))\nprint(y)\nprint(y_dot)\nprint(grad(f)(2., 3.))"
      },
      {
        "description": "Demonstrates an equivalent alternative using the `defjvps` convenience wrapper to define custom JVPs for each argument separately.",
        "code": "# Equivalent alternative using the defjvps convenience wrapper\n@custom_jvp\ndef f(x, y):\n    return jnp.sin(x) * y\n\nf.defjvps(\n    lambda x_dot, primal_out, x, y: jnp.cos(x) * x_dot * y,\n    lambda y_dot, primal_out, x, y: jnp.sin(x) * y_dot\n)"
      },
      {
        "description": "Test the function 'f' defined with `defjvps` and its custom JVP rule using jvp and grad.",
        "code": "print(f(2., 3.))\ny, y_dot = jvp(f, (2., 3.), (1., 0.))\nprint(y)\nprint(y_dot)\nprint(grad(f)(2., 3.))"
      }
    ]
  },
  {
    "title": "Example with jax.custom_vjp",
    "concepts": [
      "Demonstrates the use of jax.custom_vjp to define a custom VJP rule for a function.",
      "The @custom_vjp decorator marks a function for custom VJP.",
      "The .defvjp method associates a forward and backward pass with the function.",
      "The forward pass computes the primal output and residuals.",
      "The backward pass computes the cotangents."
    ],
    "code_examples": [
      {
        "description": "Define a function 'f' and its custom VJP rule using jax.custom_vjp. The function is f(x, y) = sin(x) * y, and the VJP rule consists of a forward pass f_fwd and a backward pass f_bwd.",
        "code": "from jax import custom_vjp\n\n@custom_vjp\ndef f(x, y):\n    return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n    # Returns primal output and residuals to be used in backward pass by f_bwd.\n    return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n    cos_x, sin_x, y = res  # Gets residuals computed in f_fwd\n    return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)"
      },
      {
        "description": "Test the function 'f' defined with `defvjp` and its custom VJP rule using grad.",
        "code": "from jax import grad\n\nprint(grad(f)(2., 3.))"
      }
    ]
  },
  {
    "title": "Improving Numerical Stability with custom_jvp: log1pexp Example",
    "concepts": [
      "jax.custom_jvp can improve numerical stability.",
      "The example demonstrates how to implement a more stable derivative for log1pexp(x) = log(1 + exp(x)).",
      "The standard autodiff rules can lead to numerical instability for large values of x.",
      "A custom JVP rule can enforce a mathematically equivalent but more numerically stable expression for the derivative."
    ],
    "code_examples": [
      {
        "description": "Naive implementation of log1pexp(x) = log(1 + exp(x)).",
        "code": "def log1pexp(x):\n    return jnp.log(1. + jnp.exp(x))\n\nlog1pexp(3.)"
      },
      {
        "description": "Demonstrates the numerical instability of the gradient of the naive log1pexp implementation.",
        "code": "from jax import grad\n\nprint(grad(log1pexp)(100.))"
      },
      {
        "description": "Shows the jaxpr for the gradient computation of the naive log1pexp implementation, revealing the source of numerical instability.",
        "code": "from jax import make_jaxpr\n\nmake_jaxpr(grad(log1pexp))(100.)"
      },
      {
        "description": "Implementation of log1pexp with a custom JVP rule for improved numerical stability.",
        "code": "from jax import custom_jvp\n\n@custom_jvp\ndef log1pexp(x):\n    return jnp.log(1. + jnp.exp(x))\n\n@log1pexp.defjvp\ndef log1pexp_jvp(primals, tangents):\n    x, = primals\n    x_dot, = tangents\n    ans = log1pexp(x)\n    ans_dot = (1 - 1 / (1 + jnp.exp(x))) * x_dot\n    return ans, ans_dot"
      },
      {
        "description": "Tests the custom JVP rule implementation.",
        "code": "from jax import grad, jit, vmap\n\nprint(grad(log1pexp)(100.))\n\nprint(jit(log1pexp)(3.))\nprint(jit(grad(log1pexp))(3.))\nprint(vmap(jit(grad(log1pexp)))(jnp.arange(3.)))"
      },
      {
        "description": "Equivalent implementation using the `defjvps` convenience wrapper.",
        "code": "@custom_jvp\ndef log1pexp(x):\n    return jnp.log(1. + jnp.exp(x))\n\nlog1pexp.defjvps(\n    lambda t, ans, x: (1 - 1 / (1 + jnp.exp(x))) * t\n)"
      },
      {
        "description": "Tests the custom JVP rule implementation using the `defjvps` convenience wrapper.",
        "code": "from jax import grad, jit, vmap\n\nprint(grad(log1pexp)(100.))\n\nprint(jit(log1pexp)(3.))\nprint(jit(grad(log1pexp))(3.))\nprint(vmap(jit(grad(log1pexp)))(jnp.arange(3.)))"
      }
    ]
  },
  {
    "title": "Enforcing Differentiation Conventions with custom_jvp",
    "concepts": [
      "jax.custom_jvp can enforce differentiation conventions, such as handling derivatives at boundaries.",
      "The example demonstrates how to define a custom derivative for f(x) = x / (1 + sqrt(x)) at x = 0, where the standard autodiff rule produces NaN.",
      "By defining a custom JVP rule, a specific derivative value can be enforced.",
      "This is useful when considering f as defined on R+ instead of R."
    ],
    "code_examples": [
      {
        "description": "Naive implementation of f(x) = x / (1 + sqrt(x)).",
        "code": "def f(x):\n    return x / (1 + jnp.sqrt(x))"
      },
      {
        "description": "Demonstrates that the gradient of the naive f implementation produces NaN at x = 0.",
        "code": "from jax import grad\n\nprint(grad(f)(0.))"
      },
      {
        "description": "Implementation of f with a custom JVP rule to enforce a derivative of 1.0 at x = 0.",
        "code": "@custom_jvp\ndef f(x):\n    return x / (1 + jnp.sqrt(x))\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n    x, = primals\n    x_dot, = tangents\n    ans = f(x)\n    ans_dot = ((jnp.sqrt(x) + 2) / (2 * (jnp.sqrt(x) + 1)**2)) * x_dot\n    return ans, ans_dot"
      },
      {
        "description": "Tests the custom JVP rule implementation.",
        "code": "from jax import grad\n\nprint(grad(f)(0.))"
      },
      {
        "description": "Equivalent implementation using the `defjvps` convenience wrapper.",
        "code": "@custom_jvp\ndef f(x):\n    return x / (1 + jnp.sqrt(x))\n\nf.defjvps(\n    lambda t, ans, x: ((jnp.sqrt(x) + 2) / (2 * (jnp.sqrt(x) + 1)**2)) * t\n)"
      },
      {
        "description": "Tests the custom JVP rule implementation using the `defjvps` convenience wrapper.",
        "code": "from jax import grad\n\nprint(grad(f)(0.))"
      }
    ]
  },
  {
    "title": "Gradient Clipping with custom_vjp",
    "concepts": [
      "jax.custom_vjp allows adjusting the computation that autodiff performs.",
      "The example demonstrates reverse-mode gradient clipping using jax.custom_vjp and jnp.clip.",
      "This involves defining a custom backward pass that clips the gradients to a specified range.",
      "This technique is motivated by development workflow rather than strict mathematical correctness."
    ],
    "code_examples": [
      {
        "description": "Implementation of gradient clipping using jax.custom_vjp.",
        "code": "from functools import partial\nfrom jax import custom_vjp\n\n@custom_vjp\ndef clip_gradient(lo, hi, x):\n    return x  # identity function\n\ndef clip_gradient_fwd(lo, hi, x):\n    return x, (lo, hi)  # save bounds as residuals\n\ndef clip_gradient_bwd(res, g):\n    lo, hi = res\n    return (None, None, jnp.clip(g, lo, hi))  # use None to indicate zero cotangents for lo and hi\n\nclip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)"
      },
      {
        "description": "Demonstrates how to plot a sine wave and its derivative with and without gradient clipping.",
        "code": "import matplotlib.pyplot as plt\nfrom jax import vmap, grad\n\nt = jnp.linspace(0, 10, 1000)\nplt.plot(jnp.sin(t))\nplt.plot(vmap(grad(jnp.sin))(t))"
      },
      {
        "description": "Defines a function that applies gradient clipping to the sine wave.",
        "code": "def clip_sin(x):\n    x = clip_gradient(-0.75, 0.75, x)\n    return jnp.sin(x)\n\nplt.plot(clip_sin(t))\nplt.plot(vmap(grad(clip_sin))(t))"
      }
    ]
  },
  {
    "title": "Reverse-Mode Differentiation of Functions with lax.while_loop using custom_vjp",
    "concepts": [
      "jax.custom_vjp can be used to differentiate functions that are JAX-transformable but not efficiently JAX-differentiable due to the presence of `lax.while_loop`.",
      "The example shows how to compute the derivative of a fixed-point iteration computed with `lax.while_loop` using the implicit function theorem.",
      "Instead of differentiating through the iterations of the `while_loop`, the VJP is computed by linearizing at the solution and solving a linear system.",
      "This approach is more memory-efficient than attempting to directly differentiate through the iterations."
    ],
    "code_examples": [
      {
        "description": "Implementation of a fixed-point iteration using `lax.while_loop`.",
        "code": "from jax.lax import while_loop\n\ndef fixed_point(f, a, x_guess):\n    def cond_fun(carry):\n        x_prev, x = carry\n        return jnp.abs(x_prev - x) > 1e-6\n\n    def body_fun(carry):\n        _, x = carry\n        return x, f(a, x)\n\n    _, x_star = while_loop(cond_fun, body_fun, (x_guess, f(a, x_guess)))\n    return x_star"
      },
      {
        "description": "Implementation of Newton's method for calculating square roots using the `fixed_point` function.",
        "code": "def newton_sqrt(a):\n    update = lambda a, x: 0.5 * (x + a / x)\n    return fixed_point(update, a, a)\n\nprint(newton_sqrt(2.))"
      },
      {
        "description": "Shows that the `newton_sqrt` function can be vmapped and jitted.",
        "code": "from jax import jit, vmap\n\nprint(jit(vmap(newton_sqrt))(jnp.array([1., 2., 3., 4.])))"
      },
      {
        "description": "Implementation of `fixed_point` with a custom VJP rule using the implicit function theorem.",
        "code": "from jax import vjp\nfrom functools import partial\n\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef fixed_point(f, a, x_guess):\n    def cond_fun(carry):\n        x_prev, x = carry\n        return jnp.abs(x_prev - x) > 1e-6\n\n    def body_fun(carry):\n        _, x = carry\n        return x, f(a, x)\n\n    _, x_star = while_loop(cond_fun, body_fun, (x_guess, f(a, x_guess)))\n    return x_star\n\ndef fixed_point_fwd(f, a, x_init):\n    x_star = fixed_point(f, a, x_init)\n    return x_star, (a, x_star)\n\ndef fixed_point_rev(f, res, x_star_bar):\n    a, x_star = res\n    _, vjp_a = vjp(lambda a: f(a, x_star), a)\n    a_bar, = vjp_a(fixed_point(partial(rev_iter, f), (a, x_star, x_star_bar), x_star_bar))\n    return a_bar, jnp.zeros_like(x_star)\n\ndef rev_iter(f, packed, u):\n    a, x_star, x_star_bar = packed\n    _, vjp_x = vjp(lambda x: f(a, x), x_star)\n    return x_star_bar + vjp_x(u)[0]\n\nfixed_point.defvjp(fixed_point_fwd, fixed_point_rev)"
      },
      {
        "description": "Tests the custom VJP implementation.",
        "code": "from jax import grad\n\nprint(newton_sqrt(2.))\nprint(grad(newton_sqrt)(2.))\nprint(grad(grad(newton_sqrt))(2.))"
      },
      {
        "description": "Checks against the derivative of jnp.sqrt.",
        "code": "print(grad(jnp.sqrt)(2.))\nprint(grad(grad(jnp.sqrt))(2.))"
      }
    ]
  },
  {
    "title": "Basic Example of jax.custom_jvp",
    "concepts": [
      "Demonstrates a canonical basic example of using jax.custom_jvp.",
      "The example defines a custom JVP rule for the function f(x) = sin(x).",
      "The primal function takes inputs of type a and produces outputs of type b.",
      "The JVP rule function takes a pair of inputs representing the primal inputs of type a and the corresponding tangent inputs of type T a, and produces a pair of outputs representing the primal outputs of type b and tangent outputs of type T b.",
      "The tangent outputs should be a linear function of the tangent inputs.",
      "Even though only a JVP rule is defined, both forward- and reverse-mode differentiation are available."
    ],
    "code_examples": [
      {
        "description": "Defines a function 'f' and its custom JVP rule using jax.custom_jvp.",
        "code": "from jax import custom_jvp\nimport jax.numpy as jnp\n\n# f :: a -> b\n@custom_jvp\ndef f(x):\n    return jnp.sin(x)\n\n# f_jvp :: (a, T a) -> (b, T b)\ndef f_jvp(primals, tangents):\n    x, = primals\n    t, = tangents\n    return f(x), jnp.cos(x) * t\n\nf.defjvp(f_jvp)"
      },
      {
        "description": "Tests the defined function 'f' and its custom JVP rule using jvp.",
        "code": "from jax import jvp\n\nprint(f(3.))\ny, y_dot = jvp(f, (3.,), (1.,))\nprint(y)\nprint(y_dot)"
      },
      {
        "description": "Demonstrates that both forward- and reverse-mode differentiation work even with only a JVP rule defined.",
        "code": "from jax import grad\n\nprint(grad(f)(3.))\nprint(grad(grad(f))(3.))"
      },
      {
        "description": "Using f.defjvp as a decorator",
        "code": "@custom_jvp\ndef f(x):\n    ...\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n    ..."
      }
    ]
  },
  {
    "title": "Multiple Arguments with custom_jvp",
    "concepts": [
      "Demonstrates how to use jax.custom_jvp with functions that have multiple arguments.",
      "The JVP rule needs to handle multiple primal and tangent inputs.",
      "The example defines a custom JVP rule for the function f(x, y) = x**2 * y.",
      "The defjvps convenience wrapper provides an alternative way to define custom JVPs for each argument separately."
    ],
    "code_examples": [
      {
        "description": "Defines a function 'f' with two arguments and its custom JVP rule.",
        "code": "@custom_jvp\ndef f(x, y):\n    return x**2 * y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n    x, y = primals\n    x_dot, y_dot = tangents\n    primal_out = f(x, y)\n    tangent_out = 2 * x * y * x_dot + x**2 * y_dot\n    return primal_out, tangent_out"
      },
      {
        "description": "Tests the defined function 'f' with multiple arguments and its custom JVP rule.",
        "code": "from jax import grad\n\nprint(grad(f)(2., 3.))"
      },
      {
        "description": "Using the defjvps convenience wrapper to define a JVP for each argument separately.",
        "code": "@custom_jvp\ndef f(x):\n    return jnp.sin(x)\n\nf.defjvps(\n    lambda t, ans, x: jnp.cos(x) * t\n)"
      },
      {
        "description": "A defjvps example with multiple arguments",
        "code": "@custom_jvp\ndef f(x, y):\n    return x**2 * y\n\nf.defjvps(\n    lambda x_dot, primal_out, x, y: 2 * x * y * x_dot,\n    lambda y_dot, primal_out, x, y: x**2 * y_dot\n)"
      },
      {
        "description": "Tests a multi-argument defjvps",
        "code": "from jax import grad\n\nprint(grad(f)(2., 3.))\nprint(grad(f, 0)(2., 3.))  # same as above\nprint(grad(f, 1)(2., 3.))"
      },
      {
        "description": "As a shorthand, with defjvps you can pass a None value to indicate that the JVP for a particular argument is zero:",
        "code": "@custom_jvp\ndef f(x, y):\n    return x**2 * y\n\nf.defjvps(\n    lambda x_dot, primal_out, x, y: 2 * x * y * x_dot,\n    None\n)"
      },
      {
        "description": "Example of using shorthand with `None` values",
        "code": "from jax import grad\n\nprint(grad(f)(2., 3.))\nprint(grad(f, 0)(2., 3.))  # same as above\nprint(grad(f, 1)(2., 3.))"
      }
    ]
  },
  {
    "title": "Behavior of custom_jvp when not differentiating",
    "concepts": [
      "When not performing differentiation, the function decorated by jax.custom_jvp is called just as if it weren't decorated.",
      "The custom JVP rule is only invoked during differentiation, whether forward or reverse.",
      "The example demonstrates this behavior by adding print statements to the primal function and its JVP rule."
    ],
    "code_examples": [
      {
        "description": "Demonstrates that the primal function is called when not differentiating.",
        "code": "@custom_jvp\ndef f(x):\n    print('called f!')  # a harmless side-effect\n    return jnp.sin(x)\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n    print('called f_jvp!')  # a harmless side-effect\n    x, = primals\n    t, = tangents\n    return f(x), jnp.cos(x) * t"
      },
      {
        "description": "Show that the print statement in the original function `f` is executed when calling `f` directly and during `vmap` and `jit`.",
        "code": "from jax import vmap, jit\n\nprint(f(3.))\nprint(vmap(f)(jnp.arange(3.)))\nprint(jit(f)(3.))"
      },
      {
        "description": "Show that the print statement in `f_jvp` is printed during differentiation with `jvp`.",
        "code": "from jax import jvp\n\ny, y_dot = jvp(f, (3.,), (1.,))\nprint(y_dot)"
      },
      {
        "description": "Demonstrates that the custom JVP rule is used during differentiation and that f_jvp calls f to compute the primal outputs.",
        "code": "from jax import grad\n\nprint(grad(f)(3.))\n"
      },
      {
        "description": "Example of higher order differentiation",
        "code": "grad(grad(f))(3.)"
      }
    ]
  },
  {
    "title": "Python Control Flow with custom_jvp",
    "concepts": [
      "You can use Python control flow with jax.custom_jvp.",
      "The example demonstrates this by defining a function with an if/else statement and a custom JVP rule that also uses an if/else statement.",
      "This allows for different JVP rules to be applied based on the input value."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using control flow within a custom_jvp function and its associated JVP rule.",
        "code": "@custom_jvp\ndef f(x):\n    if x > 0:\n        return jnp.sin(x)\n    else:\n        return jnp.cos(x)\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n    x, = primals\n    x_dot, = tangents\n    ans = f(x)\n    if x > 0:\n        return ans, 2 * x_dot\n    else:\n        return ans, 3 * x_dot"
      },
      {
        "description": "Tests the defined function with control flow and its custom JVP rule.",
        "code": "from jax import grad\n\nprint(grad(f)(1.))\nprint(grad(f)(-1.))"
      }
    ]
  },
  {
    "title": "Basic Example of jax.custom_vjp",
    "concepts": [
      "While jax.custom_jvp suffices for controlling both forward- and, via JAX\u2019s automatic transposition, reverse-mode differentiation behavior, in some cases we may want to directly control a VJP rule.",
      "A canonical basic example of using jax.custom_vjp is presented.",
      "The example defines a custom VJP rule for the function f(x) = sin(x).",
      "The custom_vjp requires a forward function `f_fwd` and a backward function `f_bwd`.",
      "This allows for different VJP rules to be applied for reverse mode differentiation."
    ],
    "code_examples": [
      {
        "description": "Defines a function 'f' and its custom VJP rule using jax.custom_vjp.",
        "code": "from jax import custom_vjp\nimport jax.numpy as jnp\n\n# f :: a -> b\n@custom_vjp\ndef f(x):\n    return jnp.sin(x)\n\n# f_fwd :: a -> (b, c)\ndef f_fwd(x):\n    return f(x), jnp.cos(x)\n\n# f_bwd :: (c, CT b) -> CT a\ndef f_bwd(cos_x, y_bar):\n    return (cos_x * y_bar,)\n\nf.defvjp(f_fwd, f_bwd)"
      },
      {
        "description": "Tests the defined function and its custom VJP rule.",
        "code": "from jax import grad\n\nprint(f(3.))\nprint(grad(f)(3.))"
      }
    ]
  },
  {
    "title": "Introduction to jax.checkpoint",
    "concepts": [
      "jax.checkpoint (aliased as jax.remat) controls which intermediates are saved or recomputed during backpropagation.",
      "It enables trading off memory usage for computational cost.",
      "Without jax.checkpoint, the forward pass saves Jacobian coefficients and other intermediates (residuals) for the backward pass."
    ],
    "code_examples": [
      {
        "description": "This example defines a function 'g' and 'f' which consists of multiple calls to function 'g'. It then prints the residuals that would be saved during the forward pass if jax.grad(f) were evaluated.  This demonstrates the default behavior of saving all intermediates without checkpointing.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f(W1, W2, W3, x):\n    x = g(W1, x)\n    x = g(W2, x)\n    x = g(W3, x)\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\n# Inspect the 'residual' values to be saved on the forward pass\n# if we were to evaluate `jax.grad(f)(W1, W2, W3, x)`\nfrom jax.ad_checkpoint import print_saved_residuals\njax.ad_checkpoint.print_saved_residuals(f, W1, W2, W3, x)"
      }
    ]
  },
  {
    "title": "Basic Usage of jax.checkpoint",
    "concepts": [
      "Applying jax.checkpoint to a sub-function prevents JAX from saving its residuals.",
      "Only the inputs of the checkpoint-decorated function are saved, and residuals are recomputed on the backward pass.",
      "This reduces memory usage at the cost of increased computation."
    ],
    "code_examples": [
      {
        "description": "This example demonstrates applying jax.checkpoint to the 'g' function within 'f2'. This forces JAX not to save any of g's residuals. Only the inputs of g are saved, and the residuals consumed on the backward pass are re-computed from these inputs.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f2(W1, W2, W3, x):\n    x = jax.checkpoint(g)(W1, x)\n    x = jax.checkpoint(g)(W2, x)\n    x = jax.checkpoint(g)(W3, x)\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\njax.ad_checkpoint.print_saved_residuals(f2, W1, W2, W3, x)"
      }
    ]
  },
  {
    "title": "Rematerialization Policies",
    "concepts": [
      "Rematerialization policies control which intermediate values are saved without modifying the function definition.",
      "A policy is a function that determines if a value should be saved based on its type-level specification.",
      "jax.checkpoint_policies provides pre-defined policies for common scenarios."
    ],
    "code_examples": [
      {
        "description": "This example shows how to use the 'dots_with_no_batch_dims_saveable' policy to save only the results of dot operations with no batch dimensions. A new function 'f3' is created using checkpointing with the specified policy.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f(W1, W2, W3, x):\n    x = g(W1, x)\n    x = g(W2, x)\n    x = g(W3, x)\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\nf3 = jax.checkpoint(f, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\njax.ad_checkpoint.print_saved_residuals(f3, W1, W2, W3, x)"
      },
      {
        "description": "This example demonstrates how to use 'checkpoint_name' to name intermediate values and then use a policy to control whether those named values are saved. The 'save_only_these_names' policy is used to save only the value named 'a'.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.ad_checkpoint import checkpoint_name\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f4(W1, W2, W3, x):\n    x = checkpoint_name(g(W1, x), name='a')\n    x = checkpoint_name(g(W2, x), name='b')\n    x = checkpoint_name(g(W3, x), name='c')\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\nf4 = jax.checkpoint(f4, policy=jax.checkpoint_policies.save_only_these_names('a'))\njax.ad_checkpoint.print_saved_residuals(f4, W1, W2, W3, x)"
      }
    ]
  },
  {
    "title": "Analyzing Forward and Backward Computations",
    "concepts": [
      "The print_fwd_bwd utility displays the forward and backward computations as JAXPRs.",
      "It helps to understand the impact of jax.checkpoint on the computation graph."
    ],
    "code_examples": [
      {
        "description": "This code defines the 'print_fwd_bwd' utility, which uses JAX's 'make_jaxpr' and 'vjp' to display the forward and backward computation graphs of a given function.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import tree_flatten, tree_unflatten\nfrom rich.console import Console\nfrom rich.table import Table\nimport rich.text\nimport rich.jupyter\n\ndef print_fwd_bwd(f, *args, **kwargs) -> None:\n    args, in_tree = tree_flatten(((args, kwargs)))\n    def f_(*args):\n        args, kwargs = tree_unflatten(in_tree, args)\n        return f(*args, **kwargs)\n    fwd = jax.make_jaxpr(lambda *args: jax.vjp(f_, *args))(*args).jaxpr\n    y, f_vjp = jax.vjp(f_, *args)\n    res, in_tree = tree_flatten(f_vjp)\n    def g_(*args):\n        *res, y = args\n        f_vjp = tree_unflatten(in_tree, res)\n        return f_vjp(y)\n    bwd = jax.make_jaxpr(g_)(*res, y).jaxpr\n\n    table = Table(show_header=False, show_lines=True, padding=(1, 2, 0, 2), box=None)\n    table.add_row(\"[bold green]forward computation:\", \"[bold green]backward computation:\")\n    table.add_row(rich.text.Text.from_ansi(str(fwd)), rich.text.Text.from_ansi(str(bwd)))\n    console = Console(width=240, force_jupyter=True)\n    console.print(table)\n\ndef _renderable_repr(self):\n    return self.html\n\nrich.jupyter.JupyterRenderable._repr_html_ = _renderable_repr\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f(W1, W2, W3, x):\n    x = g(W1, x)\n    x = g(W2, x)\n    x = g(W3, x)\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)"
      },
      {
        "description": "This uses the previously defined 'print_fwd_bwd' function to print the forward and backward computation of function 'f' without using checkpointing, and then with checkpointing using the policy 'dots_with_no_batch_dims_saveable'.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import tree_flatten, tree_unflatten\nfrom rich.console import Console\nfrom rich.table import Table\nimport rich.text\nimport rich.jupyter\n\ndef print_fwd_bwd(f, *args, **kwargs) -> None:\n    args, in_tree = tree_flatten(((args, kwargs)))\n    def f_(*args):\n        args, kwargs = tree_unflatten(in_tree, args)\n        return f(*args, **kwargs)\n    fwd = jax.make_jaxpr(lambda *args: jax.vjp(f_, *args))(*args).jaxpr\n    y, f_vjp = jax.vjp(f_, *args)\n    res, in_tree = tree_flatten(f_vjp)\n    def g_(*args):\n        *res, y = args\n        f_vjp = tree_unflatten(in_tree, res)\n        return f_vjp(y)\n    bwd = jax.make_jaxpr(g_)(*res, y).jaxpr\n\n    table = Table(show_header=False, show_lines=True, padding=(1, 2, 0, 2), box=None)\n    table.add_row(\"[bold green]forward computation:\", \"[bold green]backward computation:\")\n    table.add_row(rich.text.Text.from_ansi(str(fwd)), rich.text.Text.from_ansi(str(bwd)))\n    console = Console(width=240, force_jupyter=True)\n    console.print(table)\n\ndef _renderable_repr(self):\n    return self.html\n\nrich.jupyter.JupyterRenderable._repr_html_ = _renderable_repr\n\ndef g(W, x):\n    y = jnp.dot(W, x)\n    return jnp.sin(y)\n\ndef f(W1, W2, W3, x):\n    x = g(W1, x)\n    x = g(W2, x)\n    x = g(W3, x)\n    return x\n\nW1 = jnp.ones((5, 4))\nW2 = jnp.ones((6, 5))\nW3 = jnp.ones((7, 6))\nx = jnp.ones(4)\n\nf3 = jax.checkpoint(f, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\n\n# no use of jax.checkpoint:\nprint_fwd_bwd(f, W1, W2, W3, x)\n\n# using jax.checkpoint with policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable:\nprint_fwd_bwd(f3, W1, W2, W3, x)"
      }
    ]
  },
  {
    "title": "Memory Usage and FLOPs Trade-offs",
    "concepts": [
      "jax.checkpoint offers flexibility in how and when values are computed, trading off memory use against FLOPs.",
      "Jacobian coefficient computations can be performed on the forward pass or the backward pass.",
      "Function composition also presents memory/recomputation trade-offs, which can be controlled by jax.checkpoint."
    ],
    "code_examples": [
      {
        "description": "Illustrates two valid implementations of the VJP rule for sin(x), demonstrating the trade-off between memory and FLOPs. The first version computes cos(x) on the forward pass, while the second computes it on the backward pass.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef sin_vjp(x):\n    y = jnp.sin(x)\n    cos_x = jnp.cos(x)\n    return y, lambda y_bar: cos_x * y_bar\n\ndef sin_vjp2(x):\n    y = jnp.sin(x)\n    return y, lambda y_bar: jnp.cos(x) * y_bar"
      },
      {
        "description": "Demonstrates an alternative VJP rule for function composition, where g_vjp is computed only in the backward pass. This reduces memory usage but increases redundant work.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(x):\n  return x*2\n\ndef h(y):\n  return y**2\n\ndef f(x):\n    y = g(x)\n    z = h(y)\n    return z\n\ndef f_vjp(x):\n    y, g_vjp = jax.vjp(g, x)\n    z, h_vjp = jax.vjp(h, y)\n    def f_bwd(z_bar):\n        y_bar, = h_vjp(z_bar)\n        x_bar, = g_vjp(y_bar)\n        return x_bar\n    return z, f_bwd\n\ndef f_vjp_checkpoint(x):\n    y = g(x)\n    z, h_vjp = jax.vjp(h, y)\n    def f_bwd2(z_bar):\n        y_bar, = h_vjp(z_bar)\n        _, g_vjp = jax.vjp(g, x)\n        x_bar, = g_vjp(y_bar)\n        return x_bar\n    return z, f_bwd2"
      },
      {
        "description": "Shows how jax.checkpoint can be used within a function definition to achieve the same effect as the alternative VJP rule for function composition. This eliminates the need to write VJP functions directly.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(x):\n  return x*2\n\ndef h(y):\n  return y**2\n\ndef f_checkpoint(x):\n    y = jax.checkpoint(g)(x)\n    z = h(y)\n    return z"
      },
      {
        "description": "Illustrates that applying jax.checkpoint to the entire function 'f' or the last sub-function 'h' doesn't save any memory and introduces wasteful recomputation when evaluating jax.grad(f).",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(x):\n  return x*2\n\ndef h(y):\n  return y**2\n\ndef f(x):\n    y = g(x)\n    z = h(y)\n    return z\n\ndef f_grad_bad(x):\n    _ = f(x)\n    _, f_vjp = jax.vjp(f, x)\n    x_bar, = f_vjp(1.0)\n    return x_bar\n\ndef f_grad_bad2(x):\n    y, g_vjp = jax.vjp(g, x)\n    z = h(y)\n    _, h_vjp = jax.vjp(h, y)\n    y_bar, = h_vjp(1.0)\n    x_bar, = g_vjp(y_bar)\n    return x_bar"
      }
    ]
  },
  {
    "title": "Checkpointing Policies in More Detail",
    "concepts": [
      "Policies can be selected from attributes on jax.checkpoint_policies, such as dots_with_no_batch_dims_saveable.",
      "They enable controlling which intermediates are saved without editing the function to be differentiated.",
      "checkpoint_name can be used to name intermediate values, allowing policies to refer to specific values."
    ],
    "code_examples": [
      {
        "description": "Defines functions 'loss', 'predict', and 'layer' representing a simple neural network.  Used in the following example to show checkpointing policies.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef loss(params, x, y):\n    return jnp.sum((predict(params, x) - y) ** 2)\n\ndef predict(params, x):\n    *Ws, Wlast = params\n    for W in Ws:\n        x = layer(W, x)\n    x = jnp.dot(Wlast, x)\n    return x\n\ndef layer(W, x):\n    return jnp.sin(jnp.dot(W, x))\n\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)\n\nfrom jax.ad_checkpoint import print_saved_residuals\nprint_saved_residuals(loss, params, x, y)"
      },
      {
        "description": "This example demonstrates the 'dots_with_no_batch_dims_saveable' policy in the context of a neural network loss function. This saves matrix multiplications with no batch dimension.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef loss(params, x, y):\n    return jnp.sum((predict(params, x) - y) ** 2)\n\ndef predict(params, x):\n    *Ws, Wlast = params\n    for W in Ws:\n        x = layer(W, x)\n    x = jnp.dot(Wlast, x)\n    return x\n\ndef layer(W, x):\n    return jnp.sin(jnp.dot(W, x))\n\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)\n\nfrom jax.ad_checkpoint import print_saved_residuals\n\nloss_checkpoint = jax.checkpoint(loss, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\nprint_saved_residuals(loss_checkpoint, params, x, y)"
      },
      {
        "description": "This example illustrates using 'checkpoint_name' to name intermediate values within the 'predict' function of a neural network. This allows specific layers' outputs to be targeted for checkpointing using policies that refer to names.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.ad_checkpoint import checkpoint_name\n\ndef loss(params, x, y):\n    return jnp.sum((predict(params, x) - y) ** 2)\n\ndef predict(params, x):\n    *Ws, Wlast = params\n    for i, W in enumerate(Ws):\n        x = layer(W, x)\n        x = checkpoint_name(x, name=f'layer{i}_output')\n    x = jnp.dot(Wlast, x)\n    return x\n\ndef layer(W, x):\n    return jnp.sin(jnp.dot(W, x))\n\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)\n\nfrom jax.ad_checkpoint import print_saved_residuals\nprint_saved_residuals(loss, params, x, y)"
      },
      {
        "description": "This example uses the 'save_any_names_but_these' policy to save all named values except for 'layer1_output'. This demonstrates how to selectively exclude named values from being saved.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.ad_checkpoint import checkpoint_name\n\ndef loss(params, x, y):\n    return jnp.sum((predict(params, x) - y) ** 2)\n\ndef predict(params, x):\n    *Ws, Wlast = params\n    for i, W in enumerate(Ws):\n        x = layer(W, x)\n        x = checkpoint_name(x, name=f'layer{i}_output')\n    x = jnp.dot(Wlast, x)\n    return x\n\ndef layer(W, x):\n    return jnp.sin(jnp.dot(W, x))\n\nW1 = W2 = W3 = jnp.ones((4, 4))\nparams = [W1, W2, W3]\nx = jnp.ones(4)\ny = jnp.ones(4)\n\nfrom jax.ad_checkpoint import print_saved_residuals\n\nloss_checkpoint2 = jax.checkpoint(loss, policy=jax.checkpoint_policies.save_any_names_but_these('layer1_output'))\nprint_saved_residuals(loss_checkpoint2, params, x, y)"
      }
    ]
  },
  {
    "title": "Recursive Checkpointing",
    "concepts": [
      "Recursive checkpointing achieves memory usage scaling of O(log2 D) for a chain composition of D functions.",
      "It involves applying jax.checkpoint to a function that itself calls jax.checkpoint-decorated functions."
    ],
    "code_examples": [
      {
        "description": "Defines a function 'chain_compose' that creates a chain composition of multiple functions. Used to demonstrate the scaling of stored residuals with chain length.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.ad_checkpoint import print_saved_residuals\n\ndef chain_compose(funs):\n    def f(x):\n        for fun in funs:\n            x = fun(x)\n        return x\n    return f\n\nf = chain_compose([jnp.sin] * 8)\nprint_saved_residuals(f, 3.)"
      },
      {
        "description": "Illustrates how the number of stored residuals scales linearly with the length of the function chain when no recursive checkpointing is used.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.ad_checkpoint import print_saved_residuals\n\ndef chain_compose(funs):\n    def f(x):\n        for fun in funs:\n            x = fun(x)\n        return x\n    return f\n\nf = chain_compose([jnp.sin] * 16)\nprint_saved_residuals(f, 3.)"
      },
      {
        "description": "Implements recursive checkpointing for a chain composition of functions. The 'recursive_checkpoint' function applies jax.checkpoint recursively to achieve logarithmic memory scaling.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.ad_checkpoint import print_saved_residuals\n\ndef recursive_checkpoint(funs):\n    if len(funs) == 1:\n        return funs[0]\n    elif len(funs) == 2:\n        f1, f2 = funs\n        return lambda x: f1(f2(x))\n    else:\n        f1 = recursive_checkpoint(funs[:len(funs) // 2])\n        f2 = recursive_checkpoint(funs[len(funs) // 2:])\n        return lambda x: f1(jax.checkpoint(f2)(x))\n\nf = recursive_checkpoint([jnp.sin] * 8)\nprint_saved_residuals(f, 3.)"
      }
    ]
  },
  {
    "title": "Introduction to XLA and Jax",
    "concepts": [
      "XLA is a domain-specific compiler for linear algebra.",
      "XLA optimizes Jax code for various hardware backends (CPUs, GPUs, TPUs).",
      "Jax uses XLA's JIT compilation to transform Python functions into optimized XLA computations."
    ],
    "code_examples": []
  },
  {
    "title": "Setting XLA Flags",
    "concepts": [
      "XLA's behavior in Jax can be influenced by setting XLA_FLAGS environment variables.",
      "Flags can be set via `os.environ['XLA_FLAGS']` in Colab notebooks.",
      "Flags can be specified as part of the CLI command when running Python scripts.",
      "XLA_FLAGS should be set before importing Jax or related libraries.",
      "Changing XLA_FLAGS after backend initialization will have no effect."
    ],
    "code_examples": [
      {
        "description": "Setting multiple XLA flags in a Colab notebook.",
        "code": "import os\n\n# Set multiple flags separated by spaces\nos.environ['XLA_FLAGS'] = '--flag1=value1 --flag2=value2'"
      },
      {
        "description": "Setting multiple XLA flags in a Colab notebook. (duplicate)",
        "code": "import os\n\n# Set multiple flags separated by spaces\nos.environ['XLA_FLAGS'] = '--flag1=value1 --flag2=value2'"
      },
      {
        "description": "Setting XLA flags in a Python script using the command line.",
        "code": "XLA_FLAGS='--flag1=value1 --flag2=value2' python3 source.py"
      },
      {
        "description": "Setting XLA flags in a Python script using the command line. (duplicate)",
        "code": "XLA_FLAGS='--flag1=value1 --flag2=value2' python3 source.py"
      }
    ]
  },
  {
    "title": "Commonly Used XLA Flags",
    "concepts": [
      "XLA flags are implementation details and might change.",
      "Open-source XLA flags (CPU, GPU) are defined in `xla/debug_options_flags.cc`.",
      "TPU compiler flags are not part of OpenXLA but commonly used options are listed.",
      "Different flags can optimize performance for specific use cases."
    ],
    "code_examples": []
  },
  {
    "title": "XLA Flags for Debugging",
    "concepts": [
      "`xla_dump_to`: Specifies the directory to dump HLO files and other artifacts for debugging.",
      "`xla_disable_hlo_passes`: Allows disabling specific HLO passes during compilation."
    ],
    "code_examples": []
  },
  {
    "title": "XLA Flags for TPU",
    "concepts": [
      "`xla_tpu_enable_data_parallel_all_reduce_opt`: Optimizes data parallel all-reduces.",
      "`xla_tpu_data_parallel_opt_different_sized_ops`: Enables pipelining of data parallel ops with different output sizes.",
      "`xla_tpu_enable_async_collective_fusion`: Fuses async collective communications with compute ops.",
      "`xla_tpu_enable_async_collective_fusion_fuse_all_gather`: Enables fusing all-gathers within async collective fusion.",
      "`xla_tpu_enable_async_collective_fusion_multiple_steps`: Enables continuing async collectives in multiple steps.",
      "`xla_tpu_overlap_compute_collective_tc`: Enables overlapping compute and communication on a single TensorCore.",
      "`xla_tpu_spmd_rng_bit_generator_unsafe`: Controls partitioned execution of RngBitGenerator.",
      "`xla_tpu_megacore_fusion_allow_ags`: Allows fusing all-gathers with convolutions/all-reduces.",
      "`xla_tpu_enable_ag_backward_pipelining`: Pipelines all-gathers backwards through scan loops."
    ],
    "code_examples": []
  },
  {
    "title": "XLA Flags for GPU",
    "concepts": [
      "`xla_gpu_enable_latency_hiding_scheduler`: Enables latency hiding schedulers to overlap communication with computation.",
      "`xla_gpu_enable_triton_gemm`: Uses Triton-based matrix multiplication.",
      "`xla_gpu_graph_level`: Sets the GPU graph level (legacy flag).",
      "`xla_gpu_all_reduce_combine_threshold_bytes`: Threshold for combining small AllReduce operations.",
      "`xla_gpu_all_gather_combine_threshold_bytes`: Threshold for combining small AllGather operations.",
      "`xla_gpu_reduce_scatter_combine_threshold_bytes`: Threshold for combining small ReduceScatter operations.",
      "`xla_gpu_enable_pipelined_all_gather`: Enables pipelining of all-gather instructions.",
      "`xla_gpu_enable_pipelined_reduce_scatter`: Enables pipelining of reduce-scatter instructions.",
      "`xla_gpu_enable_pipelined_all_reduce`: Enables pipelining of all-reduce instructions.",
      "`xla_gpu_enable_while_loop_double_buffering`: Enables double-buffering for while loop.",
      "`xla_gpu_enable_all_gather_combine_by_dim`: Combine all-gather ops with the same gather dimension or irrespective of their dimension.",
      "`xla_gpu_enable_reduce_scatter_combine_by_dim`: Combine reduce-scatter ops with the same dimension or irrespective of their dimension."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Contributions",
    "concepts": [
      "JAX welcomes contributions from the community.",
      "Guides are available to help developers get set up.",
      "Developer-focused resources are available, such as JAX Enhancement Proposals.",
      "Extension guides document some of JAX's extensible internals."
    ],
    "code_examples": []
  },
  {
    "title": "Contribution Guides and Resources",
    "concepts": [
      "Contribution guides are available.",
      "Information on design and internals is provided."
    ],
    "code_examples": []
  },
  {
    "title": "Contributing to JAX",
    "concepts": [
      "There are multiple ways to contribute to JAX, including answering questions, improving documentation, contributing to the codebase, and contributing to the broader JAX ecosystem.",
      "The JAX project follows Google\u2019s Open Source Community Guidelines.",
      "Pull requests are welcome, especially for issues marked with 'contributions welcome' or 'good first issue'.",
      "For new proposals, open a GitHub Issue or Discussion first for feedback."
    ],
    "code_examples": []
  },
  {
    "title": "Setting up a Development Environment",
    "concepts": [
      "Fork the JAX repository.",
      "Install Python >= 3.10.",
      "Install your fork from source in editable mode using pip.",
      "Add the JAX repo as an upstream remote.",
      "Create a development branch."
    ],
    "code_examples": [
      {
        "description": "Cloning the forked repository and installing JAX in editable mode with testing requirements.",
        "code": "git clone https://github.com/YOUR_USERNAME/jax\ncd jax\npip install -r build/test-requirements.txt\n# Installs all testing requirements.\npip install -e \".[cpu]\"\n# Installs JAX from the current directory in editable mode."
      },
      {
        "description": "Adding the upstream JAX repository as a remote.",
        "code": "git remote add upstream https://www.github.com/jax-ml/jax"
      },
      {
        "description": "Creating a development branch.",
        "code": "git checkout -b name-of-change"
      }
    ]
  },
  {
    "title": "Linting, Type Checking, and Testing",
    "concepts": [
      "Ensure your code passes lint and type checks using pre-commit.",
      "Run the tests using pytest.",
      "You can limit tests to specific files or test names using pytest flags.",
      "JAX offers fine-grained control over running tests; see the documentation for more details."
    ],
    "code_examples": [
      {
        "description": "Installing pre-commit and running it on all files.",
        "code": "pip install pre-commit\npre-commit run --all"
      },
      {
        "description": "Running the JAX test suite.",
        "code": "pytest -n auto tests/"
      },
      {
        "description": "Running tests on a specific file.",
        "code": "pytest -n auto tests/lax_scipy_test.py"
      },
      {
        "description": "Running tests matching a specific name.",
        "code": "pytest -n auto tests/lax_scipy_test.py -k testLogSumExp"
      }
    ]
  },
  {
    "title": "Committing and Submitting Changes",
    "concepts": [
      "Add and commit your changes with a descriptive message.",
      "Sync your code with the main repo using rebase.",
      "Push your commit to a remote branch in your fork.",
      "Create a pull request from the JAX repository.",
      "Ensure your contribution is a single commit (squash if needed)."
    ],
    "code_examples": [
      {
        "description": "Adding files to the commit.",
        "code": "git add file1.py file2.py ..."
      },
      {
        "description": "Committing changes with a message.",
        "code": "git commit -m \"Your commit message\""
      },
      {
        "description": "Fetching and rebasing from the upstream repository.",
        "code": "git fetch upstream\ngit rebase upstream/main"
      },
      {
        "description": "Pushing the changes to the remote repository.",
        "code": "git push --set-upstream origin name-of-change"
      }
    ]
  },
  {
    "title": "Pull Request Checklist",
    "concepts": [
      "Sign the Google Contributor License Agreement (CLA).",
      "A git commit should be a self-contained, single change with a descriptive message.",
      "Pull requests typically comprise a single git commit (squash multiple commits if needed).",
      "Use pre-commit to statically test code quality (mypy and ruff).",
      "The PR will be run through a full test suite on GitHub CI."
    ],
    "code_examples": [
      {
        "description": "Installing and running pre-commit on all files (including documentation notebooks).",
        "code": "pip install pre-commit\npre-commit run --all-files"
      }
    ]
  },
  {
    "title": "Obtaining the JAX Source Code",
    "concepts": [
      "Clone the JAX repository from GitHub using `git clone`.",
      "Navigate to the JAX directory using `cd jax`."
    ],
    "code_examples": [
      {
        "description": "Cloning the JAX repository.",
        "code": "git clone https://github.com/jax-ml/jax\ncd jax"
      }
    ]
  },
  {
    "title": "Building JAX",
    "concepts": [
      "Building JAX involves building jaxlib and installing the jax Python package.",
      "Installing jaxlib from prebuilt wheels is recommended when only modifying Python code.",
      "To build jaxlib from source, a C++ compiler (clang recommended), and Python are required.",
      "Bazel is used for the build process and manages a hermetic Python installation."
    ],
    "code_examples": [
      {
        "description": "Installing jaxlib from a prebuilt wheel.",
        "code": "pip install jaxlib"
      },
      {
        "description": "Building jaxlib for CPU or TPU.",
        "code": "python build/build.py build --wheels=jaxlib --verbose\npip install dist/*.whl  # installs jaxlib (includes XLA)"
      },
      {
        "description": "Building a wheel for a specific Python version.",
        "code": "python build/build.py build --wheels=jaxlib --python_version=3.12 --verbose"
      },
      {
        "description": "Building jaxlib and CUDA plugins.",
        "code": "python build/build.py build --wheels=jaxlib,jax-cuda-plugin,jax-cuda-pjrt"
      },
      {
        "description": "Building jax-cuda-plugin with specific CUDA and CUDNN versions.",
        "code": "python build/build.py build --wheels=jax-cuda-plugin --cuda_version=12.3.2 \\\n--cudnn_version=9.1.1"
      },
      {
        "description": "Building jax-cuda-pjrt with specific CUDA and CUDNN versions.",
        "code": "python build/build.py build --wheels=jax-cuda-pjrt --cuda_version=12.3.2 \\\n--cudnn_version=9.1.1"
      },
      {
        "description": "Pointing to CUDA/CUDNN/NCCL redistributions on local file system.",
        "code": "python build/build.py build --wheels=jax-cuda-plugin \\\n--bazel_options=--repo_env=LOCAL_CUDA_PATH=\"/foo/bar/nvidia/cuda\" \\\n--bazel_options=--repo_env=LOCAL_CUDNN_PATH=\"/foo/bar/nvidia/cudnn\" \\\n--bazel_options=--repo_env=LOCAL_NCCL_PATH=\"/foo/bar/nvidia/nccl\""
      },
      {
        "description": "Building jaxlib with a local XLA copy using Bazel's override_repository feature.",
        "code": "python build/build.py build --wheels=jaxlib --local_xla_path=/path/to/xla"
      }
    ]
  },
  {
    "title": "Windows Build Instructions",
    "concepts": [
      "CUDA on Windows is not supported, use WSL2 instead.",
      "Visual Studio 2019 version 16.5 or newer is required.",
      "Developer Mode needs to be activated.",
      "MSYS2 is needed for bash utilities.",
      "Ensure bazel, patch and realpath are accessible."
    ],
    "code_examples": [
      {
        "description": "Installing patch and coreutils via pacman.",
        "code": "pacman -S patch coreutils"
      },
      {
        "description": "Building jaxlib on Windows.",
        "code": "python .\\build\\build.py build --wheels=jaxlib"
      },
      {
        "description": "Building jaxlib on Windows with debug information.",
        "code": "python .\\build\\build.py build --wheels=jaxlib --bazel_options='--copt=/Z7'"
      }
    ]
  },
  {
    "title": "Managing Hermetic Python",
    "concepts": [
      "JAX builds rely on hermetic Python to ensure reproducibility and platform independence.",
      "The hermetic Python version is automatically set to match the Python used to run build/build.py.",
      "A specific Python version can be selected using the --python_version argument.",
      "The HERMETIC_PYTHON_VERSION environment variable controls the hermetic Python version.",
      "JAX's Python dependencies are pinned to specific versions for reproducibility.",
      "Dependencies are specified in build/requirements_lock_<python version>.txt files.",
      "The build/requirements.in file contains the direct dependencies list.",
      "The lock files can be updated using the build/build.py requirements_update command.",
      "Dependencies on local wheels can be specified.",
      "A nightly update command is available for the latest, potentially unstable dependencies."
    ],
    "code_examples": [
      {
        "description": "Specifying the Python version for the build.",
        "code": "python build/build.py build --python_version=3.12"
      },
      {
        "description": "Setting the HERMETIC_PYTHON_VERSION environment variable in .bazelrc.",
        "code": "build --repo_env=HERMETIC_PYTHON_VERSION=3.12"
      },
      {
        "description": "Updating the requirements lock files.",
        "code": "python build/build.py requirements_update --python_version=3.12"
      },
      {
        "description": "Updating the requirements lock files with pre-release versions.",
        "code": "bazel run //build:requirements.update --repo_env=HERMETIC_PYTHON_VERSION=3.12 -- --pre"
      },
      {
        "description": "Depending on a local wheel file.",
        "code": "echo -e \"\\n$(realpath jaxlib-0.4.27.dev20240416-cp312-cp312-manylinux2014_x86_64.whl)\" >> build/requirements.in\npython build/build.py requirements_update --python_version=3.12"
      },
      {
        "description": "Updating dependencies to nightly versions.",
        "code": "python build/build.py requirements_update --python_version=3.12 --nightly_update"
      },
      {
        "description": "Configuring a custom python interpreter from tarball.",
        "code": "./configure --prefix=/path/to/python\nmake -j12\nmake altinstall\ntar -czpf my_python.tgz python"
      },
      {
        "description": "Plugging in hermetic python from a URL.",
        "code": "--repo_env=HERMETIC_PYTHON_URL=\"file:///local/path/to/my_python.tgz\"\n--repo_env=HERMETIC_PYTHON_SHA256=<file's_sha256_sum>\n\n--repo_env=HERMETIC_PYTHON_PREFIX=\"my_python/install\""
      },
      {
        "description": "Build with custom Python 3.13 from the internet.",
        "code": "bazel build <target> --repo_env=HERMETIC_PYTHON_VERSION=3.13 --repo_env=HERMETIC_PYTHON_URL=\"https://github.com/indygreg/python-build-standalone/releases/download/20241016/cpython-3.13.0+20241016-x86_64-unknown-linux-gnu-install_only.tar.gz\" --repo_env=HERMETIC_PYTHON_SHA256=\"2c8cb15c6a2caadaa98af51df6fe78a8155b8471cb3dd7b9836038e0d3657fb4\""
      },
      {
        "description": "Build with custom Python 3.13 from local file system and custom lock file.",
        "code": "bazel test <target> --repo_env=HERMETIC_PYTHON_VERSION=3.13 --repo_env=HERMETIC_PYTHON_URL=\"file:///path/to/cpython.tar.gz\" --repo_env=HERMETIC_PYTHON_PREFIX=\"prefix/to/strip/in/cython/tar/gz/archive\" --repo_env=HERMETIC_PYTHON_SHA256=<sha256_sum> --repo_env=HERMETIC_REQUIREMENTS_LOCK=\"/absolute/path/to/build:custom_requirements_lock.txt\""
      },
      {
        "description": "Custom set of dependencies using the HERMETIC_REQUIREMENTS_LOCK variable.",
        "code": "bazel test <target> --repo_env=HERMETIC_PYTHON_VERSION=3.13 --repo_env=HERMETIC_REQUIREMENTS_LOCK=\"/absolute/path/to/build:custom_requirements_lock.txt\""
      },
      {
        "description": "Multiple python locks example.",
        "code": "bazel test <target> --repo_env=HERMETIC_PYTHON_VERSION=3.13-scenario1"
      }
    ]
  },
  {
    "title": "Installing and Updating JAX",
    "concepts": [
      "Install jax after jaxlib by running `pip install -e .` in the JAX repository.",
      "Upgrade to the latest version from GitHub by pulling the latest code and rebuilding jaxlib if necessary."
    ],
    "code_examples": [
      {
        "description": "Installing JAX.",
        "code": "pip install -e .  # installs jax"
      }
    ]
  },
  {
    "title": "Running JAX Tests",
    "concepts": [
      "JAX tests can be run using Bazel or pytest.",
      "Configure the JAX build using the --configure_only flag.",
      "Use --wheel_list=jaxlib for CPU tests and CUDA/ROCM for GPU tests.",
      "To run JAX tests using Bazel, use the `bazel test` command targeting specific test suites.",
      "Preinstalled jaxlib can be used by making it available in the hermetic Python.",
      "Environment variables can be used to control test behaviors.",
      "Pytest-xdist is recommended for running tests in parallel.",
      "JAX_NUM_GENERATED_CASES environment variable controls the number of test cases generated.",
      "JAX_ENABLE_X64 enables tests with default 64-bit floats and ints.",
      "--test_targets flag allows specifying a set of tests to run.",
      "Colab notebooks are tested for errors as part of the documentation build.",
      "The hypothesis library is used in some tests."
    ],
    "code_examples": [
      {
        "description": "Configuring the JAX build for jaxlib.",
        "code": "python build/build.py build --wheels=jaxlib --configure_only"
      },
      {
        "description": "Configuring the JAX build for CUDA plugin.",
        "code": "python build/build.py build --wheels=jax-cuda-plugin --configure_only"
      },
      {
        "description": "Running JAX CPU tests using Bazel.",
        "code": "bazel test //tests:cpu_tests //tests:backend_independent_tests"
      },
      {
        "description": "Installing jaxlib within hermetic Python.",
        "code": "echo -e \"\\n jaxlib >= 0.4.26\" >> build/requirements.in\npython build/build.py requirements_update"
      },
      {
        "description": "Installing jaxlib from a local wheel within hermetic Python.",
        "code": "echo -e \"\\n$(realpath jaxlib-0.4.26-cp312-cp312-manylinux2014_x86_64.whl)\" >> build/requirements.in\npython build/build.py requirements_update --python_version=3.12"
      },
      {
        "description": "Running JAX tests with a preinstalled jaxlib.",
        "code": "bazel test --//jax:build_jaxlib=false //tests:cpu_tests //tests:backend_independent_tests"
      },
      {
        "description": "Running GPU tests with multiple accelerators.",
        "code": "bazel test //tests:gpu_tests --local_test_jobs=4 --test_tag_filters=multiaccelerator --//jax:build_jaxlib=false --test_env=XLA_PYTHON_CLIENT_ALLOCATOR=platform"
      },
      {
        "description": "Running GPU tests in parallel on multiple accelerators.",
        "code": "NB_GPUS=2\nJOBS_PER_ACC=4\nJ=$((NB_GPUS * JOBS_PER_ACC))\nMULTI_GPU=\"--run_under $PWD/build/parallel_accelerator_execute.sh --test_env=JAX_ACCELERATOR_COUNT=${NB_GPUS} --test_env=JAX_TESTS_PER_ACCELERATOR=${JOBS_PER_ACC} --local_test_jobs=$J\"\nbazel test //tests:gpu_tests //tests:backend_independent_tests --test_env=XLA_PYTHON_CLIENT_PREALLOCATE=false --test_tag_filters=-multiaccelerator $MULTI_GPU"
      },
      {
        "description": "Running JAX tests using pytest.",
        "code": "pytest -n auto tests"
      },
      {
        "description": "Running JAX tests using pytest with JAX_NUM_GENERATED_CASES.",
        "code": "JAX_NUM_GENERATED_CASES=25 pytest -n auto tests"
      },
      {
        "description": "Running JAX tests using pytest with JAX_ENABLE_X64.",
        "code": "JAX_ENABLE_X64=1 JAX_NUM_GENERATED_CASES=25 pytest -n auto tests"
      },
      {
        "description": "Running a specific test file directly.",
        "code": "JAX_NUM_GENERATED_CASES=5 python tests/lax_numpy_test.py"
      },
      {
        "description": "Running a specific test using the --test_targets flag.",
        "code": "python tests/lax_numpy_test.py --test_targets=\"testPad\""
      }
    ]
  },
  {
    "title": "Introduction to JAX Transformations",
    "concepts": [
      "JAX transformations allow interpreting functions differently.",
      "Transformations override primitive application, changing the flow of values.",
      "Examples of transformations include replacing primitive applications with their JVP rules.",
      "Multiple transformations can be composed, forming stacks of interpreters."
    ],
    "code_examples": [
      {
        "description": "Example function to be transformed.",
        "code": "def f(x):\n  y = sin(x) * 2.\n  z = -y + x\n  return z"
      }
    ]
  },
  {
    "title": "Defining Primitives",
    "concepts": [
      "Primitive operations are atomic units of processing.",
      "A `Primitive` class is defined to represent primitive operations.",
      "Functions like `add` and `sin` are wrappers around `bind1`, which calls `bind`.",
      "`bind` is the interception point for applying transformation rules.",
      "Functions use a convention of positional arguments for array data and keyword arguments for metadata."
    ],
    "code_examples": [
      {
        "description": "Defining primitive operations as instances of the Primitive class.",
        "code": "from typing import NamedTuple\nclass Primitive(NamedTuple):\n  name: str\n\nadd_p = Primitive('add')\nmul_p = Primitive('mul')\nneg_p = Primitive(\"neg\")\nsin_p = Primitive(\"sin\")\ncos_p = Primitive(\"cos\")\nreduce_sum_p = Primitive(\"reduce_sum\")\ngreater_p = Primitive(\"greater\")\nless_p = Primitive(\"less\")\ntranspose_p = Primitive(\"transpose\")\nbroadcast_p = Primitive(\"broadcast\")\n\ndef add(x, y):\n  return bind1(add_p, x, y)\n\ndef mul(x, y):\n  return bind1(mul_p, x, y)\n\ndef neg(x):\n  return bind1(neg_p, x)\n\ndef sin(x):\n  return bind1(sin_p, x)\n\ndef cos(x):\n  return bind1(cos_p, x)\n\ndef greater(x, y):\n  return bind1(greater_p, x, y)\n\ndef less(x, y):\n  return bind1(less_p, x, y)\n\ndef transpose(x, perm):\n  return bind1(transpose_p, x, perm=perm)\n\ndef broadcast(x, shape, axes):\n  return bind1(broadcast_p, x, shape=shape, axes=axes)\n\ndef reduce_sum(x, axis=None):\n  if axis is None:\n    axis = tuple(range(np.ndim(x)))\n  if type(axis) is int:\n    axis = (axis,)\n  return bind1(reduce_sum_p, x, axis=axis)\n\ndef bind1(prim, *args, **params):\n  out, = bind(prim, *args, **params)\n  return out"
      }
    ]
  },
  {
    "title": "Interpreter Stack and MainTrace",
    "concepts": [
      "Active interpreters are represented as a stack.",
      "Each stack element is a `MainTrace` instance.",
      "`MainTrace` contains the interpreter's level, trace type, and global data.",
      "`new_main` context manager pushes and pops interpreters from the stack.",
      "The interpreter stack's bottom usually contains an evaluation interpreter for standard evaluation."
    ],
    "code_examples": [
      {
        "description": "Definition of the MainTrace class and the new_main context manager.",
        "code": "from collections.abc import Sequence\nfrom contextlib import contextmanager\nfrom typing import Any\n\nclass MainTrace(NamedTuple):\n  level: int\n  trace_type: type['Trace']\n  global_data: Any | None\n\ntrace_stack: list[MainTrace] = []\ndynamic_trace: MainTrace | None = None # to be employed in Part 3\n\n@contextmanager\ndef new_main(trace_type: type['Trace'], global_data=None):\n  level = len(trace_stack)\n  main = MainTrace(level, trace_type, global_data)\n  trace_stack.append(main)\n  try:\n    yield main\n  finally:\n    trace_stack.pop()"
      }
    ]
  },
  {
    "title": "Trace and Tracer Base Classes",
    "concepts": [
      "`Trace` and `Tracer` are base classes for interpreters.",
      "`Tracer` represents a boxed-up value with context data.",
      "`Trace` handles boxing values into `Tracer` instances and primitive application.",
      "`Trace` contains a reference to its corresponding `MainTrace` instance.",
      "`Tracer` carries an abstract value and forwards infix operators to it."
    ],
    "code_examples": [
      {
        "description": "Definition of the Trace class.",
        "code": "class Trace:\n  main: MainTrace\n  def __init__(self, main: MainTrace) -> None:\n    self.main = main\n  def pure(self, val):\n    assert False  # must override\n  def lift(self, val):\n    assert False  # must override\n  def process_primitive(self, primitive, tracers, params):\n    assert False  # must override"
      },
      {
        "description": "Definition of the Tracer class.",
        "code": "import numpy as np\n\nclass Tracer:\n  _trace: Trace\n  __array_priority__ = 1000\n\n  @property\n  def aval(self):\n    assert False  # must override\n\n  def full_lower(self):\n    return self  # default implementation\n\n  def __neg__(self):\n    return self.aval._neg(self)\n\n  def __add__(self, other):\n    return self.aval._add(self, other)\n\n  def __radd__(self, other):\n    return self.aval._radd(self, other)\n\n  def __mul__(self, other):\n    return self.aval._mul(self, other)\n\n  def __rmul__(self, other):\n    return self.aval._rmul(self, other)\n\n  def __gt__(self, other):\n    return self.aval._gt(self, other)\n\n  def __lt__(self, other):\n    return self.aval._lt(self, other)\n\n  def __bool__(self):\n    return self.aval._bool(self)\n\n  def __nonzero__(self):\n    return self.aval._nonzero(self)\n\n  def __getattr__(self, name):\n    try:\n      return getattr(self.aval, name)\n    except AttributeError:\n      raise AttributeError(f\"{self.__class__.__name__} has no attribute {name}\")\n\n  def swap(f):\n    return lambda x, y: f(y, x)"
      },
      {
        "description": "Definition of the ShapedArray and ConcreteArray classes.",
        "code": "class ShapedArray:\n  array_abstraction_level = 1\n  shape: tuple[int, ...]\n  dtype: np.dtype\n\n  def __init__(self, shape, dtype):\n    self.shape = shape\n    self.dtype = dtype\n\n  @property\n  def ndim(self):\n    return len(self.shape)\n\n  _neg = staticmethod(neg)\n  _add = staticmethod(add)\n  _radd = staticmethod(swap(add))\n  _mul = staticmethod(mul)\n  _rmul = staticmethod(swap(mul))\n  _gt = staticmethod(greater)\n  _lt = staticmethod(less)\n\n  @staticmethod\n  def _bool(tracer):\n    raise Exception(\"ShapedArray can't be unambiguously converted to bool\")\n\n  @staticmethod\n  def _nonzero(tracer):\n    raise Exception(\"ShapedArray can't be unambiguously converted to bool\")\n\n  def str_short(self):\n    return f'{self.dtype.name}[{','.join(str(d) for d in self.shape)}]'\n\n  def __hash__(self):\n    return hash((self.shape, self.dtype))\n\n  def __eq__(self, other):\n    return (type(self) is type(other) and\n            self.shape == other.shape and self.dtype == other.dtype)\n\n  def __repr__(self):\n    return f\"ShapedArray(shape={self.shape}, dtype={self.dtype})\"\n\n\nclass ConcreteArray(ShapedArray):\n  array_abstraction_level = 2\n  val: np.ndarray\n\n  def __init__(self, val):\n    self.val = val\n    self.shape = val.shape\n    self.dtype = val.dtype\n\n  @staticmethod\n  def _bool(tracer):\n    return bool(tracer.aval.val)\n\n  @staticmethod\n  def _nonzero(tracer):\n    return bool(tracer.aval.val)\n\ndef get_aval(x):\n  if isinstance(x, Tracer):\n    return x.aval\n  elif type(x) in jax_types:\n    return ConcreteArray(np.asarray(x))\n  else:\n    raise TypeError(x)\n\njax_types = {bool, int, float, np.bool_, np.int32, np.int64, np.float32, np.float64, np.ndarray}"
      }
    ]
  },
  {
    "title": "The Bind Function",
    "concepts": [
      "`bind` determines which interpreter handles a primitive application.",
      "`find_top_trace` identifies the appropriate interpreter based on input arguments.",
      "`process_primitive` on the `Trace` applies the interpreter's rule.",
      "`full_raise` boxes inputs into `Tracer` instances, while `full_lower` unboxes values."
    ],
    "code_examples": [
      {
        "description": "Implementation of the bind function.",
        "code": "def bind(prim, *args, **params):\n  top_trace = find_top_trace(args)\n  tracers = [full_raise(top_trace, arg) for arg in args]\n  outs = top_trace.process_primitive(prim, tracers, params)\n  return [full_lower(out) for out in outs]"
      }
    ]
  },
  {
    "title": "Finding the Top Trace",
    "concepts": [
      "`find_top_trace` returns the highest-level interpreter associated with the `Tracer` instances.",
      "If no `Tracer` is present, it returns the bottom-of-stack interpreter.",
      "This optimization avoids applying irrelevant transformations.",
      "The current approach relies on data dependence for transformation."
    ],
    "code_examples": [
      {
        "description": "Implementation of the find_top_trace function.",
        "code": "import operator as op\n\ndef find_top_trace(xs) -> Trace:\n  top_main = max((x._trace.main for x in xs if isinstance(x, Tracer)),\n                 default=trace_stack[0],\n                 key=op.attrgetter('level'))\n  if dynamic_trace and dynamic_trace.level > top_main.level:\n    top_main = dynamic_trace\n  return top_main.trace_type(top_main)"
      }
    ]
  },
  {
    "title": "Raising and Lowering Values",
    "concepts": [
      "`full_raise` boxes values into `Tracer` instances.",
      "It calls `Trace.pure` for non-`Tracer` constants and `Trace.lift` for existing `Tracer` instances from lower-level interpreters.",
      "`full_lower` unboxes values from `Tracer` instances."
    ],
    "code_examples": [
      {
        "description": "Implementation of full_lower and full_raise functions.",
        "code": "def full_lower(val: Any):\n  if isinstance(val, Tracer):\n    return val.full_lower()\n  else:\n    return val\n\ndef full_raise(trace: Trace, val: Any) -> Tracer:\n  if not isinstance(val, Tracer):\n    assert type(val) in jax_types\n    return trace.pure(val)\n\n  level = trace.main.level\n  if val._trace.main is trace.main:\n    return val\n  elif val._trace.main.level < level:\n    return trace.lift(val)\n  elif val._trace.main.level > level:\n    raise Exception(f\"Can't lift level {val._trace.main.level} to {level}.\")\n  else:\n    # val._trace.level == level\n    raise Exception(f\"Different traces at same level: {val._trace}, {trace}.\")"
      }
    ]
  },
  {
    "title": "Evaluation Interpreter",
    "concepts": [
      "The evaluation interpreter sits at the bottom of the interpreter stack.",
      "`EvalTrace` defines `pure` and `lift` as identity functions (no boxing).",
      "`process_primitive` applies NumPy implementations of primitive operations.",
      "`impl_rules` maps primitive operations to their NumPy implementations."
    ],
    "code_examples": [
      {
        "description": "Definition of EvalTrace and the implementation rules.",
        "code": "class EvalTrace(Trace):\n  pure = lift = lambda self, x: x  # no boxing in Tracers needed\n  def process_primitive(self, primitive, tracers, params):\n    return impl_rules[primitive](*tracers, **params)\n\ntrace_stack.append(MainTrace(0, EvalTrace, None))  # special bottom of the stack\n\n# NB: in JAX, instead of a dict we attach impl rules to the Primitive instance\nimpl_rules = {}\nimpl_rules[add_p] = lambda x, y: [np.add(x, y)]\nimpl_rules[mul_p] = lambda x, y: [np.multiply(x, y)]\nimpl_rules[neg_p] = lambda x: [np.negative(x)]\nimpl_rules[sin_p] = lambda x: [np.sin(x)]\nimpl_rules[cos_p] = lambda x: [np.cos(x)]\nimpl_rules[reduce_sum_p] = lambda x, *, axis: [np.sum(x, axis)]\nimpl_rules[greater_p] = lambda x, y: [np.greater(x, y)]\nimpl_rules[less_p] = lambda x, y: [np.less(x, y)]\nimpl_rules[transpose_p] = lambda x, *, perm: [np.transpose(x, perm)]\n\ndef broadcast_impl(x, *, shape, axes):\n  for axis in sorted(axes):\n    x = np.expand_dims(x, axis)\n  return [np.broadcast_to(x, shape)]\nimpl_rules[broadcast_p] = broadcast_impl"
      },
      {
        "description": "Example of evaluating a user function.",
        "code": "def f(x):\n  y = sin(x) * 2.\n  z = -y + x\n  return z\n\nprint(f(3.0))"
      }
    ]
  },
  {
    "title": "Helper Functions",
    "concepts": [
      "`zeros_like` creates an array of zeros with the same shape and dtype as the input.",
      "`unzip2` splits a list of pairs into two separate lists.",
      "`map` and `zip` are custom implementations of built-in functions."
    ],
    "code_examples": [
      {
        "description": "Definition of helper functions.",
        "code": "import builtins\n\ndef zeros_like(val):\n  aval = get_aval(val)\n  return np.zeros(aval.shape, aval.dtype)\n\ndef unzip2(pairs):\n  lst1, lst2 = [], []\n  for x1, x2 in pairs:\n    lst1.append(x1)\n    lst2.append(x2)\n  return lst1, lst2\n\ndef map(f, *xs):\n  return list(builtins.map(f, *xs))\n\ndef zip(*args):\n  fst, *rest = args = map(list, args)\n  n = len(fst)\n  for arg in rest:\n    assert len(arg) == n\n  return list(builtins.zip(*args))"
      }
    ]
  },
  {
    "title": "Forward-Mode Automatic Differentiation (JVP)",
    "concepts": [
      "`JVPTracer` carries a primal-tangent pair.",
      "`JVPTrace` applies JVP rules.",
      "`pure` and `lift` package values into `JVPTracer` instances with zero tangent values.",
      "JVP rules define how primitive operations propagate primal and tangent values."
    ],
    "code_examples": [
      {
        "description": "Definition of JVPTracer and JVPTrace classes.",
        "code": "class JVPTracer(Tracer):\n  def __init__(self, trace, primal, tangent):\n    self._trace = trace\n    self.primal = primal\n    self.tangent = tangent\n\n  @property\n  def aval(self):\n    return get_aval(self.primal)\n\n\nclass JVPTrace(Trace):\n  pure = lift = lambda self, val: JVPTracer(self, val, zeros_like(val))\n\n  def process_primitive(self, primitive, tracers, params):\n    primals_in, tangents_in = unzip2((t.primal, t.tangent) for t in tracers)\n    jvp_rule = jvp_rules[primitive]\n    primal_outs, tangent_outs = jvp_rule(primals_in, tangents_in, **params)\n    return [JVPTracer(self, x, t) for x, t in zip(primal_outs, tangent_outs)]\n\njvp_rules = {}"
      }
    ]
  },
  {
    "title": "JVP Rules for Primitives",
    "concepts": [
      "JVP rules define the derivative computation for each primitive operation.",
      "Each rule takes primal and tangent inputs and returns primal and tangent outputs."
    ],
    "code_examples": [
      {
        "description": "JVP rules for various primitive operations.",
        "code": "def add_jvp(primals, tangents):\n  (x, y), (x_dot, y_dot) = primals, tangents\n  return [x + y], [x_dot + y_dot]\n\njvp_rules[add_p] = add_jvp\n\ndef mul_jvp(primals, tangents):\n  (x, y), (x_dot, y_dot) = primals, tangents\n  return [x * y], [x_dot * y + x * y_dot]\n\njvp_rules[mul_p] = mul_jvp\n\ndef sin_jvp(primals, tangents):\n  (x,), (x_dot,) = primals, tangents\n  return [sin(x)], [cos(x) * x_dot]\n\njvp_rules[sin_p] = sin_jvp\n\ndef cos_jvp(primals, tangents):\n  (x,), (x_dot,) = primals, tangents\n  return [cos(x)], [-sin(x) * x_dot]\n\njvp_rules[cos_p] = cos_jvp\n\ndef neg_jvp(primals, tangents):\n  (x,), (x_dot,) = primals, tangents\n  return [neg(x)], [neg(x_dot)]\n\njvp_rules[neg_p] = neg_jvp\n\ndef reduce_sum_jvp(primals, tangents, *, axis):\n  (x,), (x_dot,) = primals, tangents\n  return [reduce_sum(x, axis)], [reduce_sum(x_dot, axis)]\n\njvp_rules[reduce_sum_p] = reduce_sum_jvp\n\ndef greater_jvp(primals, tangents):\n  (x, y), _ = primals, tangents\n  out_primal = greater(x, y)\n  return [out_primal], [zeros_like(out_primal)]\n\njvp_rules[greater_p] = greater_jvp\n\ndef less_jvp(primals, tangents):\n  (x, y), _ = primals, tangents\n  out_primal = less(x, y)\n  return [out_primal], [zeros_like(out_primal)]\n\njvp_rules[less_p] = less_jvp"
      }
    ]
  },
  {
    "title": "Transformation API (jvp_v1)",
    "concepts": [
      "`jvp_v1` kicks off the JVP transformation.",
      "It creates a `JVPTrace` within a `new_main` context.",
      "It packages primal and tangent inputs into `JVPTracer` instances.",
      "It raises the output to a `JVPTracer` and extracts the primal and tangent results."
    ],
    "code_examples": [
      {
        "description": "Definition of the jvp_v1 function.",
        "code": "def jvp_v1(f, primals, tangents):\n  with new_main(JVPTrace) as main:\n    trace = JVPTrace(main)\n    tracers_in = [JVPTracer(trace, x, t) for x, t in zip(primals, tangents)]\n    out = f(*tracers_in)\n    tracer_out = full_raise(trace, out)\n    primal_out, tangent_out = tracer_out.primal, tracer_out.tangent\n    return primal_out, tangent_out"
      },
      {
        "description": "Examples of using jvp_v1 to compute derivatives.",
        "code": "x = 3.0\ny, sin_deriv_at_3 = jvp_v1(sin, (x,), (1.0,))\nprint(sin_deriv_at_3)\nprint(cos(3.0))\n\ndef f(x):\n  y = sin(x) * 2.\n  z = -y + x\n  return z\n\nx, xdot = 3., 1.\ny, ydot = jvp_v1(f, (x,), (xdot,))\nprint(y)\nprint(ydot)\n\ndef deriv(f):\n  return lambda x: jvp_v1(f, (x,), (1.,))[1]\n\nprint(deriv(sin)(3.))\nprint(deriv(deriv(sin))(3.))\nprint(deriv(deriv(deriv(sin)))(3.))\nprint(deriv(deriv(deriv(deriv(sin))))(3.))\n\ndef f(x):\n  if x > 0.:  # Python control flow\n    return 2. * x\n  else:\n    return x\n\nprint(deriv(f)(3.))\nprint(deriv(f)(-3.))"
      }
    ]
  },
  {
    "title": "Introduction to JAX Interpreters",
    "concepts": [
      "JAX is composed of primitive operations and interpreters.",
      "Interpreters provide different ways to execute JAX operations (evaluation, differentiation, compilation).",
      "A global context variable tracks the current interpreter.",
      "User-facing functions dispatch to the current interpreter."
    ],
    "code_examples": [
      {
        "description": "Defines primitive operations (Op enum), an Interpreter class, and an EvalInterpreter for concrete evaluation. Also, it defines the global current_interpreter, a context manager set_interpreter to change the interpreter, and user-facing add and mul functions.",
        "code": "from enum import Enum, auto\nfrom contextlib import contextmanager\nfrom typing import Any\n\n# The full (closed) set of primitive operations\nclass Op(Enum):\n    add = auto()  # addition on floats\n    mul = auto()  # multiplication on floats\n\n# Interpreters have rules for handling each primitive operation.\nclass Interpreter:\n    def interpret_op(self, op: Op, args: tuple[Any, ...]):\n        assert False, \"subclass should implement this\"\n\n# Our first interpreter is the \"evaluating interpreter\" which performs ordinary\n# concrete evaluation.\nclass EvalInterpreter:\n    def interpret_op(self, op, args):\n        assert all(isinstance(arg, float) for arg in args)\n        match op:\n            case Op.add:\n                x, y = args\n                return x + y\n            case Op.mul:\n                x, y = args\n                return x * y\n            case _:\n                raise ValueError(f\"Unrecognized primitive op: {op}\")\n\n# The current interpreter is initially the evaluating interpreter.\ncurrent_interpreter = EvalInterpreter()\n\n# A context manager for temporarily changing the current interpreter\n@contextmanager\ndef set_interpreter(new_interpreter):\n    global current_interpreter\n    prev_interpreter = current_interpreter\n    try:\n        current_interpreter = new_interpreter\n        yield\n    finally:\n        current_interpreter = prev_interpreter\n\n# The user-facing functions `mul` and `add` dispatch to the current interpreter.\ndef add(x, y):\n    return current_interpreter.interpret_op(Op.add, (x, y))\n\ndef mul(x, y):\n    return current_interpreter.interpret_op(Op.mul, (x, y))"
      }
    ]
  },
  {
    "title": "Example User-Defined Function",
    "concepts": [
      "A function can be defined using the primitive operations.",
      "The function's behavior changes based on the active interpreter."
    ],
    "code_examples": [
      {
        "description": "Defines a simple function `foo` using the previously defined `add` and `mul` functions and then calls this with the default EvalInterpreter.",
        "code": "def foo(x):\n    return mul(x, add(x, 3.0))\n\nprint(foo(2.0))"
      }
    ]
  },
  {
    "title": "Forward-Mode Automatic Differentiation (AD)",
    "concepts": [
      "Forward-mode AD propagates primal-tangent pairs (dual numbers).",
      "Dual numbers are represented as (primal, tangent) pairs.",
      "Rules are defined for propagating dual numbers through primitive operations.",
      "Addition of dual numbers involves adding the primal and tangent components separately.",
      "Multiplication of dual numbers involves a more complex calculation of the tangent component."
    ],
    "code_examples": [
      {
        "description": "Defines DualNumber class and corresponding add and multiply functions to handle them.",
        "code": "from dataclasses import dataclass\n\n# A primal-tangent pair is conventionally called a \"dual number\"\n@dataclass\nclass DualNumber:\n    primal: float\n    tangent: float\n\ndef add_dual(x: DualNumber, y: DualNumber) -> DualNumber:\n    return DualNumber(x.primal + y.primal, x.tangent + y.tangent)\n\ndef mul_dual(x: DualNumber, y: DualNumber) -> DualNumber:\n    return DualNumber(x.primal * y.primal, x.primal * y.tangent + x.tangent * y.primal)\n\ndef foo_dual(x: DualNumber) -> DualNumber:\n    return mul_dual(x, add_dual(x, DualNumber(3.0, 0.0)))\n\nprint(foo_dual(DualNumber(2.0, 1.0)))"
      }
    ]
  },
  {
    "title": "JVP Interpreter",
    "concepts": [
      "JVPInterpreter propagates dual numbers using the interpreter framework.",
      "The JVPInterpreter.lift method converts constants to dual numbers.",
      "The JVPInterpreter maintains a reference to the previous interpreter for nested interpretations.",
      "Higher-order AD can be implemented by carefully considering nested interpreters."
    ],
    "code_examples": [
      {
        "description": "Defines TaggedDualNumber, JVPInterpreter, and the jvp function that leverages the interpreter to perform forward-mode AD.  It also includes derivative and nth_order_derivative functions.",
        "code": "from dataclasses import dataclass\n\n# This is like DualNumber above except that is also has a pointer to the\n# interpreter it belongs to, which is needed to avoid \"perturbation confusion\"\n# in higher order differentiation.\n@dataclass\nclass TaggedDualNumber:\n    interpreter: Interpreter\n    primal: float\n    tangent: float\n\nclass JVPInterpreter(Interpreter):\n    def __init__(self, prev_interpreter: Interpreter):\n        # We keep a pointer to the interpreter that was current when this\n        # interpreter was first invoked. That's the context in which our\n        # rules should run.\n        self.prev_interpreter = prev_interpreter\n\n    def interpret_op(self, op, args):\n        args = tuple(self.lift(arg) for arg in args)\n        with set_interpreter(self.prev_interpreter):\n            match op:\n                case Op.add:\n                    # Notice that we use `add` and `mul` here, which are the\n                    # interpreter-dispatching functions defined earlier.\n                    x, y = args\n                    return self.dual_number(add(x.primal, y.primal), add(x.tangent, y.tangent))\n                case Op.mul:\n                    x, y = args\n                    x = self.lift(x)\n                    y = self.lift(y)\n                    return self.dual_number(mul(x.primal, y.primal), add(mul(x.primal, y.tangent), mul(x.tangent, y.primal)))\n\n    def dual_number(self, primal, tangent):\n        return TaggedDualNumber(self, primal, tangent)\n\n    # Lift a constant value (constant with respect to this interpreter) to\n    # a TaggedDualNumber.\n    def lift(self, x):\n        if isinstance(x, TaggedDualNumber) and x.interpreter is self:\n            return x\n        else:\n            return self.dual_number(x, 0.0)\n\ndef jvp(f, primal, tangent):\n    jvp_interpreter = JVPInterpreter(current_interpreter)\n    dual_number_in = jvp_interpreter.dual_number(primal, tangent)\n    with set_interpreter(jvp_interpreter):\n        result = f(dual_number_in)\n    dual_number_out = jvp_interpreter.lift(result)\n    return dual_number_out.primal, dual_number_out.tangent\n\n# Let's try it out:\nprint(jvp(foo, 2.0, 1.0))\n\n# Because we were careful to consider nesting interpreters, higher-order AD\n# works out of the box:\ndef derivative(f, x):\n    _, tangent = jvp(f, x, 1.0)\n    return tangent\n\ndef nth_order_derivative(n, f, x):\n    if n == 0:\n        return f(x)\n    else:\n        return derivative(lambda x: nth_order_derivative(n-1, f, x), x)\n\nprint(nth_order_derivative(0, foo, 2.0))\nprint(nth_order_derivative(1, foo, 2.0))\nprint(nth_order_derivative(2, foo, 2.0))\n# The rest are zero because `foo` is only a second-order polymonial\nprint(nth_order_derivative(3, foo, 2.0))\nprint(nth_order_derivative(4, foo, 2.0))"
      }
    ]
  },
  {
    "title": "Staging to an Internal Representation (IR)",
    "concepts": [
      "Some program transformations (e.g., dead-code elimination, reverse-mode AD) require staging the program into an IR.",
      "The IR represents the program as a data structure that can be traversed in any order.",
      "A jaxpr is an IR function containing parameters, equations (operations), and a return value.",
      "Atoms in the IR can be variables or literals.",
      "A StagingInterpreter builds the IR by recording each operation encountered during program execution."
    ],
    "code_examples": [
      {
        "description": "Defines the data structures for the IR (Jaxpr, Equation) and the StagingInterpreter to create the IR from a Python function.",
        "code": "from dataclasses import dataclass\n\nVar = str  # Variables are just strings in this untyped IR\nAtom = Var | float  # Atoms (arguments to operations) can be variables or (float) literals\n\n# Equation - a single line in our IR like `z = mul(x, y)`\n@dataclass\nclass Equation:\n    var: Var  # The variable name of the result\n    op: Op  # The primitive operation we're applying\n    args: tuple[Atom]  # The arguments we're applying the primitive operation to\n\n# We call an IR function a \"Jaxpr\", for \"JAX expression\"\n@dataclass\nclass Jaxpr:\n    parameters: list[Var]  # The function's formal parameters (arguments)\n    equations: list[Equation]  # The body of the function, a list of instructions/equations\n    return_val: Atom  # The function's return value\n\n    def __str__(self):\n        lines = []\n        lines.append(', '.join(b for b in self.parameters) + ' ->')\n        for eqn in self.equations:\n            args_str = ', '.join(str(arg) for arg in eqn.args)\n            lines.append(f'  {eqn.var} = {eqn.op}({args_str})')\n        lines.append(self.return_val)\n        return '\\n'.join(lines)\n\nclass StagingInterpreter(Interpreter):\n    def __init__(self):\n        self.equations = []  # A mutable list of all the ops we've seen so far\n        self.name_counter = 0  # Counter for generating unique names\n\n    def fresh_var(self):\n        self.name_counter += 1\n        return \"v_\" + str(self.name_counter)\n\n    def interpret_op(self, op, args):\n        binder = self.fresh_var()\n        self.equations.append(Equation(binder, op, args))\n        return binder\n\ndef build_jaxpr(f, num_args):\n    interpreter = StagingInterpreter()\n    parameters = tuple(interpreter.fresh_var() for _ in range(num_args))\n    with set_interpreter(interpreter):\n        result = f(*parameters)\n    return Jaxpr(parameters, interpreter.equations, result)"
      },
      {
        "description": "Demonstrates how to build and print the IR for the `foo` function.",
        "code": "print(build_jaxpr(foo, 1))"
      },
      {
        "description": "Defines an interpreter to evaluate Jaxpr, thereby round-tripping from Python code to IR back to interpretable Python code. It also demonstrates the re-interpretability of the resulting code by using jvp again.",
        "code": "def eval_jaxpr(jaxpr, args):\n    # An environment mapping variables to values\n    env = dict(zip(jaxpr.parameters, args))\n\n    def eval_atom(x):\n        return env[x] if isinstance(x, Var) else x\n\n    for eqn in jaxpr.equations:\n        args = tuple(eval_atom(x) for x in eqn.args)\n        env[eqn.var] = current_interpreter.interpret_op(eqn.op, args)\n    return eval_atom(jaxpr.return_val)\n\nprint(eval_jaxpr(build_jaxpr(foo, 1), (2.0,)))\n\nprint(jvp(lambda x: eval_jaxpr(build_jaxpr(foo, 1), (x,)), 2.0, 1.0))"
      }
    ]
  },
  {
    "title": "Introduction to JEPs",
    "concepts": [
      "JEPs are used for larger changes requiring more discussion.",
      "JEPs facilitate longer documents that can be discussed in pull requests.",
      "JEP structure is designed to be lightweight initially.",
      "JEPs are preferred for design documents to improve discoverability and for future reference.",
      "JEPs are useful for managing extensive discussions that are difficult to track in issues or pull requests.",
      "JEPs allow summarizing discussions within the document itself, with updates discussed in pull requests."
    ],
    "code_examples": []
  },
  {
    "title": "JEP Creation Process",
    "concepts": [
      "Create an issue with the JEP label.",
      "Link all related pull requests to the JEP issue.",
      "Create a pull request to add a file named %d-{short-title}.md, where %d is the issue number."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX PRNG Design",
    "concepts": [
      "The PRNG design should be expressive and convenient.",
      "The PRNG design should enable reproducible program execution.",
      "The PRNG design should have semantics invariant to compilation and backends.",
      "The PRNG design should enable vectorization using SIMD hardware.",
      "The PRNG design should be parallelizable without adding sequencing constraints.",
      "The PRNG design should scale to multi-replica, multi-core, and distributed computation.",
      "The PRNG design should fit with JAX and XLA semantics.",
      "JAX PRNG is based on Threefry counter PRNG and a functional array-oriented splitting model."
    ],
    "code_examples": []
  },
  {
    "title": "Stateful Global PRNG Model",
    "concepts": [
      "This model uses a global PRNG state.",
      "Reproducibility requires controlling the order of evaluation, even without data dependencies.",
      "This sequencing requirement violates parallelizability and functional semantics.",
      "Updating shared state makes parallelization difficult.",
      "Maintaining PRNG state in both Python and compiled code poses engineering challenges.",
      "The expressiveness is limited because functions cannot call others without affecting their own PRNG state."
    ],
    "code_examples": [
      {
        "description": "Example of a stateful global PRNG implementation.",
        "code": "def foo():\n    return bar() + baz()\n\ndef bar():\n    return rand(RNG, (3, 4))\n\ndef baz():\n    return rand(RNG, (3, 4))\n\ndef main():\n    global RNG\n    RNG = RandomState(0)\n    return foo()"
      }
    ]
  },
  {
    "title": "Vectorization and Sequential Equivalence",
    "concepts": [
      "The Numpy PRNG vectorization is limited by a sequential-equivalent guarantee.",
      "To allow for vectorization, the sequential-equivalent guarantee is dropped.",
      "Vectorization can be supported by the three programming models discussed, using a counter-based PRNG."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating sequential equivalence in Numpy.",
        "code": "In [1]: rng = np.random.RandomState(0)\n\nIn [2]: rng.randn(2)\nOut[2]: array([1.76405235, 0.40015721])\n\nIn [3]: rng = np.random.RandomState(0)\n\nIn [4]: np.stack([rng.randn() for _ in range(2)])\nOut[4]: array([1.76405235, 0.40015721])"
      }
    ]
  },
  {
    "title": "Functional Model with Explicit Threading",
    "concepts": [
      "This model explicitly threads the PRNG state through all functions.",
      "Every random function must accept and return the PRNG state.",
      "There is an explicit data dependence between calls, fitting with JAX's semantics.",
      "Explicit threading can make the semantics invariant to compilation boundaries.",
      "Explicit threading is inconvenient and doesn't improve expressiveness.",
      "Functions must defensively pass in and return the RNG state everywhere.",
      "It doesn't improve parallelization or scaling because everything is still sequential."
    ],
    "code_examples": [
      {
        "description": "Example of a functional PRNG model with explicit threading of the RNG state.",
        "code": "def foo(rng_1):\n    y, rng_2 = baz(rng_1)\n    z, rng_3 = bar(rng_2)\n    return y + z, rng_3\n\ndef bar(x, rng):\n    val, new_rng = rand(rng, (3, 4))\n    return val, new_rng\n\ndef baz(x, rng):\n    val, new_rng = rand(rng, (3, 4))\n    return val, new_rng\n\ndef main():\n    foo(RandomState(0))"
      }
    ]
  },
  {
    "title": "Functional Splittable PRNGs",
    "concepts": [
      "Splitting reduces sequential dependence using functional splittable PRNGs.",
      "Splitting forks a new PRNG state into two while maintaining desirable properties.",
      "New streams are computationally parallelizable and produce independent random values.",
      "Functions do not need to return updated versions of PRNGs.",
      "It is straightforward to call a random subroutine without affecting existing PRNG states.",
      "The only way to advance the PRNG state is to call split()."
    ],
    "code_examples": [
      {
        "description": "Example of a functional splittable PRNG model.",
        "code": "def foo(rng_1):\n    rng_2, rng_3 = split(rng_1, 2)\n    return bar(rng_2) + baz(rng_3)\n\ndef bar(x, rng):\n    return rand(rng, (3, 4))\n\ndef baz(x, rng):\n    return rand(rng, (3, 4))\n\ndef main():\n    foo(RandomState(0))"
      }
    ]
  },
  {
    "title": "Counter-Based PRNG and Splitting",
    "concepts": [
      "Threefry hash function is used.",
      "The counter achieves efficient vectorization.",
      "Mapping the hash function over a range of integers generates an array of values.",
      "Splitting generates two new keys from an existing one."
    ],
    "code_examples": [
      {
        "description": "Example implementation of splitting and drawing samples.",
        "code": "type Sample = Int256\ntype Key = Sample -- important identification for splitting\ntype Count = Int32\n\nhash :: Key -> Count -> Int256 -- output type equal to Key and Sample\n\nsplit :: Key -> (Key, Key)\nsplit key = (hash key 0, hash key 1)\n\ndraw_samples :: Key -> Int -> [Sample]\ndraw_samples key n = map (hash key) [1..n]"
      }
    ]
  },
  {
    "title": "Training Loop Example",
    "concepts": [
      "The user is burdened with explicit splitting of the rng.",
      "The rng does not need to be returned from the code."
    ],
    "code_examples": [
      {
        "description": "Example of a training loop using the lax.rng API for PRNG management.",
        "code": "rng = lax.rng.new_rng()\nfor i in xrange(num_steps):\n    rng, rng_input = lax.rng.split(rng)\n    params = compiled_update(rng_input, params, next(batches))"
      }
    ]
  },
  {
    "title": "Dropout Implementation with Stax",
    "concepts": [
      "The rng value is just the key used for the hash.",
      "The rng argument is passed to every apply_fun.",
      "Serial and parallel combinators handle splitting."
    ],
    "code_examples": [
      {
        "description": "Dropout layer implementation using the stax neural net builder library.",
        "code": "def Dropout(rate, mode='train'):\n    def init_fun(input_shape):\n        return input_shape, ()\n    def apply_fun(rng, params, inputs):\n        if mode == 'train':\n            keep = lax.random.bernoulli(rng, rate, inputs.shape)\n            return np.where(keep, inputs / rate, 0)\n        else:\n            return inputs\n    return init_fun, apply_fun"
      },
      {
        "description": "Serial combinator implementation for handling RNG splitting.",
        "code": "def serial(*layers):\n    init_funs, apply_funs = zip(*layers)\n    def init_fun(input_shape):\n        ...\n    def apply_fun(rng, params, inputs):\n        rngs = split(rng, len(layers))\n        for rng, param, apply_fun in zip(rngs, params, apply_funs):\n            inputs = apply_fun(rng, param, inputs)\n        return inputs\n    return init_fun, apply_fun"
      },
      {
        "description": "Parallel combinator implementation for handling RNG splitting.",
        "code": "def parallel(*layers):\n    init_funs, apply_funs = zip(*layers)\n    def init_fun(input_shape):\n        ...\n    def apply_fun(rng, params, inputs):\n        rngs = split(rng, len(layers))\n        return [f(r, p, x) for f, r, p, x in zip(apply_funs, rngs, params, inputs)]\n    return init_fun, apply_fun"
      }
    ]
  },
  {
    "title": "Hardware PRNG Considerations",
    "concepts": [
      "Device hardware PRNG is not exploited.",
      "Lack of control over hardware PRNG state for all backends.",
      "Backend-dependent and may introduce sequential dependencies.",
      "Software PRNG should not become a bottleneck for known workloads.",
      "An additional API could allow access to a hardware PRNG, sacrificing reproducibility."
    ],
    "code_examples": []
  },
  {
    "title": "Sequential Equivalence Guarantee",
    "concepts": [
      "The sequential equivalent guarantee is given up.",
      "This property is likely incompatible with vectorization.",
      "Users could write a layer on top of this API to provide this guarantee.",
      "numpy.random API cannot be followed exactly."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction",
    "concepts": [
      "This document explains the design and implementation of jax.custom_jvp and jax.custom_vjp.",
      "It focuses on defining custom differentiation rules for Python functions that are already JAX-transformable.",
      "It does not cover defining new core.Primitive instances with transformation rules."
    ],
    "code_examples": []
  },
  {
    "title": "Goals and Non-Goals",
    "concepts": [
      "The primary goals are to solve the vmap-removes-custom-jvp semantics problem and allow Python in custom VJPs for debugging.",
      "Secondary goals include cleaning up the user experience and making progress towards easier custom rules for higher-order functions.",
      "A non-goal is to provide a transformation-generic mechanism for customizing behavior beyond differentiation (JVP and VJP).",
      "Mathematical aesthetics will not be prioritized over flexibility, clarity, and simplicity.",
      "Serialization support for staged-out program representations is out of scope."
    ],
    "code_examples": []
  },
  {
    "title": "The vmap-removes-custom-jvp Semantics Problem",
    "concepts": [
      "The vmap transformation removes custom differentiation rules defined by custom_transforms.",
      "This is because vmap rewrites the function in terms of its underlying primitives, removing the custom rule.",
      "Applying vmap (or other non-differentiation transformations) to a function with custom differentiation rules leads to inconsistent semantics.",
      "The semantics of transforming a function f should be defined in terms of calls to f, not rewriting it."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the vmap-removes-custom-jvp problem using the old custom_transforms API.",
        "code": "# old custom_transforms api to be replaced\n@jax.custom_transforms\ndef f(x):\n  return 2. * x\n\n# f_vjp :: a -> (b, CT b --o CT a)\ndef f_vjp(x):\n  return f(x), lambda g: 3. * x  # 3 instead of 2\n\njax.defvjp_all(f, f_vjp)\n\ngrad(f)(1.)  # 3.\n\nvmap(grad(f))(np.ones(4))  # [3., 3., 3., 3.]\n\ngrad(lambda x: vmap(f)(x).sum())(np.ones(4))  # [2., 2., 2., 2.]"
      },
      {
        "description": "Demonstrates the vmap-removes-custom-jvp problem using the old custom_transforms API. (duplicate code)",
        "code": "# old custom_transforms api to be replaced\n@jax.custom_transforms\ndef f(x):\n  return 2. * x\n\n# f_vjp :: a -> (b, CT b --o CT a)\ndef f_vjp(x):\n  return f(x), lambda g: 3. * x  # 3 instead of 2\n\njax.defvjp_all(f, f_vjp)\n\ngrad(f)(1.)  # 3.\n\nvmap(grad(f))(np.ones(4))  # [3., 3., 3., 3.]\n\ngrad(lambda x: vmap(f)(x).sum())(np.ones(4))  # [2., 2., 2., 2.]"
      }
    ]
  },
  {
    "title": "The Python Flexibility Problem",
    "concepts": [
      "Differentiation of Python functions is performed during execution and tracing, enabling pdb-based workflows and debugging.",
      "This allows differentiation of Python native control flow.",
      "The custom_transforms machinery does not provide this Python-support flexibility, leading to abstract value tracing errors."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the Python flexibility problem with custom_transforms, leading to an abstract value tracing error.",
        "code": "# old custom_transforms api to be replaced\n@jax.custom_transforms\ndef f(x):\n  if x > 0:\n    return x\n  else:\n    return 0.\n\ndef f_vjp(x):\n  return ...\n\njax.defvjp_all(f, f_vjp)\n\ngrad(f)(1.)  # Error!"
      },
      {
        "description": "Demonstrates the Python flexibility problem with custom_transforms, leading to an abstract value tracing error. (duplicate code)",
        "code": "# old custom_transforms api to be replaced\n@jax.custom_transforms\ndef f(x):\n  if x > 0:\n    return x\n  else:\n    return 0.\n\ndef f_vjp(x):\n  return ...\n\njax.defvjp_all(f, f_vjp)\n\ngrad(f)(1.)  # Error!"
      }
    ]
  },
  {
    "title": "Solution Idea",
    "concepts": [
      "The solution involves a new Python-level call primitive called custom_jvp_call.",
      "Transformations like vmap pass through custom_jvp_call and are applied to the underlying Python callables.",
      "The jvp transformation calls f_jvp.",
      "custom_jvp_call acts like core.call, preserving Python flexibility.",
      "Evaluation and compilation (eval and jit) ignore f_jvp and behave like core.call if differentiation hasn't already been applied."
    ],
    "code_examples": []
  },
  {
    "title": "Implementation Notes",
    "concepts": [
      "Initial-style jaxpr-forming primitives (like lax.scan) require preserving custom differentiation rule semantics during round-tripping to a jaxpr.",
      "A bit of dynamic scoping is used to differentiate between final-style and initial-style processing.",
      "When staging out to a jaxpr for initial-style primitives, a custom_jvp_call_jaxpr primitive is used.",
      "This primitive traces the functions f and f_jvp to jaxprs up-front to make initial-style processing easier."
    ],
    "code_examples": []
  },
  {
    "title": "Custom JVP Definition",
    "concepts": [
      "The custom JVP for an a -> b function is specified with an (a, Ta) -> (b, T b) function.",
      "The function f is decorated with @jax.custom_jvp.",
      "The JVP rule f_jvp is defined and registered with f.defjvp(f_jvp)."
    ],
    "code_examples": [
      {
        "description": "Defines a custom JVP rule for the sine function.",
        "code": "# f :: a -> b\n@jax.custom_jvp\ndef f(x):\n  return np.sin(x)\n\n# f_jvp :: (a, T a) -> (b, T b)\ndef f_jvp(primals, tangents):\n  x, = primals\n  t, = tangents\n  return f(x), np.cos(x) * t\n\nf.defjvp(f_jvp)"
      },
      {
        "description": "Defines a custom JVP rule for the sine function. (duplicate code)",
        "code": "# f :: a -> b\n@jax.custom_jvp\ndef f(x):\n  return np.sin(x)\n\n# f_jvp :: (a, T a) -> (b, T b)\ndef f_jvp(primals, tangents):\n  x, = primals\n  t, = tangents\n  return f(x), np.cos(x) * t\n\nf.defjvp(f_jvp)"
      }
    ]
  },
  {
    "title": "Custom VJP Definition",
    "concepts": [
      "The custom VJP for an a -> b function is specified with an a -> (b, c) forward pass function and a (c, CT b) -> CT a backward pass function.",
      "The function f is decorated with @jax.custom_vjp.",
      "The forward pass function f_fwd and backward pass function f_bwd are defined and registered with f.defvjp(f_fwd, f_bwd)."
    ],
    "code_examples": [
      {
        "description": "Defines a custom VJP rule for the sine function.",
        "code": "# f :: a -> b\n@jax.custom_vjp\ndef f(x):\n  return np.sin(x)\n\n# f_fwd :: a -> (b, c)\ndef f_fwd(x):\n  return f(x), np.cos(x)\n\n# f_bwd :: (c, CT b) -> CT a\ndef f_bwd(cos_x, g):\n  return (cos_x * g,)\n\nf.defvjp(f_fwd, f_bwd)"
      },
      {
        "description": "Defines a custom VJP rule for the sine function. (duplicate code)",
        "code": "# f :: a -> b\n@jax.custom_vjp\ndef f(x):\n  return np.sin(x)\n\n# f_fwd :: a -> (b, c)\ndef f_fwd(x):\n  return f(x), np.cos(x)\n\n# f_bwd :: (c, CT b) -> CT a\ndef f_bwd(cos_x, g):\n  return (cos_x * g,)\n\nf.defvjp(f_fwd, f_bwd)"
      }
    ]
  },
  {
    "title": "API Bells and Whistles",
    "concepts": [
      "Inputs and output types a, b, and c can be arbitrary pytrees of jaxtypes.",
      "Passing arguments by name (keyword arguments) is supported when they can be resolved to positions.",
      "Arguments can be marked non-differentiable using nondiff_argnums.",
      "Non-differentiable arguments are passed in a specific order to the JVP and VJP rules."
    ],
    "code_examples": []
  },
  {
    "title": "Updated jax.experimental.odeint",
    "concepts": [
      "jax.experimental.odeint was updated to use the new custom VJP API.",
      "Improvements were made to odeint, including removing raveling/unraveling boilerplate, using lax.scan, and speeding it up on a benchmark.",
      "Custom bind methods were added on each transform for custom_jvp_call and custom_vjp_call.",
      "A custom_lin primitive was added for linear jaxprs to be transposed when using a custom VJP rule.",
      "Custom VJP rules are processed in two steps: linearization and transposition."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to custom_vjp and nondiff_argnums",
    "concepts": [
      "This document discusses changes related to jax.custom_vjp after JAX PR #4008.",
      "Previously, array-valued arguments were often passed as nondiff_argnums in custom_vjp.",
      "After JAX PR #4008, Tracer objects cannot be passed as nondiff_argnums.",
      "nondiff_argnums should now only be used for non-array values like callables, shape tuples, or strings.",
      "Array values previously handled with nondiff_argnums should be passed as regular arguments."
    ],
    "code_examples": []
  },
  {
    "title": "The Old (Incorrect) Way of Using nondiff_argnums",
    "concepts": [
      "This section presents an example of how clip_gradient was implemented before JAX PR #4008, which is now considered incorrect.",
      "The code uses nondiff_argnums to specify 'lo' and 'hi' as non-differentiable arguments.",
      "This approach will fail if 'lo' or 'hi' are Tracer objects."
    ],
    "code_examples": [
      {
        "description": "Old way to implement clip_gradient using nondiff_argnums. This approach fails when 'lo' or 'hi' are Tracer objects.",
        "code": "from functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.custom_vjp, nondiff_argnums=(0, 1))\ndef clip_gradient(lo, hi, x):\n    return x  # identity function\n\ndef clip_gradient_fwd(lo, hi, x):\n    return x, None  # no residual values to save\n\ndef clip_gradient_bwd(lo, hi, _, g):\n    return (jnp.clip(g, lo, hi),)\n\nclip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)"
      },
      {
        "description": "Redefinition of the old way to implement clip_gradient using nondiff_argnums.",
        "code": "from functools import partial\nimport jax\nimport jax.numpy as jnp\n@partial(jax.custom_vjp, nondiff_argnums=(0, 1))\ndef clip_gradient(lo, hi, x):\n    return x  # identity function\ndef clip_gradient_fwd(lo, hi, x):\n    return x, None  # no residual values to save\ndef clip_gradient_bwd(lo, hi, _, g):\n    return (jnp.clip(g, lo, hi),)\nclip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)"
      }
    ]
  },
  {
    "title": "The New (Correct) Way of Using custom_vjp",
    "concepts": [
      "This section presents the correct way to implement clip_gradient after JAX PR #4008.",
      "nondiff_argnums is no longer used for array-valued arguments.",
      "Instead, 'lo' and 'hi' are passed as regular arguments and saved as residuals in the forward pass.",
      "The backward pass retrieves 'lo' and 'hi' from the residuals and returns None for their gradients."
    ],
    "code_examples": [
      {
        "description": "Correct way to implement clip_gradient without using nondiff_argnums for array-valued arguments.  'lo' and 'hi' are now regular arguments.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.custom_vjp  # no nondiff_argnums!\ndef clip_gradient(lo, hi, x):\n    return x  # identity function\n\ndef clip_gradient_fwd(lo, hi, x):\n    return x, (lo, hi)  # save lo and hi values as residuals\n\ndef clip_gradient_bwd(res, g):\n    lo, hi = res\n    return (None, None, jnp.clip(g, lo, hi))  # return None for lo and hi\n\nclip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)"
      },
      {
        "description": "Redefinition of the correct way to implement clip_gradient.",
        "code": "import jax\nimport jax.numpy as jnp\n@jax.custom_vjp  # no nondiff_argnums!\ndef clip_gradient(lo, hi, x):\n    return x  # identity function\ndef clip_gradient_fwd(lo, hi, x):\n    return x, (lo, hi)  # save lo and hi values as residuals\ndef clip_gradient_bwd(res, g):\n    lo, hi = res\n    return (None, None, jnp.clip(g, lo, hi))  # return None for lo and hi\nclip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)"
      }
    ]
  },
  {
    "title": "When to Use nondiff_argnums",
    "concepts": [
      "nondiff_argnums is still necessary for non-array arguments, such as Python callables.",
      "The example skip_app demonstrates the use of nondiff_argnums for a function argument 'f'."
    ],
    "code_examples": [
      {
        "description": "Example of using nondiff_argnums to indicate that the function 'f' is a non-differentiable argument.",
        "code": "from functools import partial\nimport jax\n\n@partial(jax.custom_vjp, nondiff_argnums=(0,))\ndef skip_app(f, x):\n    return f(x)\n\ndef skip_app_fwd(f, x):\n    return skip_app(f, x), None\n\ndef skip_app_bwd(f, _, g):\n    return (g,)\n\nskip_app.defvjp(skip_app_fwd, skip_app_bwd)"
      },
      {
        "description": "Redefinition of the example using nondiff_argnums for function 'f'.",
        "code": "from functools import partial\nimport jax\n\n@partial(jax.custom_vjp, nondiff_argnums=(0,))\ndef skip_app(f, x):\n    return f(x)\n\ndef skip_app_fwd(f, x):\n    return skip_app(f, x), None\n\ndef skip_app_bwd(f, _, g):\n    return (g,)\n\nskip_app.defvjp(skip_app_fwd, skip_app_bwd)"
      }
    ]
  },
  {
    "title": "Lexical Closure and custom_jvp/custom_vjp",
    "concepts": [
      "Passing Tracer objects into nondiff_argnums arguments used to be buggy due to issues with lexical closure.",
      "PR #4008 fixes lexical closure issues with custom_jvp and custom_vjp.",
      "custom_jvp and custom_vjp functions and rules can now close over Tracer objects.",
      "Autodiff transformations will raise a clear error if you try to differentiate with respect to values over which a custom_jvp or custom_vjp closes.",
      "Treating array-like non-differentiable arguments as regular arguments and residuals simplifies the implementation and avoids complexity related to handling arbitrary pytrees."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Omnistaging",
    "concepts": [
      "Omnistaging is a JAX core upgrade improving memory performance and trace execution time.",
      "Omnistaging simplifies JAX internals.",
      "Omnistaging can cause existing code to break, especially buggy code.",
      "Omnistaging can be disabled temporarily as a workaround in JAX versions 0.2.0 through 0.2.11.",
      "The easiest way to tell if omnistaging is responsible for issues is to disable it and see if they go away.",
      "The most common issue with omnistaging is using jax.numpy for shape computations."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to disable omnistaging using jax.config.disable_omnistaging().",
        "code": "jax\n.\nconfig\n.\ndisable_omnistaging\n()"
      }
    ]
  },
  {
    "title": "Using jax.numpy for Shape Computations",
    "concepts": [
      "Using jax.numpy for shape computations or other trace-time constants can cause issues with omnistaging.",
      "It's better to think of using jax.numpy operations only when you want to perform a computation on an accelerator.",
      "Use the original numpy for shape calculations instead of jax.numpy."
    ],
    "code_examples": [
      {
        "description": "Example of incorrect usage of jax.numpy for shape computation.",
        "code": "@jit\ndef f(x):\n    input_size = jnp.prod(x.shape)\n    if input_size > 100:\n        ..."
      },
      {
        "description": "Example of correct usage of numpy for shape computation.",
        "code": "import numpy as np\n@jit\ndef f(x):\n    input_size = np.prod(x.shape)\n    if input_size > 100:\n        ..."
      }
    ]
  },
  {
    "title": "What is Omnistaging?",
    "concepts": [
      "Omnistaging aims to stage out more computation from Python to XLA.",
      "Omnistaging avoids trace-time constant folding in jit, pmap, and control flow primitives.",
      "Omnistaging improves JAX's memory performance by reducing fragmentation and compile-time constants.",
      "Omnistaging can improve tracing performance by eliminating op-by-op execution at tracing time.",
      "Before omnistaging, JAX staged out computation based on data dependence only.",
      "With omnistaging all jax.numpy calls in the dynamic context of a jit-transformed function are staged out to XLA."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating XLA HLO program before omnistaging.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef f(x):\n  y = jnp.add(1, 1)\n  return x * y\n\nf(3)"
      },
      {
        "description": "HLO generated before omnistaging for the function f(x).",
        "code": "ENTRY jit_f.6 {\n  constant.2 = pred[] constant(false)\n  parameter.1 = s32[] parameter(0)\n  constant.3 = s32[] constant(2)\n  multiply.4 = s32[] multiply(parameter.1, constant.3)\n  ROOT tuple.5 = (s32[]) tuple(multiply.4)\n}"
      },
      {
        "description": "Example demonstrating XLA HLO program after omnistaging.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef f(x):\n  y = jnp.add(1, 1)\n  return x * y\n\nf(3)"
      },
      {
        "description": "HLO generated after omnistaging for the function f(x).",
        "code": "ENTRY jit_f.8 {\n  constant.2 = pred[] constant(false)\n  parameter.1 = s32[] parameter(0)\n  constant.3 = s32[] constant(1)\n  constant.4 = s32[] constant(1)\n  add.5 = s32[] add(constant.3, constant.4)\n  multiply.6 = s32[] multiply(parameter.1, add.5)\n  ROOT tuple.7 = (s32[]) tuple(multiply.6)\n}"
      },
      {
        "description": "Example demonstrating boolean mask creation and lax.select before omnistaging.",
        "code": "import jax.numpy as jnp\nfrom jax import lax\n\n@jit\ndef select_tril(x):\n  mask = jnp.arange(x.shape[0])[:, None] > jnp.arange(x.shape[1])\n  return lax.select(mask, x, jnp.zeros_like(x))\n  # lax.select is like jnp.where\n\nx = np.arange(12).reshape((3, 4))\nselect_tril(x)"
      },
      {
        "description": "HLO generated before omnistaging for the function select_tril(x).",
        "code": "ENTRY jit_select_tril.8 {\n  constant.3 = pred[] constant(false)\n  constant.1 = pred[3,4]{1,0} constant({...})\n  parameter.2 = s32[3,4]{1,0} parameter(0)\n  constant.4 = s32[] constant(0)\n  broadcast.5 = s32[3,4]{1,0} broadcast(constant.4), dimensions={}\n  select.6 = s32[3,4]{1,0} select(constant.1, parameter.2, broadcast.5)\n  ROOT tuple.7 = (s32[3,4]{1,0}) tuple(select.6)\n}"
      },
      {
        "description": "HLO generated after omnistaging for the function select_tril(x).",
        "code": "ENTRY jit_select_tril.16 {\n  constant.4 = pred[] constant(false)\n  iota.1 = s32[3]{0} iota(), iota_dimension=0\n  broadcast.5 = s32[3,1]{1,0} broadcast(iota.1), dimensions={0}\n  reshape.7 = s32[3]{0} reshape(broadcast.5)\n  broadcast.8 = s32[3,4]{1,0} broadcast(reshape.7), dimensions={0}\n  iota.2 = s32[4]{0} iota(), iota_dimension=0\n  broadcast.6 = s32[1,4]{1,0} broadcast(iota.2), dimensions={1}\n  reshape.9 = s32[4]{0} reshape(broadcast.6)\n  broadcast.10 = s32[3,4]{1,0} broadcast(reshape.9), dimensions={1}\n  compare.11 = pred[3,4]{1,0} compare(broadcast.8, broadcast.10), direction=GT\n  parameter.3 = s32[3,4]{1,0} parameter(0)\n  constant.12 = s32[] constant(0)\n  broadcast.13 = s32[3,4]{1,0} broadcast(constant.12), dimensions={}\n  select.14 = s32[3,4]{1,0} select(compare.11, parameter.3, broadcast.13)\n  ROOT tuple.15 = (s32[3,4]{1,0}) tuple(select.14)\n}"
      }
    ]
  },
  {
    "title": "Issues Arising From Omnistaging",
    "concepts": [
      "Code that worked previously can raise errors with omnistaging because more computations are staged out to XLA.",
      "With omnistaging, jax.numpy cannot be used for shape computations.",
      "The solution is to use the original numpy for shape calculations.",
      "Omnistaging can cause 'ConcretizationTypeError' due to abstract tracer values where concrete values are expected.",
      "Side-effecting code can cause issues with omnistaging.",
      "Omnistaging catches more side-effect errors.",
      "Numerical behaviors can change due to reordering of floating-point operations.",
      "Code relying on internal JAX APIs can break with omnistaging.",
      "Omnistaging can trigger pre-existing XLA compile-time bugs."
    ],
    "code_examples": [
      {
        "description": "Example of code that fails with omnistaging due to using jax.numpy for shape computations.",
        "code": "from jax import jit\nimport jax.numpy as jnp\n\n@jit\ndef ex1(x):\n  size = jnp.prod(jnp.array(x.shape))\n  return x.reshape((size,))\n\nex1(jnp.ones((3, 4)))"
      },
      {
        "description": "Example of side-effecting code that exposes an escaped tracer with omnistaging.",
        "code": "from jax import jit\nfrom jax import random\n\nkey = random.PRNGKey(0)\n\ndef init():\n  global key\n  key, subkey = random.split(key)\n  return random.normal(subkey, ())\n\nprint(init())\n# -1.2515389\nprint(init())\n# -0.58665067\n\ninit = jit(init)\nprint(init())\n# 0.48648298\nprint(init())\n# 0.48648298  !!\n\nprint(key)\n# Traced<ShapedArray(uint32[2])>with<DynamicJaxprTrace(level=0/1)>\n\nrandom.normal(key, ())"
      }
    ]
  },
  {
    "title": "Introduction to New-Style RNG Keys in JAX",
    "concepts": [
      "JAX is introducing more type-safe and customizable RNG keys.",
      "Old-style RNG keys are represented as length-2 uint32 arrays.",
      "New-style RNG keys are represented as scalar arrays with a special RNG dtype.",
      "New-style RNG keys satisfy `jnp.issubdtype(key.dtype, jax.dtypes.prng_key)`.",
      "Old-style RNG keys can be created using `jax.random.PRNGKey()`.",
      "New-style RNG keys can be created using `jax.random.key()`.",
      "New-style keys can be made non-scalar by using `jax.vmap()`"
    ],
    "code_examples": [
      {
        "description": "Creating an old-style RNG key using jax.random.PRNGKey().",
        "code": "key = jax.random.PRNGKey(0)\nprint(key)\nprint(key.shape)\nprint(key.dtype)"
      },
      {
        "description": "Creating a new-style RNG key using jax.random.key().",
        "code": "key = jax.random.key(0)\nprint(key)\nprint(key.shape)\nprint(key.dtype)"
      },
      {
        "description": "Creating a non-scalar key array by using jax.vmap() with jax.random.key().",
        "code": "key_arr = jax.vmap(jax.random.key)(jnp.arange(4))\nprint(key_arr)\nprint(key_arr.shape)"
      },
      {
        "description": "Using new-style keys with jax.random.split and jax.random.uniform.",
        "code": "# split\nnew_key , subkey = jax.random.split(key)\n# random number generation\ndata = jax.random.uniform(key, shape = (5,))"
      }
    ]
  },
  {
    "title": "Transitioning to Typed Keys and Handling Potential Breakages",
    "concepts": [
      "Most PRNG-related code should continue to work as expected with new-style keys.",
      "Not all numerical operations work on key arrays, and will now raise errors.",
      "The underlying buffer of a new-style key can be recovered using `jax.random.key_data()`.",
      "Using `jax.random.key_data()` on an old-style key is an identity operation.",
      "Users are encouraged to replace `jax.random.PRNGKey()` with `jax.random.key()`.",
      "Explicit logic about `key.shape` or `key.dtype` might need updates.",
      "`dtypes.issubdtype(dtype, dtypes.prng_key)` should be used to reason about RNG dtypes.",
      "Use `raw_key = jax.random.key_data(key)` to interoperate with libraries that don't yet handle typed PRNG keys (with a TODO to remove it later).",
      "`jax.random.PRNGKey()` will be deprecated in the future.",
      "`jax.dtypes.issubdtype` or `jax.numpy.issubdtype` can be used to check if an object is a new-style typed PRNG key.",
      "`jax.Array` is the recommended type annotation for both old and new-style PRNG keys.",
      "`jax.random.KeyArray` and `jax.random.PRNGKeyArray` are deprecated and aliased to Any."
    ],
    "code_examples": [
      {
        "description": "Attempting to perform arithmetic on a new-style key will raise a TypeError.",
        "code": "key = key + 1 # This will raise a TypeError"
      },
      {
        "description": "Recovering the underlying buffer of a new-style key using jax.random.key_data().",
        "code": "jax.random.key_data(key)"
      },
      {
        "description": "Checking if a key is a typed PRNG key using jax.dtypes.issubdtype().",
        "code": "typed_key = jax.random.key(0)\njax.dtypes.issubdtype(typed_key.dtype, jax.dtypes.prng_key)\n\nraw_key = jax.random.PRNGKey(0)\njax.dtypes.issubdtype(raw_key.dtype, jax.dtypes.prng_key)"
      },
      {
        "description": "Enforcing new-style typed keys in a library function.",
        "code": "from jax import dtypes\n\ndef ensure_typed_key_array(key: Array) -> Array:\n    if dtypes.issubdtype(key.dtype, dtypes.prng_key):\n        return key\n    else:\n        raise TypeError(\"New-style typed JAX PRNG keys required\")"
      }
    ]
  },
  {
    "title": "Motivations: Customizability and Type Safety",
    "concepts": [
      "Two major motivations for the change are customizability and safety.",
      "JAX currently operates with a single, globally configured PRNG algorithm.",
      "The old design's drawbacks became clearer with alternative PRNG implementations.",
      "The new approach carries the implementation as part of the PRNG key type.",
      "The `impl` argument in `jax.random.key()` specifies the PRNG implementation (e.g., 'threefry2x32' or 'rbg').",
      "PRNG keys are meant to support key derivation (splitting) and random number generation.",
      "Manipulating key data in other ways often indicates a bug.",
      "Indexing into the key buffer, performing key arithmetic, and accidentally swapping dimensions are common misuses with old-style keys that are prevented with new-style keys.",
      "JAX's functional PRNG does not implicitly update a key when it is used; it requires explicit splitting.",
      "Tools are being developed to detect and prevent unintended key reuse, relying on typed key arrays."
    ],
    "code_examples": [
      {
        "description": "Generating pseudorandom values under different PRNG implementations using the `impl` argument in jax.random.key().",
        "code": "key = jax.random.key(0, impl='threefry2x32') # this is the default impl\nprint(key)\njax.random.uniform(key, shape=(3,))\n\nkey = jax.random.key(0, impl='rbg')\nprint(key)\njax.random.uniform(key, shape=(3,))"
      },
      {
        "description": "Incorrect way of deriving new keys by indexing into the buffer of an old-style PRNGKey which leads to identical keys.",
        "code": "key = random.PRNGKey(999)\nnew_key = random.PRNGKey(key[1]) # identical to the original key!"
      },
      {
        "description": "Correct way to derive a new key by using jax.random.split.",
        "code": "key = random.PRNGKey(999)\nkey, new_key = random.split(key)"
      },
      {
        "description": "Incorrect way of creating batched keys with old-style PRNG keys using arithmetic which can result in correlated random numbers.",
        "code": "key = random.PRNGKey(0)\nbatched_keys = key + jnp.arange(10, dtype=key.dtype)[:, None]"
      },
      {
        "description": "Correct way to create batched keys by using jax.random.split.",
        "code": "key = random.PRNGKey(0)\nbatched_keys = random.split(key, 10)"
      },
      {
        "description": "Incorrect use of vmap with old-style PRNG keys due to incorrect in_axes assignment leading to combining elements from each key buffer in the batch.",
        "code": "keys = random.split(random.PRNGKey(0))\ndata = jax.vmap(random.uniform, in_axes=1)(keys)"
      },
      {
        "description": "Correct use of vmap with old-style PRNG keys.",
        "code": "keys = random.split(random.PRNGKey(0))\ndata = jax.vmap(random.uniform, in_axes=0)(keys)"
      },
      {
        "description": "Incorrect key usage: PRNG keys must be split before each use.",
        "code": "key = random.PRNGKey(0)\nx = random.uniform(key, (100,))\ny = random.uniform(key, (100,)) # Identical values!"
      },
      {
        "description": "Correct key usage: PRNG keys must be split before each use. Everytime a random number is sampled, a new key must be generated.",
        "code": "key = random.PRNGKey(0)\nkey1, key2 = random.split(random.key(0))\nx = random.uniform(key1, (100,))\ny = random.uniform(key2, (100,))"
      }
    ]
  },
  {
    "title": "Implementation Details: Extended Dtypes and PRNG Dtypes",
    "concepts": [
      "Typed PRNG keys are implemented as an instance of extended dtypes within JAX.",
      "`jax.dtypes.issubdtype(dt, jax.dtypes.extended)` returns True for extended dtypes.",
      "Extended dtypes have a `dt.type` attribute, returning a typeclass in the `numpy.generic` hierarchy.",
      "Instances of `dt.type` scalar objects are not allowed.",
      "Extended dtypes have a private `_rules` attribute to define behavior under operations.",
      "The bounded integer type `jax._src.core.bint` is another example of an extended dtype.",
      "`jax.dtypes.prng_key` is a new public scalar type class.",
      "`jax.dtypes.issubdtype(jax.dtypes.prng_key, jax.dtypes.extended)` is True.",
      "PRNG key arrays have a dtype for which `jax.dtypes.issubdtype(key.dtype, jax.dtypes.extended)` and `jax.dtypes.issubdtype(key.dtype, jax.dtypes.prng_key)` are both True.",
      "PRNG dtypes define `key.dtype._impl`, containing metadata defining the PRNG implementation.",
      "`jax._src.prng.PRNGImpl` currently defines the PRNG implementation, but it is not a public API yet."
    ],
    "code_examples": [
      {
        "description": "Checking if jax.dtypes.prng_key is a subtype of jax.dtypes.extended",
        "code": "jax.dtypes.issubdtype(jax.dtypes.prng_key, jax.dtypes.extended)"
      },
      {
        "description": "Checking if a key dtype is a subtype of jax.dtypes.extended and jax.dtypes.prng_key",
        "code": "key = jax.random.key(0)\njax.dtypes.issubdtype(key.dtype, jax.dtypes.extended)\njax.dtypes.issubdtype(key.dtype, jax.dtypes.prng_key)"
      }
    ]
  },
  {
    "title": "Key Pull Requests",
    "concepts": [
      "The document lists several key Pull Requests that implemented the design changes related to typed PRNG keys.",
      "The main tracking issue is #9263.",
      "Specific PRs include: Implement pluggable PRNG via PRNGImpl (#6899), Implement PRNGKeyArray , without dtype (#11952), Add a \u201ccustom element\u201d dtype property to PRNGKeyArray with _rules attribute (#12167), Rename \u201ccustom element type\u201d to \u201copaque dtype\u201d (#12170), Refactor bint to use the opaque dtype infrastructure (#12707), Add jax.random.key to create typed keys directly (#16086), Add impl argument to key and PRNGKey (#16589), Rename \u201copaque dtype\u201d to  \u201cextended dtype\u201d & define jax.dtypes.extended (#16824), Introduce jax.dtypes.prng_key and unify PRNG dtype with Extended dtype (#16781), Add a jax_legacy_prng_key flag to support warning or erroring when using legacy (raw) PRNG keys (#17225)"
    ],
    "code_examples": []
  },
  {
    "title": "Introduction: The Challenge of Type Promotion",
    "concepts": [
      "Numerical computing libraries face the challenge of handling operations between values of different types.",
      "JAX's numerical computing API is modeled after NumPy but targets accelerators like GPU and TPU.",
      "NumPy's type promotion rules heavily favor 64-bit outputs, which is problematic for computation on accelerators.",
      "Accelerators often have a performance penalty or lack native support for 64-bit floating point types.",
      "JAX re-thinks NumPy-style type promotion with accelerators in mind."
    ],
    "code_examples": [
      {
        "description": "Demonstrates NumPy's type promotion between 32-bit integers and floats resulting in a 64-bit float.",
        "code": "import numpy as np\nnp.dtype(np.int32(1) + np.float32(1))"
      }
    ]
  },
  {
    "title": "Lattice Representation of Type Promotion",
    "concepts": [
      "Type promotion can be represented using a lattice, where the supremum between two nodes is the type they promote to.",
      "A lattice is a compact encoding of the information in the promotion table.",
      "The join operation finds the result of a type promotion for two inputs by tracing the graph to the first common child.",
      "An arrow in the lattice means that implicit type promotion is allowed between the source and the destination.",
      "Not every directed acyclic graph (DAG) satisfies the properties of a lattice.",
      "A lattice requires the existence of a unique least upper bound for every pair of nodes.",
      "Specifying type promotions in terms of a lattice ensures properties like existence, commutativity, and associativity.",
      "A lattice-based type promotion system is simpler to conceptualize compared to a table-based system."
    ],
    "code_examples": [
      {
        "description": "Generates the type promotion table used by Python for addition between int, float, and complex types using pandas.",
        "code": "import pandas as pd\ntypes = [int, float, complex]\nname = lambda t: t.__name__\npd.DataFrame([[name(type(t1(1) + t2(1))) for t1 in types] for t2 in types],\n             index=[name(t) for t in types],\n             columns=[name(t) for t in types])"
      },
      {
        "description": "Creates a lattice representation of Python's type promotion table (int -> float -> complex) using networkx and matplotlib.",
        "code": "import networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {'int': ['float'], 'float': ['complex']}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {'int': [0, 0], 'float': [1, 0], 'complex': [2, 0]}\nfig, ax = plt.subplots(figsize=(8, 2))\nnx.draw(graph, with_labels=True, node_size=4000, node_color='lightgray', pos=pos, ax=ax, arrowsize=20)"
      },
      {
        "description": "Shows examples of two DAGs that are not lattices, due to missing or non-unique least upper bounds.",
        "code": "import networkx as nx\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2, figsize=(10, 2))\nlattice = {'A': ['B', 'C']}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {'A': [0, 0], 'B': [1, 0.5], 'C': [1, -0.5]}\nnx.draw(graph, with_labels=True, node_size=2000, node_color='lightgray', pos=pos, ax=ax[0], arrowsize=20)\nax[0].set(xlim=[-0.5, 1.5], ylim=[-1, 1])\nlattice = {'A': ['C', 'D'], 'B': ['C', 'D']}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {'A': [0, 0.5], 'B': [0, -0.5], 'C': [1, 0.5], 'D': [1, -0.5]}\nnx.draw(graph, with_labels=True, node_size=2000, node_color='lightgray', pos=pos, ax=ax[1], arrowsize=20)\nax[1].set(xlim=[-0.5, 1.5], ylim=[-1, 1]);"
      },
      {
        "description": "Demonstrates that NumPy's type promotion can be non-associative by providing a counterexample using int8, uint8, and float16.",
        "code": "import numpy as np\na, b, c = np.int8(1), np.uint8(1), np.float16(1)\nprint(np.dtype((a + b) + c))\nprint(np.dtype(a + (b + c)))"
      }
    ]
  },
  {
    "title": "Type Categories and Same-Kind Promotion",
    "concepts": [
      "Numerical computing libraries generally provide different precisions within int, float, and complex categories.",
      "Categories considered are unsigned integers (uint8, uint16, uint32, uint64), signed integers (int8, int16, int32, int64), floating point (float16, float32, float64), and complex floating point (complex64, complex128).",
      "NumPy's type promotion semantics within each category is straightforward: the ordered hierarchy of types translates directly to separate lattices.",
      "Same-kind promotion semantics within each type category are unproblematic for JAX, as the only way to produce a 64-bit output is to have a 64-bit input."
    ],
    "code_examples": [
      {
        "description": "Creates lattice representations for same-kind type promotion within unsigned integers, signed integers, floating point, and complex types using networkx and matplotlib.",
        "code": "import networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n    'u8': ['u16'], 'u16': ['u32'], 'u32': ['u64'],\n    'i8': ['i16'], 'i16': ['i32'], 'i32': ['i64'],\n    'f16': ['f32'], 'f32': ['f64'],\n    'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n    'u8': [0, 0], 'u16': [1, 0], 'u32': [2, 0], 'u64': [3, 0],\n    'i8': [0, 1], 'i16': [1, 1], 'i32': [2, 1], 'i64': [3, 1],\n    'f16': [1, 2], 'f32': [2, 2], 'f64': [3, 2],\n    'c64': [2, 3], 'c128': [3, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 4))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)"
      }
    ]
  },
  {
    "title": "Python Scalars and NumPy Type Promotion",
    "concepts": [
      "NumPy's promotion behavior differs depending on whether the inputs are arrays or scalars.",
      "When operating on two scalars, normal promotion rules apply.",
      "In operations between Python scalars and NumPy arrays, scalars defer to the dtype of the array.",
      "When NumPy type promotion involves a scalar, the output dtype is value-dependent.",
      "Value-dependent promotion is not suitable for JAX due to JIT compilation and transformations.",
      "NumPy's signed integer type promotion can be represented using a lattice, distinguishing between scalar and array types.",
      "For simplicity, each category of scalar types can be collapsed into a single node (u*, i*, f*, c*).",
      "Putting scalars at the left of the lattice means that promotion result defers to the array type.",
      "This gives motivation to * notation for scalar values: the * is reminiscent of a wildcard.",
      "Scalar promotion semantics allow expressing sequences of operations with clean Python code, without explicit casting."
    ],
    "code_examples": [
      {
        "description": "Shows that NumPy promotes an int8 scalar and a Python int to int64.",
        "code": "x = np.int8(0) # int8 scalar\ny = 1 # Python int = int64 scalar\n(x + y).dtype"
      },
      {
        "description": "Demonstrates that in operations between a Python scalar and a NumPy array, the scalar defers to the dtype of the array.",
        "code": "x = np.zeros(1, dtype='int8') # int8 array\ny = 1 # Python int = int64 scalar\n(x + y).dtype"
      },
      {
        "description": "Shows that when the Python scalar is too large for the given dtype, it is promoted to a compatible type.",
        "code": "x = np.zeros(1, dtype='int8') # int8 array\ny = 1000 # int64 scalar\n(x + y).dtype"
      },
      {
        "description": "Creates a lattice representation of NumPy's signed integer type promotion, differentiating between scalar and array types.",
        "code": "import networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n    'i8*': ['i16*'], 'i16*': ['i32*'], 'i32*': ['i64*'], 'i64*': ['i8'],\n    'i8': ['i16'], 'i16': ['i32'], 'i32': ['i64']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n    'i8*': [0, 1], 'i16*': [2, 1], 'i32*': [4, 1], 'i64*': [6, 1],\n    'i8': [9, 1], 'i16': [11, 1], 'i32': [13, 1], 'i64': [15, 1],\n}\nfig, ax = plt.subplots(figsize=(12, 4))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)\nax.text(3, 1.6, \"Scalar Types\", ha='center', fontsize=14)\nax.text(12, 1.6, \"Array Types\", ha='center', fontsize=14)\nax.set_ylim(-1, 3);"
      },
      {
        "description": "Checks that operations between arrays of type int8, int16, int32, int64 and a scalar value produce arrays of the same type.",
        "code": "for dtype in [np.int8, np.int16, np.int32, np.int64]:\n  x = np.arange(10, dtype=dtype)\n  assert (x + 2).dtype == dtype"
      }
    ]
  },
  {
    "title": "Partial Lattice and Desired Properties",
    "concepts": [
      "Collapsing scalar types allows representing in-category lattices, simplifying the diagram.",
      "A partial lattice is one in which some pairs of nodes do not have a defined promotion behavior.",
      "A framework is set up to think about adding connections on the graph to fill-out these undefined promotion rules.",
      "Desired properties for additional connections include commutativity, associativity, preventing data component dropping, avoiding unhandled overflow, avoiding loss of precision, and avoiding types wider than the inputs."
    ],
    "code_examples": [
      {
        "description": "Combines the scalar types into a single node to create the lattice. This lattice is used to represent type promotion between Python scalars and numpy arrays",
        "code": "import networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n    'u*': ['u8'],\n    'u8': ['u16'], 'u16': ['u32'], 'u32': ['u64'],\n    'i*': ['i8'],\n    'i8': ['i16'], 'i16': ['i32'], 'i32': ['i64'],\n    'f*': ['f16'],\n    'f16': ['f32'], 'f32': ['f64'],\n    'c*': ['c64'],\n    'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n    'u*': [0, 0],\n    'u8': [3, 0], 'u16': [5, 0], 'u32': [7, 0], 'u64': [9, 0],\n    'i*': [0, 1],\n    'i8': [3, 1], 'i16': [5, 1], 'i32': [7, 1], 'i64': [9, 1],\n    'f*': [0, 2],\n    'f16': [5, 2], 'f32': [7, 2], 'f64': [9, 2],\n    'c*': [0, 3],\n    'c64': [7, 3], 'c128': [9, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 4))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)"
      },
      {
        "description": "Creates a partial lattice for type promotion between Python scalars and numpy arrays.",
        "code": "import networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n    'i*': ['f*', 'u8', 'i8'],\n    'f*': ['c*', 'f16'],\n    'c*': ['c64'],\n    'u8': ['u16'],\n    'u16': ['u32'],\n    'u32': ['u64'],\n    'i8': ['i16'],\n    'i16': ['i32'],\n    'i32': ['i64'],\n    'f16': ['f32'],\n    'f32': ['f64'],\n    'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n    'i*': [-1.25, 0.5],\n    'f*': [-0.5, 2],\n    'c*': [0, 3],\n    'u8': [0.5, 0],\n    'u16': [1.5, 0],\n    'u32': [2.5, 0],\n    'u64': [3.5, 0],\n    'i8': [0, 1],\n    'i16': [1, 1],\n    'i32': [2, 1],\n    'i64': [3, 1],\n    'f16': [0.5, 2],\n    'f32': [1.5, 2],\n    'f64': [2.5, 2],\n    'c64': [2, 3],\n    'c128': [3, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 5))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)"
      }
    ]
  },
  {
    "title": "Float and Complex Promotion",
    "concepts": [
      "Complex numbers are made up of pairs of floating point numbers, so there is a natural path of promotion between them: cast float to complex while maintaining the width of the real part.",
      "This represents exactly the semantics used by NumPy in mixed float/complex type promotion."
    ],
    "code_examples": [
      {
        "description": "Introduces the promotion between float and complex values, resulting in a more complete lattice representation.",
        "code": "import networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n    'i*': ['f*', 'u8', 'i8'],\n    'f*': ['c*', 'f16'],\n    'c*': ['c64'],\n    'u8': ['u16'],\n    'u16': ['u32'],\n    'u32': ['u64'],\n    'i8': ['i16'],\n    'i16': ['i32'],\n    'i32': ['i64'],\n    'f16': ['f32'],\n    'f32': ['f64', 'c64'],\n    'f64': ['c128'],\n    'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n    'i*': [-1.25, 0.5],\n    'f*': [-0.5, 2],\n    'c*': [0, 3],\n    'u8': [0.5, 0],\n    'u16': [1.5, 0],\n    'u32': [2.5, 0],\n    'u64': [3.5, 0],\n    'i8': [0, 1],\n    'i16': [1, 1],\n    'i32': [2, 1],\n    'i64': [3, 1],\n    'f16': [0.5, 2],\n    'f32': [1.5, 2],\n    'f64': [2.5, 2],\n    'c64': [2, 3],\n    'c128': [3, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 5))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)"
      }
    ]
  },
  {
    "title": "Signed and Unsigned Integer Promotion",
    "concepts": [
      "The promotion of unsigned integers to integers should consider the representable range.",
      "Promoting uint8 to int8 is not suitable because the largest uint8 numbers are not representable in int8.",
      "It makes more sense to promote unsigned integers to integers with twice the number of bits.",
      "Again, the connections added here are precisely the promotion semantics implemented by Numpy for mixed-integer promotion."
    ],
    "code_examples": [
      {
        "description": "Introduces connections to promote unsigned integers to signed integers with twice the number of bits in the lattice",
        "code": "import networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n    'i*': ['f*', 'u8', 'i8'],\n    'f*': ['c*', 'f16'],\n    'c*': ['c64'],\n    'u8': ['u16', 'i16'],\n    'u16': ['u32', 'i32'],\n    'u32': ['u64', 'i64'],\n    'i8': ['i16'],\n    'i16': ['i32'],\n    'i32': ['i64'],\n    'f16': ['f32'],\n    'f32': ['f64', 'c64'],\n    'f64': ['c128'],\n    'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n    'i*': [-1.25, 0.5],\n    'f*': [-0.5, 2],\n    'c*': [0, 3],\n    'u8': [0.5, 0],\n    'u16': [1.5, 0],\n    'u32': [2.5, 0],\n    'u64': [3.5, 0],\n    'i8': [0, 1],\n    'i16': [1, 1],\n    'i32': [2, 1],\n    'i64': [3, 1],\n    'f16': [0.5, 2],\n    'f32': [1.5, 2],\n    'f64': [2.5, 2],\n    'c64': [2, 3],\n    'c128': [3, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 5))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)"
      }
    ]
  },
  {
    "title": "uint64 Promotion",
    "concepts": [
      "Mixed-integer operation involving uint64 should result in int128, but this is not a standard available dtype.",
      "Numpy\u2019s choice is to promote to float64, however, this may be a surprising convention: it\u2019s the only case in which promotion of integer types does not result in an integer."
    ],
    "code_examples": []
  },
  {
    "title": "Integer to Floating Point Promotion",
    "concepts": [
      "A 16-bit signed or unsigned integer cannot be represented at full precision by a 16-bit float.",
      "It might make sense to promote integers to floats represented by twice the number of bits.",
      "This is effectively what Numpy type promotion does."
    ],
    "code_examples": [
      {
        "description": "Introduces connections to promote integers to floating points with twice the number of bits.",
        "code": "import networkx as nx\nimport matplotlib.pyplot as plt\nlattice = {\n    'i*': ['f*', 'u8', 'i8'],\n    'f*': ['c*', 'f16'],\n    'c*': ['c64'],\n    'u8': ['u16', 'i16', 'f16'],\n    'u16': ['u32', 'i32', 'f32'],\n    'u32': ['u64', 'i64', 'f64'],\n    'i8': ['i16', 'f16'],\n    'i16': ['i32', 'f32'],\n    'i32': ['i64', 'f64'],\n    'f16': ['f32'],\n    'f32': ['f64', 'c64'],\n    'f64': ['c128'],\n    'c64': ['c128']\n}\ngraph = nx.from_dict_of_lists(lattice, create_using=nx.DiGraph)\npos = {\n    'i*': [-1.25, 0.5],\n    'f*': [-0.5, 2],\n    'c*': [0, 3],\n    'u8': [0.5, 0],\n    'u16': [1.5, 0],\n    'u32': [2.5, 0],\n    'u64': [3.5, 0],\n    'i8': [0, 1],\n    'i16': [1, 1],\n    'i32': [2, 1],\n    'i64': [3, 1],\n    'f16': [0.5, 2],\n    'f32': [1.5, 2],\n    'f64': [2.5, 2],\n    'c64': [2, 3],\n    'c128': [3, 3],\n}\nfig, ax = plt.subplots(figsize=(6, 5))\nnx.draw(graph, with_labels=True, node_size=1500, node_color='lightgray', pos=pos, ax=ax)"
      }
    ]
  },
  {
    "title": "JAX and JAXLib Separation",
    "concepts": [
      "JAX is distributed as two separate Python wheels: jax (pure Python) and jaxlib (mostly C++).",
      "Separating jax and jaxlib allows independent updates of the Python parts of JAX without rebuilding the C++ code.",
      "jaxlib contains XLA, LLVM, MLIR, and JAX-specific C++ libraries.",
      "Changes to jaxlib must maintain a backward-compatible API.",
      "jax and jaxlib share the same version number in the JAX source tree but are released separately."
    ],
    "code_examples": []
  },
  {
    "title": "Version Compatibility",
    "concepts": [
      "The jax package version must be greater than or equal to the jaxlib version.",
      "The jaxlib version must be greater than or equal to the minimum jaxlib version specified by jax.",
      "Version numbers follow PEP 440 and are compared lexicographically.",
      "jax may be released on its own without updating jaxlib, but a new jaxlib release requires a corresponding jax release.",
      "Version constraints are checked at import time in jax.",
      "jax may drop compatibility with older jaxlib releases by increasing the minimum jaxlib version.",
      "jaxlib may drop compatibility with older jax releases lower than its own release version number.",
      "jax must be compatible with all jaxlib releases at least as new as the minimum version."
    ],
    "code_examples": []
  },
  {
    "title": "JAXLib Repository Structure",
    "concepts": [
      "jaxlib code is split across the main JAX repository (jaxlib/ subdirectory) and the XLA repository.",
      "JAX-specific pieces in XLA are primarily in the xla/python subdirectory.",
      "The XLA:Python bindings are in the XLA tree to allow atomic updates with the XLA C++ API.",
      "jaxlib is built using Bazel, and the XLA version is pinned in the Bazel WORKSPACE."
    ],
    "code_examples": []
  },
  {
    "title": "Fine-Grained Compatibility with _version",
    "concepts": [
      "An additional version number (_version) is maintained in xla_client.py in the XLA repository.",
      "This version is accessible to JAX Python as jax._src.lib.xla_extension_version.",
      "_version must be incremented for any change to XLA/Python code with backward compatibility implications for jax.",
      "The JAX Python code uses this version to maintain backward compatibility.",
      "This versioning is for managing compatibility during development for unreleased code, in addition to the release version constraints."
    ],
    "code_examples": [
      {
        "description": "Example of using xla_extension_version to maintain backwards compatibility.",
        "code": "from\n\njax._src.lib\n\nimport\n\nxla_extension_version\n\n# 123 is the new version number for _version in xla_client.py\n\nif\n\nxla_extension_version\n\n>=\n\n123\n\n:\n\n# Use new code path\n\n...\n\nelse\n\n:\n\n# Use old code path."
      },
      {
        "description": "Repeated Example of using xla_extension_version to maintain backwards compatibility.",
        "code": "from\n\njax._src.lib\n\nimport\n\nxla_extension_version\n\n# 123 is the new version number for _version in xla_client.py\n\nif\n\nxla_extension_version\n\n>=\n\n123\n\n:\n\n# Use new code path\n\n...\n\nelse\n\n:\n\n# Use old code path."
      }
    ]
  },
  {
    "title": "Introduction to JAX's Execution Model and Side-Effects",
    "concepts": [
      "JAX code usually pretends to be single-threaded and eagerly executed.",
      "Asynchronous execution in JAX can lead to performance optimizations.",
      "Side-effects can reveal out-of-order execution in JAX.",
      "JAX's execution model interacts with the ordering of side-effects.",
      "A mechanism is needed to enforce a single-threaded ordering of effects."
    ],
    "code_examples": [
      {
        "description": "Example of Python code with printing, where the order of prints is guaranteed.",
        "code": "def f():\n    print(\"hello\")\n    return 2\n\ndef g():\n    print(\"world\")\n    return 3\n\nf()\ng()"
      }
    ]
  },
  {
    "title": "Asynchronous Dispatch and Reordering of Side-Effects",
    "concepts": [
      "JAX can execute functions in parallel, potentially reordering their execution.",
      "Parallel execution is a performance optimization, especially with device transfers.",
      "Asynchronous dispatch can cause side-effects, such as prints, to be reordered.",
      "Reordering of side-effects breaks the illusion of a single-threaded execution model."
    ],
    "code_examples": [
      {
        "description": "JAX code where functions f and g are JIT-compiled and dispatched to different devices, potentially executing in parallel.",
        "code": "from functools import partial\nimport jax\n\n@partial(jax.jit, device=<device 0>)\ndef f():\n    return 2\n\n@partial(jax.jit, device=<device 1>)\ndef g():\n    return 3\n\nf()\ng()"
      },
      {
        "description": "JAX code with jax.print statements in JIT-compiled functions that could be executed out of order due to asynchronous dispatch.",
        "code": "from functools import partial\nimport jax\n\n@partial(jax.jit, device=<device 0>)\ndef f():\n    jax.print(\"hello\")\n    return 2\n\n@partial(jax.jit, device=<device 1>)\ndef g():\n    jax.print(\"world\")\n    return 3\n\nf()\ng()"
      },
      {
        "description": "JAX code demonstrating that the compiler is free to reorder prints if there is no data dependency.",
        "code": "import jax\n\n@jax.jit\ndef f(x):\n    jax.print(\"hello\")\n    jax.print(\"world\")\n    return x"
      }
    ]
  },
  {
    "title": "The Need for Ordered Effects and Tokens",
    "concepts": [
      "The desired behavior is to have effects occur in the same order as in a single-threaded Python program.",
      "In some cases, reordering effects is acceptable for performance.",
      "For certain side-effects, like logging, ordering is crucial.",
      "Data-dependence is the main tool to enforce ordering of computations.",
      "Tokens are used to inject artificial data-dependence for side-effects without inputs."
    ],
    "code_examples": [
      {
        "description": "JAX code with logging effects where the order of logging values is important.",
        "code": "import jax\n\n@jax.jit\ndef f(x, y):\n    log_value(x)\n    log_value(y)\n\nf(1, 2)"
      },
      {
        "description": "JAX code with tokens to enforce ordering of prints. The second print depends on the output of the first.",
        "code": "import jax\n\n@jax.jit\ndef f(token, x):\n    token = jax.print(token, \"hello\")\n    token = jax.print(token, \"world\")\n    return token, x"
      }
    ]
  },
  {
    "title": "Implementation Details: Runtime and Compiler Tokens",
    "concepts": [
      "Two types of tokens are needed: runtime tokens and compiler tokens.",
      "Runtime tokens sequence asynchronously dispatched side-effecting computations.",
      "Compiler tokens sequence effects within computations.",
      "Runtime tokens are managed in Python, while compiler tokens are created during lowering.",
      "Strict execution assumes that dispatched functions start when all inputs are ready."
    ],
    "code_examples": [
      {
        "description": "Example showing how both runtime and compiler tokens are used to sequence prints within a JIT-compiled function.",
        "code": "import jax\n\n@jax.jit\ndef f(runtime_token, x):\n    compiler_token = new_compiler_token()\n    compiler_token = jax.print(compiler_token, \"hello\")\n    compiler_token = jax.print(compiler_token, \"world\")\n    return runtime_token, x"
      }
    ]
  },
  {
    "title": "Managing Runtime Tokens with JAX's Dispatch Machinery",
    "concepts": [
      "Runtime tokens need to be injected into and extracted from JIT-ted computations.",
      "The _execute function in JAX's dispatch machinery is hooked into for token management.",
      "Runtime tokens are passed into compiled computations and updated after execution.",
      "Runtime tokens need to be copied between devices when functions are executed on different devices.",
      "Runtime tokens can be represented as (0,)-shaped arrays to minimize overhead."
    ],
    "code_examples": [
      {
        "description": "The _execute function with runtime token injection and extraction.",
        "code": "def _execute(compiled_computation, *args):\n    runtime_token = get_runtime_token()  # Grab global token\n    runtime_token, *outputs = compiled_computation.execute(runtime_token, *args)\n    update_runtime_token(runtime_token)  # Update global token\n    return outputs"
      }
    ]
  },
  {
    "title": "Blocking on Side-Effecting Computations and Output Tokens",
    "concepts": [
      "Even without ordering, it may be necessary to wait for side-effecting computations to complete.",
      "jax.block_until_ready waits for future values to be ready.",
      "Output tokens are introduced to provide a value to block on for side-effecting functions without return values.",
      "Output tokens are device-specific.",
      "effects_barrier function blocks on the output token."
    ],
    "code_examples": [
      {
        "description": "JAX code demonstrating how a function with a side effect returns a new runtime token for blocking purposes.",
        "code": "import jax\n\n@jax.jit\ndef f():\n    jax.print(\"hello world\")\n    return new_runtime_token()\n\nf()  # Executed asynchronously"
      },
      {
        "description": "Example of using output tokens with jax.block_until_ready to wait for computations on different devices.",
        "code": "from functools import partial\nimport jax\n\n@partial(jax.jit, device=<device 0>)\ndef f():\n    jax.print(\"hello\")\n    return new_runtime_token()\n\n@partial(jax.jit, device=<device 1>)\ndef g():\n    jax.print(\"world\")\n    return new_runtime_token()\n\nt0 = f()\nt1 = g()\njax.block_until_ready((t0, t1))"
      },
      {
        "description": "The _execute function with output token extraction.",
        "code": "def _execute(compiled_computation, *args):\n    output_token, *outputs = compiled_computation.execute(runtime_token, *args)\n    update_output_token(output_token, compiled_computation.device)\n    return outputs"
      },
      {
        "description": "Function to block on the output token.",
        "code": "def effects_barrier():\n    output_token.block_until_ready()"
      }
    ]
  },
  {
    "title": "Thread-Local Token Management and Effect-Specific Tokens",
    "concepts": [
      "Token management infrastructure is thread-local.",
      "Sequencing is only guaranteed at the user thread level.",
      "There is one runtime token per effect, and different instances of that effect are sequenced.",
      "Using effect-specific tokens is a trade-off against enforcing a global single-threaded Python program ordering.",
      "A hybrid approach could use both effect-specific and global tokens."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction of the New jax.checkpoint() Implementation",
    "concepts": [
      "A new implementation of jax.checkpoint() (aka jax.remat()) has been introduced.",
      "The new implementation is enabled by default.",
      "The new implementation might introduce observable differences in edge cases.",
      "The old implementation can be temporarily restored via a configuration option (jax_new_checkpoint) until jax==0.3.16.",
      "As of jax==0.3.17 the configuration option to switch back to the old checkpoint implementation is unavailable."
    ],
    "code_examples": []
  },
  {
    "title": "Reasons for the Change and Benefits of the New Implementation",
    "concepts": [
      "The new implementation provides more precise user control over memory usage vs. recomputation tradeoff using the `policy` argument.",
      "The new implementation can rematerialize computations without a data dependence on arguments to the decorated function, reducing memory consumption.",
      "The new implementation incurs less Python overhead in some cases.",
      "The change unlocks future user benefits like custom batching rules and a forward-differentiable upgrade to custom_vjp.",
      "The change reduces complexity in parts of the JAX codebase, improving maintainability and bug-fixing."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the usage of the `policy` argument in the new `jax.checkpoint` implementation to control what intermediates are saved during the forward pass.",
        "code": "from\nfunctools\nimport\npartial\nimport\njax\ndef\napply_layer\n(\nW\n,\nx\n):\n    return\n    jnp\n    .\n    sin\n    (\n    jnp\n    .\n    dot\n    (\nW\n    ,\nx\n    ))\n@partial\n(\njax\n.\ncheckpoint\n,\npolicy\n=\njax\n.\ncheckpoint_policies\n.\ncheckpoint_dots\n)\ndef\npredict\n(\nparams\n,\nx\n):\n    for\n    W\n    in\n    params\n    [:\n    -\n    1\n    ]:\n        x\n        =\n        apply_layer\n        (\nW\n        ,\nx\n        )\n    return\n    jnp\n    .\n    dot\n    (\n    params\n    [\n    -\n    1\n    ],\n    x\n    )"
      },
      {
        "description": "Example demonstrating how the old implementation of jax.checkpoint was forced to save the value of `a`, while the new implementation can rematerialize it.",
        "code": "@jax\n.\ncheckpoint\ndef\nf\n(\nx\n):\n    a\n    =\n    some_function\n    (\n    jnp\n    .\n    arange\n    (\n    10_000_000\n    ))\n    # `a` does not depend on `x`\n    return\n    a\n    *\n    x"
      }
    ]
  },
  {
    "title": "Possible Issues and Changes After the Upgrade",
    "concepts": [
      "Some code may see small numerical changes due to increased rematerialization, potentially requiring relaxation of overly tight test tolerances.",
      "The `concrete` option in the old jax.checkpoint implementation has been removed.",
      "The `static_argnums` option can be used as an alternative to `concrete=True` to support tracing on static arguments."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the old jax.checkpoint API using the `concrete=True` option to support passing an argument like `is_training`.",
        "code": "@partial\n(\njax\n.\ncheckpoint\n,\nconcrete\n=\nTrue\n)\n# OLD jax.checkpoint API\ndef\nfoo\n(\nx\n,\nis_training\n):\n    if\n    is_training\n    :\n        return\n        g\n        (\nx\n        )\n    else\n    :\n        return\n        h\n        (\nx\n        )"
      },
      {
        "description": "Example demonstrating the new jax.checkpoint API using the `static_argnums` option as an alternative to `concrete=True`.",
        "code": "@partial\n(\njax\n.\ncheckpoint\n,\nstatic_argnums\n=\n(\n1\n,)\n)\n# NEW jax.checkpoint API\ndef\nfoo\n(\nx\n,\nis_training\n):\n    if\n    is_training\n    :\n        ..."
      }
    ]
  },
  {
    "title": "Introduction to Type Annotations in JAX",
    "concepts": [
      "Python 3.0 introduced optional function annotations (PEP 3107).",
      "Python 3.5 codified annotations for static type checking (PEP 484).",
      "Type annotations and static type checking are becoming integral to Python development workflows.",
      "JAX has added annotations throughout its API.",
      "The document outlines the goals and recommendations for type annotations in JAX.",
      "Better type annotations are a frequent request from users.",
      "It's important to clearly understand the goals of type annotation to properly review code contributions to JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Levels of Type Annotation",
    "concepts": [
      "Type annotations can serve as inline documentation of function parameter and return types (Level 1).",
      "Type annotations can be used to provide inputs to intelligent code completion systems in IDEs (Level 2).",
      "Type annotations can be used for static type checking as part of a CI test suite (Level 3)."
    ],
    "code_examples": []
  },
  {
    "title": "Level 1: Type Annotations as Documentation",
    "concepts": [
      "Type annotations can be used as documentation of function parameter and return types.",
      "JAX has long utilized annotations by creating type names aliased to Any.",
      "Using `Array = Any` puts no constraint on argument values but serves as in-code documentation.",
      "The name of the alias is lost in generated documentation, limiting the documentation benefit to the source code."
    ],
    "code_examples": [
      {
        "description": "Example showing how type aliases can be created and used with `Any` to provide basic documentation of the function's expected input and output types.",
        "code": "Array = Any\nShape = core.Shape\n\ndef slice(\n    operand: Array,\n    start_indices: Sequence[int],\n    limit_indices: Sequence[int],\n    strides: Optional[Sequence[int]] = None\n) -> Array:\n    ..."
      }
    ]
  },
  {
    "title": "Level 2: Type Annotations for Code Completion",
    "concepts": [
      "Type annotations can be used as inputs to intelligent code completion systems in IDEs.",
      "Knowing that a function returns an alias of Any named Array doesn't add useful information to a code completion engine.",
      "Annotating functions with specific return types like `DeviceArray` enables more relevant autocompletions.",
      "JAX has begun to add this level of annotation in some places, such as jnp.ndarray in jax.random."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the use of jnp.ndarray as a return type annotation. This allows IDEs to provide intelligent code completion by suggesting methods and attributes of the JAX array object.",
        "code": "def shuffle(\n    key: KeyArray,\n    x: Array,\n    axis: int = 0\n) -> jnp.ndarray:\n    ..."
      }
    ]
  },
  {
    "title": "Level 3: Type Annotations for Static Type Checking",
    "concepts": [
      "Static type checking is often the first thing people think of when considering type annotations in Python code.",
      "Python doesn't do runtime type checking, but static type checking tools exist.",
      "Mypy, pytype, and pyright are important static type checkers for JAX.",
      "Full static type checking is the strictest application of type annotations and surfaces errors when annotations are not precisely correct.",
      "Strictness can make type checking brittle in packages that rely on duck-typing.",
      "The JAX codebase contains `#type: ignore` and `#pytype: disable` comments in many places to address typing problems."
    ],
    "code_examples": []
  },
  {
    "title": "Challenges in Type Annotating JAX",
    "concepts": [
      "JAX's source code poses unique challenges for Python's type annotation system.",
      "Package development must satisfy the constraints of two different static type checking systems: pytype and mypy.",
      "JAX heavily uses duck-typing, where inputs can be various types, making simple annotations insufficient.",
      "JAX's Python API relies heavily on function transformations (jit, vmap, grad, etc.), posing a challenge for static type analysis.",
      "Type annotations relate to Python class/type, but array-based languages often need to annotate array shapes and data types.",
      "A large part of JAX's user-facing API is inherited from NumPy, which follows a duck-typing style."
    ],
    "code_examples": []
  },
  {
    "title": "Dual Type Checker Constraints",
    "concepts": [
      "JAX development must satisfy both pytype (internal) and mypy (external).",
      "The two type checkers have overlapping behavior but present unique corner cases.",
      "Moving to stricter type annotations introduces potential for frustrating developer experiences due to differing type checker results."
    ],
    "code_examples": []
  },
  {
    "title": "Duck-Typing Challenges",
    "concepts": [
      "JAX heavily uses duck-typing, where inputs can be various types.",
      "Simple annotations like `def func(x: DeviceArray)` are not sufficient.",
      "Type annotations for JAX functions may require a set of JAX-specific typing extensions similar to numpy.typing."
    ],
    "code_examples": []
  },
  {
    "title": "Function Transformation Challenges",
    "concepts": [
      "JAX's function transformations (jit, vmap, grad, etc.) pose a challenge for static type analysis.",
      "Flexible annotation for decorators has been a long-standing issue.",
      "ParamSpec (PEP 612) can be used but is not fully supported due to JAX following NEP 29.",
      "Protocols can be used as a partial solution."
    ],
    "code_examples": []
  },
  {
    "title": "Array Type Granularity Challenges",
    "concepts": [
      "Type annotations relate to Python class/type, but often array shapes and data types are more important in array-based languages.",
      "Python has a plan for finer-grained type annotations via Variadic Type Generics (PEP 646).",
      "Third-party projects like jaxtyping exist but use non-standard annotations."
    ],
    "code_examples": []
  },
  {
    "title": "Duck-Typing in NumPy and JAX",
    "concepts": [
      "JAX inherits a large part of its API from NumPy.",
      "NumPy's API was developed before static type checking and uses duck-typing.",
      "When adding annotations to duck-typed code, we can annotate the intent or the implementation.",
      "Annotating intent is more useful for the user and leaves room for refactoring.",
      "Annotating implementation better serves Level 3 type checking."
    ],
    "code_examples": [
      {
        "description": "Example of a duck-typed function in NumPy. The `reps` argument is intended to be an integer or a sequence of integers, but the implementation allows it to be any iterable.",
        "code": "def tile(A, reps):\n    try:\n        tup = tuple(reps)\n    except TypeError:\n        tup = (reps,)\n    d = len(tup)\n    ..."
      }
    ]
  },
  {
    "title": "Roadmap for Type Annotations in JAX",
    "concepts": [
      "Type annotations should indicate the intent of APIs rather than the implementation.",
      "Inputs should be typed as permissively as reasonable, while outputs should be typed as strictly as possible.",
      "A `jax.typing` module will be added with common type specifications.",
      "`ArrayLike`, `DTypeLike`, and `ShapeLike` will be unions of anything that can be implicitly converted to their respective types.",
      "`Array` (or `NDArray`), `DType`, and `Shape` will be strictly-typed analogs of the permissive types.",
      "The current implementation of `jax.numpy.ndarray` may be dropped in favor of making `ndarray` an alias of `Array`.",
      "Overly-complex protocols for arguments passed to API functions should be avoided in favor of simple unions.",
      "Recently-introduced mechanisms like `ParamSpec` and `Variadic Type Generics` will be avoided until support matures.",
      "JAX will effectively strip all annotations from functions decorated by JAX transformations (jit, vmap, grad, etc.) for the time being."
    ],
    "code_examples": []
  },
  {
    "title": "jax.typing Module and Type Specifications",
    "concepts": [
      "Inputs to JAX functions should be typed as permissively as reasonable.",
      "Shapes are typically tuples, but functions accepting shapes should accept arbitrary sequences.",
      "A jax.typing module will contain common type specifications.",
      "ArrayLike, DTypeLike, and ShapeLike will be unions of types convertible to their respective types.",
      "Outputs of functions should be typed as strictly as possible.",
      "Array or NDArray is equivalent to Union[Tracer, jnp.ndarray] and should be used to annotate array outputs.",
      "DType is an alias of np.dtype.",
      "Shape is essentially Tuple[int, ...]."
    ],
    "code_examples": []
  },
  {
    "title": "Duck-Typing for Arrays in JAX",
    "concepts": [
      "Type annotation of arrays in JAX poses a unique challenge because of JAX's extensive use of duck-typing.",
      "Objects used for type annotation often overlap with objects used for runtime instance checking.",
      "For JAX, there is a need to provide duck-typed objects for both static type annotations and runtime instance checks.",
      "jax.Array is assumed to be the runtime type of on-device arrays.",
      "ArrayAnnotation is the hypothetical object used for duck-typed type annotations.",
      "ArrayInstance is the hypothetical object used for duck-typed runtime isinstance checks."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the intended use of ArrayAnnotation in a function decorated with jit, ensuring that it correctly handles Tracer objects.",
        "code": "@jit\ndef f(x: ArrayAnnotation) -> ArrayAnnotation:\n    assert isinstance(x, core.Tracer)\n    return x"
      },
      {
        "description": "Illustrates the desired behavior of ArrayInstance for runtime type checking, where both regular arrays and Tracer objects should pass the isinstance check.",
        "code": "def f(x):\n    return isinstance(x, ArrayInstance)\n\nx = jnp.array([1, 2, 3])\nassert f(x)  # x will be an array\nassert jit(f)(x)  # x will be a tracer"
      }
    ]
  },
  {
    "title": "Approaches to Array Type Handling",
    "concepts": [
      "Different mechanisms could be used to implement `ArrayAnnotation` and `ArrayInstance`.",
      "For `ArrayAnnotation`, options include type unions, interface files, or restructuring the class hierarchy.",
      "For `ArrayInstance`, options include overriding `type(ArrayInstance).__instancecheck__`, dynamically registering an abstract base class, or restructuring the class hierarchy.",
      "A key decision is whether `ArrayAnnotation` and `ArrayInstance` should be the same or different objects.",
      "Following NumPy's precedent, `jax.typing.NDArray` could be used for type annotations, while `jax.numpy.ndarray` could be used for instance checks.",
      "Alternatively, type checking and annotation could be unified via override mechanisms.",
      "Full unification could be achieved via restructuring of the class hierarchy and replacing duck-typing with OOP object hierarchies."
    ],
    "code_examples": []
  },
  {
    "title": "Preferred Approach and Next Steps",
    "concepts": [
      "Option 4 (full unification via restructuring of the class hierarchy) is the preferred approach.",
      "A new jax.Array base class will be created, with Tracer and ArrayImpl inheriting from it.",
      "jnp.ndarray will be an alias for jax.Array.",
      "A private jax._src.typing module will be created for simple types (Array, ArrayLike, DType, DTypeLike, Shape, NamedShape, ShapeLike).",
      "The functions within jax.lax will be annotated with these new typing definitions.",
      "A jax.Array base class will be re-implemented in pybind11.",
      "A public jax.typing module will be created with documentation of annotation best practices."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to shmap",
    "concepts": [
      "JAX supports compiler-driven and explicit-collectives approaches to multi-device programming.",
      "pjit is an API for compiler-driven multi-device programming.",
      "pmap has flaws and xmap doesn't provide per-device shapes.",
      "shmap is a simple multi-device parallelism API that allows writing per-device code with explicit collectives.",
      "shmap is a specialization of xmap.",
      "shmap directly surfaces the XLA SPMD Partitioner's 'manual' mode.",
      "shmap can be used within a pjit computation for manual collective optimization.",
      "shmap is an upgrade for pmap users.",
      "shmap is more expressive, performant, and composable."
    ],
    "code_examples": []
  },
  {
    "title": "shmap Examples",
    "concepts": [
      "shmap allows writing per-device code with explicit collectives.",
      "shmap doesn't require nesting for multiple axes of parallelism, unlike pmap.",
      "shmap doesn't require reshapes in the caller, and logical shapes match per-device physical shapes.",
      "shmap enables precise device placement control using mesh.",
      "shmap has only one set of axis names for logical and physical.",
      "shmap results in a jax.Array that can be efficiently passed to pjit.",
      "shmap code works efficiently inside pjit/jit.",
      "shmap code works eagerly for debugging purposes.",
      "shmap can be used to implement reduce scatter operations."
    ],
    "code_examples": [
      {
        "description": "Basic matrix multiplication example using shard_map with psum.",
        "code": "from functools import partial\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax.sharding import Mesh, PartitionSpec as P\nfrom jax.experimental.shard_map import shard_map\n\nmesh = jax.make_mesh(((4, 2),), ('i', 'j'))\na = jnp.arange(8 * 16.).reshape(8, 16)\nb = jnp.arange(16 * 32.).reshape(16, 32)\n\n@partial(\n    shard_map,\n    mesh=mesh,\n    in_specs=(P('i', 'j'), P('j', None)),\n    out_specs=P('i', None))\ndef matmul_basic(a_block, b_block):\n  # a_block: f32[2, 8]\n  # b_block: f32[8, 32]\n  z_partialsum = jnp.dot(a_block, b_block)\n  z_block = jax.lax.psum(z_partialsum, 'j')\n  return z_block\n\nc = matmul_basic(a, b)\n# c: f32[8, 32]"
      },
      {
        "description": "Matrix multiplication with reduce scatter using shard_map.",
        "code": "@partial(\n    shard_map,\n    mesh=mesh,\n    in_specs=(P('i', 'j'), P('j', None)),\n    out_specs=P('i', 'j'))\ndef matmul_reduce_scatter(a_block, b_block):\n  # c_partialsum: f32[8/X, 32]\n  c_partialsum = jnp.matmul(a_block, b_block)\n  # c_block: f32[8/X, 32/Y]\n  c_block = jax.lax.psum_scatter(\n      c_partialsum, 'j', scatter_dimension=1, tiled=True)\n  return c_block\n\nc = matmul_reduce_scatter(a, b)"
      }
    ]
  },
  {
    "title": "Comparison with pmap",
    "concepts": [
      "pmap unstacks array inputs along an axis, applies the body function, and stacks the results back together.",
      "pmap is a rank-reducing map.",
      "The number of logical applications of the pmap body function is determined by the input axis size.",
      "shmap slices inputs into blocks and concatenates the results.",
      "shmap is a rank-preserving map.",
      "The number of logical applications of the shmap body function is determined by the mesh size.",
      "in_specs identify input array axes with mesh axes.",
      "The shard sizes are determined by the mesh axis size.",
      "If an input's pspec does not mention a mesh axis name, there's no splitting over that mesh axis."
    ],
    "code_examples": [
      {
        "description": "Illustration of shard_map slicing and concatenating inputs.",
        "code": "devices = np.array(jax.devices()[:4])\nm = Mesh(devices, ('i',))\n# mesh.shape['i'] = 4\nshard_map(f, m, in_specs=P('i'), out_specs=P('i'))(y) == jnp.concatenate(\n    [f(y_blk) for y_blk in jnp.split(y, 4)])"
      },
      {
        "description": "Example demonstrating input partitioning with mesh and PartitionSpec.",
        "code": "devices = np.array(jax.devices())\nm = Mesh(devices.reshape(4, 2), ('i', 'j'))\n\n@partial(\n    shard_map,\n    mesh=m,\n    in_specs=P('i', None),\n    out_specs=P('i', 'j'))\ndef f1(x_block):\n  print(x_block.shape)\n  return x_block\n\nx1 = np.arange(12 * 12).reshape(12, 12)\ny = f1(x1)\n# prints (3,12)"
      },
      {
        "description": "Illustrates how to use jnp.tile when a mesh axis is not mentioned in an input pspec.",
        "code": "@partial(\n    shard_map,\n    mesh=m,\n    in_specs=P('i', 'j'),\n    out_specs=P('i', 'j'))\ndef f2(x_block):\n  print(x_block.shape)\n  return x_block\n\nx = np.arange(12 * 12).reshape(12, 12)\nx_ = jnp.tile(x, (1, mesh.axis_size['j']))\n# x_ has shape (12, 24)\ny = f2(x_)\n# prints (3,12), and f1(x) == f2(x_)"
      }
    ]
  },
  {
    "title": "Output Partitioning and Un-tiling",
    "concepts": [
      "out_specs identify output array axes with mesh axes.",
      "Output blocks are assembled based on the out_specs.",
      "When a mesh axis name is not mentioned in an output pspec, it represents un-tiling.",
      "Un-tiling means the output blocks are equal along that mesh axis.",
      "Physical data movement is not possible on outputs; out_specs only encodes how to assemble block outputs into Array s.",
      "shard_map has untile and block transpose capabilities built into its output."
    ],
    "code_examples": [
      {
        "description": "Examples demonstrating different out_specs and their effects on output tiling.",
        "code": "x = jnp.array([[3.]])\nz = shard_map(lambda: x, mesh=m, in_specs=(), out_specs=P('i', 'j'))()\nprint(z)\n# prints the same as jnp.tile(x, (4, 2))\n\nz = shard_map(lambda: x, mesh=m, in_specs=(), out_specs=P('i', None))()\nprint(z)\n# prints the same as jnp.tile(x, (4, 1)), or just jnp.tile(x, (4,))\n\nz = shard_map(lambda: x, mesh=m, in_specs=(), out_specs=P(None, None))()\nprint(z)\n# prints the same as jnp.tile(x, (1, 1)), or just x"
      },
      {
        "description": "Illustrates the use of psum with shard_map and its effect on the output shape.",
        "code": "@partial(\n    shard_map,\n    mesh=m,\n    in_specs=P('i', 'j'),\n    out_specs=P('i', None))\ndef f3(x_block):\n  return jax.lax.psum(x_block, 'j')\n\nx = np.arange(12 * 12).reshape(12, 12)\ny3 = f3(x)\nprint(y3.shape)\n# (12,6)"
      },
      {
        "description": "Demonstrates varying mesh axes in out_specs, combined with psum for data aggregation.",
        "code": "@partial(\n    shard_map,\n    mesh=m,\n    in_specs=P('i', 'j'),\n    out_specs=P(None, 'j'))\ndef f4(x_block):\n  return jax.lax.psum(x_block, 'i')\n\nx = np.arange(12 * 12).reshape(12, 12)\ny4 = f4(x)\nprint(y4.shape)\n# (3,12)\n\n@partial(\n    shard_map,\n    mesh=m,\n    in_specs=P('i', 'j'),\n    out_specs=P(None, None))\ndef f5(x_block):\n  return jax.lax.psum(x_block, ('i', 'j'))\n\ny5 = f5(x)\nprint(y5.shape)\n# (3,6)"
      }
    ]
  },
  {
    "title": "shmap API Definition",
    "concepts": [
      "The shard_map function takes a function, mesh, in_specs, and out_specs as arguments.",
      "mesh encodes devices arranged in an array and with associated axis names.",
      "in_specs and out_specs are PartitionSpec s that express slicing/unconcatenation and concatenation of inputs and outputs.",
      "Unmentioned names in in_specs and out_specs correspond to replication and untiling, respectively.",
      "The shapes of the arguments passed to f have the same ranks as the arguments passed to shard_map(f).",
      "The body of f can apply collectives using names from mesh.",
      "shmap is eager by default, allowing for Python control flow and interactive debugging.",
      "shmap can be staged out and end-to-end compiled with jit.",
      "Lowering shmap to StableHLO involves switching into 'manual SPMD mode'."
    ],
    "code_examples": [
      {
        "description": "Definition of the shard_map function.",
        "code": "from jax.sharding import Mesh\nfrom typing import Callable, Sequence, Tuple\nimport jax.numpy as jnp\nfrom jax.sharding import PartitionSpec\nfrom jax import lax\nfrom jax.experimental import attention\nfrom typing import Any\nfrom flax import traverse_util\n\nPyTree = Any\n\nSpecs = PyTree[PartitionSpec]\ndef shard_map(\n    f: Callable,\n    mesh: Mesh,\n    in_specs: Specs,\n    out_specs: Specs\n) -> Callable:\n  ...\n"
      }
    ]
  },
  {
    "title": "shmap in a Transformer Layer",
    "concepts": [
      "shmap can be used to optimize transformer layers.",
      "Manual implementations of matmul can be used with shard_map.",
      "shmap enables overlapping compute and communication.",
      "shmap can be integrated with pjit."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the integration of shard_map within a transformer layer for manual optimization.",
        "code": "def matmul_2D_wg_manual(xnorm, q_wi, layer):\n  '''Calls a custom manual implementation of matmul_reducescatter'''\n  # [batch, maxlen, embed.X] @ [heads.YZ, embed.X, q_wi_per_head]\n  # -> (matmul)\n  # -> [batch, maxlen, heads.YZ, q_wi_per_head]{x unreduced}\n  # -> (reducescatter over x into X heads, B batches)\n  # -> [batch, maxlen, heads.YZX, q_wi_per_head]\n  with jax.named_scope('q_wi'):\n    xnorm = intermediate_dtype(xnorm)\n    q_wi = matmul_reducescatter(\n        'bte,hed->bthd',\n        xnorm,\n        params.q_wi,\n        scatter_dimension=(0, 2),\n        axis_name='i',\n        layer=layer)\n    return q_wi\n\nimport partitioning.logical_to_physical as l2phys\n\ndef pjit_transformer_layer(\n    hparams: HParams,\n    layer: int,\n    params: weights.Layer,\n    sin: jnp.ndarray,\n    cos: jnp.ndarray,\n    kv_caches: Sequence[attention.KVCache],\n    x: jnp.ndarray\n) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n  \"\"\"Forward pass through a single layer, returning output, K, V.\"\"\"\n\n  def my_layer(t, axis=0):\n    \"\"\"Gets the parameters corresponding to a given layer.\"\"\"\n    return lax.dynamic_index_in_dim(t, layer, axis=axis, keepdims=False)\n\n  # 2D: [batch.Z, time, embed.XY]\n  x = _with_sharding_constraint(x, ('residual_batch', 'residual_time', 'residual_embed'))\n  xnorm = _layernorm(x)\n  # 2D: [batch, time, embed.X]\n  xnorm = _with_sharding_constraint(xnorm, ('post_norm_batch', 'time', 'post_norm_embed'))\n\n  # jump into manual mode where you want to optimise\n  if manual:\n    q_wi = shard_map(\n        matmul_2D_wg_manual,\n        mesh\n        in_specs=(\n            l2phys('post_norm_batch', 'time', 'post_norm_embed'),\n            l2phys('layers', 'heads', 'embed', 'q_wi_per_head')),\n        out_specs=l2phys('post_norm_batch', 'time', 'heads', 'q_wi_per_head'))(\n            xnorm, q_wi, layer)\n  else:\n    q_wi = jnp.einsum('bte,hed->bthd', xnorm, my_layer(params.q_wi))\n\n  # 2D: [batch, time, heads.YZX, None]\n  q_wi = _with_sharding_constraint(q_wi, ('post_norm_batch', 'time', 'heads', 'qkv'))\n  q = q_wi[:, :, :, :hparams.qkv]\n  q = _rope(sin, cos, q)\n  # unlike in https://arxiv.org/pdf/2002.05202.pdf, PaLM implements\n  # swiGLU with full d_ff dimension, rather than 2/3 scaled\n  wi0 = q_wi[:, :, :, hparams.qkv:hparams.qkv + (hparams.ff // hparams.heads)]\n  wi1 = q_wi[:, :, :, hparams.qkv + (hparams.ff // hparams.heads):]\n  kv = jnp.einsum('bte,ezd->btzd', xnorm, my_layer(params.kv))\n  k = kv[:, :, 0, :hparams.qkv]\n  v = kv[:, :, 0, hparams.qkv:]\n  k = _rope(sin, cos, k)\n\n  y_att = jnp.bfloat16(attention.attend(q, k, v, kv_caches, layer))\n  y_mlp = special2.swish2(wi0) * wi1\n\n  # 2D: [batch, time, heads.YZX, None]\n  y_mlp = _with_sharding_constraint(y_mlp, ('post_norm_batch', 'time', 'heads', None))\n\n  y_fused = jnp.concatenate([y_att, y_mlp], axis=-1)\n  # do the second half of the mlp and the self-attn projection in parallel\n  y_out = jnp.einsum('bthd,hde->bte', y_fused, my_layer(params.o_wo))\n\n  # 2D: [batch.Z, time, embed.XY]\n  y_out = _with_sharding_constraint(y_out, ('residual_batch', 'residual_time', 'residual_embed'))\n\n  z = y_out + x\n  z = _with_sharding_constraint(z, ('residual_batch', 'residual_time', 'residual_embed'))\n  return z, k, v"
      }
    ]
  },
  {
    "title": "Why shmap and not pmap?",
    "concepts": [
      "pmap has shortcomings that make it unsuitable for modern programs.",
      "Mapping multiple axes in pmap required nested pmap s, which are cumbersome and make device placement difficult.",
      "Controlling device placement was impossible with pmap.",
      "jit / pjit composability with pmap is problematic because sharding is not preserved.",
      "pmap outputs are not jax.Array compatible, hindering pjit compatibility.",
      "pmap has multi-controller semantics that differ from single-controller pmap.",
      "pmap was not designed for eager mode.",
      "pmap requires reshapes in the caller, potentially increasing memory usage.",
      "xmap paved the way for solving these issues.",
      "shmap solves these problems in similar ways to xmap and is a specialized subset of xmap."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.extend",
    "concepts": [
      "Several projects depend on JAX internals for core machinery and extending JAX.",
      "Using JAX internals is challenging due to design and lack of API support.",
      "The proposal introduces jax.extend module for a library view of JAX internals.",
      "jax.extend is a second-tier API with no compatibility policy.",
      "The audience includes JAX-adjacent libraries and projects experimenting with transformations and compilers.",
      "jax.extend is different from jax.experimental; it's not a staging ground for new features.",
      "jax.extend would not follow the public API compatibility policy and may break without notice.",
      "Pinning JAX versions is suggested for projects relying on jax.extend.",
      "Implementation starts with moving symbols from internal packages to jax.extend."
    ],
    "code_examples": []
  },
  {
    "title": "Possible Modules within jax.extend",
    "concepts": [
      "jax.extend may eventually include modules like core, interpreters, random, and sharding.",
      "core will contain primitives, the Jaxpr IR, etc.",
      "interpreters will contain core transformations and lowerings.",
      "random will contain random bit generation, key splitting and folding, key arrays.",
      "sharding will contain extra functionality around distributed arrays.",
      "Temporary symbols like jex.api_util and jex.lib may exist initially."
    ],
    "code_examples": []
  },
  {
    "title": "jax.extend.core: Primitives and Jaxpr IR",
    "concepts": [
      "jax.extend.core enables defining new JAX primitives and processing the Jaxpr IR.",
      "It provides access to existing core system primitives and IR types.",
      "It provides functions for checking and building jaxprs.",
      "Initially, it contains more symbols than needed, including names used in setting up \"final-style transformations\".",
      "The module could potentially include make_jaxpr itself."
    ],
    "code_examples": []
  },
  {
    "title": "jax.extend.interpreters: Transformation Rules",
    "concepts": [
      "jax.extend.interpreters provides a means of registering transformation rules for primitives.",
      "It initially reflects jax._src.interpreters, including ad, batching, partial_eval, mlir, pxla, and xla modules.",
      "The first three modules (ad, batching, partial_eval) might be replaceable by a single primitive extension API in jex.core.",
      "The latter three modules (mlir, pxla, xla), used for lowering, could be simplified into one module.",
      "Tracer types may be removed from jex to simplify transformation rules."
    ],
    "code_examples": []
  },
  {
    "title": "Example: Defining a Primitive and JIT Compilation",
    "concepts": [
      "jax.extend.core and jax.extend.interpreters should suffice for custom primitive tutorials.",
      "It is possible to define a primitive and its behavior under jax.jit."
    ],
    "code_examples": [
      {
        "description": "Defining a custom primitive 'mul_add' and registering its lowering rule for JIT compilation.",
        "code": "from\njax.extend\nimport\ncore\n# Previously: from jax import core\nfrom\njax.extend.interpreters\nimport\nmlir\n# ... and similarly\nmul_add_p\n=\ncore\n.\nPrimitive\n(\n'mul_add'\n)\nmul_add_p\n.\ndef_impl\n(\nlambda\nx\n,\ny\n,\nz\n:\nx\n*\ny\n+\nz\n)\n@mul_add_p\n.\ndef_abstract_eval\ndef\nmul_add_abstract\n(\nx_sa\n,\ny_sa\n,\nz_sa\n):\n  return\ncore\n.\nShapedArray\n(\nx_sa\n.\nshape\n,\nx_sa\n.\ndtype\n)\ndef\nmul_add_mlir\n(\nctx\n,\nxc\n,\nyc\n,\nzc\n):\n  add\n=\nmlir\n.\nhlo\n.\nAddOp\n  mul\n=\nmlir\n.\nhlo\n.\nMulOp\n  return\nadd\n(\nmul\n(\nxc\n,\nyc\n),\nzc\n)\n.\nresults\nmlir\n.\nregister_lowering\n(\nmul_add_p\n,\nmul_add_mlir\n)\nimport\njax\nprint\n(\nmul_add_p\n.\nbind\n(\n2\n,\n3\n,\n4\n))\n# -> 10\nprint\n(\njax\n.\njit\n(\nmul_add_p\n.\nbind\n)(\n2\n,\n3\n,\n4\n))\n# -> Array(10, dtype=int32)"
      },
      {
        "description": "Defining a custom primitive 'mul_add' and registering its lowering rule for JIT compilation. (Duplicated Code Snippet)",
        "code": "from\njax.extend\nimport\ncore\n# Previously: from jax import core\nfrom\njax.extend.interpreters\nimport\nmlir\n# ... and similarly\nmul_add_p\n=\ncore\n.\nPrimitive\n(\n'mul_add'\n)\nmul_add_p\n.\ndef_impl\n(\nlambda\nx\n,\ny\n,\nz\n:\nx\n*\ny\n+\nz\n)\n@mul_add_p\n.\ndef_abstract_eval\ndef\nmul_add_abstract\n(\nx_sa\n,\ny_sa\n,\nz_sa\n):\n  return\ncore\n.\nShapedArray\n(\nx_sa\n.\nshape\n,\nx_sa\n.\ndtype\n)\ndef\nmul_add_mlir\n(\nctx\n,\nxc\n,\nyc\n,\nzc\n):\n  add\n=\nmlir\n.\nhlo\n.\nAddOp\n  mul\n=\nmlir\n.\nhlo\n.\nMulOp\n  return\nadd\n(\nmul\n(\nxc\n,\nyc\n),\nzc\n)\n.\nresults\nmlir\n.\nregister_lowering\n(\nmul_add_p\n,\nmul_add_mlir\n)\nimport\njax\nprint\n(\nmul_add_p\n.\nbind\n(\n2\n,\n3\n,\n4\n))\n# -> 10\nprint\n(\njax\n.\njit\n(\nmul_add_p\n.\nbind\n)(\n2\n,\n3\n,\n4\n))\n# -> Array(10, dtype=int32)"
      }
    ]
  },
  {
    "title": "jax.extend.random: RNG Implementations",
    "concepts": [
      "jax.extend.random could expose mechanisms for defining new RNG implementations.",
      "It could expose functions for working with PRNG key internals.",
      "It could expose keyed hash functions underlying the built-in RNG implementations."
    ],
    "code_examples": []
  },
  {
    "title": "jax.extend.sharding: Distributed Array Utilities",
    "concepts": [
      "jax.extend.sharding could expose low-level utilities for sharding distributed arrays.",
      "It could provide jex.sharding.XlaOpShardingProto, corresponding to jax._src.lib.xla_client.OpSharding internally."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction and Problem Definition",
    "concepts": [
      "An efficiency problem exists in automatically transposing shmaps containing certain collectives, specifically psum and all_gather.",
      "The issue arises when the output of the collective is returned to the caller as an unmapped output.",
      "This situation occurs in batch data parallel neural network loss functions using psum to compute the total loss.",
      "The document aims to define the transpose efficiency problem and propose a solution affecting only the shmap implementation."
    ],
    "code_examples": []
  },
  {
    "title": "Illustrative Examples of the Transpose Problem",
    "concepts": [
      "Example 1 demonstrates an inefficient transpose with psum and an unmapped output.",
      "Example 2 shows an efficient transpose with psum and a mapped output.",
      "Example 3 illustrates a cursed identity function leading to increasingly inefficient transposes.",
      "Example 4 demonstrates an inefficient transpose with all_gather to an unmapped output.",
      "Example 5 shows an efficient transpose with all_gather to a mapped output."
    ],
    "code_examples": [
      {
        "description": "Example 1: shmap involving psum and unmapped output with inefficient transpose",
        "code": "f1 = shmap(\n    lambda x: psum(g(x), 'i'),\n    in_specs=P('i'),\n    out_specs=P()\n)"
      },
      {
        "description": "An efficient \"transpose\" of Example 1 (but don't transpose this again!)",
        "code": "\u00bff1_transpose? = shmap(\n    t(g),\n    in_specs=P(),\n    out_specs=P('i')\n)"
      },
      {
        "description": "The transpose we currently get for Example 1 (which is fine to transpose again)",
        "code": "t(f1) = shmap(\n    lambda ybar: t(g)(psum(ybar / 8, 'i')),\n    in_specs=P(),\n    out_specs=P('i')\n)"
      },
      {
        "description": "Example 2: shmap involving psum and *mapped* output with efficient transpose",
        "code": "f2 = shmap(\n    lambda x, y: psum(g(x), 'i') * y,\n    in_specs=(P('i'), P('i')),\n    out_specs=P('i')\n)"
      },
      {
        "description": "The transpose we currently get for Example 2 is efficient",
        "code": "t(f2, 0) = shmap(\n    lambda y, zbar: t(g)(psum(zbar * y, 'i')),\n    in_specs=(P('i'), P('i')),\n    out_specs=P('i')\n)"
      },
      {
        "description": "Example 3: cursed identity",
        "code": "cursed_identity = shmap(\n    lambda x: x,\n    P(),\n    P()\n)"
      },
      {
        "description": "Currently we get these inefficient transposes",
        "code": "t(cursed_identity) = shmap(\n    lambda x: psum(x / 8, 'i'),\n    P(),\n    P()\n)\nt(t(cursed_identity)) = shmap(\n    lambda x: psum(psum(x / 8 / 8, 'i'), 'i'),\n    P(),\n    P()\n)\n..."
      },
      {
        "description": "Example 4: all_gather to an unmapped output",
        "code": "f4 = shmap(\n    lambda x: all_gather(x, 'i'),\n    P('i'),\n    P()\n)"
      },
      {
        "description": "Currently we get this inefficient transpose",
        "code": "t(f4) = shmap(\n    lambda ybar: psum_scatter(ybar / 8, 'i'),\n    P(),\n    P('i')\n)"
      },
      {
        "description": "Example 5: all_gather to a mapped output",
        "code": "f5 = shmap(\n    lambda x, y: all_gather(x, 'i') * y,\n    in_specs=(P('i'), P('i')),\n    out_specs=P('i')\n)"
      },
      {
        "description": "Currently we get this efficient transpose",
        "code": "t(f5, 0) = shmap(\n    lambda y, zbar: psum_scatter(zbar * y, 'i'),\n    in_specs=(P('i'), P('i')),\n    out_specs=P('i')\n)"
      }
    ]
  },
  {
    "title": "Initial Solution Idea: Extending Out_specs",
    "concepts": [
      "A partial solution involves extending out_specs to express summing, allowing for a built-in psum.",
      "This approach fixes the transpose problem for Example 1 but doesn't fully address the cursed identity transpose in Example 3.",
      "The solution still results in wasteful communication in some scenarios."
    ],
    "code_examples": [
      {
        "description": "Example 4 again",
        "code": "f4 = shmap(\n    lambda x: all_gather(x, 'i'),\n    P('i'),\n    P()\n)\n\n# Why didn't we just write it like this?\nf4_better = shmap(\n    lambda x: x,\n    P('i'),\n    P('i')\n)"
      },
      {
        "description": "Example 1 again",
        "code": "f1 = shmap(\n    lambda x: psum(g(x), 'i'),\n    in_specs=P('i'),\n    out_specs=P()\n)\n\n# What if we could write an output sum like this?\nf1_better = shmap(\n    g,\n    in_specs=P('i'),\n    out_specs=P(sum='i')\n)\n\n# sum='i' means sum over that axis\n# Then it could transpose like this:\nt(f1_better) = shmap(\n    t(g),\n    in_specs=P(),\n    out_specs=P('i')\n)\nt(t(f1_better)) = shmap(\n    t(t(g)),\n    in_specs=P('i'),\n    P(sum='i')\n)"
      },
      {
        "description": "Example 3 again",
        "code": "cursed_identity = shmap(\n    lambda x: x,\n    P(),\n    P()\n)\n\n# How it would transpose with the P-sum partial solution:\nt(cursed_identity) = shmap(\n    lambda x: x / 8,\n    P(),\n    P(sum='i')\n)\nt(t(cursed_identity)) = shmap(\n    lambda x: x / 8,\n    P(),\n    P(sum='i')\n)"
      }
    ]
  },
  {
    "title": "Improved Solution: Device Variance Tracking and Collective Decomposition",
    "concepts": [
      "The improved solution involves tracking device-invariant vs. device-varying values and decomposing psum into pbroadcast and psum2.",
      "Device variance is encoded in types like x:f32[3,4]{i}, indicating variance along mesh axis i.",
      "Typing rules are updated to handle device variance, including rules for first-order primitives, higher-order primitives, and collectives.",
      "New primitives are introduced: pbroadcast, all_gather_invariant, and pscatter.",
      "Pbroadcast lowers to a no-op, while all_gather_invariant lowers to the same thing as all_gather but has a different device variance type.",
      "The psum and pbroadcast transpose pair corresponds to the psum_idrev and id_psumrev used in LLM training with pmap.",
      "Automatic pbroadcast insertion is proposed to avoid users having to write them explicitly."
    ],
    "code_examples": [
      {
        "description": "Example 1 with intermediate device variance types annotated",
        "code": "@partial(\n    shmap,\n    in_specs=P('i'),\n    out_specs=P()\n)\ndef f1(x: f32[3, 4]{i}):\n  w: f32[]{i} = g(x)\n  y: f32[]{} = psum(w, 'i')\n  return y"
      },
      {
        "description": "Example 1 transpose using device variance types (go ahead and transpose this again!)",
        "code": "t(f1) = shmap(\n    lambda ybar: t(g)(pbroadcast(ybar, 'i')),\n    in_specs=P(),\n    out_specs=P('i')\n)"
      },
      {
        "description": "Example 1 transpose with intermediate device variance types annotated",
        "code": "@partial(\n    shmap,\n    in_specs=P('i'),\n    out_specs=P()\n)\ndef f1_transpose(ybar: f32[]):\n  wbar: f32[]{i} = pbroadcast(ybar, 'i')\n  xbar: f32[3, 4]{i} = transpose(g)(wbar)\n  return xbar"
      },
      {
        "description": "Example 2 rewritten with explicit pbroadcast",
        "code": "f2 = shmap(\n    lambda x, y: pbroadcast(psum(g(x), 'i'), 'i') * y,\n    in_specs=(P('i'), P('i')),\n    out_specs=P('i')\n)"
      },
      {
        "description": "Example 2 transpose using device variance types",
        "code": "t(f2, 0) = shmap(\n    lambda y, zbar: t(g)(pbroadcast(psum(zbar * y, 'i'), 'i')),\n    in_specs=(P('i'), P('i')),\n    out_specs=P('i')\n)"
      },
      {
        "description": "Example 3 again",
        "code": "cursed_identity = shmap(\n    lambda x: x,\n    P(),\n    P()\n)\n\n# Notice here the body is `f32[...] -> f32[...]`, i.e. no device varying type.\n# Example 3 transpose using device variance types\nt(cursed_identity) = shmap(\n    lambda x: x,\n    P(),\n    P()\n)\nt(t(cursed_identity)) = shmap(\n    lambda x: x,\n    P(),\n    P()\n)"
      },
      {
        "description": "Example 4 rewritten with explicit all_reduce_invariant",
        "code": "f4 = shmap(\n    lambda x: all_gather_invariant(x, 'i'),\n    P('i'),\n    P()\n)"
      },
      {
        "description": "Example 4 with intermediate device variance types annotated",
        "code": "@partial(\n    shmap,\n    P('i'),\n    P()\n)\ndef f4(x: f32[1]{i}):\n  y: f32[8]{} = all_gather_invariant(x, 'i')\n  return y"
      },
      {
        "description": "Example 4 transpose with intermediate device variance types annotated",
        "code": "@partial(\n    shmap,\n    in_specs=P(),\n    out_specs=P('i')\n)\ndef f4_transpose(ybar: f32[8]):\n  xbar: f32[1]{i} = pscatter(ybar, 'i')\n  return xbar"
      },
      {
        "description": "Example 5 with intermediate device variance types annotated",
        "code": "@partial(\n    shmap,\n    in_specs=(P('i'), P('i')),\n    out_specs=P('i')\n)\ndef f5(x: f32[1]{i}, y: f32[8]{i}):\n  z: f32[8]{i} = all_gather(x, 'i')\n  w: f32[8]{i} = z * y\n  return w"
      },
      {
        "description": "Transpose with respect to first argument",
        "code": "@partial(\n    shmap,\n    in_specs=(P('i'), P('i')),\n    out_specs=P('i')\n)\ndef f5_transpose(y: f32[8]{i}, wbar: f32[8]{i}):\n  zbar: f32[8]{i} = wbar * y\n  xbar: f32[1]{i} = psum_scatter(zbar, 'i')\n  return xbar"
      }
    ]
  },
  {
    "title": "Implementation Details and Unmapped Inputs/Outputs",
    "concepts": [
      "The implementation will keep the extended types as metadata internal to shmap, similar to the existing replication checking machinery.",
      "PBroadcast insertion can be performed just before transposition or in every shmap body.",
      "The discussion focuses on shmap, but the ideas apply to pmap and xmap.",
      "Unmapped inputs and outputs are crucial for expressiveness, matching pjit capabilities.",
      "Unmapped inputs and outputs are essential for closed-over inputs and closure under transposition."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction and Motivation",
    "concepts": [
      "jax.numpy and jax.scipy have an ill-defined scope.",
      "A well-defined scope is needed to guide future contributions.",
      "A well-defined scope is needed to motivate the removal of out-of-scope code.",
      "JAX aims to provide a NumPy-like API for executing code in XLA.",
      "Some parts of numpy and scipy are out-of-scope for JAX.",
      "Including code in JAX incurs a maintenance burden.",
      "Project success depends on maintainability of all parts.",
      "Maintainers must carefully weigh the benefits of contributions against their goals and resources."
    ],
    "code_examples": []
  },
  {
    "title": "Rubric for API Inclusion in JAX",
    "concepts": [
      "A rubric of six axes is proposed to judge the scope of numpy or scipy APIs for inclusion in JAX.",
      "Strong alignment across all axes is ideal for inclusion.",
      "Significant weakness along any axis is a reason against inclusion."
    ],
    "code_examples": []
  },
  {
    "title": "Axis 1: XLA Alignment and Pure Function Semantics",
    "concepts": [
      "The first axis is the degree to which the proposed API aligns with native XLA operations.",
      "Functions that directly mirror XLA operations (e.g., jax.numpy.exp()) are preferred.",
      "Functions like numpy.unique(), which do not correspond to XLA operations and return value-dependent dynamic array shapes, are disfavored.",
      "The need for pure function semantics is also considered; state-based RNGs like numpy.random are incompatible with JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Axis 2: Python Array API Standard",
    "concepts": [
      "The second axis focuses on the Python Array API Standard.",
      "APIs listed in the Array API Standard are strong candidates for inclusion.",
      "Even if an API isn't aligned with XLA, its presence in the Array API standard suggests it's important to the Python user community.",
      "numpy.unique and its variants are included in the Array API standard."
    ],
    "code_examples": []
  },
  {
    "title": "Axis 3: Availability in Downstream Packages",
    "concepts": [
      "For functionality not aligned with Axes 1 or 2, consider if well-supported downstream packages provide it.",
      "scipy.optimize functionality is largely covered by JAXopt, so JAX itself doesn't need to fully re-implement it.",
      "Users and contributors should be directed to these specialized packages."
    ],
    "code_examples": []
  },
  {
    "title": "Axis 4: Implementation Complexity",
    "concepts": [
      "The complexity of the proposed implementation is an important consideration.",
      "Complex implementations can be difficult to validate and maintain.",
      "jax.scipy.special.bessel_jn() is an example of a complex implementation with convergence issues."
    ],
    "code_examples": []
  },
  {
    "title": "Axis 5: Functional vs. Object-Oriented APIs",
    "concepts": [
      "JAX works best with functional APIs.",
      "Object-oriented APIs can hide impure semantics and are often difficult to implement well.",
      "When both functional and object-oriented APIs exist, JAX should prefer the functional APIs.",
      "numpy.polynomial.Polynomial is an example of an object-oriented API."
    ],
    "code_examples": []
  },
  {
    "title": "Axis 6: Importance to the User Community",
    "concepts": [
      "The importance of the algorithm to the general user community should be considered.",
      "Quantifying importance is difficult but necessary.",
      "Existing API usage on GitHub can be used to establish importance.",
      "jax.scipy.special.bessel_jn() has few uses on GitHub, potentially due to accuracy issues."
    ],
    "code_examples": []
  },
  {
    "title": "NumPy API Evaluation",
    "concepts": [
      "The main numpy namespace is generally in-scope for JAX due to its alignment with XLA, the Array API, and its importance to users.",
      "Some functions, like numpy.intersect1d(), are borderline but are included for simplicity.",
      "numpy.linalg and numpy.fft are in-scope, even with complicated device-specific lowerings, due to their importance to stakeholders.",
      "numpy.random is out-of-scope because state-based RNGs are incompatible with JAX; jax.random is the alternative.",
      "numpy.ma and numpy.polynomial are out-of-scope because they are mostly object-oriented interfaces.",
      "NumPy's testing functionality is out-of-scope, but JAX arrays are compatible with numpy.testing."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: Overview",
    "concepts": [
      "SciPy has no functions in the top-level namespace, but includes a number of submodules.",
      "The following evaluations leave out modules which have been deprecated."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.cluster",
    "concepts": [
      "scipy.cluster (hierarchical clustering, k-means) is weak along several axes and better served by a downstream package.",
      "jax.scipy.cluster.vq.vq() has no obvious usage on github.",
      "Recommendation: deprecate and remove jax.scipy.cluster.vq()."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.constants and scipy.datasets",
    "concepts": [
      "scipy.constants (mathematical and physical constants) can be used directly with JAX, so no re-implementation is needed.",
      "scipy.datasets (tools to fetch and load datasets) can be used directly with JAX, so no re-implementation is needed."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.fft",
    "concepts": [
      "scipy.fft aligns with XLA functionality and fares well along other axes.",
      "It is in-scope for JAX."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.integrate",
    "concepts": [
      "scipy.integrate (numerical integration) is largely out-of-scope for JAX (axes 1 & 4) due to loopy algorithms and dynamic evaluations.",
      "jax.experimental.ode.odeint() is related but limited and not actively developed.",
      "jax.scipy.integrate.trapezoid() could be replaced with a one-line jax.numpy expression.",
      "Recommendation: remove jax.scipy.integrate.trapezoid() .",
      "Based on Axes 1, 2, 4, and 6, scipy.integrate should be considered out-of-scope for JAX."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.interpolate",
    "concepts": [
      "scipy.interpolate (interpolation routines) rates poorly along a number of axes.",
      "These APIs are class-based rather than low-level.",
      "Only the simplest methods can be efficiently expressed in terms of XLA operations.",
      "JAX currently has wrappers for scipy.interpolate.RegularGridInterpolator.",
      "Recommendation: other members of scipy.interpolate should be considered out-of-scope for JAX."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.io",
    "concepts": [
      "scipy.io (file input/output) is out-of-scope for JAX; no re-implementation is needed."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.linalg",
    "concepts": [
      "scipy.linalg (linear algebra) aligns with XLA functionality and is important to the JAX user community.",
      "It is in-scope for JAX."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.ndimage",
    "concepts": [
      "scipy.ndimage (image processing) has overlaps with scipy.signal.",
      "JAX provides jax.scipy.ndimage.map_coordinates().",
      "jax.image and dm-pix (deepmind ecosystem) provide more tools for image manipulation.",
      "Recommendation: scipy.ndimage should be considered out-of-scope for JAX core; consider moving map_coordinates to dm-pix or another package."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.odr",
    "concepts": [
      "scipy.odr (orthogonal distance regression) is object-oriented and not easily expressed using JAX primitives.",
      "It is out of scope for JAX itself."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.optimize",
    "concepts": [
      "scipy.optimize (optimization) is important to JAX users, and JAX created wrappers early on.",
      "The scipy.optimize API was too constraining.",
      "JAXopt and Optimistix provide more comprehensive optimization routines in JAX.",
      "scipy.optimize is out-of-scope for JAX.",
      "Recommendation: deprecate jax.scipy.optimize and/or make it a lightweight wrapper around an optional JAXopt or Optimistix dependency."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.signal",
    "concepts": [
      "scipy.signal (signal processing) is mixed.",
      "Functions like correlate and convolve are in-scope.",
      "Domain-specific tools with no XLA lowering path are out-of-scope.",
      "Potential contributions to jax.scipy.signal will have to be weighed on a case-by-case basis."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.sparse",
    "concepts": [
      "scipy.sparse (sparse matrices) contains data structures and solvers.",
      "Data structures are out-of-scope for JAX because they don't align with JAX's computational model.",
      "jax.experimental.sparse provides alternative data structures.",
      "scipy.sparse.linalg solvers (bicgstab, cg, gmres) are useful but don't fare well along other axes.",
      "Recommendation: explore moving sparse solvers into Lineax, and otherwise treat scipy.sparse as out-of-scope for JAX."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.spatial",
    "concepts": [
      "scipy.spatial (spatial computations) contains mainly object-oriented interfaces and is mostly out-of-scope for JAX.",
      "scipy.spatial.transform provides tools for manipulating three-dimensional spatial rotations.",
      "JAX contains partial implementations of Rotation and Slerp within jax.scipy.spatial.transform.",
      "These are object-oriented wrappers with a large API surface and few users, therefore out-of-scope for JAX.",
      "scipy.spatial.distance contains distance metrics.",
      "Recommendation: consider deprecating and removing the Rotation and Slerp APIs, and consider scipy.spatial as a whole out-of-scope for future contributions."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.special",
    "concepts": [
      "scipy.special (special functions) includes functions that are often squarely in-scope (e.g., gammaln, betainc, digamma) because they correspond to XLA primitives.",
      "Other functions require more complicated implementations (e.g., bessel_jn).",
      "Functions in scipy.special tend to be very strong along Axis 6: they provide fundamental functions necessary for computation in a variety of domains.",
      "Existing function wrappers that may need review: jax.scipy.special.lpmn(), jax.scipy.special.lpmn_values(), jax.scipy.special.sph_harm(), jax.scipy.special.bessel_jn().",
      "Recommendation: refactor and improve robustness & test coverage for bessel_jn . Consider deprecating lpmn , lpmn_values , and sph_harm if they cannot be modified to more closely match the scipy APIs."
    ],
    "code_examples": []
  },
  {
    "title": "SciPy API Evaluation: scipy.stats",
    "concepts": [
      "scipy.stats (statistical functions) contains distributions, summary statistics, and hypothesis testing.",
      "JAX wraps a number of these in jax.scipy.stats, including statistical distributions and some other functions.",
      "These are generally well-aligned with JAX because they are expressible in terms of efficient XLA operations.",
      "JAX doesn't currently wrap hypothesis testing functions, probably because these are less useful to the primary user-base of JAX.",
      "tensorflow_probability provides similar functionality for distributions; deprecating scipy.stats distributions could be considered in the future.",
      "Recommendation: going forward, we should treat statistical distributions and summary statistics as in-scope, and consider hypothesis tests and related functionality generally out-of-scope."
    ],
    "code_examples": []
  },
  {
    "title": "Effort-Based Versioning (EffVer) Proposal",
    "concepts": [
      "JAX core library should adopt Effort-based versioning (EffVer).",
      "EffVer is a three-number versioning system (MACRO.MESO.MICRO) indicating the effort required to adapt to changes.",
      "Micro version increments signal little to no effort needed for adaptation.",
      "Meso version increments signal some small effort will be required.",
      "Macro version increments signal significant effort may be required.",
      "EffVer captures the essence of semantic versioning but avoids hard-to-meet compatibility guarantees.",
      "EffVer gives special meaning to the Zero version (0.X.Y).",
      "Bumping from 0.X.Y to 1.0.0 is recommended when a certain level of stability has been reached.",
      "EffVer concisely communicates the intent of a change.",
      "EffVer correctly describes JAX\u2019s release strategy prior to this proposal.",
      "EffVer provides a concrete recommendation for how to think about JAX 1.0."
    ],
    "code_examples": []
  },
  {
    "title": "EffVer Examples",
    "concepts": [
      "Software with current version 2.3.4",
      "Increasing the micro version (i.e. releasing 2.3.5) signals to users that little to no effort is necessary on their part to adapt to the changes.",
      "Increasing the meso version (i.e. releasing 2.4.0) signals to users that some small effort will be required for existing code to work with the changes.",
      "Increasing the macro version (i.e. releasing 3.0.0) signals to users that significant effort may be required to update to the changes."
    ],
    "code_examples": []
  },
  {
    "title": "Alternatives Considered: Non-Semantic Versioning",
    "concepts": [
      "JAX's current versioning uses three numbers with no formal semantic meaning beyond simple orderability.",
      "In practice, JAX\u2019s version numbers up until now have been semantically quite similar to the EffVer zero-version case.",
      "One option would be to explicitly formalize this non-semantic versioning."
    ],
    "code_examples": []
  },
  {
    "title": "Alternatives Considered: Semantic Versioning (SemVer)",
    "concepts": [
      "SemVer encodes versions with three numbers: MAJOR.MINOR.MICRO.",
      "Micro version increments indicate bug fixes.",
      "Minor version increments indicate bug fixes and new features.",
      "Major version increments indicate bug fixes, new features, and breaking changes.",
      "SemVer makes no special accommodation for zero version.",
      "JAX\u2019s existing releases violate the guarantees of SemVer because JAX has generally used the micro version for feature releases, and the minor version for significant backward-incompatible changes."
    ],
    "code_examples": []
  },
  {
    "title": "SemVer Examples",
    "concepts": [
      "Software with current version 2.3.4",
      "Increasing the micro version (i.e. releasing 2.3.5) indicates the release includes only bug fixes.",
      "Increasing the minor version (i.e. releasing 2.4.0) indicates the release includes bug fixes as well as new features.",
      "Increasing the major version (i.e. releasing 3.0.0) indicates the release includes bug fixes, new features, as well as breaking changes."
    ],
    "code_examples": []
  },
  {
    "title": "Alternatives Considered: Calendar-Based Versioning (CalVer)",
    "concepts": [
      "CalVer is typically represented by three numbers YEAR.MONTH.DAY.",
      "These numbers contain no semantic meaning regarding the included changes, but rather encode the calendar data on which the software was released.",
      "CalVer immediately communicates the timestamp of the particular release.",
      "CalVer version numbers do not provide any signal to users regarding the degree of severity of the changes it includes."
    ],
    "code_examples": []
  },
  {
    "title": "CalVer Example",
    "concepts": [
      "The 2024.12.16 release indicates that it reflects the state of the main branch on December 16, 2024."
    ],
    "code_examples": []
  },
  {
    "title": "Overview",
    "concepts": [
      "JAX can be extended to add new functionalities.",
      "JAX is used for building libraries and interfaces."
    ],
    "code_examples": []
  },
  {
    "title": "Extensible JAX internals",
    "concepts": [
      "JAX internals can be extended."
    ],
    "code_examples": []
  },
  {
    "title": "Libraries and extensions",
    "concepts": [
      "Libraries can be built on top of JAX.",
      "JAX can be extended with extensions."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Function Transformations in JAX",
    "concepts": [
      "JAX offers composable function transformations like jit, grad, and vmap.",
      "These transformations enable writing concise and accelerated code.",
      "Custom function transformations can be added to the system by writing a custom Jaxpr interpreter.",
      "Composable transformations can be achieved for free."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of the `jit` transformation to accelerate a function `f`.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\n\nx = random.normal(random.key(0), (5000, 5000))\n\ndef f(w, b, x):\n  return jnp.tanh(jnp.dot(x, w) + b)\n\nfast_f = jit(f)\n\nx = random.normal(random.key(0), (5000, 5000))\n\ndef f(w, b, x):\n  return jnp.tanh(jnp.dot(x, w) + b)\n\nfast_f = jit(f)"
      }
    ]
  },
  {
    "title": "Understanding Jaxprs",
    "concepts": [
      "When a JAX transformed function is called, JAX traces the function and constructs an XLA computation graph.",
      "Jaxpr is a data structure that can be evaluated like a mini functional programming language.",
      "Jaxprs are a useful intermediate representation for function transformation.",
      "The `make_jaxpr` transformation transforms a function into one that produces a Jaxpr representation of its computation, useful for debugging and introspection."
    ],
    "code_examples": [
      {
        "description": "Illustrates how to use `jax.make_jaxpr` to inspect the Jaxpr representation of simple functions `foo` and `bar`.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\n\ndef examine_jaxpr(closed_jaxpr):\n  jaxpr = closed_jaxpr.jaxpr\n  print(\"invars:\", jaxpr.invars)\n  print(\"outvars:\", jaxpr.outvars)\n  print(\"constvars:\", jaxpr.constvars)\n  for eqn in jaxpr.eqns:\n    print(\"equation:\", eqn.invars, eqn.primitive, eqn.outvars, eqn.params)\n  print()\n  print(\"jaxpr:\", jaxpr)\n\ndef foo(x):\n  return x + 1\n\nprint(\"foo\")\nprint(\"=====\")\nexamine_jaxpr(jax.make_jaxpr(foo)(5))\nprint()\n\ndef bar(w, b, x):\n  return jnp.dot(w, x) + b + jnp.ones(5), x\n\nprint(\"bar\")\nprint(\"=====\")\nexamine_jaxpr(jax.make_jaxpr(bar)(jnp.ones((5, 10)), jnp.ones(5), jnp.ones(10)))"
      }
    ]
  },
  {
    "title": "Jaxpr Components",
    "concepts": [
      "jaxpr.invars are the input variables to the Jaxpr, analogous to arguments in Python functions.",
      "jaxpr.outvars are the variables that are returned by the Jaxpr. Every Jaxpr has multiple outputs.",
      "jaxpr.constvars are variables that are also inputs to the Jaxpr, but correspond to constants from the trace.",
      "jaxpr.eqns is a list of equations, which are essentially let-bindings. Each equation is a list of input variables, a list of output variables, and a primitive which is used to evaluate inputs to produce outputs. Each equation also has a params, a dictionary of parameters."
    ],
    "code_examples": []
  },
  {
    "title": "Implementing a Function Inverter",
    "concepts": [
      "Goal is to implement a function inverter that takes the output of a function and returns the input that produced it.",
      "The implementation involves tracing the function into a Jaxpr and interpreting the Jaxpr backwards.",
      "While interpreting the Jaxpr backwards, the primitive's inverse is looked up in a table and applied."
    ],
    "code_examples": [
      {
        "description": "Example of the desired functionality of the inverse function.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\n\ndef f(x):\n  return jnp.exp(jnp.tanh(x))\n\nf_inv = inverse(f)\nassert jnp.allclose(f_inv(f(1.0)), 1.0)"
      }
    ]
  },
  {
    "title": "Tracing a Function into a Jaxpr",
    "concepts": [
      "jax.make_jaxpr returns a closed Jaxpr, which is a Jaxpr that has been bundled with the constants (literals) from the trace.",
      "Demonstrates how to trace a function into a Jaxpr using jax.make_jaxpr.",
      "Shows how to access the jaxpr and literals attributes of the closed Jaxpr."
    ],
    "code_examples": [
      {
        "description": "Shows how to trace a function into a Jaxpr using `jax.make_jaxpr` and access the `jaxpr` and `literals` attributes.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\nfrom\tfunctools\nimport\nwraps\nfrom\njax\nimport\ncore\nfrom\njax\nimport\nlax\nfrom\njax._src.util\nimport\nsafe_map\n\ndef f(x):\n  return jnp.exp(jnp.tanh(x))\n\nclosed_jaxpr = jax.make_jaxpr(f)(jnp.ones(5))\nprint(closed_jaxpr.jaxpr)\nprint(closed_jaxpr.literals)"
      }
    ]
  },
  {
    "title": "Implementing the Default Jaxpr Interpreter (eval_jaxpr)",
    "concepts": [
      "The eval_jaxpr function evaluates the Jaxpr as-is, computing the same values as the original, un-transformed Python function would.",
      "It uses an environment to store the values for each variable.",
      "It evaluates primitives using the `bind` method."
    ],
    "code_examples": [
      {
        "description": "Implementation of the `eval_jaxpr` function that evaluates a Jaxpr.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\nfrom\tfunctools\nimport\nwraps\nfrom\njax\nimport\ncore\nfrom\njax\nimport\nlax\nfrom\njax._src.util\nimport\nsafe_map\n\ndef eval_jaxpr(jaxpr, consts, *args):\n  # Mapping from variable -> value\n  env = {}\n\n  def read(var):\n    # Literals are values baked into the Jaxpr\n    if type(var) is core.Literal:\n      return var.val\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n\n  # Bind args and consts to environment\n  safe_map(write, jaxpr.invars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Loop through equations and evaluate primitives using `bind`\n  for eqn in jaxpr.eqns:\n    # Read inputs to equation from environment\n    invals = safe_map(read, eqn.invars)\n    # `bind` is how a primitive is called\n    outvals = eqn.primitive.bind(*invals, **eqn.params)\n    # Primitives may return multiple outputs or not\n    if not eqn.primitive.multiple_results:\n      outvals = [outvals]\n    # Write the results of the primitive into the environment\n    safe_map(write, eqn.outvars, outvals)\n\n  # Read the final result of the Jaxpr from the environment\n  return safe_map(read, jaxpr.outvars)"
      },
      {
        "description": "Demonstrates the usage of `eval_jaxpr` with a closed Jaxpr.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\nfrom\tfunctools\nimport\nwraps\nfrom\njax\nimport\ncore\nfrom\njax\nimport\nlax\nfrom\njax._src.util\nimport\nsafe_map\n\ndef eval_jaxpr(jaxpr, consts, *args):\n  # Mapping from variable -> value\n  env = {}\n\n  def read(var):\n    # Literals are values baked into the Jaxpr\n    if type(var) is core.Literal:\n      return var.val\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n\n  # Bind args and consts to environment\n  safe_map(write, jaxpr.invars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Loop through equations and evaluate primitives using `bind`\n  for eqn in jaxpr.eqns:\n    # Read inputs to equation from environment\n    invals = safe_map(read, eqn.invars)\n    # `bind` is how a primitive is called\n    outvals = eqn.primitive.bind(*invals, **eqn.params)\n    # Primitives may return multiple outputs or not\n    if not eqn.primitive.multiple_results:\n      outvals = [outvals]\n    # Write the results of the primitive into the environment\n    safe_map(write, eqn.outvars, outvals)\n\n  # Read the final result of the Jaxpr from the environment\n  return safe_map(read, jaxpr.outvars)\n\ndef f(x):\n  return jnp.exp(jnp.tanh(x))\n\nclosed_jaxpr = jax.make_jaxpr(f)(jnp.ones(5))\neval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.literals, jnp.ones(5))"
      }
    ]
  },
  {
    "title": "Implementing the Inverse Jaxpr Interpreter",
    "concepts": [
      "An inverse interpreter is implemented by setting up a registry mapping primitives to their inverses.",
      "The interpreter walks through the Jaxpr backward and inverts primitives when it can.",
      "This interpreter is similar to the transpose interpreter used in reverse-mode autodifferentiation.",
      "The inverse interpreter traces the function, then custom-interprets the Jaxpr."
    ],
    "code_examples": [
      {
        "description": "Registers inverses for the `lax.exp_p` and `lax.tanh_p` primitives.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\nfrom\tfunctools\nimport\nwraps\nfrom\njax\nimport\ncore\nfrom\njax\nimport\nlax\nfrom\njax._src.util\nimport\nsafe_map\n\ninverse_registry = {}\ninverse_registry[lax.exp_p] = jnp.log\ninverse_registry[lax.tanh_p] = jnp.arctanh"
      },
      {
        "description": "Sets up the skeleton for the `inverse` function, which traces the function and then calls `inverse_jaxpr`.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\nfrom\tfunctools\nimport\nwraps\nfrom\njax\nimport\ncore\nfrom\njax\nimport\nlax\nfrom\njax._src.util\nimport\nsafe_map\n\ndef inverse(fun):\n  @wraps(fun)\n  def wrapped(*args, **kwargs):\n    # Since we assume unary functions, we won't worry about flattening and\n    # unflattening arguments.\n    closed_jaxpr = jax.make_jaxpr(fun)(*args, **kwargs)\n    out = inverse_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.literals, *args)\n    return out[0]\n\n  return wrapped"
      },
      {
        "description": "Defines the `inverse_jaxpr` function, which walks through the Jaxpr backward and inverts primitives.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\nfrom\tfunctools\nimport\nwraps\nfrom\njax\nimport\ncore\nfrom\njax\nimport\nlax\nfrom\njax._src.util\nimport\nsafe_map\n\ndef inverse_jaxpr(jaxpr, consts, *args):\n  env = {}\n\n  def read(var):\n    if type(var) is core.Literal:\n      return var.val\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n\n  # Args now correspond to Jaxpr outvars\n  safe_map(write, jaxpr.outvars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Looping backward\n  for eqn in jaxpr.eqns[::-1]:\n    #  outvars are now invars\n    invals = safe_map(read, eqn.outvars)\n    if eqn.primitive not in inverse_registry:\n      raise NotImplementedError(f\"{eqn.primitive} does not have registered inverse.\")\n    # Assuming a unary function\n    outval = inverse_registry[eqn.primitive](*invals)\n    safe_map(write, eqn.invars, [outval])\n  return safe_map(read, jaxpr.invars)"
      },
      {
        "description": "Shows the final usage of the inverse function and confirms that it works.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\nfrom\tfunctools\nimport\nwraps\nfrom\njax\nimport\ncore\nfrom\njax\nimport\nlax\nfrom\njax._src.util\nimport\nsafe_map\n\ndef inverse(fun):\n  @wraps(fun)\n  def wrapped(*args, **kwargs):\n    # Since we assume unary functions, we won't worry about flattening and\n    # unflattening arguments.\n    closed_jaxpr = jax.make_jaxpr(fun)(*args, **kwargs)\n    out = inverse_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.literals, *args)\n    return out[0]\n\n  return wrapped\n\ndef inverse_jaxpr(jaxpr, consts, *args):\n  env = {}\n\n  def read(var):\n    if type(var) is core.Literal:\n      return var.val\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n\n  # Args now correspond to Jaxpr outvars\n  safe_map(write, jaxpr.outvars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Looping backward\n  for eqn in jaxpr.eqns[::-1]:\n    #  outvars are now invars\n    invals = safe_map(read, eqn.outvars)\n    if eqn.primitive not in inverse_registry:\n      raise NotImplementedError(f\"{eqn.primitive} does not have registered inverse.\")\n    # Assuming a unary function\n    outval = inverse_registry[eqn.primitive](*invals)\n    safe_map(write, eqn.invars, [outval])\n  return safe_map(read, jaxpr.invars)\n\ninverse_registry = {}\ninverse_registry[lax.exp_p] = jnp.log\ninverse_registry[lax.tanh_p] = jnp.arctanh\n\ndef f(x):\n  return jnp.exp(jnp.tanh(x))\n\nf_inv = inverse(f)\nassert jnp.allclose(f_inv(f(1.0)), 1.0)"
      }
    ]
  },
  {
    "title": "Composition with other JAX transformations",
    "concepts": [
      "The custom transformation is composable with other JAX transformations like jit, vmap, and grad.",
      "Demonstrates the composition of jit, vmap, and grad with the inverse transformation.",
      "Tracing through a Jaxpr interpreter is possible."
    ],
    "code_examples": [
      {
        "description": "Illustrates the composition of jit, vmap, and grad with the inverse transformation.",
        "code": "import\njax\nimport\njax.numpy\nas\njnp\nfrom\njax\nimport\njit\n,\ngrad\n,\nvmap\nfrom\njax\nimport\nrandom\nfrom\tfunctools\nimport\nwraps\nfrom\njax\nimport\ncore\nfrom\njax\nimport\nlax\nfrom\njax._src.util\nimport\nsafe_map\n\ndef inverse(fun):\n  @wraps(fun)\n  def wrapped(*args, **kwargs):\n    # Since we assume unary functions, we won't worry about flattening and\n    # unflattening arguments.\n    closed_jaxpr = jax.make_jaxpr(fun)(*args, **kwargs)\n    out = inverse_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.literals, *args)\n    return out[0]\n\n  return wrapped\n\ndef inverse_jaxpr(jaxpr, consts, *args):\n  env = {}\n\n  def read(var):\n    if type(var) is core.Literal:\n      return var.val\n    return env[var]\n\n  def write(var, val):\n    env[var] = val\n\n  # Args now correspond to Jaxpr outvars\n  safe_map(write, jaxpr.outvars, args)\n  safe_map(write, jaxpr.constvars, consts)\n\n  # Looping backward\n  for eqn in jaxpr.eqns[::-1]:\n    #  outvars are now invars\n    invals = safe_map(read, eqn.outvars)\n    if eqn.primitive not in inverse_registry:\n      raise NotImplementedError(f\"{eqn.primitive} does not have registered inverse.\")\n    # Assuming a unary function\n    outval = inverse_registry[eqn.primitive](*invals)\n    safe_map(write, eqn.invars, [outval])\n  return safe_map(read, jaxpr.invars)\n\ninverse_registry = {}\ninverse_registry[lax.exp_p] = jnp.log\ninverse_registry[lax.tanh_p] = jnp.arctanh\n\ndef f(x):\n  return jnp.exp(jnp.tanh(x))\n\n\njit(vmap(grad(inverse(f))))((jnp.arange(5) + 1.) / 5.)"
      }
    ]
  },
  {
    "title": "Limitations and Future Improvements",
    "concepts": [
      "The current implementation has limitations.",
      "It needs to handle primitives with multiple arguments where inputs are partially known.",
      "It needs to handle xla_call and xla_pmap primitives."
    ],
    "code_examples": []
  },
  {
    "title": "Core JAX Data Structures",
    "concepts": [
      "ClosedJaxpr represents a JAX program with closed variables and constants.",
      "Jaxpr defines the core structure of a JAX program, containing input, output, and equations.",
      "JaxprEqn represents an equation within a Jaxpr, specifying inputs, outputs, the primitive used, and parameters.",
      "Literal represents a constant value within a JAX program.",
      "Primitive represents a built-in JAX operation.",
      "Token represents a dependency used for sequencing operations.",
      "Var represents a variable within a JAX program."
    ],
    "code_examples": []
  },
  {
    "title": "Utility Functions and Objects",
    "concepts": [
      "array_types represents a set of array types.",
      "set() creates a new empty set object.",
      "set(iterable) creates a new set object from an iterable.",
      "jaxpr_as_fun converts a Jaxpr into a Python function.",
      "primitives refers to a collection of primitive operations."
    ],
    "code_examples": []
  },
  {
    "title": "Jaxpr Class Overview",
    "concepts": [
      "The Jaxpr class represents a JAX program in an intermediate representation.",
      "It has attributes for constants, equations, input and output abstract values, and effects.",
      "It provides methods for initialization, mapping functions over the Jaxpr, pretty printing, and replacing components."
    ],
    "code_examples": []
  },
  {
    "title": "Jaxpr Class Methods",
    "concepts": [
      "__init__(jaxpr, consts): Initializes a Jaxpr object with a Jaxpr and constants.",
      "map_jaxpr(f): Applies a function to each element of the Jaxpr.",
      "pretty_print(*[, source_info, print_shapes, ...]): Returns a human-readable representation of the Jaxpr.",
      "replace(*[, jaxpr, consts]): Returns a new Jaxpr with specified components replaced."
    ],
    "code_examples": []
  },
  {
    "title": "Jaxpr Class Attributes",
    "concepts": [
      "consts: Stores the constants used in the Jaxpr.",
      "effects: Represents side effects in the jaxpr.",
      "eqns: Represents the equations in the jaxpr.",
      "in_avals: Stores the abstract values of the inputs to the Jaxpr.",
      "jaxpr: The core jaxpr structure.",
      "literals: Stores the literal values used in the Jaxpr.",
      "out_avals: Stores the abstract values of the outputs of the Jaxpr."
    ],
    "code_examples": []
  },
  {
    "title": "Jaxpr Structure",
    "concepts": [
      "Jaxpr consists of constvars, invars, outvars, eqns, effects, and debug_info.",
      "constvars are variables introduced for constants.",
      "invars are the input variables to the Jaxpr.",
      "outvars are the output atoms.",
      "eqns are the list of equations.",
      "effects are the set of effects.",
      "debug_info is debugging information."
    ],
    "code_examples": []
  },
  {
    "title": "Jaxpr Attributes",
    "concepts": [
      "constvars attribute represents variables introduced for constants.",
      "debug_info attribute holds debugging information.",
      "effects attribute stores the set of effects.",
      "eqns attribute contains the list of equations.",
      "invars attribute represents the input variables.",
      "outvars attribute represents the output atoms."
    ],
    "code_examples": []
  },
  {
    "title": "Equation Object Properties",
    "concepts": [
      "The equation object has several properties related to input variables.",
      "The equation object has several properties related to output variables.",
      "The equation object is associated with a primitive operation.",
      "The equation object includes parameters for the primitive operation.",
      "The equation object includes side effects that the operation may have.",
      "The equation object stores source code location information.",
      "The equation object is associated with a context during Jaxpr execution."
    ],
    "code_examples": []
  },
  {
    "title": "Equation Object Attributes",
    "concepts": [
      "invars: List of Atom objects representing input variables.",
      "outvars: List of Var objects representing output variables.",
      "primitive: The Primitive operation performed by the equation.",
      "params: Dictionary of parameters for the primitive.",
      "effects: Side effects of the equation's operation.",
      "source_info: Source code information about the equation.",
      "ctx: JaxprEqnContext associated with the equation."
    ],
    "code_examples": []
  },
  {
    "title": "Equation Object Methods",
    "concepts": [
      "__init__: Constructor to create a new equation object.",
      "replace: Method to create a new equation with replaced attributes."
    ],
    "code_examples": []
  },
  {
    "title": "Class val",
    "concepts": [
      "The class `val` takes `val` of type Any and `aval` of type AbstractValue as input."
    ],
    "code_examples": []
  },
  {
    "title": "Methods of Class val",
    "concepts": [
      "The class `val` has an `__init__` method that initializes the object with `val` and `aval`."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes of Class val",
    "concepts": [
      "The class `val` has attributes `val` and `aval`.",
      "The class `val` has a hash attribute."
    ],
    "code_examples": []
  },
  {
    "title": "Class Definition and Methods",
    "concepts": [
      "Defines a class with a name attribute.",
      "Includes methods for initialization, abstract evaluation, binding, and implementation.",
      "Provides decorators for defining abstract evaluation and binding with trace.",
      "Offers functionality for effectful abstract evaluation and retrieving binding parameters."
    ],
    "code_examples": [
      {
        "description": "Illustrates the structure of the __init__ method, taking 'name' as a parameter.",
        "code": "__init__ (name)"
      },
      {
        "description": "Shows the structure of the abstract_eval method, taking variable arguments and keyword parameters.",
        "code": "abstract_eval (*args,\u00a0**params)"
      },
      {
        "description": "Demonstrates the bind method with variable arguments and keyword parameters.",
        "code": "bind (*args,\u00a0**params)"
      },
      {
        "description": "Illustrates the bind_with_trace method, taking trace, args, and params as input.",
        "code": "bind_with_trace (trace,\u00a0args,\u00a0params)"
      },
      {
        "description": "Shows the def_abstract_eval decorator, taking abstract_eval as a parameter.",
        "code": "def_abstract_eval (abstract_eval)"
      },
      {
        "description": "Demonstrates the def_bind_with_trace decorator, taking bind_with_trace as a parameter.",
        "code": "def_bind_with_trace (bind_with_trace)"
      },
      {
        "description": "Illustrates the impl method, taking variable arguments and keyword parameters.",
        "code": "impl (*args,\u00a0**params)"
      },
      {
        "description": "Shows the get_bind_params method, taking params as input.",
        "code": "get_bind_params (params)"
      }
    ]
  },
  {
    "title": "Attributes",
    "concepts": [
      "Defines various attributes related to primitive operations.",
      "Includes attributes such as call_primitive, map_primitive, ref_primitive.",
      "Indicates whether canonicalization should be skipped.",
      "Holds the name of the class or object."
    ],
    "code_examples": []
  },
  {
    "title": "Methods Overview",
    "concepts": [
      "__init__(buf) is a method, likely the constructor for a class, taking 'buf' as an argument.",
      "block_until_ready() is a method, likely used for synchronization or waiting for a resource to be available."
    ],
    "code_examples": []
  },
  {
    "title": "Class Initialization",
    "concepts": [
      "The class is initialized with a suffix (string) and an abstract value (AbstractValue)."
    ],
    "code_examples": [
      {
        "description": "Class constructor that takes a suffix (string) and an AbstractValue object.",
        "code": "__init__ (suffix,  aval)"
      }
    ]
  },
  {
    "title": "Attributes",
    "concepts": [
      "The class has attributes for count, suffix, and aval."
    ],
    "code_examples": []
  },
  {
    "title": "Attribute Declaration",
    "concepts": [
      "suffix is a string.",
      "aval is an AbstractValue object."
    ],
    "code_examples": [
      {
        "description": "Declaration of the 'suffix' attribute as a string.",
        "code": "suffix ( str )"
      },
      {
        "description": "Declaration of the 'aval' attribute as an AbstractValue object.",
        "code": "aval ( AbstractValue )"
      }
    ]
  },
  {
    "title": "Set Creation",
    "concepts": [
      "Creates a new empty set object using set().",
      "Creates a new set object from an iterable using set(iterable).",
      "Sets are unordered collections of unique elements."
    ],
    "code_examples": []
  },
  {
    "title": "ClosedJaxpr",
    "concepts": [
      "The document refers to 'ClosedJaxpr'."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to the Document",
    "concepts": [
      "This section introduces the purpose and scope of the document.",
      "It provides a high-level overview of the topics covered."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Data Types",
    "concepts": [
      "Integers are used to represent whole numbers.",
      "Floats are used to represent numbers with decimal points.",
      "Strings are used to represent text."
    ],
    "code_examples": []
  },
  {
    "title": "Variables and Assignment",
    "concepts": [
      "Variables are used to store data.",
      "The assignment operator (=) is used to assign values to variables.",
      "Variable names are case-sensitive."
    ],
    "code_examples": [
      {
        "description": "Assigning an integer value to a variable.",
        "code": "x = 10"
      },
      {
        "description": "Assigning a floating-point value to a variable.",
        "code": "y = 3.14"
      },
      {
        "description": "Assigning a string value to a variable.",
        "code": "name = \"Alice\""
      }
    ]
  },
  {
    "title": "Arithmetic Operators",
    "concepts": [
      "The addition operator (+) adds two numbers.",
      "The subtraction operator (-) subtracts two numbers.",
      "The multiplication operator (*) multiplies two numbers.",
      "The division operator (/) divides two numbers.",
      "The modulo operator (%) returns the remainder of a division."
    ],
    "code_examples": [
      {
        "description": "Demonstrating addition, subtraction, multiplication, and division.",
        "code": "a = 10\nb = 5\n\nsum_result = a + b\ndiff_result = a - b\nproduct_result = a * b\ndivision_result = a / b\n\nprint(sum_result)\nprint(diff_result)\nprint(product_result)\nprint(division_result)"
      },
      {
        "description": "Demonstrating the modulo operator.",
        "code": "c = 17\nd = 5\n\nremainder = c % d\n\nprint(remainder)"
      }
    ]
  },
  {
    "title": "Control Flow Statements",
    "concepts": [
      "If statements execute a block of code if a condition is true.",
      "Else statements execute a block of code if the if condition is false.",
      "Elif statements check multiple conditions in sequence.",
      "For loops iterate over a sequence of items.",
      "While loops execute a block of code as long as a condition is true."
    ],
    "code_examples": [
      {
        "description": "Demonstrating an if-else statement.",
        "code": "age = 20\n\nif age >= 18:\n    print(\"You are an adult.\")\nelse:\n    print(\"You are a minor.\")"
      },
      {
        "description": "Demonstrating a for loop.",
        "code": "numbers = [1, 2, 3, 4, 5]\n\nfor number in numbers:\n    print(number)"
      },
      {
        "description": "Demonstrating a while loop.",
        "code": "count = 0\n\nwhile count < 5:\n    print(count)\n    count += 1"
      }
    ]
  },
  {
    "title": "StoreException",
    "concepts": [],
    "code_examples": []
  },
  {
    "title": "WrappedFun",
    "concepts": [
      "Represents a function f to which transforms are to be applied."
    ],
    "code_examples": []
  },
  {
    "title": "cache",
    "concepts": [
      "Memoization decorator for functions taking a WrappedFun as first argument."
    ],
    "code_examples": []
  },
  {
    "title": "merge_linear_aux",
    "concepts": [],
    "code_examples": []
  },
  {
    "title": "transformation",
    "concepts": [],
    "code_examples": []
  },
  {
    "title": "transformation_with_aux",
    "concepts": [],
    "code_examples": []
  },
  {
    "title": "wrap_init",
    "concepts": [],
    "code_examples": []
  },
  {
    "title": "Function Transformations Overview",
    "concepts": [
      "The document describes a mechanism for applying transformations to a function.",
      "The transformations are defined as a tuple of generator functions and their static arguments.",
      "Auxiliary outputs of the transformations are stored in a list of stores.",
      "Extra parameters can be passed to the function as keyword arguments."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters Description",
    "concepts": [
      "f (Callable): The original function to be transformed.",
      "f_transformed (Callable): The transformed function.",
      "transforms (tuple): A tuple of (generator, static_arguments) tuples representing transformations.",
      "stores (tuple): A list of stores for the auxiliary output of the transforms.",
      "params (tuple): A tuple of (name, param) tuples representing extra parameters for the function.",
      "in_type (core.InputType | None): Optional input type.",
      "debug_info (DebugInfo): Debugging info about the function being wrapped."
    ],
    "code_examples": []
  },
  {
    "title": "Methods Summary",
    "concepts": [
      "__init__(f, f_transformed, transforms, ...): Initializes the transformation setup.",
      "call_wrapped(*args, **kwargs): Calls the transformed function.",
      "populate_stores(stores): Copies values from the stores into self.stores.",
      "wrap(gen, gen_static_args, out_store): Adds another transform and its store."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes Overview",
    "concepts": [
      "f: The original function.",
      "f_transformed: The transformed function.",
      "transforms: The tuple of transformations.",
      "stores: The list of stores.",
      "params: The extra parameters.",
      "in_type: The input type.",
      "debug_info: The debugging information."
    ],
    "code_examples": []
  },
  {
    "title": "Memoization Decorator Overview",
    "concepts": [
      "Memoization is applied to functions with a WrappedFun as the first argument.",
      "The memoization cache key is based on the transforms and params of the WrappedFun.",
      "A callable 'call' is memoized.",
      "An optional 'explain' function logs explanations upon cache misses, providing details about the WrappedFun, cache state, and key."
    ],
    "code_examples": []
  },
  {
    "title": "Fun and WrappedFun",
    "concepts": [
      "The document introduces concepts named 'Fun' and 'WrappedFun'."
    ],
    "code_examples": []
  },
  {
    "title": "WrappedFun",
    "concepts": [
      "The document mentions a function or type called 'WrappedFun'."
    ],
    "code_examples": []
  },
  {
    "title": "Function Wrapper",
    "concepts": [
      "A wrapper around a callable object.",
      "It encapsulates a function (or other callable) and may modify its behavior.",
      "The type of the wrapped function is `Callable`.",
      "The variable name representing the wrapped function is `WrappedFun`."
    ],
    "code_examples": []
  },
  {
    "title": "Dialects",
    "concepts": [
      "Dialects are a way to extend the IR with new operations and types.",
      "Dialects provide a namespace for operations and types."
    ],
    "code_examples": []
  },
  {
    "title": "IR",
    "concepts": [
      "IR stands for Intermediate Representation.",
      "The IR is the core data structure in MLIR.",
      "It represents the program as a graph of operations.",
      "Operations can have attributes and operands."
    ],
    "code_examples": []
  },
  {
    "title": "Pass Manager",
    "concepts": [
      "The Pass Manager is responsible for running optimization passes.",
      "Passes transform the IR to improve performance or reduce code size.",
      "The Pass Manager allows specifying the order in which passes are run."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to the Document",
    "concepts": [
      "The document provides a high-level overview of a system or process.",
      "It introduces core functionalities and components."
    ],
    "code_examples": []
  },
  {
    "title": "Data Structures and Definitions",
    "concepts": [
      "Data structures are defined to represent entities within the system.",
      "Example: A 'User' object with attributes like 'id', 'name', and 'email'."
    ],
    "code_examples": []
  },
  {
    "title": "Core Functionalities",
    "concepts": [
      "The system supports user authentication.",
      "Users can register with their email and password.",
      "Users can log in using their credentials.",
      "After successful login, a session token is generated."
    ],
    "code_examples": []
  },
  {
    "title": "User Authentication Example",
    "concepts": [
      "Demonstrates the authentication process using a Python-like syntax.",
      "Illustrates how to create a User object, verify credentials, and generate a token."
    ],
    "code_examples": [
      {
        "description": "Example of user authentication and token generation.",
        "code": "class User:\n    def __init__(self, id, name, email, password):\n        self.id = id\n        self.name = name\n        self.email = email\n        self.password = password\n\n    def verify_password(self, password):\n        return self.password == password\n\ndef authenticate_user(email, password):\n    # Assume user is retrieved from a database based on the email.\n    user = User(1, \"John Doe\", email, \"password123\")  \n\n    if user and user.verify_password(password):\n        return generate_token(user.id)\n    else:\n        return None\n\ndef generate_token(user_id):\n    # In a real system, a secure token generation method should be used.\n    return f\"token_{user_id}\""
      }
    ]
  },
  {
    "title": "PRNG Implementations",
    "concepts": [
      "Defines a PRNG implementation.",
      "Specifies the key shape for PRNGs.",
      "Includes operations for seeding and splitting PRNGs."
    ],
    "code_examples": []
  },
  {
    "title": "Threefry Hash Function",
    "concepts": [
      "Applies the Threefry 2x32 hash function.",
      "The function `threefry_2x32` takes a keypair and a count as input.",
      "A `threefry2x32_p` is defined."
    ],
    "code_examples": []
  },
  {
    "title": "PRNG Implementations Details",
    "concepts": [
      "Details for `threefry_prng_impl` are provided.",
      "Details for `rbg_prng_impl` are provided.",
      "Details for `unsafe_rbg_prng_impl` are provided.",
      "These specify the PRNG key shape and operations."
    ],
    "code_examples": []
  },
  {
    "title": "Key Shape",
    "concepts": [
      "key_shape is a tuple of integers.",
      "The tuple represents the shape of the cryptographic key."
    ],
    "code_examples": []
  },
  {
    "title": "Seed Function",
    "concepts": [
      "seed is a callable function that takes an array as input.",
      "It returns an array."
    ],
    "code_examples": []
  },
  {
    "title": "Split Function",
    "concepts": [
      "split is a callable function that takes an array and a tuple of integers as input.",
      "It returns an array.",
      "The tuple of integers likely specifies how the array should be split."
    ],
    "code_examples": []
  },
  {
    "title": "Random Bits Function",
    "concepts": [
      "random_bits is a callable function that takes an array, an integer, and a tuple of integers as input.",
      "It returns an array.",
      "The integer likely represents the number of random bits to generate.",
      "The tuple of integers likely represents the shape of the output array."
    ],
    "code_examples": []
  },
  {
    "title": "Fold In Function",
    "concepts": [
      "fold_in is a callable function that takes an array and an integer as input.",
      "It returns an array.",
      "It likely incorporates the integer into the array."
    ],
    "code_examples": []
  },
  {
    "title": "Name and Tag",
    "concepts": [
      "name is a string.",
      "tag is a string."
    ],
    "code_examples": []
  },
  {
    "title": "Hashable",
    "concepts": [
      "Hashable is a type or class indicating that the object can be used as a key in a dictionary or as an element in a set."
    ],
    "code_examples": []
  },
  {
    "title": "PRNG Implementation",
    "concepts": [
      "Implementation details for a Pseudo-Random Number Generator.",
      "The PRNGImpl structure or class."
    ],
    "code_examples": []
  },
  {
    "title": "Seeding the PRNG",
    "concepts": [
      "The 'seed' function is used to initialize the PRNG with a seed value.",
      "The seed can be an integer or an array-like object.",
      "Seeding is crucial for reproducibility in random number generation."
    ],
    "code_examples": []
  },
  {
    "title": "PRNGKeyArray",
    "concepts": [
      "Represents an array of keys for the PRNG.",
      "Used for managing multiple PRNG states."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction",
    "concepts": [
      "The document introduces the concept of representing data and operations in a structured format.",
      "It likely aims to describe a system for data serialization or a domain-specific language."
    ],
    "code_examples": []
  },
  {
    "title": "Threefry 2x32 Hash Application",
    "concepts": [
      "Applies the Threefry 2x32 hash function.",
      "keypair: A pair of 32-bit unsigned integers used as the key.",
      "count: An array of dtype uint32 used as the counts.",
      "Returns an array of dtype uint32 with the same shape as the count array."
    ],
    "code_examples": []
  },
  {
    "title": "PRNG Key Shape and Operations",
    "concepts": [
      "PRNG implementation is determined by a key type K and a collection of functions.",
      "Key type K is an array type with element type uint32 and shape specified by key_shape.",
      "The type signature of operations: seed, fold_in, split, random_bits.",
      "A PRNG implementation is adapted to an array-like object of keys K by the PRNGKeyArray class.",
      "PRNGKeyArray should be created via the random_seed function."
    ],
    "code_examples": []
  },
  {
    "title": "PRNG Key Shape and Operations",
    "concepts": [
      "PRNG implementation is determined by a key type K and functions operating on K.",
      "The key type K is an array with element type uint32 and shape specified by key_shape.",
      "The document specifies the type signatures of seed, fold_in, split, and random_bits operations.",
      "A PRNG implementation is adapted to array-like objects of keys K by the PRNGKeyArray class, created via the random_seed function."
    ],
    "code_examples": []
  },
  {
    "title": "PRNG Key Shape and Operations",
    "concepts": [
      "PRNG implementation is determined by a key type K and operations on those keys.",
      "Key type K is an array type with element type uint32 and shape specified by key_shape.",
      "seed operation has type signature: int[] -> K",
      "fold_in operation has type signature: K -> int[] -> K",
      "split operation has type signature: K -> K[*shape]",
      "random_bits operation has type signature: K -> uint<bit_width>[*shape]",
      "A PRNG implementation is adapted to an array-like object of keys K by the PRNGKeyArray class, which should be created via the random_seed function."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Usage in Libraries",
    "concepts": [
      "Learning advanced JAX usage involves examining how other libraries utilize it.",
      "JAX's features facilitate accelerated computation in various domains.",
      "Easy gradient calculation is a key feature of JAX and is used in libraries like JaxOpt.",
      "Models defined in JAX can be compiled for speedup using JIT compiling and deployed on different devices.",
      "JAX simplifies parallelizing computation with pmap and vmap."
    ],
    "code_examples": []
  },
  {
    "title": "Gradient Calculation with JAX",
    "concepts": [
      "JaxOpt uses value and grad directly for optimization algorithms.",
      "Dynamax and Optax pairing facilitates estimation methods using gradients."
    ],
    "code_examples": []
  },
  {
    "title": "JIT Compilation for Speedup",
    "concepts": [
      "Models in JAX can be compiled to enable single computation speedup through JIT compiling.",
      "Compiled code can be deployed on CPU, GPU, or TPU devices.",
      "In Dynamax, the computationally expensive part of a Linear State Space Model solver is jitted.",
      "PyTensor compiles a JAX function dynamically and then jits the constructed function."
    ],
    "code_examples": []
  },
  {
    "title": "Parallelization with pmap and vmap",
    "concepts": [
      "JAX simplifies parallelizing computation using pmap and vmap.",
      "Dynamax state space models are parallelized with a VMAP decorator, as used for multi-object tracking."
    ],
    "code_examples": []
  },
  {
    "title": "Patterns of JAX Usage",
    "concepts": [
      "JAX can be used in a standalone pattern where users define all calculations.",
      "Libraries built on JAX can provide specific functionality, such as model definition or optimization.",
      "Flax simplifies the construction of Neural Networks and is paired with Optax for optimization.",
      "Dynamax facilitates easy definition of state space models and estimation using Optax or Blackjax.",
      "Some libraries, like PyMC and Pytensor, wrap JAX functions within their specific APIs."
    ],
    "code_examples": []
  },
  {
    "title": "Direct JAX Usage",
    "concepts": [
      "JAX can be directly imported and used to build models from scratch.",
      "This is suitable when prebuilt code isn't available or to reduce dependencies."
    ],
    "code_examples": []
  },
  {
    "title": "Libraries Built on JAX",
    "concepts": [
      "Packages provide prebuilt functionality for model definition or computation.",
      "Combinations of packages can be used for end-to-end workflows.",
      "Flax simplifies the construction of Neural Networks.",
      "Flax is typically paired with Optax.",
      "Dynamax facilitates easy definition of state space models.",
      "Dynamax parameters can be estimated using Maximum Likelihood using Optax or full Bayesian Posterior can be estimating using MCMC from Blackjax."
    ],
    "code_examples": []
  },
  {
    "title": "Wrapping JAX in Model-Specific APIs",
    "concepts": [
      "Some libraries completely wrap JAX in their model-specific API.",
      "PyMC and Pytensor are examples where users might not directly see JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Notes",
    "concepts": [
      "This section provides shorter notes on using JAX.",
      "For longer design discussions, refer to JAX Enhancement Proposals (JEPs)."
    ],
    "code_examples": []
  },
  {
    "title": "API Compatibility",
    "concepts": [
      "JAX has policies regarding API compatibility across releases."
    ],
    "code_examples": []
  },
  {
    "title": "Python and NumPy Version Support",
    "concepts": [
      "JAX has policies regarding compatibility with Python and NumPy versions."
    ],
    "code_examples": []
  },
  {
    "title": "jax.Array Migration",
    "concepts": [
      "This section summarizes the changes to the default array type in jax v0.4.1."
    ],
    "code_examples": []
  },
  {
    "title": "Asynchronous Dispatch",
    "concepts": [
      "JAX uses an asynchronous dispatch model."
    ],
    "code_examples": []
  },
  {
    "title": "Concurrency",
    "concepts": [
      "This section describes how JAX interacts with other Python concurrency mechanisms."
    ],
    "code_examples": []
  },
  {
    "title": "GPU Memory Allocation",
    "concepts": [
      "This section describes how JAX interacts with memory allocation on GPUs."
    ],
    "code_examples": []
  },
  {
    "title": "Rank Promotion Warning",
    "concepts": [
      "This section describes how to configure jax.numpy to avoid implicit rank promotion."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Versioning and Breaking Changes",
    "concepts": [
      "JAX uses Effort-based versioning.",
      "Version 0.X.Y: Incrementing Y introduces minor breaking changes, incrementing X introduces major breaking changes.",
      "JAX follows a 3-month deprecation policy for breaking changes.",
      "Breaking changes are announced in CHANGELOG.md and the deprecated API's docstring, issuing a DeprecationWarning.",
      "Deprecated APIs may be removed 3 months after the JAX release that deprecated them.",
      "The policy may change at any time."
    ],
    "code_examples": []
  },
  {
    "title": "Public vs. Private APIs",
    "concepts": [
      "Only public JAX APIs are covered by the compatibility policy.",
      "Public JAX APIs are documented in the JAX documentation.",
      "Non-public APIs should ideally have names prefixed with underscores.",
      "APIs or import paths prefixed with an underscore are explicitly private and may change without warning.",
      "Private APIs are being moved into jax._src.",
      "Legacy modules like jax.core, jax.interpreters, jax.lib, and jax.util expose private APIs without an underscore and are being deprecated."
    ],
    "code_examples": []
  },
  {
    "title": "Experimental and Extend Modules",
    "concepts": [
      "jax.experimental and jax.example_libraries modules contain code for experimental or demonstration purposes.",
      "APIs in these modules may change between releases without warning.",
      "Generally, the 3-month deprecation period is followed for changes in jax.experimental.",
      "jax.extend contains semi-public JAX internal APIs meant for use by downstream projects.",
      "jax.extend APIs do not have the same stability guarantees as the main JAX package.",
      "CI tests against JAX nightly releases are recommended for code using jax.extend."
    ],
    "code_examples": []
  },
  {
    "title": "Numerical Stability and Random Numbers",
    "concepts": [
      "Exact values of numerical operations are not guaranteed to be stable across JAX releases.",
      "Exact numerics are not necessarily stable at a given JAX version, across accelerator platforms, within or without jax.jit.",
      "For a fixed PRNG key input, the outputs of pseudorandom functions in jax.random may vary across JAX versions.",
      "The compatibility policy applies only to the output distribution of pseudorandom functions.",
      "Changes to pseudorandom values are announced in the changelog but do not follow a deprecation cycle.",
      "JAX might expose a transient configuration flag to revert to older behavior for a deprecation window's amount of time."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating that the exact value returned by jax.random.gumbel may change across JAX releases, but it will still sample from a Gumbel distribution.",
        "code": "jax.random.gumbel(jax.random.key(72))"
      }
    ]
  },
  {
    "title": "JAX Version Support Policy",
    "concepts": [
      "JAX follows the Python scientific community\u2019s SPEC 0 for NumPy and SciPy version support.",
      "JAX supports Python versions for at least nine months longer than SPEC-0 recommends.",
      "JAX supports Python feature releases in the 45 months prior to each JAX release.",
      "JAX supports NumPy feature releases in the 24 months prior to each JAX release.",
      "JAX supports SciPy feature releases in the 24 months prior to each JAX release.",
      "JAX releases may support older versions of Python, NumPy, and SciPy than strictly required by this policy, but support for older versions may be dropped at any time beyond the listed dates."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.Array",
    "concepts": [
      "JAX's default array implementation has switched to jax.Array as of version 0.4.1.",
      "jax.Array unifies DeviceArray, ShardedDeviceArray, and GlobalDeviceArray types.",
      "This change makes parallelism a core feature of JAX and simplifies its internals.",
      "Code that explicitly uses DeviceArray, ShardedDeviceArray, or GlobalDeviceArray may need to be updated.",
      "jax.Array will eventually be the only array type in JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Enabling and Disabling jax.Array",
    "concepts": [
      "jax.Array can be enabled by setting the JAX_ARRAY environment variable to a true-like value, using the jax_array boolean flag with absl, or using jax.config.update('jax_array', True).",
      "jax.Array can be disabled until March 15, 2023, using similar methods with falsey values.",
      "Disabling jax.Array can help identify if it's causing any issues."
    ],
    "code_examples": [
      {
        "description": "Enables jax.Array",
        "code": "import jax\njax.config.update('jax_array', True)"
      },
      {
        "description": "Disables jax.Array (until March 15, 2023)",
        "code": "import jax\njax.config.update('jax_array', False)"
      }
    ]
  },
  {
    "title": "Benefits of jax.Array",
    "concepts": [
      "jax.Array merges DeviceArray, ShardedDeviceArray and GlobalDeviceArray, simplifying JAX's internals.",
      "jax.Array adds new parallelism features.",
      "A new Sharding abstraction describes how a logical Array is physically sharded across devices.",
      "The change simplifies and merges the parallelism features of pjit into jit.",
      "jit functions can operate over sharded arrays without copying data to a single device."
    ],
    "code_examples": []
  },
  {
    "title": "jax.Array Usage Example",
    "concepts": [
      "This example demonstrates the use of jax.Array with sharding and JIT compilation.",
      "The example showcases how to create a sharded array and perform operations on it.",
      "The example shows that jit can operate on sharded arrays without copying to a single device, and jnp.copy preserves sharding.",
      "The example requires the jax, jax.numpy, jax.sharding, and numpy libraries"
    ],
    "code_examples": [
      {
        "description": "Illustrates sharding an array and performing JIT-compiled operations on it. It also shows that jnp.copy preserves the sharding.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\n\nx = jnp.arange(8)\n# Let's say there are 8 devices in jax.devices()\nmesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(4, 2), ('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x'))\nsharded_x = jax.device_put(x, sharding)\n# `matmul_sharded_x` and `sin_sharded_x` are sharded. `jit` is able to operate over a\n# sharded array without copying data to a single device.\nmatmul_sharded_x = sharded_x @ sharded_x.T\nsin_sharded_x = jnp.sin(sharded_x)\n# Even jnp.copy preserves the sharding on the output.\ncopy_sharded_x = jnp.copy(sharded_x)\n# double_out is also sharded\ndouble_out = jax.jit(lambda x: x * 2)(sharded_x)"
      },
      {
        "description": "Illustrates sharding an array and performing JIT-compiled operations on it. It also shows that jnp.copy preserves the sharding.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\n\nx = jnp.arange(8)\n# Let's say there are 8 devices in jax.devices()\nmesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(4, 2), ('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x'))\nsharded_x = jax.device_put(x, sharding)\n# `matmul_sharded_x` and `sin_sharded_x` are sharded. `jit` is able to operate over a\n# sharded array without copying data to a single device.\nmatmul_sharded_x = sharded_x @ sharded_x.T\nsin_sharded_x = jnp.sin(sharded_x)\n# Even jnp.copy preserves the sharding on the output.\ncopy_sharded_x = jnp.copy(sharded_x)\n# double_out is also sharded\ndouble_out = jax.jit(lambda x: x * 2)(sharded_x)"
      }
    ]
  },
  {
    "title": "Migrating Code to use jax.Array",
    "concepts": [
      "Replace isinstance(..., jnp.DeviceArray) or isinstance(.., jax.xla.DeviceArray) with isinstance(..., jax.Array).",
      "Differentiate DA, SDA, and GDA within jax.Array using x.is_fully_addressable and len(x.sharding.device_set).",
      "Replace GDA's local_shards and local_data with addressable_shards and addressable_data.",
      "Use jax.make_array_from_callback() or jax.make_array_from_single_device_arrays() instead of GlobalDeviceArray.from_callback(), make_sharded_device_array, or make_device_array."
    ],
    "code_examples": []
  },
  {
    "title": "Specific Migrations for GDA and SDA",
    "concepts": [
      "GlobalDeviceArray.from_callback(shape, mesh, pspec, callback) becomes jax.make_array_from_callback(shape, jax.sharding.NamedSharding(mesh, pspec), callback).",
      "GlobalDeviceArray(shape, mesh, pspec, buffers) becomes jax.make_array_from_single_device_arrays(shape, jax.sharding.NamedSharding(mesh, pspec), buffers).",
      "make_sharded_device_array(aval, sharding_spec, device_buffers, indices) becomes jax.make_array_from_single_device_arrays(shape, sharding, device_buffers).",
      "Use jax.sharding.PmapSharding or jax.sharding.NamedSharding to create the appropriate sharding for pmap or pjit inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Breaking Change: pjit Input Requirements",
    "concepts": [
      "With jax.Array enabled, all inputs to pjit must be globally shaped.",
      "pjit no longer concatenates process-local arguments into a global value.",
      "This change makes the representation of how local shards fit into a global whole more explicit.",
      "Explicit representation unlocks additional flexibility."
    ],
    "code_examples": []
  },
  {
    "title": "Migrating pjit with Host-Local Inputs",
    "concepts": [
      "Running multi-process pjit computations with host-local inputs when jax.Array is enabled can lead to errors.",
      "Use jax.experimental.multihost_utils.host_local_array_to_global_array to convert host-local inputs to global arrays for pjit.",
      "Use jax.experimental.multihost_utils.global_array_to_host_local_array to convert global outputs back to host-local arrays.",
      "Passing fully replicated inputs with P(None) as in_axis_resources is still supported without host_local_array_to_global_array."
    ],
    "code_examples": [
      {
        "description": "Illustrates how to convert host-local arrays to global arrays and vice versa for pjit.",
        "code": "from jax.experimental import multihost_utils\n\nglobal_inps = multihost_utils.host_local_array_to_global_array(\n    local_inputs,\n    mesh,\n    in_pspecs\n)\n\nglobal_outputs = pjit(\n    f,\n    in_shardings=in_pspecs,\n    out_shardings=out_pspecs\n)(global_inps)\n\nlocal_outs = multihost_utils.global_array_to_host_local_array(\n    global_outputs,\n    mesh,\n    out_pspecs\n)"
      },
      {
        "description": "Illustrates the usage of `pjit` when the input is fully replicated.",
        "code": "key = jax.random.PRNGKey(1)\n# As you can see, using host_local_array_to_global_array is not required since in_axis_resources says\n# that the input is fully replicated via P(None)\npjit(\n    f,\n    in_shardings=None,\n    out_shardings=None\n)(key)"
      },
      {
        "description": "Illustrates mixing inputs and converting them using `host_local_array_to_global_array`",
        "code": "key = jax.random.PRNGKey(1)\n# Mixing inputs\nglobal_inp = multihost_utils.host_local_array_to_global_array(\n    local_inp,\n    mesh,\n    P('data')\n)\nglobal_out = pjit(\n    f,\n    in_shardings=(P(None), P('data')),\n    out_shardings=...\n)(key, global_inp)"
      }
    ]
  },
  {
    "title": "Replacing FROM_GDA in pjit",
    "concepts": [
      "With jax.Array enabled, there is no need to pass anything to in_axis_resources as jax.Array will follow computation follows sharding semantics when FROM_GDA was used.",
      "Replace pjit(f, in_shardings=FROM_GDA, out_shardings=...) with pjit(f, out_shardings=...).",
      "If you have PartitionSpecs mixed in with FROM_GDA, use host_local_array_to_global_array to convert numpy arrays to jax.Array."
    ],
    "code_examples": [
      {
        "description": "Replaces FROM_GDA with host_local_array_to_global_array and removes in_shardings.",
        "code": "pjitted_f = pjit(\n    f,\n    out_shardings=...\n)\n\narray2, array3 = multihost_utils.host_local_array_to_global_array(\n    (np_array1, np_array2),\n    mesh,\n    (P('x'), P(None))\n)\n\npjitted_f(\n    array1,\n    array2,\n    array3,\n    array4\n)"
      }
    ]
  },
  {
    "title": "Deprecated Attributes and Functions",
    "concepts": [
      "The live_buffers attribute on jax Device has been deprecated; use jax.live_arrays() instead.",
      "Use multihost_utils.host_local_array_to_global_array to convert host-local inputs to global jax.Arrays for pjit in multi-process environments."
    ],
    "code_examples": [
      {
        "description": "Illustrates how to convert a batch to a global jax.Array and then pass that to pjit.",
        "code": "from jax.experimental import multihost_utils\nbatch = multihost_utils.host_local_array_to_global_array(\n    batch,\n    mesh,\n    batch_partition_spec\n)"
      }
    ]
  },
  {
    "title": "RecursionError due to jax.Array Mismatch",
    "concepts": [
      "A RecursionError can occur if jax.Array is disabled in one part of the code and enabled in another.",
      "This can happen when using third-party libraries that return DeviceArray while jax.Array is enabled in your code.",
      "This error should be resolved when jax.Array is enabled by default."
    ],
    "code_examples": []
  },
  {
    "title": "Asynchronous Dispatch in JAX",
    "concepts": [
      "JAX uses asynchronous dispatch to hide Python overheads.",
      "JAX returns a jax.Array, a future, without waiting for operations to complete.",
      "Inspecting the value of a jax.Array forces JAX to wait for the computation to complete.",
      "Asynchronous dispatch allows Python code to run ahead of the accelerator."
    ],
    "code_examples": [
      {
        "description": "Demonstrates asynchronous dispatch by performing a matrix multiplication and addition. The result is not immediately available.",
        "code": "import numpy as np\nimport jax.numpy as jnp\nfrom jax import random\n\nx = random.uniform(random.key(0), (1000, 1000))\n\n# Printing the result (i.e. evaluating `repr(result)` or `str(result)`)\n# will block until the value is ready.\n\njnp.dot(x, x) + 3."
      }
    ]
  },
  {
    "title": "Microbenchmarking and Asynchronous Dispatch",
    "concepts": [
      "Asynchronous dispatch can be misleading in microbenchmarks.",
      "The initial timing might only reflect the dispatch time, not the execution time.",
      "To measure the true cost, force the computation to complete using `np.asarray()` or `block_until_ready()`.",
      "Blocking without transferring the result back to Python is usually faster for microbenchmarks."
    ],
    "code_examples": [
      {
        "description": "Shows the misleadingly small time for matrix multiplication due to asynchronous dispatch.",
        "code": "import numpy as np\nimport jax.numpy as jnp\nfrom jax import random\n\nx = random.uniform(random.key(0), (1000, 1000))\n\n%time jnp.dot(x, x)"
      },
      {
        "description": "Demonstrates how to accurately time the matrix multiplication using `np.asarray()` to force computation.",
        "code": "import numpy as np\nimport jax.numpy as jnp\nfrom jax import random\n\nx = random.uniform(random.key(0), (1000, 1000))\n\n%time np.asarray(jnp.dot(x, x))"
      },
      {
        "description": "Demonstrates how to accurately time the matrix multiplication using `block_until_ready()` to wait for completion.",
        "code": "import numpy as np\nimport jax.numpy as jnp\nfrom jax import random\n\nx = random.uniform(random.key(0), (1000, 1000))\n\n%time jnp.dot(x, x).block_until_ready()"
      }
    ]
  },
  {
    "title": "Concurrency Support in JAX",
    "concepts": [
      "JAX has limited support for Python concurrency.",
      "JAX APIs can be called concurrently from separate Python threads.",
      "Manipulating JAX trace values concurrently from multiple threads is not permitted.",
      "It is permissible to call functions that use JAX tracing (e.g., jit()) from multiple threads.",
      "Do not use threading to manipulate JAX values inside functions passed to jit()."
    ],
    "code_examples": []
  },
  {
    "title": "NumPy Rank Promotion",
    "concepts": [
      "NumPy broadcasting rules allow automatic rank promotion.",
      "Rank promotion can lead to unexpected bugs.",
      "jax.numpy is configurable to control rank promotion behavior.",
      "The jax_numpy_rank_promotion option can be set to allow, warn, or raise.",
      "The default setting for jax_numpy_rank_promotion is allow."
    ],
    "code_examples": [
      {
        "description": "Demonstrates NumPy rank promotion.",
        "code": "import numpy as np\n\nx = np.arange(12).reshape(4, 3)\ny = np.array([0, 1, 0])\n\nx + y"
      },
      {
        "description": "Demonstrates NumPy rank promotion.",
        "code": "import numpy as np\n\nx = np.arange(12).reshape(4, 3)\ny = np.array([0, 1, 0])\n\nx + y"
      }
    ]
  },
  {
    "title": "Configuring Rank Promotion in JAX",
    "concepts": [
      "jax.numpy_rank_promotion() context manager enables local configuration.",
      "jax.config.update() allows global configuration in code.",
      "The JAX_NUMPY_RANK_PROMOTION environment variable can be used for global configuration.",
      "absl-py command-line flags can be used for configuration."
    ],
    "code_examples": [
      {
        "description": "Enables rank promotion warning using a context manager.",
        "code": "import jax\nimport numpy as np\n\nx = np.arange(12).reshape(4, 3)\ny = np.array([0, 1, 0])\n\nwith jax.numpy_rank_promotion(\"warn\"):\n    z = x + y"
      },
      {
        "description": "Enables rank promotion warning using a context manager.",
        "code": "import jax\nimport numpy as np\n\nx = np.arange(12).reshape(4, 3)\ny = np.array([0, 1, 0])\n\nwith jax.numpy_rank_promotion(\"warn\"):\n    z = x + y"
      },
      {
        "description": "Globally configures rank promotion to 'warn' using jax.config.update().",
        "code": "import jax\njax.config.update(\"jax_numpy_rank_promotion\", \"warn\")"
      },
      {
        "description": "Globally configures rank promotion to 'warn' using jax.config.update().",
        "code": "import jax\njax.config.update(\"jax_numpy_rank_promotion\", \"warn\")"
      }
    ]
  },
  {
    "title": "Configuration Context Managers",
    "concepts": [
      "Context managers are used to temporarily modify JAX's configuration.",
      "These context managers control various aspects of JAX's behavior, like debugging, tracing, and numerical precision."
    ],
    "code_examples": []
  },
  {
    "title": "JIT Compilation and Tracing",
    "concepts": [
      "Just-In-Time (JIT) compilation optimizes JAX functions for performance.",
      "Tracing captures the operations performed by a JAX function.",
      "JAXPR is a representation of the computation performed by a JAX function."
    ],
    "code_examples": []
  },
  {
    "title": "Shape and Device Handling",
    "concepts": [
      "ShapeDtypeStruct represents the shape and data type of an array.",
      "device_put transfers data to a specific device.",
      "device_get transfers data from a device to the host.",
      "default_backend returns the default XLA backend."
    ],
    "code_examples": []
  },
  {
    "title": "Naming and Scoping",
    "concepts": [
      "Functions and computations can be named for easier debugging and profiling.",
      "named_call adds a user specified name to a function.",
      "named_scope provides a context for naming operations."
    ],
    "code_examples": []
  },
  {
    "title": "Blocking and Asynchronous Operations",
    "concepts": [
      "block_until_ready ensures that a computation is complete before proceeding.",
      "copy_to_host_async copies data to the host asynchronously."
    ],
    "code_examples": []
  },
  {
    "title": "Mesh Creation",
    "concepts": [
      "Meshes are used for distributed computation and data parallelism.",
      "make_mesh creates a mesh with specified shape and axis names."
    ],
    "code_examples": []
  },
  {
    "title": "Automatic Differentiation",
    "concepts": [
      "Automatic differentiation (AD) is used to compute gradients of functions.",
      "grad computes the gradient of a function.",
      "value_and_grad computes both the function value and its gradient.",
      "jacobian computes the Jacobian matrix.",
      "jvp computes the Jacobian-vector product (forward mode AD).",
      "vjp computes the vector-Jacobian product (reverse mode AD).",
      "hessian computes the Hessian matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Custom Derivatives and Checkpointing",
    "concepts": [
      "Custom derivatives allow defining custom gradient rules for functions.",
      "custom_gradient is a utility for defining custom VJP rules.",
      "custom_jvp allows defining custom JVP rules.",
      "custom_vjp allows defining custom VJP rules.",
      "Checkpointing reduces memory usage during AD by recomputing intermediate values."
    ],
    "code_examples": []
  },
  {
    "title": "Custom Batching",
    "concepts": [
      "Custom batching allows customizing the behavior of `vmap` for specific functions.",
      "custom_batching.custom_vmap allows customizing `vmap` behavior.",
      "custom_batching.sequential_vmap is a special case using a loop."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Arrays",
    "concepts": [
      "JAX arrays are the fundamental data structure for representing numerical data.",
      "JAX arrays provide methods for various operations, including arithmetic, linear algebra, and reduction.",
      "JAX Arrays have attributes to determine its sharding, shape and data type."
    ],
    "code_examples": []
  },
  {
    "title": "Vectorization and Parallelization",
    "concepts": [
      "vmap vectorizes a function over its inputs.",
      "pmap parallelizes a function across multiple devices.",
      "numpy.vectorize allows using numpy broadcasting with python functions"
    ],
    "code_examples": []
  },
  {
    "title": "Device Management",
    "concepts": [
      "Functions for querying and managing JAX devices.",
      "devices returns a list of all devices.",
      "local_devices returns devices local to the current process.",
      "device_count returns the total number of devices.",
      "process_count returns the number of JAX processes."
    ],
    "code_examples": []
  },
  {
    "title": "Callbacks and Debugging",
    "concepts": [
      "Callbacks allow calling Python functions from JAX code.",
      "pure_callback calls a pure Python callback.",
      "experimental.io_callback calls an impure Python callback.",
      "debug.callback calls a stageable Python callback.",
      "debug.print prints values during JAX execution."
    ],
    "code_examples": []
  },
  {
    "title": "Device Information and Caching",
    "concepts": [
      "Device describes an available device.",
      "print_environment_info provides information about the JAX environment.",
      "live_arrays returns all live arrays on a platform.",
      "clear_caches clears compilation and staging caches."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX NumPy Implementation",
    "concepts": [
      "JAX implements the NumPy API using primitives from jax.lax.",
      "JAX aims to closely follow the NumPy API but has limitations.",
      "JAX arrays are immutable, preventing in-place array mutations.",
      "JAX provides purely functional alternatives to in-place operations, such as `x.at[i].set(y)` for indexed updates.",
      "JAX functions like `transpose()` and `reshape()` return copies instead of views, but XLA can optimize these away.",
      "JAX is less aggressive than NumPy in promoting values to `float64` type.",
      "JAX requires array shapes to be known at compile time, posing challenges for functions with data-dependent output shapes.",
      "Functions like `unique()` and `nonzero()` in JAX accept an optional `size` argument for JIT compatibility.",
      "Most NumPy functions are implemented in the `jax.numpy` namespace."
    ],
    "code_examples": []
  },
  {
    "title": "Indexed Update Example",
    "concepts": [
      "JAX provides `x.at[i].set(y)` as a functional alternative to in-place array updates like `x[i] = y`."
    ],
    "code_examples": []
  },
  {
    "title": "NumPy API in JAX NumPy",
    "concepts": [
      "Listing of nearly all applicable NumPy functions implemented in the jax.numpy namespace."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of the 'at' Property",
    "concepts": [
      "The 'at' property provides a functionally pure equivalent of in-place array modifications.",
      "It allows setting, adding, subtracting, multiplying, dividing, powering, finding the minimum or maximum, and applying a ufunc at specific indices of an array.",
      "The 'at' expressions do not modify the original array; they return a modified copy.",
      "Inside a jit() compiled function, expressions like x = x.at[idx].set(y) are guaranteed to be applied in-place.",
      "Multiple updates to the same index will all be applied, unlike NumPy.",
      "The order of conflicting updates is implementation-defined and may be nondeterministic."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the alternate syntax and equivalent in-place expressions.",
        "code": "x = x.at[idx].set(y)  # Equivalent to: x[idx] = y\nx = x.at[idx].add(y)  # Equivalent to: x[idx] += y\nx = x.at[idx].subtract(y) # Equivalent to: x[idx] -= y\nx = x.at[idx].multiply(y) # Equivalent to: x[idx] *= y\nx = x.at[idx].divide(y) # Equivalent to: x[idx] /= y\nx = x.at[idx].power(y) # Equivalent to: x[idx] **= y\nx = x.at[idx].min(y) # Equivalent to: x[idx] = minimum(x[idx], y)\nx = x.at[idx].max(y) # Equivalent to: x[idx] = maximum(x[idx], y)\nx = x.at[idx].apply(ufunc) # Equivalent to: ufunc.at(x, idx)\nx = x.at[idx].get() # Equivalent to: x = x[idx]"
      }
    ]
  },
  {
    "title": "Out-of-Bounds Indexing and Modes",
    "concepts": [
      "By default, JAX assumes all indices are in-bounds.",
      "Alternative out-of-bound index semantics can be specified via the 'mode' parameter.",
      "Available 'mode' options are: 'promise_in_bounds', 'clip', 'drop', and 'fill'.",
      "'promise_in_bounds' does no additional checking, potentially clipping out-of-bounds indices in get() and dropping them in set(), add(), etc.",
      "'clip' clamps out-of-bounds indices into valid range.",
      "'drop' ignores out-of-bound indices.",
      "'fill' (alias for 'drop') allows specifying a fill value for out-of-bounds indices in get().",
      "The 'fill_value' argument specifies the value returned for out-of-bounds slices when mode is 'fill'."
    ],
    "code_examples": []
  },
  {
    "title": "Additional Parameters",
    "concepts": [
      "indices_are_sorted: If True, the implementation will assume that the indices passed to at[] are sorted in ascending order, which can lead to more efficient execution on some backends.",
      "unique_indices: If True, the implementation will assume that the indices passed to at[] are unique, which can result in more efficient execution on some backends."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Examples",
    "concepts": [
      "Demonstrates how to use the 'at' property with various methods like 'add' and 'get'.",
      "Shows how to handle out-of-bounds indices using 'clip' and 'fill' modes.",
      "Demonstrates how to set a custom fill value when using the 'fill' mode."
    ],
    "code_examples": [
      {
        "description": "Demonstrates adding a value to an element at a specific index.",
        "code": "x = jnp.arange(5.0)\nx.at[2].add(10)"
      },
      {
        "description": "Demonstrates that out-of-bounds indices are ignored by default.",
        "code": "x = jnp.arange(5.0)\nx.at[10].add(10)"
      },
      {
        "description": "Demonstrates clipping out-of-bounds indices.",
        "code": "x = jnp.arange(5.0)\nx.at[20].add(10, mode='clip')"
      },
      {
        "description": "Demonstrates getting the value at a specific index.",
        "code": "x = jnp.arange(5.0)\nx.at[2].get()"
      },
      {
        "description": "Demonstrates clipping out-of-bounds indices when getting a value.",
        "code": "x = jnp.arange(5.0)\nx.at[20].get()"
      },
      {
        "description": "Demonstrates filling out-of-bounds indices with NaN when getting a value.",
        "code": "x = jnp.arange(5.0)\nx.at[20].get(mode='fill')"
      },
      {
        "description": "Demonstrates filling out-of-bounds indices with a custom value when getting a value.",
        "code": "x = jnp.arange(5.0)\nx.at[20].get(mode='fill', fill_value=-1)"
      }
    ]
  },
  {
    "title": "Alias and Input Array",
    "concepts": [
      "The function is an alias of jax.numpy.absolute().",
      "The function takes an input array 'x'."
    ],
    "code_examples": []
  },
  {
    "title": "Absolute Value Calculation with JAX",
    "concepts": [
      "Calculates the absolute value element-wise of an array.",
      "JAX implementation is the same as jax.numpy.abs().",
      "Handles both real and complex numbers.",
      "For complex numbers a + ib, the absolute value is sqrt(a^2 + b^2)."
    ],
    "code_examples": [
      {
        "description": "Calculate the absolute value of a 1D JAX array.",
        "code": "x1 = jnp.array([5, -2, 0, 12])\njnp.absolute(x1)"
      },
      {
        "description": "Calculate the absolute value of a 2D JAX array.",
        "code": "x2 = jnp.array([[8, -3, 1],[0, 9, -6]])\njnp.absolute(x2)"
      },
      {
        "description": "Calculate the absolute value of a 1D JAX array of complex numbers.",
        "code": "x3 = jnp.array([8 + 15j, 3 - 4j, -5 + 0j])\njnp.absolute(x3)"
      }
    ]
  },
  {
    "title": "Alias of jax.numpy.arccos()",
    "concepts": [
      "The document describes an alias for the jax.numpy.arccos() function.",
      "The function takes an ArrayLike input denoted as 'x'."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.arccosh",
    "concepts": [
      "Alias of jax.numpy.arccosh()",
      "The function takes an ArrayLike input x",
      "The function returns an Array"
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jnp.add",
    "concepts": [
      "jnp.add is a JAX implementation of numpy.add.",
      "It is a universal function (ufunc).",
      "It implements the + operator for JAX arrays.",
      "The function performs element-wise addition of two arrays.",
      "The input arrays must be broadcastable to a common shape."
    ],
    "code_examples": []
  },
  {
    "title": "Explicitly Calling jnp.add",
    "concepts": [
      "Demonstrates how to use the jnp.add function directly."
    ],
    "code_examples": [
      {
        "description": "Adds 10 to each element of a JAX array created using jnp.arange().",
        "code": "x = jnp.arange(4)\njnp.add(x, 10)"
      },
      {
        "description": "Adds 10 to each element of a JAX array created using jnp.arange().",
        "code": "x = jnp.arange(4)\njnp.add(x, 10)"
      }
    ]
  },
  {
    "title": "Using the + Operator",
    "concepts": [
      "Demonstrates using the + operator for element-wise addition of a JAX array and a scalar."
    ],
    "code_examples": [
      {
        "description": "Adds 10 to each element of a JAX array using the + operator.",
        "code": "x + 10"
      },
      {
        "description": "Adds 10 to each element of a JAX array using the + operator.",
        "code": "x + 10"
      }
    ]
  },
  {
    "title": "Introduction to jnp.all",
    "concepts": [
      "The function jnp.all tests whether all array elements along a given axis evaluate to True.",
      "It is a JAX implementation of numpy.all().",
      "The 'a' parameter is the input array.",
      "The 'axis' parameter specifies the axis along which to test (default: None, tests along all axes).",
      "The 'keepdims' parameter indicates whether reduced axes should be left in the result with size 1 (default: False).",
      "The 'where' parameter specifies the elements to include in the test. It should be broadcast compatible to the input (default: None).",
      "The 'out' parameter is unused by JAX.",
      "The function returns an array of boolean values."
    ],
    "code_examples": []
  },
  {
    "title": "Testing all elements without specifying an axis",
    "concepts": [
      "By default, jnp.all tests for True values along all the axes.",
      "If any element in the array is False, the result will be False."
    ],
    "code_examples": [
      {
        "description": "Demonstrates testing all elements in an array for True values.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\nprint(jnp.all(x))"
      },
      {
        "description": "Demonstrates testing all elements in an array for True values.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\nprint(jnp.all(x))"
      }
    ]
  },
  {
    "title": "Testing along a specific axis",
    "concepts": [
      "The axis parameter specifies the axis along which to perform the 'all' test.",
      "When axis=0, it tests for True values along axis 0 (columns)."
    ],
    "code_examples": [
      {
        "description": "Demonstrates testing for True values along axis 0.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\nprint(jnp.all(x, axis=0))"
      },
      {
        "description": "Demonstrates testing for True values along axis 0.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\nprint(jnp.all(x, axis=0))"
      }
    ]
  },
  {
    "title": "Keeping dimensions in the output",
    "concepts": [
      "The keepdims parameter, when set to True, ensures that the output array has the same number of dimensions as the input array.",
      "The reduced axes are left in the result with size 1."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using keepdims=True to maintain the input array's dimensions.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\nprint(jnp.all(x, axis=0, keepdims=True))"
      },
      {
        "description": "Demonstrates using keepdims=True to maintain the input array's dimensions.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\nprint(jnp.all(x, axis=0, keepdims=True))"
      }
    ]
  },
  {
    "title": "Using the 'where' parameter",
    "concepts": [
      "The 'where' parameter allows including specific elements in testing for True values.",
      "The 'where' array should be broadcast compatible with the input array.",
      "Only the elements where 'where' is True are considered in the 'all' test."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using the 'where' parameter to include specific elements in the test.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 0, 1, 1],\n                   [1, 1, 1, 0]], dtype=bool)\n\nprint(jnp.all(x, axis=0, keepdims=True, where=where))"
      },
      {
        "description": "Demonstrates using the 'where' parameter to include specific elements in the test.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 0, 1, 1],\n                   [1, 1, 1, 0]], dtype=bool)\n\nprint(jnp.all(x, axis=0, keepdims=True, where=where))"
      }
    ]
  },
  {
    "title": "Description of jnp.allclose()",
    "concepts": [
      "The function checks if two arrays are element-wise approximately equal within a tolerance.",
      "It's a JAX implementation of numpy.allclose().",
      "The condition for approximate equality is |a - b| <= atol + rtol * |b|.",
      "jnp.inf in a will be considered equal to jnp.inf in b.",
      "The function takes two arrays (a, b), relative tolerance (rtol), absolute tolerance (atol), and a boolean (equal_nan) as input.",
      "The function returns a boolean scalar array indicating whether the input arrays are element-wise approximately equal within the specified tolerances.",
      "See also jax.numpy.isclose() and jax.numpy.equal()."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.allclose() Usage",
    "concepts": [
      "Demonstrates the usage of jnp.allclose() with different arrays, rtol, atol, and equal_nan parameters."
    ],
    "code_examples": [
      {
        "description": "Compares two arrays that are not close and shows default behaviour of jnp.allclose.",
        "code": "jnp.allclose(jnp.array([1e6, 2e6, 3e6]), jnp.array([1e6, 2e6, 3e7]))"
      },
      {
        "description": "Compares two arrays with rtol = 1e3. These arrays are considered close.",
        "code": "jnp.allclose(jnp.array([1e6, 2e6, 3e6]),\n             jnp.array([1.00008e6, 2.00008e7, 3.00008e8]), rtol=1e3)"
      },
      {
        "description": "Compares two arrays with atol = 1e3. These arrays are considered close.",
        "code": "jnp.allclose(jnp.array([1e6, 2e6, 3e6]),\n             jnp.array([1.00001e6, 2.00002e6, 3.00009e6]), atol=1e3)"
      },
      {
        "description": "Compares two arrays containing NaNs with equal_nan = True. NaNs are considered equal.",
        "code": "jnp.allclose(jnp.array([jnp.nan, 1, 2]),\n             jnp.array([jnp.nan, 1, 2]), equal_nan=True)"
      },
      {
        "description": "Compares two arrays that are not close and shows default behaviour of jnp.allclose.",
        "code": "jnp.allclose(jnp.array([1e6, 2e6, 3e6]), jnp.array([1e6, 2e6, 3e7]))"
      },
      {
        "description": "Compares two arrays with rtol = 1e3. These arrays are considered close.",
        "code": "jnp.allclose(jnp.array([1e6, 2e6, 3e6]),\n             jnp.array([1.00008e6, 2.00008e7, 3.00008e8]), rtol=1e3)"
      },
      {
        "description": "Compares two arrays with atol = 1e3. These arrays are considered close.",
        "code": "jnp.allclose(jnp.array([1e6, 2e6, 3e6]),\n             jnp.array([1.00001e6, 2.00002e6, 3.00009e6]), atol=1e3)"
      },
      {
        "description": "Compares two arrays containing NaNs with equal_nan = True. NaNs are considered equal.",
        "code": "jnp.allclose(jnp.array([jnp.nan, 1, 2]),\n             jnp.array([jnp.nan, 1, 2]), equal_nan=True)"
      }
    ]
  },
  {
    "title": "Alias of jax.numpy.max()",
    "concepts": [
      "This is an alias for the jax.numpy.max() function.",
      "The function likely calculates the maximum value within an array.",
      "It accepts arguments such as the input array (a), axis, output array (out), keepdims, initial value, and a where condition."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.min() Alias",
    "concepts": [
      "Alias of jax.numpy.min().",
      "Function takes an ArrayLike object 'a' as input.",
      "Function takes an optional 'axis' argument of type Axis or None.",
      "Function takes an optional 'out' argument of type None.",
      "Function takes a boolean argument 'keepdims'.",
      "Function takes an optional 'initial' argument of type ArrayLike or None.",
      "Function takes an optional 'where' argument of type ArrayLike or None.",
      "Function returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jnp.angle()",
    "concepts": [
      "The jnp.angle() function returns the angle of a complex number or array.",
      "It is a JAX implementation of numpy.angle().",
      "The input can be a single complex number or an array of complex numbers.",
      "The function can return the angle in radians (default) or degrees.",
      "The output is an array of the same shape as the input, with dtype float."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with a Single Complex Number",
    "concepts": [
      "Demonstrates using jnp.angle() with a single complex number.",
      "The function returns the angle in radians."
    ],
    "code_examples": [
      {
        "description": "Calculating the angle of a single complex number.",
        "code": "z1 = 2 + 3j\njnp.angle(z1)"
      },
      {
        "description": "Calculating the angle of a single complex number (repeated).",
        "code": "z1 = 2 + 3j\njnp.angle(z1)"
      }
    ]
  },
  {
    "title": "Examples with a Complex Number Array",
    "concepts": [
      "Demonstrates using jnp.angle() with an array of complex numbers.",
      "jnp.printoptions is used to format the output for better readability.",
      "The function returns an array of angles corresponding to each complex number."
    ],
    "code_examples": [
      {
        "description": "Calculating the angles of elements in a complex number array.",
        "code": "import jax.numpy as jnp\n\nz2 = jnp.array([[1 + 3j, 2 - 5j],\n                [4 - 3j, 3 + 2j]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.angle(z2))"
      },
      {
        "description": "Calculating the angles of elements in a complex number array (repeated).",
        "code": "import jax.numpy as jnp\n\nz2 = jnp.array([[1 + 3j, 2 - 5j],\n                [4 - 3j, 3 + 2j]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.angle(z2))"
      }
    ]
  },
  {
    "title": "Example with Degrees",
    "concepts": [
      "Demonstrates using jnp.angle() with the deg=True parameter.",
      "The function returns the angles in degrees instead of radians."
    ],
    "code_examples": [
      {
        "description": "Calculating the angles of elements in a complex number array, returning degrees.",
        "code": "import jax.numpy as jnp\n\nz2 = jnp.array([[1 + 3j, 2 - 5j],\n                [4 - 3j, 3 + 2j]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.angle(z2, deg=True))"
      },
      {
        "description": "Calculating the angles of elements in a complex number array, returning degrees (repeated).",
        "code": "import jax.numpy as jnp\n\nz2 = jnp.array([[1 + 3j, 2 - 5j],\n                [4 - 3j, 3 + 2j]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.angle(z2, deg=True))"
      }
    ]
  },
  {
    "title": "Overview of jnp.any()",
    "concepts": [
      "jnp.any() tests if any array elements along a given axis evaluate to True.",
      "It is a JAX implementation of numpy.any().",
      "It accepts an input array, axis, keepdims, and where arguments.",
      "The function returns an array of boolean values."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.any()",
    "concepts": [
      "By default, jnp.any tests along all the axes.",
      "The example demonstrates the basic usage of jnp.any() on a 2D boolean array without specifying an axis."
    ],
    "code_examples": [
      {
        "description": "Tests if any element in the 2D array 'x' is True.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\njnp.any(x)"
      },
      {
        "description": "Tests if any element in the 2D array 'x' is True.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\njnp.any(x)"
      }
    ]
  },
  {
    "title": "Specifying the Axis",
    "concepts": [
      "The axis argument specifies the axis along which to perform the 'any' test.",
      "The example demonstrates how to test along axis 0."
    ],
    "code_examples": [
      {
        "description": "Tests if any element along axis 0 of the 2D array 'x' is True.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\njnp.any(x, axis=0)"
      },
      {
        "description": "Tests if any element along axis 0 of the 2D array 'x' is True.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\njnp.any(x, axis=0)"
      }
    ]
  },
  {
    "title": "Using keepdims",
    "concepts": [
      "The keepdims argument, if True, preserves the dimensions of the input array in the output.",
      "The example demonstrates how to use keepdims to maintain the original number of dimensions."
    ],
    "code_examples": [
      {
        "description": "Tests if any element along axis 0 of the 2D array 'x' is True, keeping the dimensions of the original array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\njnp.any(x, axis=0, keepdims=True)"
      },
      {
        "description": "Tests if any element along axis 0 of the 2D array 'x' is True, keeping the dimensions of the original array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\njnp.any(x, axis=0, keepdims=True)"
      }
    ]
  },
  {
    "title": "Conditional Testing with 'where'",
    "concepts": [
      "The 'where' argument allows for conditional testing of elements.",
      "Only elements where the 'where' array is True are considered in the test."
    ],
    "code_examples": [
      {
        "description": "Tests if any element along axis 0 of the 2D array 'x' is True, considering only elements where the corresponding 'where' array element is True.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 1, 0, 1],\n                   [1, 0, 1, 0]], dtype=bool)\n\njnp.any(x, axis=0, keepdims=True, where=where)"
      },
      {
        "description": "Tests if any element along axis 0 of the 2D array 'x' is True, considering only elements where the corresponding 'where' array element is True.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[True, True, True, False],\n               [True, False, True, False],\n               [True, True, False, False]])\n\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 1, 0, 1],\n                   [1, 0, 1, 0]], dtype=bool)\n\njnp.any(x, axis=0, keepdims=True, where=where)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.append()",
    "concepts": [
      "The function appends values to the end of an array.",
      "It is a JAX implementation of numpy.append().",
      "The 'arr' argument is the original array.",
      "The 'values' argument contains the values to append.",
      "The 'axis' argument specifies the axis along which to append.",
      "If axis is None, both arrays are flattened before appending."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Appending Examples",
    "concepts": [
      "Demonstrates appending one 1D array to another.",
      "The result is a new 1D array containing all elements."
    ],
    "code_examples": [
      {
        "description": "Appends array 'b' to array 'a'.",
        "code": "a = jnp.array([1, 2, 3])\nb = jnp.array([4, 5, 6])\njnp.append(a, b)"
      },
      {
        "description": "Appends array 'b' to array 'a'.",
        "code": "a = jnp.array([1, 2, 3])\nb = jnp.array([4, 5, 6])\njnp.append(a, b)"
      }
    ]
  },
  {
    "title": "Appending Along a Specific Axis",
    "concepts": [
      "Illustrates appending arrays along a specified axis (axis=0).",
      "The arrays must have compatible shapes along other axes."
    ],
    "code_examples": [
      {
        "description": "Appends array 'b' to array 'a' along axis 0.",
        "code": "a = jnp.array([[1, 2],\n              [3, 4]])\nb = jnp.array([[5, 6]])\njnp.append(a, b, axis=0)"
      },
      {
        "description": "Appends array 'b' to array 'a' along axis 0.",
        "code": "a = jnp.array([[1, 2],\n              [3, 4]])\nb = jnp.array([[5, 6]])\njnp.append(a, b, axis=0)"
      }
    ]
  },
  {
    "title": "Appending Along a Trailing Axis",
    "concepts": [
      "Demonstrates appending arrays along a trailing axis (axis=1).",
      "The arrays are appended column-wise."
    ],
    "code_examples": [
      {
        "description": "Appends array 'b' to array 'a' along axis 1.",
        "code": "a = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\nb = jnp.array([[7],\n              [8]])\njnp.append(a, b, axis=1)"
      },
      {
        "description": "Appends array 'b' to array 'a' along axis 1.",
        "code": "a = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\nb = jnp.array([[7],\n              [8]])\njnp.append(a, b, axis=1)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.apply_along_axis",
    "concepts": [
      "Applies a function to 1D slices of an array along a specified axis.",
      "JAX implementation uses jax.vmap for efficiency.",
      "The func1d must be compatible with jax.vmap.",
      "Additional arguments can be passed to func1d via apply_along_axis.",
      "Similar functionalities can be achieved through jax.vmap, jax.numpy.apply_over_axes and jax.numpy.vectorize."
    ],
    "code_examples": []
  },
  {
    "title": "Two-Dimensional Array Example",
    "concepts": [
      "Demonstrates applying a function row-wise and column-wise to a 2D array.",
      "Shows how to compute the sum of squares along different axes."
    ],
    "code_examples": [
      {
        "description": "Applies a function to calculate the sum of squares along axis 0 and axis 1 of a 2D array.",
        "code": "import jax.numpy as jnp\nimport jax\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\ndef func1d(x):\n    return jnp.sum(x**2)\n\nprint(jnp.apply_along_axis(func1d, 0, x))\nprint(jnp.apply_along_axis(func1d, 1, x))"
      }
    ]
  },
  {
    "title": "Equivalence with jax.vmap for 2D inputs",
    "concepts": [
      "Illustrates the equivalent functionality of apply_along_axis using jax.vmap for 2D arrays.",
      "Highlights the difference in axis specification between apply_along_axis and jax.vmap."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the equivalence of apply_along_axis with vmap for 2D arrays, showing how to achieve the same results by mapping along different axes using vmap.",
        "code": "import jax.numpy as jnp\nimport jax\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\ndef func1d(x):\n    return jnp.sum(x**2)\n\nprint(jax.vmap(func1d, in_axes=1)(x))\nprint(jax.vmap(func1d, in_axes=0)(x))"
      }
    ]
  },
  {
    "title": "Three-Dimensional Array Example",
    "concepts": [
      "Extends the application of apply_along_axis to a 3D array.",
      "Demonstrates the equivalence of apply_along_axis with nested jax.vmap calls."
    ],
    "code_examples": [
      {
        "description": "Applies a function along axis 2 of a 3D array and shows its equivalent implementation using nested jax.vmap calls.",
        "code": "import jax.numpy as jnp\nimport jax\n\nx_3d = jnp.arange(24).reshape(2, 3, 4)\n\ndef func1d(x):\n    return jnp.sum(x**2)\n\nprint(jnp.apply_along_axis(func1d, 2, x_3d))\nprint(jax.vmap(jax.vmap(func1d))(x_3d))"
      }
    ]
  },
  {
    "title": "Passing Additional Arguments to the Applied Function",
    "concepts": [
      "Shows how to pass additional positional or keyword arguments to the function applied by apply_along_axis.",
      "Demonstrates passing an 'exponent' argument to a function that calculates the sum of x raised to that exponent."
    ],
    "code_examples": [
      {
        "description": "Demonstrates passing an additional 'exponent' argument to the func1d function via apply_along_axis.",
        "code": "import jax.numpy as jnp\nimport jax\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\ndef func1d(x, exponent):\n    return jnp.sum(x**exponent)\n\nprint(jnp.apply_along_axis(func1d, 0, x, exponent=3))"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.apply_over_axes",
    "concepts": [
      "The function `jax.numpy.apply_over_axes` applies a function repeatedly over specified axes of an array.",
      "It mimics the behavior of NumPy's `apply_over_axes` function.",
      "The function argument `func` should accept an array and an axis index, returning an array with dimension either equal to the input array's dimension or one less.",
      "The `axes` argument is a sequence of axis indices to apply the function over.",
      "It has similar semantics to associative jax.numpy reductions over one or more axes with keepdims=True."
    ],
    "code_examples": [
      {
        "description": "Example showcasing the usage of `jnp.apply_over_axes` with `jnp.sum` along axis 0, demonstrating similar behavior to `jnp.sum` with `keepdims=True`.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\n\njnp.apply_over_axes(jnp.sum, x, [0])\n\njnp.sum(x, [0], keepdims=True)"
      },
      {
        "description": "Example showcasing the usage of `jnp.apply_over_axes` with `jnp.min` along axis 1, demonstrating similar behavior to `jnp.min` with `keepdims=True`.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\n\njnp.apply_over_axes(jnp.min, x, [1])\n\njnp.min(x, [1], keepdims=True)"
      },
      {
        "description": "Example showcasing the usage of `jnp.apply_over_axes` with `jnp.prod` along axes 0 and 1, demonstrating similar behavior to `jnp.prod` with `keepdims=True`.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\n\njnp.apply_over_axes(jnp.prod, x, [0, 1])\n\njnp.prod(x, [0, 1], keepdims=True)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.arange",
    "concepts": [
      "jnp.arange() creates an array of evenly spaced values.",
      "It is a JAX implementation of numpy.arange().",
      "It is implemented using jax.lax.iota().",
      "It can be called with one, two, or three positional arguments, similar to Python's range().",
      "The starting value is inclusive, and the stop value is exclusive.",
      "The default step size is 1."
    ],
    "code_examples": []
  },
  {
    "title": "Usage and Parameters of jnp.arange",
    "concepts": [
      "The function can take start, stop, and step arguments.",
      "The dtype of the resulting array can be specified.",
      "The device or sharding can be specified for the created array.",
      "The function returns an array of evenly-spaced values."
    ],
    "code_examples": []
  },
  {
    "title": "Precision Considerations with Floating-Point Steps",
    "concepts": [
      "Using arange with a floating-point step can lead to precision errors.",
      "This is especially true with lower-precision data types.",
      "It's better to generate a range of integers and scale them to avoid precision errors."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the potential for precision errors when using a floating-point step size.",
        "code": "jnp.arange(-1, 1, 0.01, dtype='bfloat16')"
      },
      {
        "description": "Shows the more accurate approach of generating a sequence of integers and scaling them.",
        "code": "(jnp.arange(-100, 100) * 0.01).astype('bfloat16')"
      }
    ]
  },
  {
    "title": "Examples of jnp.arange Usage",
    "concepts": [
      "Demonstrates the single-argument usage with stop value.",
      "Shows the effect of passing a floating-point stop value.",
      "Illustrates the two-argument usage with start and stop values.",
      "Provides an example of the three-argument usage with start, stop, and step values."
    ],
    "code_examples": [
      {
        "description": "Single argument version specifying only the stop value.",
        "code": "jnp.arange(4)"
      },
      {
        "description": "Passing a floating-point stop value leads to a floating-point result.",
        "code": "jnp.arange(4.0)"
      },
      {
        "description": "Two-argument version specifies start and stop, with step=1.",
        "code": "jnp.arange(1, 6)"
      },
      {
        "description": "Three-argument version specifies start, stop, and step.",
        "code": "jnp.arange(0, 2, 0.5)"
      }
    ]
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.linspace() generates a fixed number of evenly-spaced values.",
      "jax.lax.iota() directly generates integer sequences in XLA."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jnp.arccos",
    "concepts": [
      "Computes the element-wise inverse trigonometric cosine of an input array.",
      "It is a JAX implementation of numpy.arccos.",
      "The input can be an array or scalar.",
      "The output is an array containing the inverse trigonometric cosine of each element, in radians, within the range [0, pi].",
      "The output dtype is promoted to inexact.",
      "jnp.arccos returns NaN when the input is real-valued and outside the interval [-1, 1].",
      "It follows the same branch cut convention as numpy.arccos for complex inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Real-valued Input Example",
    "concepts": [
      "Demonstrates the usage of jnp.arccos with a real-valued array.",
      "Illustrates the behavior of jnp.arccos when the input is outside the range [-1, 1], resulting in NaN values."
    ],
    "code_examples": [
      {
        "description": "Calculates the inverse cosine of a real-valued array, demonstrating the output for values within and outside the domain [-1, 1].",
        "code": "x = jnp.array([-2, -1, -0.5, 0, 0.5, 1, 2])\nwith jnp.printoptions(precision=3, suppress=True):\n  jnp.arccos(x)"
      },
      {
        "description": "Calculates the inverse cosine of a real-valued array, demonstrating the output for values within and outside the domain [-1, 1].",
        "code": "x = jnp.array([-2, -1, -0.5, 0, 0.5, 1, 2])\nwith jnp.printoptions(precision=3, suppress=True):\n  jnp.arccos(x)"
      }
    ]
  },
  {
    "title": "Complex Input Example",
    "concepts": [
      "Shows the usage of jnp.arccos with a complex number as input.",
      "Demonstrates the output format for complex inputs."
    ],
    "code_examples": [
      {
        "description": "Calculates the inverse cosine of a complex number and prints the result with specified precision.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n  jnp.arccos(4 - 1j)"
      },
      {
        "description": "Calculates the inverse cosine of a complex number and prints the result with specified precision.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n  jnp.arccos(4 - 1j)"
      }
    ]
  },
  {
    "title": "Definition and Usage",
    "concepts": [
      "The function calculates the element-wise inverse of the hyperbolic cosine of the input.",
      "It's a JAX implementation of numpy.arccosh.",
      "The formula for inverse hyperbolic cosine is arccosh(x) = ln(x + sqrt(x^2 - 1)).",
      "The input is an array-like object.",
      "The output is an array of the same shape as the input, with the inverse hyperbolic cosine of each element.",
      "The output dtype promotes to inexact."
    ],
    "code_examples": []
  },
  {
    "title": "Important Notes",
    "concepts": [
      "jnp.arccosh returns NaN for real values in the range [-inf, 1).",
      "jnp.arccosh follows the branch cut convention of numpy.arccosh for complex inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.cosh() computes the element-wise hyperbolic cosine of the input.",
      "jax.numpy.arcsinh() computes the element-wise inverse of hyperbolic sine of the input.",
      "jax.numpy.arctanh() computes the element-wise inverse of hyperbolic tangent of the input."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Real-valued Arrays",
    "concepts": [
      "Demonstrates jnp.arccosh with a real-valued array.",
      "Illustrates the output, including NaN values for inputs less than 1."
    ],
    "code_examples": [
      {
        "description": "Computes the inverse hyperbolic cosine of a real-valued array, printing the result with a specified precision and suppressing scientific notation.",
        "code": "x = jnp.array([[1, 3, -4],\n               [-5, 2, 7]])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.arccosh(x)"
      },
      {
        "description": "Computes the inverse hyperbolic cosine of a real-valued array, printing the result with a specified precision and suppressing scientific notation.",
        "code": "x = jnp.array([[1, 3, -4],\n               [-5, 2, 7]])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.arccosh(x)"
      }
    ]
  },
  {
    "title": "Examples with Complex-valued Arrays",
    "concepts": [
      "Demonstrates jnp.arccosh with a complex-valued array.",
      "Illustrates the output for complex inputs."
    ],
    "code_examples": [
      {
        "description": "Computes the inverse hyperbolic cosine of a complex-valued array, printing the result with a specified precision and suppressing scientific notation.",
        "code": "x1 = jnp.array([-jnp.inf + 0j, 1 + 2j, -5 + 0j])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.arccosh(x1)"
      },
      {
        "description": "Computes the inverse hyperbolic cosine of a complex-valued array, printing the result with a specified precision and suppressing scientific notation.",
        "code": "x1 = jnp.array([-jnp.inf + 0j, 1 + 2j, -5 + 0j])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.arccosh(x1)"
      }
    ]
  },
  {
    "title": "Description of jnp.arcsin",
    "concepts": [
      "Calculates the element-wise inverse of the trigonometric sine.",
      "It's a JAX implementation of numpy.arcsin.",
      "The input can be an array or a scalar.",
      "The output is an array containing the inverse trigonometric sine of each element, in radians within the range [-pi/2, pi/2].",
      "The output data type is promoted to inexact dtype.",
      "jnp.arcsin returns nan when x is real-valued and not in the closed interval [-1, 1].",
      "jnp.arcsin follows the branch cut convention of numpy.arcsin for complex inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.arcsin with real inputs",
    "concepts": [
      "Demonstrates the usage of jnp.arcsin with a jax array as input.",
      "Shows how values outside the range [-1, 1] result in 'nan'.",
      "Demonstrates the use of jnp.printoptions for formatting the output."
    ],
    "code_examples": [
      {
        "description": "Calculates the arcsin of a jax array with values outside the range [-1, 1].",
        "code": "x = jnp.array([-2, -1, -0.5, 0, 0.5, 1, 2])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.arcsin(x)\n"
      },
      {
        "description": "Calculates the arcsin of a jax array with values outside the range [-1, 1].",
        "code": "x = jnp.array([-2, -1, -0.5, 0, 0.5, 1, 2])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.arcsin(x)"
      }
    ]
  },
  {
    "title": "Examples of jnp.arcsin with complex inputs",
    "concepts": [
      "Illustrates the behavior of jnp.arcsin when the input is a complex number.",
      "Demonstrates the output for a complex input value."
    ],
    "code_examples": [
      {
        "description": "Calculates the arcsin of a complex number.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    jnp.arcsin(3 + 4j)"
      },
      {
        "description": "Calculates the arcsin of a complex number.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    jnp.arcsin(3 + 4j)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.arcsinh",
    "concepts": [
      "Calculates the element-wise inverse hyperbolic sine of an input array.",
      "JAX implementation of numpy.arcsinh.",
      "The inverse hyperbolic sine is defined as arcsinh(x) = ln(x + sqrt(1 + x^2)).",
      "The input can be an array or scalar.",
      "Returns an array of the same shape as the input, with elements representing the inverse hyperbolic sine.",
      "The output data type is inexact.",
      "jnp.arcsinh returns nan for values outside the range (-inf, inf).",
      "jnp.arcsinh follows the branch cut convention of numpy.arcsinh for complex inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Real-valued Arrays",
    "concepts": [
      "Demonstrates the usage of jnp.arcsinh with a real-valued JAX array.",
      "Shows how to format the output using jnp.printoptions for precision and suppression."
    ],
    "code_examples": [
      {
        "description": "Calculates the element-wise inverse hyperbolic sine of a real-valued JAX array and prints the result with specified formatting.",
        "code": "x = jnp.array([[-2, 3, 1],\n              [4, 9, -5]])\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.arcsinh(x))"
      },
      {
        "description": "Calculates the element-wise inverse hyperbolic sine of a real-valued JAX array and prints the result with specified formatting.",
        "code": "x = jnp.array([[-2, 3, 1],\n              [4, 9, -5]])\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.arcsinh(x))"
      }
    ]
  },
  {
    "title": "Examples with Complex-valued Arrays",
    "concepts": [
      "Demonstrates the usage of jnp.arcsinh with a complex-valued JAX array.",
      "Shows how to calculate the inverse hyperbolic sine of complex numbers.",
      "The result is a complex-valued array."
    ],
    "code_examples": [
      {
        "description": "Calculates the element-wise inverse hyperbolic sine of a complex-valued JAX array and prints the result with specified formatting.",
        "code": "x1 = jnp.array([4-3j, 2j])\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.arcsinh(x1))"
      },
      {
        "description": "Calculates the element-wise inverse hyperbolic sine of a complex-valued JAX array and prints the result with specified formatting.",
        "code": "x1 = jnp.array([4-3j, 2j])\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.arcsinh(x1))"
      }
    ]
  },
  {
    "title": "Description of jnp.arctan",
    "concepts": [
      "Computes the element-wise inverse trigonometric tangent of an input array.",
      "The input can be a scalar or an array-like object.",
      "The output array contains angles in radians, within the range [-pi/2, pi/2].",
      "The function promotes to inexact dtype.",
      "It follows NumPy's branch cut convention for complex inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Examples using jnp.arctan",
    "concepts": [
      "Demonstrates the application of jnp.arctan to a jax array.",
      "Illustrates the use of jnp.printoptions for formatted output.",
      "Shows the behavior of jnp.arctan with infinite and finite inputs.",
      "Demonstrates the computation of the inverse tangent for complex numbers."
    ],
    "code_examples": [
      {
        "description": "Example of applying jnp.arctan to a jax array with different values including infinity and printing the result with specific precision.",
        "code": "x = jnp.array([-jnp.inf, -20, -1, 0, 1, 20, jnp.inf])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.arctan(x)"
      },
      {
        "description": "Example of applying jnp.arctan to a jax array with different values including infinity and printing the result with specific precision. (Duplicated in source doc)",
        "code": "x = jnp.array([-jnp.inf, -20, -1, 0, 1, 20, jnp.inf])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.arctan(x)"
      },
      {
        "description": "Example of applying jnp.arctan to a complex number and printing the result with specific precision.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    jnp.arctan(2 + 7j)"
      },
      {
        "description": "Example of applying jnp.arctan to a complex number and printing the result with specific precision. (Duplicated in source doc)",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    jnp.arctan(2 + 7j)"
      }
    ]
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.tan(): Computes the trigonometric tangent of each element of the input.",
      "jax.numpy.arcsin() and jax.numpy.asin(): Computes the inverse of trigonometric sine of each element of the input.",
      "jax.numpy.arccos() and jax.numpy.atan(): Computes the inverse of trigonometric cosine of each element of the input."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to arctan2",
    "concepts": [
      "arctan2 computes the arctangent of x1/x2, considering the correct quadrant.",
      "It is a JAX implementation of numpy.arctan2().",
      "x1 is the numerator array, and x2 is the denominator array.",
      "x2 should be broadcast-compatible with x1.",
      "It returns the elementwise arctangent of x1 / x2, tracking the correct quadrant."
    ],
    "code_examples": []
  },
  {
    "title": "Using arctan2 to reconstruct angles",
    "concepts": [
      "arctan2 can be used to reconstruct angles from (x, y) coordinates on a unit circle.",
      "The simple arctan approach (arctan(y/x)) loses information about the quadrant.",
      "arctan2 addresses the ambiguity of y/x by considering the signs of both x and y.",
      "arctan2 returns values between -pi and +pi inclusive."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the issue of using arctan for angle reconstruction and how arctan2 solves it.",
        "code": "import jax.numpy as jnp\n\ntheta = jnp.linspace(-jnp.pi, jnp.pi, 9)\nwith jnp.printoptions(precision=2, suppress=True):\n    print(theta)\n\nx, y = jnp.cos(theta), jnp.sin(theta)\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.arctan(y / x))\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.arctan2(y, x))"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.argmax()",
    "concepts": [
      "Returns the index of the maximum value in an array.",
      "It is a JAX implementation of numpy.argmax().",
      "The input array is specified by the 'a' parameter.",
      "The axis along which to find the maximum value can be specified using the 'axis' parameter.",
      "If 'axis' is not specified, the input array is flattened.",
      "The 'out' parameter is unused by JAX.",
      "The 'keepdims' parameter determines whether the output array retains the same number of dimensions as the input.",
      "jax.numpy.argmin() returns the index of the minimum value.",
      "jax.numpy.nanargmax() computes argmax while ignoring NaN values."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.argmax() usage",
    "concepts": [
      "Demonstrates finding the index of the maximum value in a 1D array.",
      "Shows how to specify the axis when finding the maximum value in a 2D array.",
      "Illustrates the usage of the 'keepdims' parameter to maintain the original number of dimensions."
    ],
    "code_examples": [
      {
        "description": "Finds the index of the maximum value in a 1D array.",
        "code": "x = jnp.array([1, 3, 5, 4, 2])\njnp.argmax(x)"
      },
      {
        "description": "Finds the index of the maximum value in a 1D array.",
        "code": "x = jnp.array([1, 3, 5, 4, 2])\njnp.argmax(x)"
      },
      {
        "description": "Finds the index of the maximum value along axis 1 in a 2D array.",
        "code": "x = jnp.array([[1, 3, 2],\n                [5, 4, 1]])\njnp.argmax(x, axis=1)"
      },
      {
        "description": "Finds the index of the maximum value along axis 1 in a 2D array.",
        "code": "x = jnp.array([[1, 3, 2],\n                [5, 4, 1]])\njnp.argmax(x, axis=1)"
      },
      {
        "description": "Finds the index of the maximum value along axis 1 in a 2D array, keeping the dimensions.",
        "code": "x = jnp.array([[1, 3, 2],\n                [5, 4, 1]])\njnp.argmax(x, axis=1, keepdims=True)"
      },
      {
        "description": "Finds the index of the maximum value along axis 1 in a 2D array, keeping the dimensions.",
        "code": "x = jnp.array([[1, 3, 2],\n                [5, 4, 1]])\njnp.argmax(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.argmin()",
    "concepts": [
      "Returns the index of the minimum value of an array.",
      "JAX implementation of numpy.argmin().",
      "The 'axis' parameter specifies the axis along which to find the minimum value.",
      "If axis is not specified, the input array is flattened.",
      "The 'keepdims' parameter determines whether the output array retains the same number of dimensions as the input."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.argmin() usage",
    "concepts": [
      "Demonstrates finding the index of the minimum value in a 1D array.",
      "Demonstrates finding the index of the minimum value along a specific axis in a 2D array.",
      "Demonstrates using the 'keepdims' parameter to preserve the dimensions of the input array."
    ],
    "code_examples": [
      {
        "description": "Finds the index of the minimum value in a 1D array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 3, 5, 4, 2])\n\njnp.argmin(x)"
      },
      {
        "description": "Finds the index of the minimum value in a 1D array (duplicate example).",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 3, 5, 4, 2])\n\njnp.argmin(x)"
      },
      {
        "description": "Finds the index of the minimum value along axis 1 in a 2D array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 2],\n               [5, 4, 1]])\n\njnp.argmin(x, axis=1)"
      },
      {
        "description": "Finds the index of the minimum value along axis 1 in a 2D array (duplicate example).",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 2],\n               [5, 4, 1]])\n\njnp.argmin(x, axis=1)"
      },
      {
        "description": "Finds the index of the minimum value along axis 1 in a 2D array, keeping the dimensions.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 2],\n               [5, 4, 1]])\n\njnp.argmin(x, axis=1, keepdims=True)"
      },
      {
        "description": "Finds the index of the minimum value along axis 1 in a 2D array, keeping the dimensions (duplicate example).",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 2],\n               [5, 4, 1]])\n\njnp.argmin(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.argpartition",
    "concepts": [
      "Returns indices that partially sort an array.",
      "JAX implementation of numpy.argpartition.",
      "NaNs with the negative bit set are sorted to the beginning of the array.",
      "Requires the kth argument to be a static integer.",
      "Implemented via two calls to jax.lax.top_k().",
      "Consider using jax.lax.top_k() directly for top/bottom k values."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Example of jax.numpy.argpartition",
    "concepts": [
      "Demonstrates how to use jax.numpy.argpartition to find indices that partially sort an array.",
      "Shows how to partition an array around a given kth index.",
      "Illustrates that values before the kth index are smaller, and values after are larger.",
      "The order within the smaller and larger values is arbitrary."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the use of jnp.argpartition to find indices that partially sort an array. It then shows how to access the partitioned elements using the returned indices.",
        "code": "x = jnp.array([6, 8, 4, 3, 1, 9, 7, 5, 2, 3])\nkth = 4\nidx = jnp.argpartition(x, kth)\nprint(idx)"
      },
      {
        "description": "Example demonstrating the use of jnp.argpartition to find indices that partially sort an array. It then shows how to access the partitioned elements using the returned indices.",
        "code": "x = jnp.array([6, 8, 4, 3, 1, 9, 7, 5, 2, 3])\nkth = 4\nidx = jnp.argpartition(x, kth)\nprint(idx)"
      },
      {
        "description": "This example builds upon the previous one, extracting the smallest values, pivot value, and largest values from the partitioned array based on the indices returned by `jnp.argpartition`.",
        "code": "x = jnp.array([6, 8, 4, 3, 1, 9, 7, 5, 2, 3])\nkth = 4\nidx = jnp.argpartition(x, kth)\nx_partitioned = x[idx]\nsmallest_values = x_partitioned[:kth]\npivot_value = x_partitioned[kth]\nlargest_values = x_partitioned[kth + 1:]\nprint(smallest_values, pivot_value, largest_values)"
      },
      {
        "description": "This example builds upon the previous one, extracting the smallest values, pivot value, and largest values from the partitioned array based on the indices returned by `jnp.argpartition`.",
        "code": "x = jnp.array([6, 8, 4, 3, 1, 9, 7, 5, 2, 3])\nkth = 4\nidx = jnp.argpartition(x, kth)\nx_partitioned = x[idx]\nsmallest_values = x_partitioned[:kth]\npivot_value = x_partitioned[kth]\nlargest_values = x_partitioned[kth + 1:]\nprint(smallest_values, pivot_value, largest_values)"
      }
    ]
  },
  {
    "title": "Description of jnp.argsort",
    "concepts": [
      "Returns indices that sort an array.",
      "JAX implementation of numpy.argsort().",
      "The 'a' parameter is the array to sort.",
      "The 'axis' parameter specifies the axis along which to sort.",
      "The 'stable' parameter specifies whether a stable sort should be used.",
      "The 'descending' parameter specifies whether to sort in descending order.",
      "The 'kind' and 'order' parameters from NumPy are not supported or are deprecated in JAX.",
      "Returns an array of indices that sort the input array."
    ],
    "code_examples": []
  },
  {
    "title": "Simple 1-Dimensional Sort",
    "concepts": [
      "Demonstrates sorting a 1D JAX array using jnp.argsort.",
      "Shows how to retrieve the sorted array using the returned indices."
    ],
    "code_examples": [
      {
        "description": "Sorts a 1D JAX array and prints the sorted indices and the sorted array.",
        "code": "x = jnp.array([1, 3, 5, 4, 2, 1])\nindices = jnp.argsort(x)\nprint(indices)\nprint(x[indices])"
      },
      {
        "description": "Sorts a 1D JAX array and prints the sorted indices and the sorted array (repeated).",
        "code": "x = jnp.array([1, 3, 5, 4, 2, 1])\nindices = jnp.argsort(x)\nprint(indices)\nprint(x[indices])"
      }
    ]
  },
  {
    "title": "Sort Along the Last Axis of an Array",
    "concepts": [
      "Demonstrates sorting along a specific axis of a multi-dimensional array.",
      "Uses jnp.take_along_axis to construct the sorted array from the indices."
    ],
    "code_examples": [
      {
        "description": "Sorts a 2D JAX array along the last axis (axis=1) using jnp.argsort and jnp.take_along_axis to retrieve the sorted array.",
        "code": "x = jnp.array([[2, 1, 3],\n               [6, 4, 3]])\nindices = jnp.argsort(x, axis=1)\nprint(indices)\nprint(jnp.take_along_axis(x, indices, axis=1))"
      },
      {
        "description": "Sorts a 2D JAX array along the last axis (axis=1) using jnp.argsort and jnp.take_along_axis to retrieve the sorted array (repeated).",
        "code": "x = jnp.array([[2, 1, 3],\n               [6, 4, 3]])\nindices = jnp.argsort(x, axis=1)\nprint(indices)\nprint(jnp.take_along_axis(x, indices, axis=1))"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.numpy.sort(): return sorted values directly.",
      "jax.numpy.lexsort(): lexicographical sort of multiple arrays.",
      "jax.lax.sort(): lower-level function wrapping XLA\u2019s Sort operator."
    ],
    "code_examples": []
  },
  {
    "title": "Alias and Parameters",
    "concepts": [
      "This documentation refers to an alias of jax.numpy.round().",
      "The function takes an ArrayLike object as input, named 'a'.",
      "The function takes an integer 'decimals' as input.",
      "The function takes 'None' as input for the 'out' parameter.",
      "The function returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jnp.array",
    "concepts": [
      "Converts an object to a JAX array.",
      "It is a JAX implementation of numpy.array().",
      "It can handle various input types including JAX arrays, NumPy arrays, Python scalars, and Python collections.",
      "The dtype of the output array can be specified or inferred.",
      "The copy parameter controls whether to force a copy of the input.",
      "The ndmin parameter specifies the minimum number of dimensions in the output array.",
      "The device parameter specifies the Device or Sharding to which the created array will be committed."
    ],
    "code_examples": []
  },
  {
    "title": "Constructing JAX arrays from Python scalars",
    "concepts": [
      "JAX arrays can be constructed from Python scalars such as booleans, integers, floats, and complex numbers."
    ],
    "code_examples": [
      {
        "description": "Creating a JAX array from a boolean.",
        "code": "jnp.array(True)"
      },
      {
        "description": "Creating a JAX array from an integer.",
        "code": "jnp.array(42)"
      },
      {
        "description": "Creating a JAX array from a float.",
        "code": "jnp.array(3.5)"
      },
      {
        "description": "Creating a JAX array from a complex number.",
        "code": "jnp.array(1 + 1j)"
      },
      {
        "description": "Creating a JAX array from a boolean.",
        "code": "jnp.array(True)"
      },
      {
        "description": "Creating a JAX array from an integer.",
        "code": "jnp.array(42)"
      },
      {
        "description": "Creating a JAX array from a float.",
        "code": "jnp.array(3.5)"
      },
      {
        "description": "Creating a JAX array from a complex number.",
        "code": "jnp.array(1 + 1j)"
      }
    ]
  },
  {
    "title": "Constructing JAX arrays from Python collections",
    "concepts": [
      "JAX arrays can be constructed from Python collections like lists and tuples.",
      "The structure of the collection determines the shape of the resulting array."
    ],
    "code_examples": [
      {
        "description": "Creating a 1D JAX array from a list of integers.",
        "code": "jnp.array([1, 2, 3])"
      },
      {
        "description": "Creating a 2D JAX array from a list of tuples of integers.",
        "code": "jnp.array([(1, 2, 3), (4, 5, 6)])"
      },
      {
        "description": "Creating a JAX array from a range object.",
        "code": "jnp.array(range(5))"
      },
      {
        "description": "Creating a 1D JAX array from a list of integers.",
        "code": "jnp.array([1, 2, 3])"
      },
      {
        "description": "Creating a 2D JAX array from a list of tuples of integers.",
        "code": "jnp.array([(1, 2, 3), (4, 5, 6)])"
      },
      {
        "description": "Creating a JAX array from a range object.",
        "code": "jnp.array(range(5))"
      }
    ]
  },
  {
    "title": "Constructing JAX arrays from NumPy arrays",
    "concepts": [
      "JAX arrays can be constructed from NumPy arrays.",
      "NumPy arrays are converted to JAX arrays."
    ],
    "code_examples": [
      {
        "description": "Creating a JAX array from a NumPy array generated using linspace.",
        "code": "import numpy as np\njnp.array(np.linspace(0, 2, 5))"
      },
      {
        "description": "Creating a JAX array from a NumPy array generated using linspace.",
        "code": "import numpy as np\njnp.array(np.linspace(0, 2, 5))"
      }
    ]
  },
  {
    "title": "Constructing JAX arrays via the Python buffer interface",
    "concepts": [
      "JAX arrays can be constructed from objects that implement the Python buffer interface.",
      "The array module in Python can be used to create such objects."
    ],
    "code_examples": [
      {
        "description": "Creating a JAX array from a Python array.array object.",
        "code": "from array import array\npybuffer = array('i', [2, 3, 5, 7])\njnp.array(pybuffer)"
      },
      {
        "description": "Creating a JAX array from a Python array.array object.",
        "code": "from array import array\npybuffer = array('i', [2, 3, 5, 7])\njnp.array(pybuffer)"
      }
    ]
  },
  {
    "title": "Description and Usage of jax.numpy.array_equal",
    "concepts": [
      "The function jax.numpy.array_equal() checks if two arrays are element-wise equal.",
      "It is a JAX implementation of numpy.array_equal().",
      "The function takes two array-like objects (a1 and a2) as input.",
      "The equal_nan parameter determines if NaNs are considered equal (default is False).",
      "The function returns a boolean scalar array.",
      "The function can be used to determine element-wise equality of arrays of the same shape.",
      "The function returns False if arrays have different shapes.",
      "By default, NaN values are not considered equal.",
      "Setting equal_nan=True will consider NaN values as equal."
    ],
    "code_examples": [
      {
        "description": "Demonstrates array_equal with equal arrays.",
        "code": "jnp.array_equal(jnp.array([1, 2, 3]), jnp.array([1, 2, 3]))"
      },
      {
        "description": "Demonstrates array_equal with arrays of different shapes.",
        "code": "jnp.array_equal(jnp.array([1, 2, 3]), jnp.array([1, 2]))"
      },
      {
        "description": "Demonstrates array_equal with arrays that have different values.",
        "code": "jnp.array_equal(jnp.array([1, 2, 3]), jnp.array([1, 2, 4]))"
      },
      {
        "description": "Demonstrates array_equal with NaN values, where equal_nan is False (default).",
        "code": "jnp.array_equal(jnp.array([1, 2, float('nan')]),\n... jnp.array([1, 2, float('nan')]))"
      },
      {
        "description": "Demonstrates array_equal with NaN values, where equal_nan is True.",
        "code": "jnp.array_equal(jnp.array([1, 2, float('nan')]),\n... jnp.array([1, 2, float('nan')]),\nequal_nan=True)"
      },
      {
        "description": "Demonstrates array_equal with equal arrays.",
        "code": "jnp.array_equal(jnp.array([1, 2, 3]), jnp.array([1, 2, 3]))"
      },
      {
        "description": "Demonstrates array_equal with arrays of different shapes.",
        "code": "jnp.array_equal(jnp.array([1, 2, 3]), jnp.array([1, 2]))"
      },
      {
        "description": "Demonstrates array_equal with arrays that have different values.",
        "code": "jnp.array_equal(jnp.array([1, 2, 3]), jnp.array([1, 2, 4]))"
      },
      {
        "description": "Demonstrates array_equal with NaN values, where equal_nan is False (default).",
        "code": "jnp.array_equal(jnp.array([1, 2, float('nan')]),\n... jnp.array([1, 2, float('nan')]))"
      },
      {
        "description": "Demonstrates array_equal with NaN values, where equal_nan is True.",
        "code": "jnp.array_equal(jnp.array([1, 2, float('nan')]),\n... jnp.array([1, 2, float('nan')]),\nequal_nan=True)"
      }
    ]
  },
  {
    "title": "Description of jnp.array_equiv",
    "concepts": [
      "The function checks if two arrays are element-wise equal.",
      "It is a JAX implementation of numpy.array_equiv().",
      "The function returns False if the arrays cannot be broadcasted to the same shape.",
      "It takes two ArrayLike objects as input.",
      "It returns a boolean scalar array indicating element-wise equality after broadcasting."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.array_equiv usage",
    "concepts": [
      "Demonstrates how to use jnp.array_equiv() with different array inputs.",
      "Shows cases where the arrays are element-wise equal and not equal.",
      "Illustrates the broadcasting behavior of jnp.array_equiv()."
    ],
    "code_examples": [
      {
        "description": "Compares two equal 1D arrays.",
        "code": "jnp.array_equiv(jnp.array([1, 2, 3]), jnp.array([1, 2, 3]))"
      },
      {
        "description": "Compares two different 1D arrays.",
        "code": "jnp.array_equiv(jnp.array([1, 2, 3]), jnp.array([1, 2, 4]))"
      },
      {
        "description": "Compares a 2D array with a 1D array that can be broadcasted.",
        "code": "jnp.array_equiv(jnp.array([[1, 2, 3],[1, 2, 3]]), jnp.array([1, 2, 3]))"
      },
      {
        "description": "Compares two equal 1D arrays.",
        "code": "jnp.array_equiv(jnp.array([1, 2, 3]), jnp.array([1, 2, 3]))"
      },
      {
        "description": "Compares two different 1D arrays.",
        "code": "jnp.array_equiv(jnp.array([1, 2, 3]), jnp.array([1, 2, 4]))"
      },
      {
        "description": "Compares a 2D array with a 1D array that can be broadcasted.",
        "code": "jnp.array_equiv(jnp.array([[1, 2, 3],[1, 2, 3]]), jnp.array([1, 2, 3]))"
      }
    ]
  },
  {
    "title": "Description of numpy.array_repr",
    "concepts": [
      "The function returns the string representation of an array.",
      "It accepts an array as input.",
      "It has optional parameters to control the maximum line width, precision, and suppression of small values.",
      "The return value is a string representation of the array."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Examples",
    "concepts": [
      "Demonstrates the basic usage of `np.array_repr` with simple arrays.",
      "Shows how it represents masked arrays.",
      "Illustrates the representation of empty arrays with a specific data type."
    ],
    "code_examples": [
      {
        "description": "Shows how to represent a basic numpy array.",
        "code": "import numpy as np\nnp.array_repr(np.array([1, 2]))\n"
      },
      {
        "description": "Demonstrates how to represent a masked array.",
        "code": "import numpy as np\nnp.array_repr(np.ma.array([0.]))"
      },
      {
        "description": "Shows how an empty array with a specific datatype is represented.",
        "code": "import numpy as np\nnp.array_repr(np.array([], np.int32))"
      }
    ]
  },
  {
    "title": "Precision and Suppress Small Examples",
    "concepts": [
      "Demonstrates the usage of `precision` and `suppress_small` parameters.",
      "Shows how to control the floating-point precision in the string representation.",
      "Illustrates how to suppress small values close to zero.",
      "Uses the suppress_small argument to set values near zero to zero in the string representation."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of `precision` and `suppress_small` to format the array representation.",
        "code": "import numpy as np\nx = np.array([1e-6, 4e-7, 2, 3])\nnp.array_repr(x, precision=6, suppress_small=True)"
      },
      {
        "description": "Demonstrates the use of `precision` and `suppress_small` to format the array representation.",
        "code": "import numpy as np\nx = np.array([1e-6, 4e-7, 2, 3])\nnp.array_repr(x, precision=6, suppress_small=True)"
      }
    ]
  },
  {
    "title": "Array Splitting with jax.numpy.array_split",
    "concepts": [
      "jax.numpy.array_split() splits an array into sub-arrays.",
      "It's a JAX implementation of numpy.array_split().",
      "array_split is equivalent to split but allows integer indices_or_sections that do not evenly divide the split axis.",
      "It can be used to split arrays along a specified axis.",
      "The function returns a list of arrays."
    ],
    "code_examples": [
      {
        "description": "Splitting a JAX array into 4 chunks.",
        "code": "x = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\nchunks = jnp.array_split(x, 4)\nprint(*chunks)"
      },
      {
        "description": "Splitting a JAX array into 4 chunks (repeated example).",
        "code": "x = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\nchunks = jnp.array_split(x, 4)\nprint(*chunks)"
      }
    ]
  },
  {
    "title": "Description of array_str function",
    "concepts": [
      "The array_str function returns a string representation of an array's data.",
      "It differs from array_repr by not including array type and data type information.",
      "It accepts an ndarray as input.",
      "It accepts optional parameters for controlling the output format: max_line_width, precision, and suppress_small."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of array_str function",
    "concepts": [
      "a (ndarray): Input array",
      "max_line_width (int, optional): Inserts newlines if text is longer than max_line_width. Defaults to numpy.get_printoptions()['linewidth']",
      "precision (int, optional): Floating point precision. Defaults to numpy.get_printoptions()['precision']",
      "suppress_small (bool, optional): Represent numbers 'very close' to zero as zero; default is False."
    ],
    "code_examples": []
  },
  {
    "title": "Related functions",
    "concepts": [
      "array2string is a related function",
      "array_repr is a related function",
      "set_printoptions is a related function"
    ],
    "code_examples": []
  },
  {
    "title": "Examples of array_str usage",
    "concepts": [
      "Demonstrates how to use array_str to convert a NumPy array to a string."
    ],
    "code_examples": [
      {
        "description": "Example showing the basic usage of array_str with a simple NumPy array.",
        "code": "import numpy as np\nnp.array_str(np.arange(3))\n'[0 1 2]'"
      },
      {
        "description": "Example showing the basic usage of array_str with a simple NumPy array. (Duplicate example).",
        "code": "import numpy as np\nnp.array_str(np.arange(3))\n'[0 1 2]'"
      }
    ]
  },
  {
    "title": "Overview of jnp.asarray",
    "concepts": [
      "Converts an object to a JAX array.",
      "JAX implementation of numpy.asarray().",
      "Handles JAX arrays, NumPy arrays, Python scalars, Python collections, objects with an __array__ method, and objects supporting the Python buffer protocol.",
      "Optionally specifies the dtype of the output array.",
      "The 'order' parameter is not implemented in JAX.",
      "Optionally specifies the copy mode (True, False, or None).",
      "Optionally specifies the Device or Sharding to which the created array will be committed.",
      "Returns a JAX array constructed from the input."
    ],
    "code_examples": []
  },
  {
    "title": "JAX array construction from Python scalars",
    "concepts": [
      "Demonstrates how to create JAX arrays from Python boolean, integer, float and complex scalars."
    ],
    "code_examples": [
      {
        "description": "Creating JAX arrays from Python boolean scalar.",
        "code": "jnp.asarray(True)\n# Output: Array(True, dtype=bool)"
      },
      {
        "description": "Creating JAX arrays from Python integer scalar.",
        "code": "jnp.asarray(42)\n# Output: Array(42, dtype=int32, weak_type=True)"
      },
      {
        "description": "Creating JAX arrays from Python float scalar.",
        "code": "jnp.asarray(3.5)\n# Output: Array(3.5, dtype=float32, weak_type=True)"
      },
      {
        "description": "Creating JAX arrays from Python complex scalar.",
        "code": "jnp.asarray(1 + 1j)\n# Output: Array(1.+1.j, dtype=complex64, weak_type=True)"
      },
      {
        "description": "Creating JAX arrays from Python boolean scalar.",
        "code": "jnp.asarray(True)\n# Output: Array(True, dtype=bool)"
      },
      {
        "description": "Creating JAX arrays from Python integer scalar.",
        "code": "jnp.asarray(42)\n# Output: Array(42, dtype=int32, weak_type=True)"
      },
      {
        "description": "Creating JAX arrays from Python float scalar.",
        "code": "jnp.asarray(3.5)\n# Output: Array(3.5, dtype=float32, weak_type=True)"
      },
      {
        "description": "Creating JAX arrays from Python complex scalar.",
        "code": "jnp.asarray(1 + 1j)\n# Output: Array(1.+1.j, dtype=complex64, weak_type=True)"
      }
    ]
  },
  {
    "title": "JAX array construction from Python collections",
    "concepts": [
      "Demonstrates how to create JAX arrays from Python lists and tuples."
    ],
    "code_examples": [
      {
        "description": "Creating a 1D JAX array from a list of integers.",
        "code": "jnp.asarray([1, 2, 3])\n# Output: Array([1, 2, 3], dtype=int32)"
      },
      {
        "description": "Creating a 2D JAX array from a list of tuples of integers.",
        "code": "jnp.asarray([(1, 2, 3), (4, 5, 6)])\n# Output: Array([[1, 2, 3],\n#               [4, 5, 6]], dtype=int32)"
      },
      {
        "description": "Creating a JAX array from a Python range object.",
        "code": "jnp.asarray(range(5))\n# Output: Array([0, 1, 2, 3, 4], dtype=int32)"
      },
      {
        "description": "Creating a 1D JAX array from a list of integers.",
        "code": "jnp.asarray([1, 2, 3])\n# Output: Array([1, 2, 3], dtype=int32)"
      },
      {
        "description": "Creating a 2D JAX array from a list of tuples of integers.",
        "code": "jnp.asarray([(1, 2, 3), (4, 5, 6)])\n# Output: Array([[1, 2, 3],\n#               [4, 5, 6]], dtype=int32)"
      },
      {
        "description": "Creating a JAX array from a Python range object.",
        "code": "jnp.asarray(range(5))\n# Output: Array([0, 1, 2, 3, 4], dtype=int32)"
      }
    ]
  },
  {
    "title": "JAX array construction from NumPy arrays",
    "concepts": [
      "Demonstrates how to create JAX arrays from NumPy arrays."
    ],
    "code_examples": [
      {
        "description": "Creating a JAX array from a NumPy array generated by linspace.",
        "code": "import numpy as np\n\njnp.asarray(np.linspace(0, 2, 5))\n# Output: Array([0. , 0.5, 1. , 1.5, 2. ], dtype=float32)"
      },
      {
        "description": "Creating a JAX array from a NumPy array generated by linspace.",
        "code": "import numpy as np\n\njnp.asarray(np.linspace(0, 2, 5))\n# Output: Array([0. , 0.5, 1. , 1.5, 2. ], dtype=float32)"
      }
    ]
  },
  {
    "title": "JAX array construction via Python buffer interface",
    "concepts": [
      "Demonstrates how to create JAX arrays using the Python buffer interface and the array module."
    ],
    "code_examples": [
      {
        "description": "Creating a JAX array from a Python buffer.",
        "code": "from array import array\n\npybuffer = array('i', [2, 3, 5, 7])\njnp.asarray(pybuffer)\n# Output: Array([2, 3, 5, 7], dtype=int32)"
      },
      {
        "description": "Creating a JAX array from a Python buffer.",
        "code": "from array import array\n\npybuffer = array('i', [2, 3, 5, 7])\njnp.asarray(pybuffer)\n# Output: Array([2, 3, 5, 7], dtype=int32)"
      }
    ]
  },
  {
    "title": "arcsin alias",
    "concepts": [
      "jax.numpy.arcsin() is an alias.",
      "The function takes an ArrayLike object as input, denoted as 'x'."
    ],
    "code_examples": []
  },
  {
    "title": "arcsinh Function",
    "concepts": [
      "The function is an alias of jax.numpy.arcsinh().",
      "The function takes an ArrayLike object as input.",
      "The input is represented by the variable x."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jax.numpy.astype()",
    "concepts": [
      "The function converts a JAX array to a specified data type (dtype).",
      "It uses jax.lax.convert_element_type() for the conversion.",
      "The behavior might slightly differ from numpy.astype() in some cases.",
      "Float-to-int and int-to-float casts are implementation-dependent.",
      "The function accepts ArrayLike input, a DTypeLike, a copy boolean, and optionally a device.",
      "The function returns an array with the same shape as the input, but with the specified dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of using jax.numpy.astype()",
    "concepts": [
      "Demonstrates converting an integer array to a float array.",
      "Shows the truncation behavior when converting a float array to an integer array."
    ],
    "code_examples": [
      {
        "description": "Converting an integer array to a float array",
        "code": "x = jnp.array([0, 1, 2, 3])\nprint(x)\nprint(x.astype('float32'))"
      },
      {
        "description": "Converting a float array to an integer array (truncation)",
        "code": "y = jnp.array([0.0, 0.5, 1.0])\nprint(y.astype(int)) # truncates fractional values"
      }
    ]
  },
  {
    "title": "Alias of jax.numpy.arctan()",
    "concepts": [
      "This function is an alias of jax.numpy.arctan().",
      "The function takes an ArrayLike object as input.",
      "The input is named 'x' and represents an array."
    ],
    "code_examples": []
  },
  {
    "title": "Alias of jax.numpy.arctanh()",
    "concepts": [
      "The document describes an alias for the jax.numpy.arctanh() function.",
      "The input 'x' is an ArrayLike object.",
      "The input 'x' represents an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Alias of jax.numpy.arctan2()",
    "concepts": [
      "This documentation refers to an alias of jax.numpy.arctan2().",
      "The function takes two ArrayLike arguments: x1 and x2.",
      "The function returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to atleast_1d",
    "concepts": [
      "Converts inputs to arrays with at least 1 dimension.",
      "JAX implementation of numpy.atleast_1d().",
      "Accepts zero or more array-like arguments.",
      "Arrays of shape () are converted to shape (1,).",
      "Other shapes are returned unchanged.",
      "Returns a list of arrays when multiple arguments are given."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Scalar Arguments",
    "concepts": [
      "Scalar arguments are converted to 1D, length-1 arrays."
    ],
    "code_examples": [
      {
        "description": "Demonstrates converting a scalar to a 1D array using jnp.atleast_1d().",
        "code": "x = jnp.float32(1.0)\njnp.atleast_1d(x)"
      },
      {
        "description": "Demonstrates converting a scalar to a 1D array using jnp.atleast_1d().",
        "code": "x = jnp.float32(1.0)\njnp.atleast_1d(x)"
      }
    ]
  },
  {
    "title": "Examples with Higher Dimensional Inputs",
    "concepts": [
      "Higher dimensional inputs are returned unchanged."
    ],
    "code_examples": [
      {
        "description": "Demonstrates that 1D array remains unchanged when passed to jnp.atleast_1d().",
        "code": "y = jnp.arange(4)\njnp.atleast_1d(y)"
      },
      {
        "description": "Demonstrates that 1D array remains unchanged when passed to jnp.atleast_1d().",
        "code": "y = jnp.arange(4)\njnp.atleast_1d(y)"
      }
    ]
  },
  {
    "title": "Examples with Multiple Arguments",
    "concepts": [
      "Multiple arguments can be passed to the function at once.",
      "A list of results is returned when multiple arguments are passed."
    ],
    "code_examples": [
      {
        "description": "Demonstrates passing two arguments to jnp.atleast_1d() and receiving a list of arrays.",
        "code": "jnp.atleast_1d(x, y)"
      },
      {
        "description": "Demonstrates passing two arguments to jnp.atleast_1d() and receiving a list of arrays.",
        "code": "jnp.atleast_1d(x, y)"
      }
    ]
  },
  {
    "title": "JAX implementation of atleast_2d",
    "concepts": [
      "Converts input(s) to arrays with at least 2 dimensions.",
      "Scalar inputs are converted to 2D arrays with shape (1, 1).",
      "1D arrays are converted to 2D arrays with shape (1, N).",
      "Arrays with more than one dimension are returned unchanged.",
      "Accepts multiple array-like inputs and returns a list of arrays."
    ],
    "code_examples": [
      {
        "description": "Demonstrates converting a scalar to a 2D array.",
        "code": "x = jnp.float32(1.0)\njnp.atleast_2d(x)"
      },
      {
        "description": "Demonstrates converting a 1D array to a 2D array.",
        "code": "y = jnp.arange(4)\njnp.atleast_2d(y)"
      },
      {
        "description": "Demonstrates that higher dimensional arrays are returned unchanged.",
        "code": "z = jnp.ones((2, 3))\njnp.atleast_2d(z)"
      },
      {
        "description": "Demonstrates passing multiple arguments to atleast_2d.",
        "code": "x = jnp.float32(1.0)\ny = jnp.arange(4)\njnp.atleast_2d(x, y)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.atleast_3d",
    "concepts": [
      "The function converts input(s) to arrays with at least 3 dimensions.",
      "Scalar inputs are converted to arrays of shape (1, 1, 1).",
      "1D arrays of shape (N,) are converted to shape (1, N, 1).",
      "2D arrays of shape (M, N) are converted to shape (M, N, 1).",
      "Arrays with more than 2 dimensions are returned unchanged.",
      "Multiple arguments can be passed to the function, returning a list of results."
    ],
    "code_examples": [
      {
        "description": "Converting a scalar to a 3D array.",
        "code": "x = jnp.float32(1.0)\njnp.atleast_3d(x)"
      },
      {
        "description": "Converting a 1D array to a 3D array and checking its shape.",
        "code": "y = jnp.arange(4)\njnp.atleast_3d(y).shape"
      },
      {
        "description": "Converting a 2D array to a 3D array and checking its shape.",
        "code": "z = jnp.ones((2, 3))\njnp.atleast_3d(z).shape"
      },
      {
        "description": "Passing multiple arguments to the function.",
        "code": "x = jnp.float32(1.0)\ny = jnp.arange(4)\nx3, y3 = jnp.atleast_3d(x, y)\nprint(x3)\nprint(y3)"
      }
    ]
  },
  {
    "title": "Introduction to Weighted Average in JAX",
    "concepts": [
      "Computes the weighted average of an array using JAX.",
      "The function is a JAX implementation of numpy.average().",
      "It takes an array, optional axis, optional weights, returned flag, and keepdims flag as input.",
      "It returns the average, or a tuple of (average, normalization) if returned is True.",
      "Weights must be broadcast-compatible with the input array."
    ],
    "code_examples": []
  },
  {
    "title": "Simple Average Example",
    "concepts": [
      "Demonstrates calculating the average of an array.",
      "Uses jnp.average() to compute the average.",
      "Shows the basic usage of the function without weights."
    ],
    "code_examples": [
      {
        "description": "Calculates the average of a JAX array.",
        "code": "x = jnp.array([1, 2, 3, 2, 4])\njnp.average(x)"
      },
      {
        "description": "Calculates the average of a JAX array.",
        "code": "x = jnp.array([1, 2, 3, 2, 4])\njnp.average(x)"
      }
    ]
  },
  {
    "title": "Weighted Average Example",
    "concepts": [
      "Demonstrates calculating the weighted average of an array.",
      "Uses the weights parameter to specify the weights for each element.",
      "Shows how the weights affect the resulting average."
    ],
    "code_examples": [
      {
        "description": "Calculates the weighted average of a JAX array using specified weights.",
        "code": "weights = jnp.array([2, 1, 3, 2, 2])\nx = jnp.array([1, 2, 3, 2, 4])\njnp.average(x, weights=weights)"
      },
      {
        "description": "Calculates the weighted average of a JAX array using specified weights.",
        "code": "weights = jnp.array([2, 1, 3, 2, 2])\nx = jnp.array([1, 2, 3, 2, 4])\njnp.average(x, weights=weights)"
      }
    ]
  },
  {
    "title": "Returning Average and Normalization",
    "concepts": [
      "Demonstrates returning both the average and the normalization factor (sum of weights).",
      "Uses the returned=True parameter to request both values.",
      "Shows how to access both the average and normalization from the returned tuple."
    ],
    "code_examples": [
      {
        "description": "Calculates the average and normalization factor of a JAX array, returning both.",
        "code": "x = jnp.array([1, 2, 3, 2, 4])\njnp.average(x, returned=True)"
      },
      {
        "description": "Calculates the weighted average and normalization factor of a JAX array, returning both.",
        "code": "x = jnp.array([1, 2, 3, 2, 4])\nweights = jnp.array([2, 1, 3, 2, 2])\njnp.average(x, weights=weights, returned=True)"
      },
      {
        "description": "Calculates the average and normalization factor of a JAX array, returning both.",
        "code": "x = jnp.array([1, 2, 3, 2, 4])\njnp.average(x, returned=True)"
      },
      {
        "description": "Calculates the weighted average and normalization factor of a JAX array, returning both.",
        "code": "x = jnp.array([1, 2, 3, 2, 4])\nweights = jnp.array([2, 1, 3, 2, 2])\njnp.average(x, weights=weights, returned=True)"
      }
    ]
  },
  {
    "title": "Weighted Average along an Axis",
    "concepts": [
      "Demonstrates calculating the weighted average along a specified axis of a multi-dimensional array.",
      "Uses the axis parameter to specify the axis along which to compute the average.",
      "Shows how the weights and axis parameters combine to compute the average along a specific dimension."
    ],
    "code_examples": [
      {
        "description": "Calculates the weighted average along axis 1 of a 2D JAX array.",
        "code": "x = jnp.array([[8, 2, 7],\n               [3, 6, 4]])\nweights = jnp.array([1, 2, 3])\njnp.average(x, weights=weights, axis=1)"
      },
      {
        "description": "Calculates the weighted average along axis 1 of a 2D JAX array.",
        "code": "x = jnp.array([[8, 2, 7],\n               [3, 6, 4]])\nweights = jnp.array([1, 2, 3])\njnp.average(x, weights=weights, axis=1)"
      }
    ]
  },
  {
    "title": "Bartlett Window Function",
    "concepts": [
      "Returns a Bartlett window of size M.",
      "JAX implementation of numpy.bartlett().",
      "The function takes the window size M as an integer input.",
      "The function returns an array of size M containing the Bartlett window."
    ],
    "code_examples": [
      {
        "description": "Example usage of jnp.bartlett() to generate a Bartlett window of size 4 and print the result with specified precision.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.bartlett(4))"
      },
      {
        "description": "Another example usage of jnp.bartlett() to generate a Bartlett window of size 4 and print the result with specified precision.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.bartlett(4))"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.bincount",
    "concepts": [
      "jax.numpy.bincount counts the number of occurrences of each value in an integer array.",
      "It is a JAX implementation of numpy.bincount().",
      "For an array x of positive integers, it returns an array counts where counts[i] is the number of occurrences of i in x.",
      "Negative values in the input array are clipped to zero in JAX.",
      "JAX adds an optional length parameter to statically specify the length of the output array.",
      "Items larger than the specified length are dropped."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jax.numpy.bincount",
    "concepts": [
      "Demonstrates the basic usage of jax.numpy.bincount to count occurrences of values in an array."
    ],
    "code_examples": [
      {
        "description": "Counts the occurrences of each value in the array x.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 1, 2, 3, 3, 3])\njnp.bincount(x)"
      },
      {
        "description": "Counts the occurrences of each value in the array x.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 1, 2, 3, 3, 3])\njnp.bincount(x)"
      }
    ]
  },
  {
    "title": "Weighted jax.numpy.bincount",
    "concepts": [
      "Demonstrates how to use weights with jax.numpy.bincount to calculate the sum of weights for each value in the array."
    ],
    "code_examples": [
      {
        "description": "Calculates the weighted bincount using the array x and corresponding weights.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 1, 2, 3, 3, 3])\nweights = jnp.array([1, 2, 3, 4, 5, 6])\njnp.bincount(x, weights)"
      },
      {
        "description": "Calculates the weighted bincount using the array x and corresponding weights.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 1, 2, 3, 3, 3])\nweights = jnp.array([1, 2, 3, 4, 5, 6])\njnp.bincount(x, weights)"
      }
    ]
  },
  {
    "title": "jax.numpy.bincount with JIT Compilation",
    "concepts": [
      "Demonstrates how to use jax.numpy.bincount with JIT compilation by specifying a static length.",
      "Using a static length makes the function compatible with JAX transformations like jax.jit()."
    ],
    "code_examples": [
      {
        "description": "Compiles jnp.bincount with jax.jit, specifying 'length' as a static argument, and calls the compiled function.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.array([1, 1, 2, 3, 3, 3])\njit_bincount = jax.jit(jnp.bincount, static_argnames=['length'])\njit_bincount(x, length=5)"
      },
      {
        "description": "Compiles jnp.bincount with jax.jit, specifying 'length' as a static argument, and calls the compiled function.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.array([1, 1, 2, 3, 3, 3])\njit_bincount = jax.jit(jnp.bincount, static_argnames=['length'])\njit_bincount(x, length=5)"
      }
    ]
  },
  {
    "title": "Clipping and Length Handling in jax.numpy.bincount",
    "concepts": [
      "Demonstrates how negative numbers are clipped to zero and numbers exceeding the specified length are dropped when using jax.numpy.bincount with a length parameter."
    ],
    "code_examples": [
      {
        "description": "Shows how negative numbers are clipped to the first bin (index 0) and numbers greater than the specified length are ignored.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([-1, -1, 1, 3, 10])\njnp.bincount(x, length=5)"
      },
      {
        "description": "Shows how negative numbers are clipped to the first bin (index 0) and numbers greater than the specified length are ignored.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([-1, -1, 1, 3, 10])\njnp.bincount(x, length=5)"
      }
    ]
  },
  {
    "title": "Description of bitwise_and function",
    "concepts": [
      "Computes the bitwise AND operation elementwise.",
      "JAX implementation of numpy.bitwise_and.",
      "It is a universal function (ufunc).",
      "Provides the implementation of the & operator for JAX arrays.",
      "The function takes two integer or boolean arrays as input.",
      "Arrays must be broadcastable to a common shape.",
      "Returns an array containing the result of the element-wise bitwise AND."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of bitwise_and usage",
    "concepts": [
      "Demonstrates calling bitwise_and explicitly using jnp.bitwise_and.",
      "Demonstrates calling bitwise_and implicitly using the & operator."
    ],
    "code_examples": [
      {
        "description": "Calling bitwise_and explicitly:",
        "code": "x = jnp.arange(4)\njnp.bitwise_and(x, 1)"
      },
      {
        "description": "Calling bitwise_and explicitly:",
        "code": "x = jnp.arange(4)\njnp.bitwise_and(x, 1)"
      },
      {
        "description": "Calling bitwise_and via the & operator:",
        "code": "x & 1"
      },
      {
        "description": "Calling bitwise_and via the & operator:",
        "code": "x & 1"
      }
    ]
  },
  {
    "title": "Description of jnp.bitwise_count",
    "concepts": [
      "The function counts the number of 1 bits in the binary representation of the absolute value of each element in the input array.",
      "It is a JAX implementation of numpy.bitwise_count.",
      "The input array should be of integer subtype.",
      "The output array has the same shape as the input array and its dtype is uint8."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.bitwise_count with 1D arrays",
    "concepts": [
      "Demonstrates the usage of jnp.bitwise_count with a 1D array of positive integers.",
      "Illustrates how the function counts the set bits in the binary representation of each number.",
      "Demonstrates the usage of jnp.bitwise_count with a 1D array containing negative integers. It counts set bits in the absolute value."
    ],
    "code_examples": [
      {
        "description": "Counts the number of set bits in a 1D array of positive integers.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([64, 32, 31, 20])\n# 64 = 0b1000000, 32 = 0b100000, 31 = 0b11111, 20 = 0b10100\n\njnp.bitwise_count(x1)"
      },
      {
        "description": "Counts the number of set bits in a 1D array containing negative integers. It counts set bits in the absolute value.",
        "code": "import jax.numpy as jnp\n\nx2 = jnp.array([-16, -7, 7])\n# |-16| = 0b10000, |-7| = 0b111, 7 = 0b111\n\njnp.bitwise_count(x2)"
      }
    ]
  },
  {
    "title": "Examples of jnp.bitwise_count with 2D arrays",
    "concepts": [
      "Demonstrates the usage of jnp.bitwise_count with a 2D array of integers, both positive and negative.",
      "The function operates element-wise on the array, counting the set bits in the absolute value of each element.",
      "The output is a 2D array with the same shape as the input, containing the bit counts."
    ],
    "code_examples": [
      {
        "description": "Counts the number of set bits in a 2D array of integers (both positive and negative).",
        "code": "import jax.numpy as jnp\n\nx3 = jnp.array([[2, -7],[-9, 7]])\n# 2 = 0b10, |-7| = 0b111, |-9| = 0b1001, 7 = 0b111\n\njnp.bitwise_count(x3)"
      }
    ]
  },
  {
    "title": "Alias of jax.numpy.invert()",
    "concepts": [
      "The document describes an alias for jax.numpy.invert().",
      "The input 'x' is an ArrayLike."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.left_shift",
    "concepts": [
      "Alias of jax.numpy.left_shift().",
      "x is an ArrayLike.",
      "y is an ArrayLike.",
      "The function returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.invert() Alias",
    "concepts": [
      "This document describes jax.numpy.invert() alias.",
      "x is an ArrayLike object, which serves as input."
    ],
    "code_examples": []
  },
  {
    "title": "Bitwise OR Operation in JAX",
    "concepts": [
      "Computes the bitwise OR operation elementwise.",
      "JAX implementation of numpy.bitwise_or.",
      "It is a universal function, and supports the additional APIs described at jax.numpy.ufunc.",
      "Implements the | operator for JAX arrays.",
      "Input arrays must be broadcastable to a common shape."
    ],
    "code_examples": []
  },
  {
    "title": "Explicit bitwise_or examples",
    "concepts": [
      "Demonstrates explicit calling of jnp.bitwise_or."
    ],
    "code_examples": [
      {
        "description": "Calling bitwise_or explicitly.",
        "code": "x = jnp.arange(4)\njnp.bitwise_or(x, 1)"
      },
      {
        "description": "Calling bitwise_or explicitly.",
        "code": "x = jnp.arange(4)\njnp.bitwise_or(x, 1)"
      }
    ]
  },
  {
    "title": "Bitwise OR with the | operator",
    "concepts": [
      "Demonstrates calling bitwise_or using the | operator."
    ],
    "code_examples": [
      {
        "description": "Calling bitwise_or via the | operator",
        "code": "x | 1"
      },
      {
        "description": "Calling bitwise_or via the | operator",
        "code": "x | 1"
      }
    ]
  },
  {
    "title": "Right Shift Operation",
    "concepts": [
      "Alias of jax.numpy.right_shift().",
      "x1 is an ArrayLike object.",
      "x2 is an ArrayLike object.",
      "The function returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Bitwise XOR in JAX",
    "concepts": [
      "jax.numpy.bitwise_xor is a JAX implementation of numpy.bitwise_xor.",
      "It is a universal function (ufunc) and supports ufunc APIs.",
      "This function implements the ^ operator for JAX arrays.",
      "The inputs 'x' and 'y' must be integer or boolean arrays.",
      "The inputs 'x' and 'y' must be broadcastable to a common shape."
    ],
    "code_examples": []
  },
  {
    "title": "Explicitly Calling bitwise_xor",
    "concepts": [
      "Demonstrates how to call jnp.bitwise_xor directly.",
      "It shows the bitwise XOR operation between a JAX array and a scalar value."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of jnp.bitwise_xor with a jax array and scalar value.",
        "code": "x = jnp.arange(4)\njnp.bitwise_xor(x, 1)\n"
      },
      {
        "description": "Demonstrates the use of jnp.bitwise_xor with a jax array and scalar value.",
        "code": "x = jnp.arange(4)\njnp.bitwise_xor(x, 1)"
      }
    ]
  },
  {
    "title": "Calling bitwise_xor via the ^ operator",
    "concepts": [
      "Demonstrates how to use the ^ operator to perform bitwise XOR.",
      "It shows the bitwise XOR operation between a JAX array and a scalar value using the ^ operator."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of ^ operator for bitwise XOR of a jax array and a scalar.",
        "code": "x ^ 1"
      },
      {
        "description": "Demonstrates the use of ^ operator for bitwise XOR of a jax array and a scalar.",
        "code": "x ^ 1"
      }
    ]
  },
  {
    "title": "Blackman Window in JAX",
    "concepts": [
      "Returns a Blackman window of a specified size.",
      "JAX implementation of numpy.blackman().",
      "The input M specifies the window size.",
      "The output is an array of size M containing the Blackman window."
    ],
    "code_examples": [
      {
        "description": "Example usage of jnp.blackman() with window size 4, printing the result with specified precision.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.blackman(4))"
      },
      {
        "description": "Example usage of jnp.blackman() with window size 4, printing the result with specified precision (duplicate).",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.blackman(4))"
      }
    ]
  },
  {
    "title": "JAX Array Blocking",
    "concepts": [
      "The jnp.block() function creates an array from a list of blocks.",
      "The input can be an array or a nested list of arrays.",
      "A single array passed to block() returns the array itself.",
      "A simple list of arrays concatenates them along the last axis.",
      "Doubly-nested lists concatenate inner lists along the last axis and outer lists along the second-to-last axis.",
      "Blocks need to align in the dimensions along the axis of concatenation.",
      "This logic generalizes to blocks in 3 or more dimensions."
    ],
    "code_examples": [
      {
        "description": "Demonstrates passing a single array to jnp.block().",
        "code": "import jax.numpy as jnp\n\nzeros = jnp.zeros((2, 2))\n\njnp.block(zeros)"
      },
      {
        "description": "Demonstrates concatenating a simple list of arrays along the last axis.",
        "code": "import jax.numpy as jnp\n\nzeros = jnp.zeros((2, 2))\nones = jnp.ones((2, 2))\n\njnp.block([zeros, ones])"
      },
      {
        "description": "Demonstrates concatenating a doubly-nested list of arrays.",
        "code": "import jax.numpy as jnp\n\nzeros = jnp.zeros((2, 2))\nones = jnp.ones((2, 2))\ntwos = jnp.full((2, 2), 2)\nthrees = jnp.full((2, 2), 3)\n\njnp.block([[zeros, ones],\n           [twos, threes]])"
      },
      {
        "description": "Demonstrates using jnp.block() with blocks of non-uniform shape.",
        "code": "import jax.numpy as jnp\n\na = jnp.zeros((2, 1))\nb = jnp.ones((2, 3))\nc = jnp.full((1, 2), 2)\nd = jnp.full((1, 2), 3)\n\njnp.block([[a, b],\n           [c, d]])"
      },
      {
        "description": "Demonstrates using jnp.block() with a 3-dimensional block-wise array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(6).reshape((1, 2, 3))\nblocks = [[[x for i in range(3)] for j in range(4)] for k in range(5)]\n\njnp.block(blocks).shape"
      }
    ]
  },
  {
    "title": "Alias of bool",
    "concepts": [
      "The document describes 'bool' as an alias.",
      "The document does not provide further context."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jnp.broadcast_to",
    "concepts": [
      "The function `jnp.broadcast_to()` is JAX's implementation of NumPy's `broadcast_to()` function.",
      "It broadcasts an array to a specified shape according to NumPy's broadcasting rules.",
      "The function takes an array and a shape as input.",
      "It returns a copy of the input array broadcast to the specified shape."
    ],
    "code_examples": []
  },
  {
    "title": "Broadcasting a scalar to a 2D array",
    "concepts": [
      "A scalar value can be broadcast to a higher-dimensional array.",
      "The scalar value is replicated to fill the new array."
    ],
    "code_examples": [
      {
        "description": "Broadcasting the integer 1 to a (1, 4) array.",
        "code": "x = jnp.int32(1)\njnp.broadcast_to(x, (1, 4))"
      },
      {
        "description": "Broadcasting the integer 1 to a (1, 4) array.",
        "code": "x = jnp.int32(1)\njnp.broadcast_to(x, (1, 4))"
      }
    ]
  },
  {
    "title": "Broadcasting a 1D array to a 2D array",
    "concepts": [
      "A 1D array can be broadcast to a 2D array.",
      "The 1D array is replicated along the new axis."
    ],
    "code_examples": [
      {
        "description": "Broadcasting the 1D array [1, 2, 3] to a (2, 3) array.",
        "code": "x = jnp.array([1, 2, 3])\njnp.broadcast_to(x, (2, 3))"
      },
      {
        "description": "Broadcasting the 1D array [1, 2, 3] to a (2, 3) array.",
        "code": "x = jnp.array([1, 2, 3])\njnp.broadcast_to(x, (2, 3))"
      }
    ]
  },
  {
    "title": "Broadcasting a 2D array to a larger 2D array",
    "concepts": [
      "A 2D array can be broadcast to a larger 2D array if dimensions are compatible.",
      "Dimensions must either be equal or one of them must be 1.",
      "The array is replicated along axes where the dimension is 1."
    ],
    "code_examples": [
      {
        "description": "Broadcasting a (2, 1) array to a (2, 4) array.",
        "code": "x = jnp.array([[2],\n              [4]])\njnp.broadcast_to(x, (2, 4))"
      },
      {
        "description": "Broadcasting a (2, 1) array to a (2, 4) array.",
        "code": "x = jnp.array([[2],\n              [4]])\njnp.broadcast_to(x, (2, 4))"
      }
    ]
  },
  {
    "title": "Concatenating Arrays Along the Last Axis",
    "concepts": [
      "jnp.c_ concatenates slices, scalars and array-like objects along the last axis.",
      "It provides a LAX-backend implementation of numpy.c_ .",
      "jnp.r_ concatenates along the first axis."
    ],
    "code_examples": [
      {
        "description": "Concatenating two arrays along the last axis using jnp.c_.",
        "code": "a = jnp.arange(6).reshape((2, 3))\njnp.c_[a, a]"
      },
      {
        "description": "Concatenating two arrays along the last axis using jnp.c_.",
        "code": "a = jnp.arange(6).reshape((2, 3))\njnp.c_[a, a]"
      }
    ]
  },
  {
    "title": "String Directive for Concatenation",
    "concepts": [
      "Using a string directive specifies concatenation axis, minimum dimensions, and position of original dimensions.",
      "The string directive format is 'axis:dims:trans1d'."
    ],
    "code_examples": [
      {
        "description": "Concatenating arrays with the '0,2' string directive.",
        "code": "jnp.c_['0,2', [1, 2, 3], [4, 5, 6]]"
      },
      {
        "description": "Concatenating arrays with the '0,2' string directive.",
        "code": "jnp.c_['0,2', [1, 2, 3], [4, 5, 6]]"
      },
      {
        "description": "Concatenating arrays with the '0,2,-1' string directive.",
        "code": "jnp.c_['0,2,-1', [1, 2, 3], [4, 5, 6]]"
      },
      {
        "description": "Concatenating arrays with the '0,2,-1' string directive.",
        "code": "jnp.c_['0,2,-1', [1, 2, 3], [4, 5, 6]]"
      }
    ]
  },
  {
    "title": "Special Directives 'r' and 'c'",
    "concepts": [
      "The 'r' and 'c' directives stack flat inputs along the last axis.",
      "These are used as the first argument to jnp.c_."
    ],
    "code_examples": [
      {
        "description": "Using the 'r' directive to concatenate flat inputs.",
        "code": "jnp.c_['r',[1, 2, 3], [4, 5, 6]]"
      },
      {
        "description": "Using the 'r' directive to concatenate flat inputs.",
        "code": "jnp.c_['r',[1, 2, 3], [4, 5, 6]]"
      }
    ]
  },
  {
    "title": "Overview of numpy.can_cast",
    "concepts": [
      "The function `numpy.can_cast` determines if a cast between data types can occur.",
      "It takes a 'from' data type, a 'to' data type, and an optional 'casting' rule as input.",
      "The 'from' argument can be a dtype, a dtype specifier, a NumPy scalar, or an array.",
      "The 'to' argument can be a dtype or dtype specifier.",
      "The 'casting' argument controls what kind of data casting may occur, with options like 'no', 'equiv', 'safe', 'same_kind', and 'unsafe'.",
      "'no' means the data types should not be cast at all.",
      "'equiv' means only byte-order changes are allowed.",
      "'safe' means only casts which can preserve values are allowed.",
      "'same_kind' means only safe casts or casts within a kind, like float64 to float32, are allowed.",
      "'unsafe' means any data conversions may be done.",
      "The function returns True if the cast can occur according to the specified casting rule.",
      "Python scalars are not supported anymore since version 2.0."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Examples",
    "concepts": [
      "Demonstrates how to use `numpy.can_cast` with different data types and specifiers to check if casting is possible."
    ],
    "code_examples": [
      {
        "description": "Checks if an int32 can be cast to an int64.",
        "code": "import numpy as np\n\nnp.can_cast(np.int32, np.int64)"
      },
      {
        "description": "Checks if a float64 can be cast to a complex type.",
        "code": "import numpy as np\n\nnp.can_cast(np.float64, complex)"
      },
      {
        "description": "Checks if a complex type can be cast to a float.",
        "code": "import numpy as np\n\nnp.can_cast(complex, float)"
      },
      {
        "description": "Checks if an int32 can be cast to an int64 using data type specifiers.",
        "code": "import numpy as np\n\nnp.can_cast('i8', 'f8')"
      },
      {
        "description": "Checks if an int8 can be cast to float4.",
        "code": "import numpy as np\n\nnp.can_cast('i8', 'f4')"
      },
      {
        "description": "Checks if an int4 can be cast to string4.",
        "code": "import numpy as np\n\nnp.can_cast('i4', 'S4')"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.cbrt",
    "concepts": [
      "Calculates the element-wise cube root of an input array.",
      "JAX implementation of numpy.cbrt.",
      "Complex dtypes are not supported.",
      "Returns an array containing the cube root of the elements."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.cbrt usage",
    "concepts": [
      "Shows how to calculate the cube root of a JAX array using jnp.cbrt.",
      "Demonstrates the use of jnp.printoptions to control the output precision."
    ],
    "code_examples": [
      {
        "description": "Calculates the cube root of a JAX array and prints the result with specified precision.",
        "code": "x = jnp.array([[216, 125, 64],\n               [-27, -8, -1]])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.cbrt(x)"
      },
      {
        "description": "Calculates the cube root of a JAX array and prints the result with specified precision.",
        "code": "x = jnp.array([[216, 125, 64],\n               [-27, -8, -1]])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.cbrt(x)"
      }
    ]
  },
  {
    "title": "Definition of complex128 Alias",
    "concepts": [
      "complex128 is an alias.",
      "It represents a 128-bit complex number."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.numpy.ceil",
    "concepts": [
      "The function rounds input to the nearest integer upwards.",
      "The input array must not have a complex dtype.",
      "The output array has the same shape and dtype as the input array.",
      "The output contains values rounded to the nearest integer that is greater than or equal to the value itself."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of jax.numpy.ceil",
    "concepts": [
      "Demonstrates how to use jax.numpy.ceil with a JAX array.",
      "Shows how to generate a random JAX array using jax.random.uniform.",
      "Illustrates the effect of jax.numpy.ceil on positive and negative floating-point numbers.",
      "jnp.printoptions is used to control the printing precision."
    ],
    "code_examples": [
      {
        "description": "Demonstrates jax.numpy.ceil with randomly generated numbers.",
        "code": "key = jax.random.key(1)\nx = jax.random.uniform(key, (3, 3), minval=-5, maxval=5)\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(x)\n\nprint(jnp.ceil(x))"
      },
      {
        "description": "Demonstrates jax.numpy.ceil with randomly generated numbers.",
        "code": "key = jax.random.key(1)\nx = jax.random.uniform(key, (3, 3), minval=-5, maxval=5)\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(x)\n\nprint(jnp.ceil(x))"
      }
    ]
  },
  {
    "title": "Overview of Character String Scalar Type",
    "concepts": [
      "This is an abstract base class for character string scalar types.",
      "It provides methods and attributes that are identical to corresponding array attributes.",
      "The class includes standard methods like all, any, argmax, argmin, astype, etc.",
      "It also has attributes like T, base, data, dtype, flags, etc."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "The class contains various methods to manipulate scalar values.",
      "Methods such as argmax, argmin, sort, reshape, and others provide functionality similar to NumPy array operations.",
      "These scalar methods are identical to their corresponding array attribute counterparts."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "The class contains various attributes describing the scalar value.",
      "Attributes such as base, data, dtype, flags, shape and size describe the scalar's properties.",
      "These scalar attributes are identical to their corresponding array attribute counterparts."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Implementation of `numpy.choose()`",
    "concepts": [
      "The function constructs an array by stacking slices of choice arrays based on an index array.",
      "The function can be used with 1D index array and 2D choice array, or with N-dimensional index array and a sequence of broadcast-compatible arrays.",
      "The `mode` argument controls the behavior for out-of-bound indices ('raise', 'wrap', or 'clip').",
      "The default mode of 'raise' is not compatible with JAX transformations.",
      "jax.lax.switch() can choose between N functions based on an index."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the simplest case where a is a 1D array and choices is a 2D array.",
        "code": "import jax.numpy as jnp\n\nchoices = jnp.array([[1, 2, 3, 4],\n                     [5, 6, 7, 8],\n                     [9, 10, 11, 12]])\na = jnp.array([2, 0, 1, 0])\n\njnp.choose(a, choices)"
      },
      {
        "description": "Demonstrates the usage of the 'clip' mode when the index is out of bounds.",
        "code": "import jax.numpy as jnp\n\nchoices = jnp.array([[1, 2, 3, 4],\n                     [5, 6, 7, 8],\n                     [9, 10, 11, 12]])\na2 = jnp.array([2, 0, 1, 4])  # last index out-of-bound\n\njnp.choose(a2, choices, mode='clip')"
      },
      {
        "description": "Demonstrates the usage of the 'wrap' mode when the index is out of bounds.",
        "code": "import jax.numpy as jnp\n\nchoices = jnp.array([[1, 2, 3, 4],\n                     [5, 6, 7, 8],\n                     [9, 10, 11, 12]])\na2 = jnp.array([2, 0, 1, 4])  # last index out-of-bound\n\njnp.choose(a2, choices, mode='wrap')"
      },
      {
        "description": "Illustrates the function with a sequence of broadcast-compatible arrays as choices, including a scalar.",
        "code": "import jax.numpy as jnp\n\nchoice_1 = jnp.array([1, 2, 3, 4])\nchoice_2 = 99\nchoice_3 = jnp.array([[10],\n                     [20],\n                     [30]])\na = jnp.array([[0, 1, 2, 0],\n              [1, 2, 0, 1],\n              [2, 0, 1, 2]])\n\njnp.choose(a, [choice_1, choice_2, choice_3], mode='wrap')"
      }
    ]
  },
  {
    "title": "Implementation Details (Illustrative)",
    "concepts": [
      "The function can be implemented using list comprehension for simple cases.",
      "The function can be implemented using jnp.broadcast_arrays and np.ndindex for the general case."
    ],
    "code_examples": [
      {
        "description": "Implementation of choose for 1D array a and 2D array choices.",
        "code": "import jax.numpy as jnp\n\ndef choose(a, choices):\n    return jnp.array([choices[a_i, i] for i, a_i in enumerate(a)])"
      },
      {
        "description": "Implementation of choose for the general case with N-dimensional array a and a sequence of broadcast-compatible arrays.",
        "code": "import jax.numpy as jnp\nimport numpy as np\n\ndef choose(a, choices):\n    a, *choices = jnp.broadcast_arrays(a, *choices)\n    choices = jnp.array(choices)\n    return jnp.array([choices[a[idx], *idx] for idx in np.ndindex(a.shape)])"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.clip()",
    "concepts": [
      "The function clips array values to a specified range.",
      "It is a JAX implementation of numpy.clip().",
      "The arr argument is the N-dimensional array to be clipped.",
      "The min argument is the optional minimum value of the clipped range.",
      "The max argument is the optional maximum value of the clipped range.",
      "Deprecated aliases for arr, min, and max are a, a_min, and a_max, respectively."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of using jax.numpy.clip()",
    "concepts": [
      "The jnp.clip() function is used to limit values in an array to a specified range.",
      "Values below the minimum are set to the minimum, and values above the maximum are set to the maximum."
    ],
    "code_examples": [
      {
        "description": "Clips the array `arr` between the values 2 and 5.",
        "code": "arr = jnp.array([0, 1, 2, 3, 4, 5, 6, 7])\njnp.clip(arr, 2, 5)"
      },
      {
        "description": "Clips the array `arr` between the values 2 and 5 (repeated example).",
        "code": "arr = jnp.array([0, 1, 2, 3, 4, 5, 6, 7])\njnp.clip(arr, 2, 5)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.column_stack",
    "concepts": [
      "Stacks arrays column-wise.",
      "Equivalent to jax.numpy.concatenate() with axis=1 for arrays with two or more dimensions.",
      "Input arrays are promoted to at least rank 2.",
      "The function can take a sequence of arrays to stack.  Each must have the same leading dimension.",
      "The optional dtype argument specifies the data type of the resulting array."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Scalar Values",
    "concepts": [
      "Demonstrates column_stack with scalar inputs.",
      "Scalar inputs are stacked into a 2D array."
    ],
    "code_examples": [
      {
        "description": "Stacks scalar values 1, 2, and 3 column-wise.",
        "code": "jnp.column_stack([1, 2, 3])"
      },
      {
        "description": "Stacks scalar values 1, 2, and 3 column-wise.",
        "code": "jnp.column_stack([1, 2, 3])"
      }
    ]
  },
  {
    "title": "Examples with 1D Arrays",
    "concepts": [
      "Demonstrates column_stack with 1D array inputs.",
      "1D arrays are stacked as columns of a 2D array."
    ],
    "code_examples": [
      {
        "description": "Stacks two 1D arrays x and y column-wise.",
        "code": "x = jnp.arange(3)\ny = jnp.ones(3)\njnp.column_stack([x, y])"
      },
      {
        "description": "Stacks two 1D arrays x and y column-wise.",
        "code": "x = jnp.arange(3)\ny = jnp.ones(3)\njnp.column_stack([x, y])"
      }
    ]
  },
  {
    "title": "Examples with 2D Arrays",
    "concepts": [
      "Demonstrates column_stack with 2D array inputs.",
      "2D arrays are stacked column-wise."
    ],
    "code_examples": [
      {
        "description": "Stacks two 2D arrays x and y column-wise.  Arrays are reshaped to (3,1) first.",
        "code": "x = x.reshape(3, 1)\ny = y.reshape(3, 1)\njnp.column_stack([x, y])"
      },
      {
        "description": "Stacks two 2D arrays x and y column-wise.  Arrays are reshaped to (3,1) first.",
        "code": "x = x.reshape(3, 1)\ny = y.reshape(3, 1)\njnp.column_stack([x, y])"
      }
    ]
  },
  {
    "title": "Definition of complex128 Alias",
    "concepts": [
      "complex128 is an alias.",
      "The document defines 'complex128'."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Complex128 Scalar Constructor",
    "concepts": [
      "JAX provides a scalar constructor of type complex128.",
      "NumPy defines scalar types for each data type.",
      "JAX represents scalars as zero-dimensional arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Complex to Real dtype Casting Warning",
    "concepts": [
      "A warning is raised when casting a complex dtype to a real dtype.",
      "Casting a complex number to a real number discards the imaginary part.",
      "Discarding the imaginary part may not be the desired behavior for the user."
    ],
    "code_examples": []
  },
  {
    "title": "Overview",
    "concepts": [
      "The function compresses an array along a specified axis based on a boolean condition.",
      "It is a JAX implementation of numpy.compress().",
      "It takes a 1D boolean condition array and an N-dimensional array as input.",
      "The axis parameter specifies the axis to compress along.",
      "If axis is None, the input array is flattened before compression.",
      "The size parameter allows for specifying a static output size, which is needed for JAX transformations like jit() and vmap().",
      "The fill_value parameter is used to pad entries when size is specified."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Compression Examples",
    "concepts": [
      "Compressing a 2D array along rows using a boolean condition.",
      "Using the compress() method of JAX arrays for equivalent functionality.",
      "Demonstrates that the condition length does not need to match the shape of the specified axis; values beyond the condition's size are ignored."
    ],
    "code_examples": [
      {
        "description": "Compressing a 2D array along the rows based on a boolean condition.",
        "code": "a = jnp.array([[1, 2, 3, 4],\n               [5, 6, 7, 8],\n               [9, 10, 11, 12]])\ncondition = jnp.array([True, False, True])\njnp.compress(condition, a, axis=0)"
      },
      {
        "description": "Compressing a 2D array along the rows based on a boolean condition using compress array method.",
        "code": "a = jnp.array([[1, 2, 3, 4],\n               [5, 6, 7, 8],\n               [9, 10, 11, 12]])\ncondition = jnp.array([True, False, True])\na.compress(condition, axis=0)"
      },
      {
        "description": "Compressing a 2D array along the columns based on a boolean condition.",
        "code": "a = jnp.array([[1, 2, 3, 4],\n               [5, 6, 7, 8],\n               [9, 10, 11, 12]])\ncondition = jnp.array([True, False, True])\njnp.compress(condition, a, axis=1)"
      }
    ]
  },
  {
    "title": "Static Shape and JAX Transformations",
    "concepts": [
      "The size argument can be used to specify a static output size.",
      "Using static output size allows compatibility with JAX transformations like jit() and vmap().",
      "Demonstrates the use of jnp.extract with size and fill_value inside a function that is then vmapped."
    ],
    "code_examples": [
      {
        "description": "Example using jnp.extract with size and fill_value to create a function compatible with jax.vmap.",
        "code": "f = lambda c, a: jnp.extract(c, a, size=len(a), fill_value=0)\nmask = (a % 3 == 0)\njax.vmap(f)(mask, a)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.concatenate",
    "concepts": [
      "jax.numpy.concatenate() is JAX's implementation of numpy.concatenate().",
      "It joins arrays along an existing axis.",
      "The arrays to be concatenated must have the same shape except along the specified axis.",
      "The axis parameter specifies the axis along which to concatenate.",
      "The dtype parameter specifies the data type of the resulting array."
    ],
    "code_examples": []
  },
  {
    "title": "One-Dimensional Concatenation Example",
    "concepts": [
      "Demonstrates concatenating one-dimensional JAX arrays.",
      "Uses jnp.arange() to create a sequence of numbers.",
      "Uses jnp.zeros() to create an array filled with zeros.",
      "Concatenates the two arrays using jnp.concatenate()."
    ],
    "code_examples": [
      {
        "description": "Concatenates a jnp.arange array with a jnp.zeros array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(3)\ny = jnp.zeros(3, dtype=int)\njnp.concatenate([x, y])"
      },
      {
        "description": "Concatenates a jnp.arange array with a jnp.zeros array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(3)\ny = jnp.zeros(3, dtype=int)\njnp.concatenate([x, y])"
      }
    ]
  },
  {
    "title": "Two-Dimensional Concatenation Example",
    "concepts": [
      "Demonstrates concatenating two-dimensional JAX arrays along a specified axis.",
      "Uses jnp.ones() to create an array filled with ones.",
      "Uses jnp.zeros() to create an array filled with zeros.",
      "Concatenates the two arrays along axis=1 using jnp.concatenate()."
    ],
    "code_examples": [
      {
        "description": "Concatenates a jnp.ones array with a jnp.zeros array along axis 1.",
        "code": "import jax.numpy as jnp\n\nx = jnp.ones((2, 3))\ny = jnp.zeros((2, 1))\njnp.concatenate([x, y], axis=1)"
      },
      {
        "description": "Concatenates a jnp.ones array with a jnp.zeros array along axis 1.",
        "code": "import jax.numpy as jnp\n\nx = jnp.ones((2, 3))\ny = jnp.zeros((2, 1))\njnp.concatenate([x, y], axis=1)"
      }
    ]
  },
  {
    "title": "Alias of jax.numpy.conjugate()",
    "concepts": [
      "This section describes the alias of jax.numpy.conjugate().",
      "The function takes an ArrayLike object as input.",
      "The input array is represented by the variable 'x'."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.numpy.conjugate",
    "concepts": [
      "The function returns the complex conjugate of a JAX array.",
      "It is a JAX implementation of numpy.conjugate.",
      "The input can be an array or a scalar.",
      "The output is an array containing the complex conjugate of the input."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.conjugate with Integers",
    "concepts": [
      "Demonstrates the use of jnp.conjugate with an integer input.",
      "The complex conjugate of a real number is the number itself."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the conjugate of an integer.",
        "code": "jnp.conjugate(3)"
      }
    ]
  },
  {
    "title": "Examples of jax.numpy.conjugate with Complex Numbers",
    "concepts": [
      "Demonstrates the use of jnp.conjugate with a complex number array.",
      "The complex conjugate changes the sign of the imaginary part."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the conjugate of a complex array.",
        "code": "x = jnp.array([2 - 1j, 3 + 5j, 7])\njnp.conjugate(x)"
      }
    ]
  },
  {
    "title": "More Examples of jax.numpy.conjugate with Integers",
    "concepts": [
      "Demonstrates the use of jnp.conjugate with an integer input.",
      "The complex conjugate of a real number is the number itself."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the conjugate of an integer.",
        "code": "jnp.conjugate(3)"
      }
    ]
  },
  {
    "title": "More Examples of jax.numpy.conjugate with Complex Numbers",
    "concepts": [
      "Demonstrates the use of jnp.conjugate with a complex number array.",
      "The complex conjugate changes the sign of the imaginary part."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the conjugate of a complex array.",
        "code": "x = jnp.array([2 - 1j, 3 + 5j, 7])\njnp.conjugate(x)"
      }
    ]
  },
  {
    "title": "Introduction to 1D Convolution with JAX",
    "concepts": [
      "JAX implementation of numpy.convolve().",
      "Convolution of one dimensional arrays is defined as c_k = sum_j a_{k - j} v_j.",
      "The inputs `a` and `v` must have a dimension of 1.",
      "The `mode` parameter controls the output size with options 'full', 'same', and 'valid'."
    ],
    "code_examples": []
  },
  {
    "title": "Convolution Modes: Full, Same, and Valid",
    "concepts": [
      "Mode 'full' returns the complete convolution.",
      "Mode 'same' returns a centered portion of the full convolution with the same size as the first input array `a`.",
      "Mode 'valid' returns only the part of the convolution where the arrays fully overlap, without padding.",
      "The `precision` argument specifies the precision of computation.",
      "The `preferred_element_type` argument dictates the data type to be used for accumulating results."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Convolution Examples with jnp.convolve",
    "concepts": [
      "Illustrates 1D convolution using jnp.convolve with different modes.",
      "Demonstrates the default 'full' mode.",
      "Demonstrates the 'same' mode.",
      "Demonstrates the 'valid' mode."
    ],
    "code_examples": [
      {
        "description": "Demonstrates full convolution with jnp.convolve. By default, returns full convolution using implicit zero-padding at the edges.",
        "code": "x = jnp.array([1, 2, 3, 2, 1])\ny = jnp.array([4, 1, 2])\njnp.convolve(x, y)"
      },
      {
        "description": "Demonstrates convolution with mode='same'. Returns a centered convolution the same size as the first input.",
        "code": "x = jnp.array([1, 2, 3, 2, 1])\ny = jnp.array([4, 1, 2])\njnp.convolve(x, y, mode='same')"
      },
      {
        "description": "Demonstrates convolution with mode='valid'. Returns only the portion where the two arrays fully overlap.",
        "code": "x = jnp.array([1, 2, 3, 2, 1])\ny = jnp.array([4, 1, 2])\njnp.convolve(x, y, mode='valid')"
      }
    ]
  },
  {
    "title": "Convolution with Complex-Valued Inputs",
    "concepts": [
      "Demonstrates convolution of complex-valued arrays using jnp.convolve."
    ],
    "code_examples": [
      {
        "description": "Illustrates convolution of two complex-valued arrays.",
        "code": "x1 = jnp.array([3+1j, 2, 4-3j])\ny1 = jnp.array([1, 2-3j, 4+5j])\njnp.convolve(x1, y1)"
      }
    ]
  },
  {
    "title": "JAX `copysign` Function",
    "concepts": [
      "The `copysign` function copies the sign of elements from one array to another.",
      "It is a JAX implementation of numpy.copysign.",
      "The function takes two array-like inputs, x1 and x2.",
      "x2 determines the sign to be applied to x1.",
      "x1 and x2 must be broadcast-compatible.",
      "The function promotes to inexact dtype.",
      "The output shape is determined by broadcasting the shapes of x1 and x2."
    ],
    "code_examples": [
      {
        "description": "Copies the sign of -1 to the elements of the array [5, 2, 0].",
        "code": "x1 = jnp.array([5, 2, 0])\nx2 = -1\njnp.copysign(x1, x2)"
      },
      {
        "description": "Copies the sign of 2 to the elements of the array [6, 8, 0].",
        "code": "x1 = jnp.array([6, 8, 0])\nx2 = 2\njnp.copysign(x1, x2)"
      },
      {
        "description": "Copies the sign of the elements of the array [[1], [-4], [5]] to the elements of the array [2, -3]. Demonstrates broadcasting.",
        "code": "x1 = jnp.array([2, -3])\nx2 = jnp.array([[1], [-4], [5]])\njnp.copysign(x1, x2)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.cos",
    "concepts": [
      "Calculates the trigonometric cosine of each element in an array.",
      "It's a JAX implementation of numpy.cos.",
      "Input is an array-like object representing angles in radians.",
      "Output is an array containing the cosine of each input element, promoting to inexact dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.sin() computes the trigonometric sine.",
      "jax.numpy.tan() computes the trigonometric tangent.",
      "jax.numpy.arccos() and jax.numpy.acos() compute the inverse trigonometric cosine."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.cos Usage",
    "concepts": [
      "Demonstrates the calculation of cosine values for an array of angles.",
      "Uses jnp.printoptions to format the output with specific precision.",
      "Illustrates the basic usage of jnp.cos with a sample array.",
      "The example is repeated twice in the document."
    ],
    "code_examples": [
      {
        "description": "Calculates the cosine of elements in an array of angles and prints the result with a specified precision.",
        "code": "pi = jnp.pi\nx = jnp.array([pi / 4, pi / 2, 3 * pi / 4, 5 * pi / 6])\nwith jnp.printoptions(precision=3, suppress=True):\n  print(jnp.cos(x))"
      },
      {
        "description": "Calculates the cosine of elements in an array of angles and prints the result with a specified precision. This is a duplicate example.",
        "code": "pi = jnp.pi\nx = jnp.array([pi / 4, pi / 2, 3 * pi / 4, 5 * pi / 6])\nwith jnp.printoptions(precision=3, suppress=True):\n  print(jnp.cos(x))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.cosh",
    "concepts": [
      "Calculates the element-wise hyperbolic cosine of an input array.",
      "The hyperbolic cosine is defined as (e^x + e^{-x}) / 2.",
      "It is equivalent to computing jnp.cos(1j * x)."
    ],
    "code_examples": []
  },
  {
    "title": "jnp.cosh with Real-valued Input Array",
    "concepts": [
      "Demonstrates calculating hyperbolic cosine for a real-valued JAX array.",
      "Shows the output with specified precision and suppression using jnp.printoptions."
    ],
    "code_examples": [
      {
        "description": "Calculates the hyperbolic cosine of a real-valued array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[3, -1, 0],\n               [4, 7, -5]])\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.cosh(x))"
      },
      {
        "description": "Shows that jnp.cosh(x) is equivalent to jnp.cos(1j * x) for real inputs",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[3, -1, 0],\n               [4, 7, -5]])\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.cos(1j * x))"
      },
      {
        "description": "Calculates the hyperbolic cosine of a real-valued array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[3, -1, 0],\n               [4, 7, -5]])\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.cosh(x))"
      },
      {
        "description": "Shows that jnp.cosh(x) is equivalent to jnp.cos(1j * x) for real inputs",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[3, -1, 0],\n               [4, 7, -5]])\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.cos(1j * x))"
      }
    ]
  },
  {
    "title": "jnp.cosh with Complex-valued Input",
    "concepts": [
      "Demonstrates calculating hyperbolic cosine for a complex-valued input.",
      "Illustrates the equivalence of jnp.cosh(x) and jnp.cos(1j * x) for complex numbers."
    ],
    "code_examples": [
      {
        "description": "Calculates the hyperbolic cosine of a complex number.",
        "code": "import jax.numpy as jnp\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.cosh(5 + 1j))"
      },
      {
        "description": "Shows that jnp.cosh(x) is equivalent to jnp.cos(1j * x) for complex inputs",
        "code": "import jax.numpy as jnp\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.cos(1j * (5 + 1j)))"
      },
      {
        "description": "Calculates the hyperbolic cosine of a complex number.",
        "code": "import jax.numpy as jnp\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.cosh(5 + 1j))"
      },
      {
        "description": "Shows that jnp.cosh(x) is equivalent to jnp.cos(1j * x) for complex inputs",
        "code": "import jax.numpy as jnp\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.cos(1j * (5 + 1j)))"
      }
    ]
  },
  {
    "title": "Overview of Covariance Estimation",
    "concepts": [
      "Covariance measures the joint variability of two random variables.",
      "The sample covariance estimates the covariance from a set of observations.",
      "The formula for sample covariance is provided.",
      "The function `jax.numpy.cov()` provides a JAX implementation of NumPy's `cov()` function."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of jnp.cov",
    "concepts": [
      "The `m` parameter is an array of observations, with shape (M, N) or (N, M) depending on `rowvar`, or (N,) for a single variable.",
      "The `y` parameter is an optional array of additional observations, combined with `m`.",
      "The `rowvar` parameter determines if rows (True) or columns (False) represent variables.",
      "The `bias` parameter determines the normalization factor (N or N-1).",
      "The `ddof` parameter specifies the degrees of freedom (defaults to 1 if bias is False, 0 if bias is True).",
      "The `fweights` parameter allows for frequency weights for each observation.",
      "The `aweights` parameter allows for assigning relative weights to observations.",
      "The function returns the covariance matrix of shape (M, M) or a scalar if M=1."
    ],
    "code_examples": []
  },
  {
    "title": "Covariance Examples",
    "concepts": [
      "Demonstrates the calculation of covariance for perfectly correlated variables.",
      "Illustrates the covariance calculation for perfectly anti-correlated variables.",
      "Shows how to specify sequences as separate arguments to jnp.cov.",
      "Presents an example using a 3-dimensional standard normal distribution."
    ],
    "code_examples": [
      {
        "description": "Covariance matrix for perfectly correlated variables.",
        "code": ">>> x = jnp.array([[0, 1, 2],\n...                [0, 1, 2]])\n>>> jnp.cov(x)\nArray([[1., 1.],\n       [1., 1.]], dtype=float32)"
      },
      {
        "description": "Covariance matrix for perfectly anti-correlated variables.",
        "code": ">>> x = jnp.array([[-1, 0, 1],\n...                [ 1, 0,-1]])\n>>> jnp.cov(x)\nArray([[ 1., -1.],\n       [-1.,  1.]], dtype=float32)"
      },
      {
        "description": "Covariance calculation by stacking separate arguments.",
        "code": ">>> x = jnp.array([-1, 0, 1])\n>>> y = jnp.array([ 1, 0,-1])\n>>> jnp.cov(x, y)\nArray([[ 1., -1.],\n       [-1.,  1.]], dtype=float32)"
      },
      {
        "description": "Covariance matrix of 100 points drawn from a 3-dimensional standard normal distribution.",
        "code": ">>> key = jax.random.key(0)\n>>> x = jax.random.normal(key, shape=(3, 100))\n>>> with jnp.printoptions(precision=2):\n...   print(jnp.cov(x))\n[[0.9  0.03 0.1 ]\n [0.03 1.   0.01]\n [0.1  0.01 0.85]]"
      }
    ]
  },
  {
    "title": "Overview",
    "concepts": [
      "Computes the batched cross product of two arrays.",
      "It's a JAX implementation of numpy.cross().",
      "Computes either 2-dimensional or 3-dimensional cross product.",
      "In 3D, the result is a length-3 array; in 2D, it's a scalar.",
      "The function takes two N-dimensional arrays, 'a' and 'b', as input.",
      "Arguments axisa, axisb, axisc specify the axis of a, b and the resulting cross product.",
      "The 'axis' argument overrides axisa, axisb and axisc."
    ],
    "code_examples": []
  },
  {
    "title": "2-Dimensional Cross Product Example",
    "concepts": [
      "Demonstrates the cross product of two 2-dimensional vectors.",
      "The result is a scalar value."
    ],
    "code_examples": [
      {
        "description": "Computes the cross product of two 2D arrays using jnp.cross().",
        "code": "import jax.numpy as jnp\n\na = jnp.array([1, 2])\nb = jnp.array([3, 4])\n\njnp.cross(a, b)"
      }
    ]
  },
  {
    "title": "3-Dimensional Cross Product Example",
    "concepts": [
      "Demonstrates the cross product of two 3-dimensional vectors.",
      "The result is a vector of length 3."
    ],
    "code_examples": [
      {
        "description": "Computes the cross product of two 3D arrays using jnp.cross().",
        "code": "import jax.numpy as jnp\n\na = jnp.array([1, 2, 3])\nb = jnp.array([4, 5, 6])\n\njnp.cross(a, b)"
      }
    ]
  },
  {
    "title": "Batched 3-Dimensional Cross Product",
    "concepts": [
      "Illustrates a batched cross product operation.",
      "The cross product is computed along the last axis by default (rows).",
      "It shows the cross product operating on the rows of the input arrays."
    ],
    "code_examples": [
      {
        "description": "Computes a batched cross product of two multi-dimensional arrays along the last axis.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 2, 3],\n               [3, 4, 3]])\nb = jnp.array([[2, 3, 2],\n               [4, 5, 6]])\n\njnp.cross(a, b)"
      }
    ]
  },
  {
    "title": "Batched 2-Dimensional Cross Product with Specified Axis",
    "concepts": [
      "Shows how to compute a batched cross product along a specific axis.",
      "Demonstrates the use of the `axis` parameter to operate on columns instead of rows.",
      "The example uses axis=0 to perform the cross product on the columns of the input arrays."
    ],
    "code_examples": [
      {
        "description": "Computes a batched cross product along axis 0 (columns).",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 2, 3],\n               [3, 4, 3]])\nb = jnp.array([[2, 3, 2],\n               [4, 5, 6]])\n\njnp.cross(a, b, axis=0)"
      }
    ]
  },
  {
    "title": "Cross Product with Independently Specified Axes",
    "concepts": [
      "Illustrates how to independently specify the axes for the inputs 'a' and 'b' and the output 'c'.",
      "Demonstrates the use of `axisa`, `axisb`, and `axisc` parameters.",
      "It produces the same result as specifying `axis=0` in the previous example."
    ],
    "code_examples": [
      {
        "description": "Computes a batched cross product by specifying the axes for input arrays a and b, and the output array c.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 2, 3],\n               [3, 4, 3]])\nb = jnp.array([[2, 3, 2],\n               [4, 5, 6]])\n\njnp.cross(a, b, axisa=0, axisb=0, axisc=0)"
      }
    ]
  },
  {
    "title": "Definition of complex64 Alias",
    "concepts": [
      "complex64 is an alias.",
      "The document defines the alias of complex64."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.cumprod",
    "concepts": [
      "Calculates the cumulative product of array elements along a specified axis.",
      "It is a JAX implementation of numpy.cumprod().",
      "The axis parameter specifies the axis along which the cumulative product is computed; if None, the array is flattened.",
      "The dtype parameter allows specifying the output data type.",
      "The out parameter is unused by JAX.",
      "Returns an array containing the accumulated product along the given axis."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.cumprod usage",
    "concepts": [
      "Demonstrates calculating the cumulative product of a JAX array.",
      "Shows both flattened and axis-specific cumulative product calculations."
    ],
    "code_examples": [
      {
        "description": "Calculates the flattened cumulative product and the cumulative product along axis 1 of a 2D JAX array.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\njnp.cumprod(x) # flattened cumulative product\n# Array([  1,   2,   6,  24, 120, 720], dtype=int32)\n\njnp.cumprod(x, axis=1) # cumulative product along axis 1\n# Array([[  1,   2,   6],\n#        [  4,  20, 120]], dtype=int32)"
      },
      {
        "description": "Repeats the previous example for jax.numpy.cumprod demonstrating cumulative product along flattened array and along axis 1.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\njnp.cumprod(x) # flattened cumulative product\n# Array([  1,   2,   6,  24, 120, 720], dtype=int32)\n\njnp.cumprod(x, axis=1) # cumulative product along axis 1\n# Array([[  1,   2,   6],\n#        [  4,  20, 120]], dtype=int32)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.cumsum()",
    "concepts": [
      "jax.numpy.cumsum() is a JAX implementation of numpy.cumsum().",
      "It calculates the cumulative sum of elements along a specified axis.",
      "The 'a' parameter is the input array.",
      "The 'axis' parameter specifies the axis along which to accumulate.",
      "If axis is None, the array is flattened before accumulation.",
      "The 'dtype' parameter optionally specifies the output dtype.",
      "The 'out' parameter is unused by JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.cumsum() usage",
    "concepts": [
      "The examples demonstrate how to calculate the cumulative sum of a JAX array.",
      "It shows the flattened cumulative sum when axis is not specified.",
      "It shows cumulative sum along a specific axis."
    ],
    "code_examples": [
      {
        "description": "Demonstrates flattened cumulative sum and cumulative sum along axis 1.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\njnp.cumsum(x) # flattened cumulative sum\n\n# Array([ 1,  3,  6, 10, 15, 21], dtype=int32)\n\njnp.cumsum(x, axis=1) # cumulative sum along axis 1\n\n# Array([[ 1,  3,  6],\n#        [ 4,  9, 15]], dtype=int32)"
      },
      {
        "description": "Demonstrates flattened cumulative sum and cumulative sum along axis 1.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\njnp.cumsum(x) # flattened cumulative sum\n\n# Array([ 1,  3,  6, 10, 15, 21], dtype=int32)\n\njnp.cumsum(x, axis=1) # cumulative sum along axis 1\n\n# Array([[ 1,  3,  6],\n#        [ 4,  9, 15]], dtype=int32)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.cumulative_prod",
    "concepts": [
      "Calculates the cumulative product along a specified axis of an array using JAX.",
      "It is a JAX implementation of numpy.cumulative_prod().",
      "The function takes an N-dimensional array and an optional axis argument.",
      "The dtype of the output array can be optionally specified.",
      "The include_initial parameter determines whether to include the initial value in the cumulative product.",
      "Returns an array containing the accumulated values."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.cumulative_prod usage",
    "concepts": [
      "Demonstrates the usage of cumulative_prod along a specific axis.",
      "Shows how to include the initial value in the cumulative product.",
      "Illustrates the resulting array after applying cumulative_prod with and without include_initial."
    ],
    "code_examples": [
      {
        "description": "Calculates cumulative product along axis 1 without including the initial value.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\njnp.cumulative_prod(x, axis=1)"
      },
      {
        "description": "Calculates cumulative product along axis 1 including the initial value.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\njnp.cumulative_prod(x, axis=1, include_initial=True)"
      },
      {
        "description": "Calculates cumulative product along axis 1 without including the initial value.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\njnp.cumulative_prod(x, axis=1)"
      },
      {
        "description": "Calculates cumulative product along axis 1 including the initial value.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\njnp.cumulative_prod(x, axis=1, include_initial=True)"
      }
    ]
  },
  {
    "title": "Degrees to Radians Conversion",
    "concepts": [
      "Converts angles from degrees to radians using JAX.",
      "The conversion formula is: deg2rad(x) = x * pi / 180.",
      "jax.numpy.deg2rad() is the JAX implementation of numpy.deg2rad.",
      "The input can be a scalar or an array representing the angle in degrees.",
      "The output is an array containing the angles in radians."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the conversion of angles from degrees to radians using jnp.deg2rad() and the equivalent formula.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([60, 90, 120, 180])\n\nprint(jnp.deg2rad(x))\nprint(x * jnp.pi / 180)"
      },
      {
        "description": "Example showing that the output of jnp.deg2rad() is the same as x * jnp.pi / 180.",
        "code": "import jax.numpy as jnp\nx = jnp.array([60, 90, 120, 180])\n\nprint(jnp.deg2rad(x))\nprint(x * jnp.pi / 180)"
      }
    ]
  },
  {
    "title": "Alias of jax.numpy.rad2deg()",
    "concepts": [
      "This is an alias for the jax.numpy.rad2deg() function.",
      "The function likely converts angles from radians to degrees.",
      "The input is an ArrayLike object."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jnp.delete",
    "concepts": [
      "The function deletes entries from an array.",
      "It is a JAX implementation of numpy.delete().",
      "It takes an array, indices to delete, and an optional axis as input.",
      "The function returns a copy of the array with the specified indices deleted.",
      "Index specification usually needs to be static.",
      "If indices are guaranteed to be unique you can set assume_unique_indices=True."
    ],
    "code_examples": []
  },
  {
    "title": "Deleting entries from a 1D array",
    "concepts": [
      "Deleting a single element from a 1D array using its index.",
      "Deleting a slice of elements from a 1D array.",
      "Deleting elements from a 1D array using a step in the slice."
    ],
    "code_examples": [
      {
        "description": "Deleting an element at index 2 from a 1D array.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([4, 5, 6, 7, 8, 9])\n\njnp.delete(a, 2)"
      },
      {
        "description": "Deleting a slice of elements (from index 1 up to but not including 4) from a 1D array.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([4, 5, 6, 7, 8, 9])\n\njnp.delete(a, slice(1, 4))"
      },
      {
        "description": "Deleting elements with a step of 2 from a 1D array.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([4, 5, 6, 7, 8, 9])\n\njnp.delete(a, slice(None, None, 2))"
      }
    ]
  },
  {
    "title": "Deleting entries from a 2D array along a specified axis",
    "concepts": [
      "Deleting a column from a 2D array using the axis parameter."
    ],
    "code_examples": [
      {
        "description": "Deleting the column at index 1 from a 2D array.",
        "code": "import jax.numpy as jnp\n\na2 = jnp.array([[4, 5, 6],\n               [7, 8, 9]])\n\njnp.delete(a2, 1, axis=1)"
      }
    ]
  },
  {
    "title": "Deleting multiple entries via a sequence of indices",
    "concepts": [
      "Deleting multiple elements from an array using an array of indices.",
      "The output shape cannot be known with the possibility of duplicate indices under jit().",
      "Using `assume_unique_indices=True` when the indices are unique allows execution under JIT."
    ],
    "code_examples": [
      {
        "description": "Deleting elements at indices 0, 1, and 3 from a 1D array.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([4, 5, 6, 7, 8, 9])\nindices = jnp.array([0, 1, 3])\n\njnp.delete(a, indices)"
      },
      {
        "description": "Demonstrates the failure under jax.jit due to non-static indices.",
        "code": "import jax\nimport jax.numpy as jnp\n\na = jnp.array([4, 5, 6, 7, 8, 9])\nindices = jnp.array([0, 1, 3])\n\njax.jit(jnp.delete)(a, indices)"
      },
      {
        "description": "Deleting elements at indices 0, 1, and 3 using jit and assuming unique indices.",
        "code": "import jax\nimport jax.numpy as jnp\n\na = jnp.array([4, 5, 6, 7, 8, 9])\nindices = jnp.array([0, 1, 3])\n\njit_delete = jax.jit(jnp.delete, static_argnames=['assume_unique_indices'])\njit_delete(a, indices, assume_unique_indices=True)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.diag",
    "concepts": [
      "The function returns a specified diagonal or constructs a diagonal array.",
      "It's a JAX implementation of numpy.diag().",
      "The JAX version always returns a copy of the input.",
      "The parameter 'v' is the input array which can be 1-D or 2-D.",
      "The parameter 'k' is the diagonal offset."
    ],
    "code_examples": []
  },
  {
    "title": "Creating a Diagonal Matrix",
    "concepts": [
      "A 1-D array can be used to create a diagonal matrix.",
      "jax.numpy.diag is used to create the diagonal matrix."
    ],
    "code_examples": [
      {
        "description": "Creating a diagonal matrix from a 1-D array [1, 2, 3].",
        "code": "jnp.diag(jnp.array([1, 2, 3]))"
      },
      {
        "description": "Creating a diagonal matrix from a 1-D array [1, 2, 3].",
        "code": "jnp.diag(jnp.array([1, 2, 3]))"
      }
    ]
  },
  {
    "title": "Specifying Diagonal Offset",
    "concepts": [
      "The 'k' parameter is used to specify the diagonal offset.",
      "Positive 'k' values place the diagonal above the main diagonal.",
      "Negative 'k' values place the diagonal below the main diagonal."
    ],
    "code_examples": [
      {
        "description": "Creating a diagonal matrix from a 1-D array [1, 2, 3] with a diagonal offset of k=1.",
        "code": "jnp.diag(jnp.array([1, 2, 3]), k=1)"
      },
      {
        "description": "Creating a diagonal matrix from a 1-D array [1, 2, 3] with a diagonal offset of k=1.",
        "code": "jnp.diag(jnp.array([1, 2, 3]), k=1)"
      }
    ]
  },
  {
    "title": "Extracting a Diagonal",
    "concepts": [
      "A 2-D array can be used to extract a diagonal.",
      "jax.numpy.diag is used to extract the diagonal."
    ],
    "code_examples": [
      {
        "description": "Extracting the main diagonal from a 2-D array.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\njnp.diag(x)"
      },
      {
        "description": "Extracting the main diagonal from a 2-D array.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\njnp.diag(x)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.diag_indices",
    "concepts": [
      "Returns indices for accessing the main diagonal of a multidimensional array.",
      "JAX implementation of numpy.diag_indices().",
      "The function takes the size of each dimension (n) and the number of dimensions (ndim) as input.",
      "It returns a tuple of arrays containing the indices to access the main diagonal."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.diag_indices Usage",
    "concepts": [
      "Demonstrates the usage of jax.numpy.diag_indices with different input parameters.",
      "Shows how to get indices for a 2D and 3D array."
    ],
    "code_examples": [
      {
        "description": "Example with n=3 and default ndim=2",
        "code": "jnp.diag_indices(3)\n(Array([0, 1, 2], dtype=int32), Array([0, 1, 2], dtype=int32))"
      },
      {
        "description": "Example with n=4 and ndim=3",
        "code": "jnp.diag_indices(4, ndim=3)\n(Array([0, 1, 2, 3], dtype=int32),\nArray([0, 1, 2, 3], dtype=int32),\nArray([0, 1, 2, 3], dtype=int32))"
      },
      {
        "description": "Example with n=3 and default ndim=2 (repeated)",
        "code": "jnp.diag_indices(3)\n(Array([0, 1, 2], dtype=int32), Array([0, 1, 2], dtype=int32))"
      },
      {
        "description": "Example with n=4 and ndim=3 (repeated)",
        "code": "jnp.diag_indices(4, ndim=3)\n(Array([0, 1, 2, 3], dtype=int32),\nArray([0, 1, 2, 3], dtype=int32),\nArray([0, 1, 2, 3], dtype=int32))"
      }
    ]
  },
  {
    "title": "diag_indices_from",
    "concepts": [
      "Returns indices for accessing the main diagonal of a given array.",
      "JAX implementation of numpy.diag_indices_from().",
      "The input array must be at least 2-dimensional.",
      "The input array must have equal length along all dimensions."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the usage of jnp.diag_indices_from() with a 2D array.",
        "code": "arr = jnp.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\njnp.diag_indices_from(arr)"
      },
      {
        "description": "Example demonstrating the usage of jnp.diag_indices_from() with a 3D array.",
        "code": "arr = jnp.array([[[1, 2], [3, 4]],\n              [[5, 6], [7, 8]]])\njnp.diag_indices_from(arr)"
      },
      {
        "description": "Example demonstrating the usage of jnp.diag_indices_from() with a 2D array.",
        "code": "arr = jnp.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\njnp.diag_indices_from(arr)"
      },
      {
        "description": "Example demonstrating the usage of jnp.diag_indices_from() with a 3D array.",
        "code": "arr = jnp.array([[[1, 2], [3, 4]],\n              [[5, 6], [7, 8]]])\njnp.diag_indices_from(arr)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.diagflat",
    "concepts": [
      "Creates a 2D array with a flattened input array on the diagonal.",
      "JAX implementation of numpy.diagflat().",
      "Always returns a 2D array, unlike NumPy which may return a scalar.",
      "The 'v' parameter is the input array, which can be N-dimensional but is flattened to 1D.",
      "The 'k' parameter is the diagonal offset.",
      "Positive 'k' values place the diagonal above the main diagonal.",
      "Negative 'k' values place the diagonal below the main diagonal."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.diagflat Usage",
    "concepts": [
      "Demonstrates how to create a diagonal matrix from a 1D array using jnp.diagflat.",
      "Shows how to use the 'k' parameter to offset the diagonal.",
      "Illustrates how jnp.diagflat flattens a multi-dimensional array before placing its elements on the diagonal."
    ],
    "code_examples": [
      {
        "description": "Creates a diagonal matrix from a 1D JAX array.",
        "code": "jnp.diagflat(jnp.array([1, 2, 3]))\nArray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]], dtype=int32)"
      },
      {
        "description": "Creates a diagonal matrix with an offset using the 'k' parameter.",
        "code": "jnp.diagflat(jnp.array([1, 2, 3]), k=1)\nArray([[0, 1, 0, 0],\n       [0, 0, 2, 0],\n       [0, 0, 0, 3],\n       [0, 0, 0, 0]], dtype=int32)"
      },
      {
        "description": "Creates a diagonal matrix from a 2D JAX array. Notice that the 2D array is flattened before populating the diagonal.",
        "code": "a = jnp.array([[1, 2],\n              [3, 4]])\njnp.diagflat(a)\nArray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]], dtype=int32)"
      },
      {
        "description": "Creates a diagonal matrix from a 1D JAX array.",
        "code": "jnp.diagflat(jnp.array([1, 2, 3]))\nArray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]], dtype=int32)"
      },
      {
        "description": "Creates a diagonal matrix with an offset using the 'k' parameter.",
        "code": "jnp.diagflat(jnp.array([1, 2, 3]), k=1)\nArray([[0, 1, 0, 0],\n       [0, 0, 2, 0],\n       [0, 0, 0, 3],\n       [0, 0, 0, 0]], dtype=int32)"
      },
      {
        "description": "Creates a diagonal matrix from a 2D JAX array. Notice that the 2D array is flattened before populating the diagonal.",
        "code": "a = jnp.array([[1, 2],\n              [3, 4]])\njnp.diagflat(a)\nArray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]], dtype=int32)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.diagonal()",
    "concepts": [
      "Returns the specified diagonal of an array.",
      "JAX implementation of numpy.diagonal().",
      "The JAX version always returns a copy of the input.",
      "Input array must be at least 2-dimensional.",
      "offset: Diagonal offset from the main diagonal.",
      "axis1: The first axis along which to take the diagonal.",
      "axis2: The second axis along which to take the diagonal.",
      "Returns a 1D array for 2D input, and in general a N-1 dimensional array for N-dimensional input."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.diagonal()",
    "concepts": [
      "Demonstrates the usage of jnp.diagonal() with different offsets."
    ],
    "code_examples": [
      {
        "description": "Basic example of extracting the main diagonal of a 2D array.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\njnp.diagonal(x)"
      },
      {
        "description": "Extracting a diagonal with a positive offset.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\njnp.diagonal(x, offset=1)"
      },
      {
        "description": "Extracting a diagonal with a negative offset.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\njnp.diagonal(x, offset=-1)"
      },
      {
        "description": "Basic example of extracting the main diagonal of a 2D array.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\njnp.diagonal(x)"
      },
      {
        "description": "Extracting a diagonal with a positive offset.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\njnp.diagonal(x, offset=1)"
      },
      {
        "description": "Extracting a diagonal with a negative offset.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\njnp.diagonal(x, offset=-1)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.diff",
    "concepts": [
      "Calculates the n-th order difference between array elements along a given axis.",
      "It is a JAX implementation of numpy.diff().",
      "The first order difference is computed by a[i+1] - a[i].",
      "The n-th order difference is computed n times recursively.",
      "If n=0, no difference is computed and the input is returned as is.",
      "The difference is computed along axis -1 by default.",
      "Values can be prepended or appended to the array before computing the difference."
    ],
    "code_examples": []
  },
  {
    "title": "First Order Difference Example",
    "concepts": [
      "Demonstrates the basic usage of jnp.diff to compute the first order difference along the default axis.",
      "The default axis is -1."
    ],
    "code_examples": [
      {
        "description": "Computes the first order difference of a 2D array along the default axis.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 5, 2, 9],\n               [3, 8, 7, 4]])\n\njnp.diff(a)"
      },
      {
        "description": "Computes the first order difference of a 2D array along the default axis.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 5, 2, 9],\n               [3, 8, 7, 4]])\n\njnp.diff(a)"
      }
    ]
  },
  {
    "title": "Second Order Difference Example",
    "concepts": [
      "Illustrates how to compute the second order difference using the n parameter.",
      "The second order difference is computed recursively."
    ],
    "code_examples": [
      {
        "description": "Computes the second order difference (n=2) of the array.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 5, 2, 9],\n               [3, 8, 7, 4]])\n\njnp.diff(a, n=2)"
      },
      {
        "description": "Computes the second order difference (n=2) of the array.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 5, 2, 9],\n               [3, 8, 7, 4]])\n\njnp.diff(a, n=2)"
      }
    ]
  },
  {
    "title": "Using the prepend Parameter",
    "concepts": [
      "Shows how to prepend values to the array before computing the difference.",
      "The prepend parameter takes a scalar or array."
    ],
    "code_examples": [
      {
        "description": "Prepends the value 2 to the array along the default axis before computing the difference.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 5, 2, 9],\n               [3, 8, 7, 4]])\n\njnp.diff(a, prepend=2)"
      },
      {
        "description": "Prepends the value 2 to the array along the default axis before computing the difference.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 5, 2, 9],\n               [3, 8, 7, 4]])\n\njnp.diff(a, prepend=2)"
      }
    ]
  },
  {
    "title": "Using the append Parameter",
    "concepts": [
      "Demonstrates how to append values to the array before computing the difference.",
      "The append parameter takes a scalar or array.",
      "The shape of the appended array must be compatible with the axis along which the difference is computed."
    ],
    "code_examples": [
      {
        "description": "Appends an array to the original array before computing the difference.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 5, 2, 9],\n               [3, 8, 7, 4]])\n\njnp.diff(a, append=jnp.array([[3],[1]]))"
      },
      {
        "description": "Appends an array to the original array before computing the difference.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 5, 2, 9],\n               [3, 8, 7, 4]])\n\njnp.diff(a, append=jnp.array([[3],[1]]))"
      }
    ]
  },
  {
    "title": "Description",
    "concepts": [
      "Alias of jax.numpy.true_divide().",
      "x1 is an ArrayLike.",
      "x2 is an ArrayLike.",
      "The function returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jnp.divmod",
    "concepts": [
      "Calculates the element-wise integer quotient and remainder of two arrays.",
      "It is the JAX implementation of NumPy's divmod function.",
      "The function takes two ArrayLike inputs: the dividend (x1) and the divisor (x2).",
      "The function returns a tuple containing the floor division (x1 // x2) and the remainder (x1 % x2)."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.divmod with integer arrays",
    "concepts": [
      "Demonstrates the usage of jnp.divmod with integer arrays.",
      "Shows the calculation of quotient and remainder for positive integers.",
      "Shows the calculation of quotient and remainder for negative integers."
    ],
    "code_examples": [
      {
        "description": "Example of jnp.divmod with positive integer arrays",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([10, 20, 30])\nx2 = jnp.array([3, 4, 7])\njnp.divmod(x1, x2)"
      },
      {
        "description": "Example of jnp.divmod with mixed positive and negative integers",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5])\nx2 = 3\njnp.divmod(x1, x2)"
      }
    ]
  },
  {
    "title": "Examples of jnp.divmod with mixed integer and float arrays",
    "concepts": [
      "Demonstrates the usage of jnp.divmod with mixed integer and float arrays.",
      "Shows how integers are divided by floats, resulting in float quotients and remainders.",
      "Illustrates the type promotion that occurs when dividing integers by floats."
    ],
    "code_examples": [
      {
        "description": "Example of jnp.divmod where an integer array is divided by a float array, resulting in a float output.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([6, 6, 6], dtype=jnp.int32)\nx2 = jnp.array([1.9, 2.5, 3.1], dtype=jnp.float32)\njnp.divmod(x1, x2)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.dot",
    "concepts": [
      "jax.numpy.dot computes the dot product of two arrays.",
      "It is a JAX implementation of numpy.dot().",
      "For scalar inputs, jax.numpy.dot is equivalent to jax.numpy.multiply().",
      "For arrays with more than 2 dimensions, jax.numpy.dot stacks batch indices, while jax.numpy.matmul broadcasts them.",
      "The first input array `a` has a shape of (..., N).",
      "The second input array `b` has a shape of (N,) or (..., N, M).",
      "Leading dimensions of `b` must be broadcast-compatible with `a`.",
      "The `precision` argument controls numerical precision.",
      "The `preferred_element_type` argument sets the accumulation datatype."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.dot",
    "concepts": [
      "Dot product with a scalar.",
      "Dot product of vectors and matrices.",
      "Demonstration of batch dimension stacking vs. broadcasting in matmul."
    ],
    "code_examples": [
      {
        "description": "Demonstrates dot product with a scalar.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3])\nprint(jnp.dot(x, 2))"
      },
      {
        "description": "Demonstrates dot product of a matrix and a vector, and a matrix and a matrix.",
        "code": "import jax.numpy as jnp\n\nM = jnp.array([[2, 3, 4],\n               [5, 6, 7],\n               [8, 9, 0]])\nx = jnp.array([1, 2, 3])\nprint(jnp.dot(M, x))\nprint(jnp.dot(M, M))"
      },
      {
        "description": "Shows the difference between jnp.dot and jnp.matmul with higher-dimensional arrays, specifically how they handle batch dimensions (stacking vs. broadcasting).",
        "code": "import jax.numpy as jnp\n\na = jnp.zeros((3, 2, 4))\nb = jnp.zeros((3, 4, 1))\nprint(jnp.dot(a, b).shape)\nprint(jnp.matmul(a, b).shape)"
      }
    ]
  },
  {
    "title": "Definition of alias of float64",
    "concepts": [
      "The document defines 'alias' as being equivalent to 'float64'."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jax.numpy.dstack",
    "concepts": [
      "jax.numpy.dstack stacks arrays depth-wise (along the third axis).",
      "For arrays with three or more dimensions, it's equivalent to jax.numpy.concatenate(axis=2).",
      "Input arrays are promoted to at least rank 3.",
      "The function accepts a sequence of arrays (or a single array) as input.",
      "The dtype of the resulting array can be optionally specified.",
      "It is similar to jax.numpy.stack, jax.numpy.concatenate, jax.numpy.vstack and jax.numpy.hstack."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Scalar Values",
    "concepts": [
      "Demonstrates the use of jax.numpy.dstack with scalar values."
    ],
    "code_examples": [
      {
        "description": "Stacks scalar values 1, 2, and 3 using jnp.dstack.",
        "code": "jnp.dstack([1, 2, 3])"
      },
      {
        "description": "Stacks scalar values 1, 2, and 3 using jnp.dstack.",
        "code": "jnp.dstack([1, 2, 3])"
      }
    ]
  },
  {
    "title": "Examples with 1D Arrays",
    "concepts": [
      "Demonstrates the use of jax.numpy.dstack with 1D arrays.",
      "jnp.arange(3) creates a 1D array [0, 1, 2].",
      "jnp.ones(3) creates a 1D array [1, 1, 1]."
    ],
    "code_examples": [
      {
        "description": "Stacks two 1D arrays, x and y, using jnp.dstack.",
        "code": "x = jnp.arange(3)\ny = jnp.ones(3)\njnp.dstack([x, y])"
      },
      {
        "description": "Stacks two 1D arrays, x and y, using jnp.dstack.",
        "code": "x = jnp.arange(3)\ny = jnp.ones(3)\njnp.dstack([x, y])"
      }
    ]
  },
  {
    "title": "Examples with 2D Arrays",
    "concepts": [
      "Demonstrates the use of jax.numpy.dstack with 2D arrays.",
      "Reshaping 1D arrays into 2D arrays with shape (1, 3)."
    ],
    "code_examples": [
      {
        "description": "Stacks two 2D arrays, x and y, using jnp.dstack after reshaping them.",
        "code": "x = x.reshape(1, 3)\ny = y.reshape(1, 3)\njnp.dstack([x, y])"
      },
      {
        "description": "Stacks two 2D arrays, x and y, using jnp.dstack after reshaping them.",
        "code": "x = x.reshape(1, 3)\ny = y.reshape(1, 3)\njnp.dstack([x, y])"
      }
    ]
  },
  {
    "title": "Introduction to Data Type Objects",
    "concepts": [
      "A NumPy array is homogeneous.",
      "A NumPy array contains elements described by a dtype object.",
      "A dtype object can be constructed from fundamental numeric types.",
      "The dtype object can be created from different combinations of the above types."
    ],
    "code_examples": []
  },
  {
    "title": "Creating Data Type Objects with numpy.dtype",
    "concepts": [
      "The numpy.dtype function is used to create data type objects.",
      "Data type objects can be created using array-scalar types, structured types, array-protocol type strings, comma-separated field formats, tuples and dictionaries."
    ],
    "code_examples": [
      {
        "description": "Create a data type object from an array-scalar type (int16).",
        "code": "import numpy as np\n\nnp.dtype(np.int16)"
      },
      {
        "description": "Create a structured data type object with one field named 'f1' containing int16.",
        "code": "import numpy as np\n\nnp.dtype([('f1', np.int16)])"
      },
      {
        "description": "Create a structured data type object with one field named 'f1', itself containing a structured type with one field.",
        "code": "import numpy as np\n\nnp.dtype([('f1', [('f1', np.int16)])])"
      },
      {
        "description": "Create a structured data type object with two fields: 'f1' (uint64) and 'f2' (int32).",
        "code": "import numpy as np\n\nnp.dtype([('f1', np.uint64), ('f2', np.int32)])"
      },
      {
        "description": "Create a data type object using array-protocol type strings.",
        "code": "import numpy as np\n\nnp.dtype([('a', 'f8'),('b', 'S10')])"
      },
      {
        "description": "Create a data type object using comma-separated field formats with a shape of (2,3).",
        "code": "import numpy as np\n\nnp.dtype(\"i4, (2,3)f8\")"
      },
      {
        "description": "Create a data type object using tuples.",
        "code": "import numpy as np\n\nnp.dtype([('hello',(np.int64,3)),('world',np.void,10)])"
      },
      {
        "description": "Subdivide int16 into 2 int8's, called x and y. 0 and 1 are the offsets in bytes.",
        "code": "import numpy as np\n\nnp.dtype((np.int16,{'x':(np.int8,0), 'y':(np.int8,1)}))"
      },
      {
        "description": "Create a data type object using dictionaries.",
        "code": "import numpy as np\n\nnp.dtype({'names':['gender','age'], 'formats':['S1', np.uint8]})"
      },
      {
        "description": "Create a data type object using dictionaries with offsets in bytes.",
        "code": "import numpy as np\n\nnp.dtype({'surname':('S25',0), 'age':(np.uint8, 25)})"
      }
    ]
  },
  {
    "title": "Methods and Attributes of dtype Objects",
    "concepts": [
      "dtype objects have methods like newbyteorder() to change byte order.",
      "dtype objects have attributes like alignment, base, byteorder, char, descr, fields, flags, hasobject, isalignedstruct, isbuiltin, isnative, itemsize, kind, metadata, name, names, ndim, num, shape, str, subdtype, and type."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of einsum_path",
    "concepts": [
      "The function evaluates the optimal contraction path for einsum without actually performing the einsum operation.",
      "It's a JAX implementation of numpy.einsum_path().",
      "It utilizes the opt_einsum package for optimization.",
      "It takes subscripts and operands as input, similar to einsum.",
      "The optimize argument controls the optimization strategy, defaulting to \"auto\" in JAX.",
      "The function returns the optimal path and a printable object representing the path."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of einsum_path",
    "concepts": [
      "Demonstrates how to use jnp.einsum_path to find the optimal contraction path for a given einsum expression.",
      "Shows how to generate random JAX arrays for use with einsum.",
      "Illustrates how to interpret the output of jnp.einsum_path, including the contraction path and path information.",
      "The path information includes details like naive and optimized scaling, FLOP counts, theoretical speedup, and largest intermediate size."
    ],
    "code_examples": [
      {
        "description": "Generates random arrays and finds the optimal einsum path for the expression \"ij,jk,kl\".",
        "code": "key1, key2, key3 = jax.random.split(jax.random.key(0), 3)\nx = jax.random.randint(key1, minval=-5, maxval=5, shape=(2, 3))\ny = jax.random.randint(key2, minval=-5, maxval=5, shape=(3, 100))\nz = jax.random.randint(key3, minval=-5, maxval=5, shape=(100, 5))\npath, path_info = jnp.einsum_path(\"ij,jk,kl\", x, y, z, optimize=\"optimal\")\nprint(path)\nprint(path_info)"
      }
    ]
  },
  {
    "title": "Using the Computed Path in einsum",
    "concepts": [
      "Demonstrates how to use the contraction path obtained from jnp.einsum_path as the optimize argument in jnp.einsum.",
      "This allows for efficient computation of the einsum expression based on the optimized path."
    ],
    "code_examples": [
      {
        "description": "Computes the einsum of the generated arrays using the previously computed optimal path.",
        "code": "key1, key2, key3 = jax.random.split(jax.random.key(0), 3)\nx = jax.random.randint(key1, minval=-5, maxval=5, shape=(2, 3))\ny = jax.random.randint(key2, minval=-5, maxval=5, shape=(3, 100))\nz = jax.random.randint(key3, minval=-5, maxval=5, shape=(100, 5))\npath, path_info = jnp.einsum_path(\"ij,jk,kl\", x, y, z, optimize=\"optimal\")\nprint(jnp.einsum(\"ij,jk,kl\", x, y, z, optimize=path))"
      }
    ]
  },
  {
    "title": "JAX numpy.empty() Function",
    "concepts": [
      "jax.numpy.empty() creates an array with a specified shape and dtype.",
      "Due to XLA limitations, the array is initialized with zeros instead of being uninitialized.",
      "The function takes shape, dtype, and device as input parameters.",
      "It returns an array of the specified shape and dtype."
    ],
    "code_examples": [
      {
        "description": "Create an empty array of shape (4) with the default float32 dtype.",
        "code": "jnp.empty(4)\nArray([0., 0., 0., 0.], dtype=float32)"
      },
      {
        "description": "Create an empty array of shape (2, 3) with a boolean dtype.",
        "code": "jnp.empty((2, 3), dtype=bool)\nArray([[False, False, False],\n       [False, False, False]], dtype=bool)"
      },
      {
        "description": "Create an empty array of shape (4) with the default float32 dtype. (duplicate)",
        "code": "jnp.empty(4)\nArray([0., 0., 0., 0.], dtype=float32)"
      },
      {
        "description": "Create an empty array of shape (2, 3) with a boolean dtype. (duplicate)",
        "code": "jnp.empty((2, 3), dtype=bool)\nArray([[False, False, False],\n       [False, False, False]], dtype=bool)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.equal",
    "concepts": [
      "Returns element-wise truth value of x == y.",
      "JAX implementation of numpy.equal.",
      "Implements the == operator for JAX arrays.",
      "Inputs x and y should have the same shape or be broadcast compatible.",
      "Returns a boolean array indicating where elements of x and y are equal."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jax.numpy.equal",
    "concepts": [
      "Demonstrates element-wise equality comparison with scalar values.",
      "Shows equality comparison between integers and floats.",
      "Illustrates the use of jnp.array to create JAX arrays."
    ],
    "code_examples": [
      {
        "description": "Compares 0.0 and -0.0 for equality.",
        "code": "jnp.equal(0., -0.)"
      },
      {
        "description": "Compares 1 and 1.0 for equality.",
        "code": "jnp.equal(1, 1.)"
      },
      {
        "description": "Compares 5 and jnp.array(5) for equality.",
        "code": "jnp.equal(5, jnp.array(5))"
      },
      {
        "description": "Compares 2 and -2 for equality.",
        "code": "jnp.equal(2, -2)"
      }
    ]
  },
  {
    "title": "Broadcasting with jax.numpy.equal",
    "concepts": [
      "Demonstrates broadcasting behavior of jnp.equal with arrays of different shapes.",
      "Shows element-wise comparison between a 2D array and a 1D array.",
      "Illustrates using the == operator as a shorthand for jnp.equal.",
      "Broadcasting aligns the shapes of the arrays to make the comparison."
    ],
    "code_examples": [
      {
        "description": "Compares a 2D array 'x' with a 1D array 'y' using jnp.equal, demonstrating broadcasting.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\ny = jnp.array([1, 5, 9])\njnp.equal(x, y)"
      },
      {
        "description": "Compares a 2D array 'x' with a 1D array 'y' using the == operator, demonstrating broadcasting.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\ny = jnp.array([1, 5, 9])\nx == y"
      }
    ]
  },
  {
    "title": "Repeated Examples",
    "concepts": [
      "Demonstrates element-wise equality comparison with scalar values.",
      "Shows equality comparison between integers and floats.",
      "Illustrates the use of jnp.array to create JAX arrays.",
      "Demonstrates broadcasting behavior of jnp.equal with arrays of different shapes.",
      "Shows element-wise comparison between a 2D array and a 1D array.",
      "Illustrates using the == operator as a shorthand for jnp.equal.",
      "Broadcasting aligns the shapes of the arrays to make the comparison."
    ],
    "code_examples": [
      {
        "description": "Compares 0.0 and -0.0 for equality.",
        "code": "jnp.equal(0., -0.)"
      },
      {
        "description": "Compares 1 and 1.0 for equality.",
        "code": "jnp.equal(1, 1.)"
      },
      {
        "description": "Compares 5 and jnp.array(5) for equality.",
        "code": "jnp.equal(5, jnp.array(5))"
      },
      {
        "description": "Compares 2 and -2 for equality.",
        "code": "jnp.equal(2, -2)"
      },
      {
        "description": "Compares a 2D array 'x' with a 1D array 'y' using jnp.equal, demonstrating broadcasting.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\ny = jnp.array([1, 5, 9])\njnp.equal(x, y)"
      },
      {
        "description": "Compares a 2D array 'x' with a 1D array 'y' using the == operator, demonstrating broadcasting.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\ny = jnp.array([1, 5, 9])\nx == y"
      }
    ]
  },
  {
    "title": "Introduction to jnp.exp",
    "concepts": [
      "jnp.exp calculates the element-wise exponential of an input array.",
      "It is a JAX implementation of numpy.exp.",
      "The input can be an array or a scalar.",
      "The function promotes to inexact dtype.",
      "Related functions include jax.numpy.log(), jax.numpy.expm1(), and jax.numpy.exp2()."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.exp Usage",
    "concepts": [
      "jnp.exp follows the properties of exponential, such as e^(a+b) = e^a * e^b.",
      "The function can be used with jnp.printoptions to control the output format.",
      "The property e^(a+b) = e^a * e^b holds for complex input as well."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the property e^(a+b) = e^a * e^b with jnp.exp.",
        "code": "x1 = jnp.array([2, 4, 3, 1])\nx2 = jnp.array([1, 3, 2, 3])\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.exp(x1 + x2))\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.exp(x1) * jnp.exp(x2))"
      },
      {
        "description": "Demonstrates the property e^(a+b) = e^a * e^b with jnp.exp and complex numbers.",
        "code": "jnp.allclose(jnp.exp(3 - 4j), jnp.exp(3) * jnp.exp(-4j))"
      }
    ]
  },
  {
    "title": "Description of jnp.exp2",
    "concepts": [
      "Calculates the element-wise base-2 exponential of an input array.",
      "It's a JAX implementation of numpy.exp2.",
      "The input can be an array or a scalar.",
      "The output is an array containing the base-2 exponential of each element in the input.",
      "The output dtype is inexact.",
      "Related functions include jax.numpy.log2(), jax.numpy.exp(), and jax.numpy.expm1()."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.exp2 usage",
    "concepts": [
      "jnp.exp2 follows the properties of exponentials, such as 2^(a+b) = 2^a * 2^b."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the property 2^(a+b) = 2^a * 2^b using jnp.exp2.",
        "code": "x1 = jnp.array([2, -4, 3, -1])\nx2 = jnp.array([-1, 3, -2, 3])\n\njnp.exp2(x1 + x2)\n# Array([2. , 0.5, 2. , 4. ], dtype=float32)\n\njnp.exp2(x1) * jnp.exp2(x2)\n# Array([2. , 0.5, 2. , 4. ], dtype=float32)"
      },
      {
        "description": "Demonstrates the property 2^(a+b) = 2^a * 2^b using jnp.exp2. Repeated example.",
        "code": "x1 = jnp.array([2, -4, 3, -1])\nx2 = jnp.array([-1, 3, -2, 3])\n\njnp.exp2(x1 + x2)\n# Array([2. , 0.5, 2. , 4. ], dtype=float32)\n\njnp.exp2(x1) * jnp.exp2(x2)\n# Array([2. , 0.5, 2. , 4. ], dtype=float32)"
      }
    ]
  },
  {
    "title": "Overview of jnp.expand_dims",
    "concepts": [
      "The function `jnp.expand_dims()` inserts new dimensions of length 1 into an array.",
      "It is a JAX implementation of `numpy.expand_dims()` using `jax.lax.expand_dims()`.",
      "The `axis` argument specifies the position(s) where the new dimension(s) should be added.",
      "It returns a copy of the array with added dimensions, but the compiler may optimize away copies under JIT.",
      "`jax.numpy.squeeze()` is the inverse operation, removing length-1 dimensions.",
      "`jax.lax.expand_dims()` is the XLA version of this functionality."
    ],
    "code_examples": []
  },
  {
    "title": "Expanding Dimensions: Examples",
    "concepts": [
      "Expanding the leading dimension of an array using `jnp.expand_dims(x, 0)`.",
      "Expanding the trailing dimension of an array using `jnp.expand_dims(x, 1)`.",
      "Expanding multiple dimensions of an array using `jnp.expand_dims(x, (0, 1, 3))`.",
      "Expanding dimensions using indexing with `None` which is equivalent to `jnp.expand_dims()`."
    ],
    "code_examples": [
      {
        "description": "Initialize a JAX array.",
        "code": "x = jnp.array([1, 2, 3])\nprint(x.shape)"
      },
      {
        "description": "Expand the leading dimension of the array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3])\nprint(jnp.expand_dims(x, 0))\nprint(jnp.expand_dims(x, 0).shape)"
      },
      {
        "description": "Expand the trailing dimension of the array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3])\nprint(jnp.expand_dims(x, 1))\nprint(jnp.expand_dims(x, 1).shape)"
      },
      {
        "description": "Expand multiple dimensions of the array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3])\nprint(jnp.expand_dims(x, (0, 1, 3)))\nprint(jnp.expand_dims(x, (0, 1, 3)).shape)"
      },
      {
        "description": "Expand the leading dimension using None indexing.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3])\nprint(x[None])"
      },
      {
        "description": "Expand the trailing dimension using None indexing.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3])\nprint(x[:, None])"
      },
      {
        "description": "Expand multiple dimensions using None indexing.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3])\nprint(x[None, None, :, None])"
      }
    ]
  },
  {
    "title": "Description of jnp.expm1",
    "concepts": [
      "Calculates exp(x)-1 for each element of the input array.",
      "It's a JAX implementation of numpy.expm1.",
      "The input can be an array or a scalar.",
      "The output is an array containing exp(x)-1 of each element in x, promoted to inexact dtype.",
      "jnp.expm1 provides higher precision than the naive computation of exp(x)-1 for small values of x."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.expm1",
    "concepts": [
      "Demonstrates the basic usage of jnp.expm1 with an example array.",
      "Compares the output of jnp.expm1(x) with jnp.exp(x) - 1 for an array x.",
      "Uses jnp.printoptions to format the output for better readability."
    ],
    "code_examples": [
      {
        "description": "Basic usage example showing jnp.expm1 calculation.",
        "code": "x = jnp.array([2, -4, 3, -1])\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.expm1(x))"
      },
      {
        "description": "Comparing the result of jnp.expm1(x) with jnp.exp(x) - 1.",
        "code": "x = jnp.array([2, -4, 3, -1])\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.exp(x) - 1)"
      }
    ]
  },
  {
    "title": "Precision Comparison: jnp.expm1 vs jnp.exp(x) - 1 for Small Values",
    "concepts": [
      "Illustrates the higher precision of jnp.expm1(x) compared to jnp.exp(x) - 1 when x is close to 0.",
      "Demonstrates that for very small values, the naive computation exp(x) - 1 can lose precision, while jnp.expm1 maintains accuracy."
    ],
    "code_examples": [
      {
        "description": "Comparing jnp.expm1(x) and jnp.exp(x) - 1 for small values of x.",
        "code": "x1 = jnp.array([1e-4, 1e-6, 2e-10])\nprint(jnp.expm1(x1))\nprint(jnp.exp(x1) - 1)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.extract",
    "concepts": [
      "The function extracts elements from an array based on a condition.",
      "It is a JAX implementation of numpy.extract().",
      "The condition and the array of values are flattened to 1D.",
      "An optional size argument can be specified for JAX transformations.",
      "The fill_value argument pads the output if size is specified.",
      "Shape agreement between condition and arr is not strictly required."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jax.numpy.extract",
    "concepts": [
      "Extracting values from a 1D array based on a boolean mask.",
      "The mask determines which elements of the array are extracted.",
      "The result is a 1D array containing only the extracted elements."
    ],
    "code_examples": [
      {
        "description": "Extracts even numbers from an array using a boolean mask.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5, 6])\nmask = (x % 2 == 0)\njnp.extract(mask, x)"
      }
    ]
  },
  {
    "title": "Comparison with Boolean Indexing",
    "concepts": [
      "jax.numpy.extract is similar to boolean indexing.",
      "Both methods extract elements based on a boolean condition.",
      "Boolean indexing provides similar functionality as extract in simple cases."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how boolean indexing achieves the same result as jnp.extract in a simple case.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5, 6])\nmask = (x % 2 == 0)\nx[mask]"
      }
    ]
  },
  {
    "title": "Using Size and Fill Value for JAX Transformations",
    "concepts": [
      "The `size` argument allows specifying a static shape for the output.",
      "Specifying `size` is required for JAX transformations like jit() or vmap().",
      "The `fill_value` argument specifies the value to use for padding when `size` is specified.",
      "The default value for `fill_value` is 0."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use the `size` and `fill_value` arguments to pad the output of `jnp.extract` to a specific size.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5, 6])\nmask = (x % 2 == 0)\njnp.extract(mask, x, size=len(x), fill_value=0)"
      }
    ]
  },
  {
    "title": "Handling Mismatched Condition and Array Sizes",
    "concepts": [
      "Unlike boolean indexing, jax.numpy.extract does not require strict size agreement.",
      "If the condition is larger than the array, the condition is truncated.",
      "If the array is larger than the condition, the array is truncated."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how jnp.extract handles a condition array that is shorter than the data array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5, 6])\nshort_mask = jnp.array([False, True])\njnp.extract(short_mask, x)"
      },
      {
        "description": "Demonstrates how jnp.extract handles a condition array that is longer than the data array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5, 6])\nlong_mask = jnp.array([True, False, True, False, False, False, False, False])\njnp.extract(long_mask, x)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.eye()",
    "concepts": [
      "Creates a square or rectangular identity matrix.",
      "JAX implementation of numpy.eye().",
      "N specifies the first dimension of the array.",
      "M specifies the second dimension of the array, defaulting to N if not provided.",
      "k specifies the offset of the diagonal.",
      "dtype specifies the data type of the array, defaulting to floating point.",
      "device specifies the device or sharding to which the array will be committed.",
      "Returns an identity array of shape (N, M) or (N, N)."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.eye() Usage",
    "concepts": [
      "Demonstrates creating a simple 3x3 identity matrix.",
      "Demonstrates creating integer identity matrices with offset diagonals.",
      "Demonstrates creating a non-square identity matrix."
    ],
    "code_examples": [
      {
        "description": "Creates a simple 3x3 identity matrix using jnp.eye().",
        "code": "jnp.eye(3)"
      },
      {
        "description": "Creates an integer identity matrix with an upper diagonal offset of 1.",
        "code": "jnp.eye(3, k=1, dtype=int)"
      },
      {
        "description": "Creates an integer identity matrix with a lower diagonal offset of -1.",
        "code": "jnp.eye(3, k=-1, dtype=int)"
      },
      {
        "description": "Creates a non-square (3x5) identity matrix with an upper diagonal offset of 1.",
        "code": "jnp.eye(3, 5, k=1)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.fabs",
    "concepts": [
      "jnp.fabs computes the element-wise absolute values of a real-valued array.",
      "The input array must not have a complex dtype.",
      "The output array has the same shape as the input and dtype float.",
      "jnp.fabs is the JAX implementation of numpy.fabs."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Integer Inputs",
    "concepts": [
      "Demonstrates the use of jnp.fabs with integer arrays.",
      "The absolute value of each integer element is computed."
    ],
    "code_examples": [
      {
        "description": "Computes the absolute values of an integer array using jnp.fabs.",
        "code": "x = jnp.array([-5, -9, 1, 10, 15])\njnp.fabs(x)"
      },
      {
        "description": "Computes the absolute values of an integer array using jnp.fabs.",
        "code": "x = jnp.array([-5, -9, 1, 10, 15])\njnp.fabs(x)"
      }
    ]
  },
  {
    "title": "Examples with Float Inputs",
    "concepts": [
      "Demonstrates the use of jnp.fabs with float arrays.",
      "The absolute value of each float element is computed."
    ],
    "code_examples": [
      {
        "description": "Computes the absolute values of a float array using jnp.fabs.",
        "code": "x1 = jnp.array([-1.342, 5.649, 3.927])\njnp.fabs(x1)"
      },
      {
        "description": "Computes the absolute values of a float array using jnp.fabs.",
        "code": "x1 = jnp.array([-1.342, 5.649, 3.927])\njnp.fabs(x1)"
      }
    ]
  },
  {
    "title": "Examples with Boolean Inputs",
    "concepts": [
      "Demonstrates the use of jnp.fabs with boolean arrays.",
      "True is treated as 1 and False as 0.",
      "The absolute value of each boolean element is computed."
    ],
    "code_examples": [
      {
        "description": "Computes the absolute values of a boolean array using jnp.fabs.",
        "code": "x2 = jnp.array([True, False])\njnp.fabs(x2)"
      },
      {
        "description": "Computes the absolute values of a boolean array using jnp.fabs.",
        "code": "x2 = jnp.array([True, False])\njnp.fabs(x2)"
      }
    ]
  },
  {
    "title": "Floating Point Limits and Properties",
    "concepts": [
      "Describes machine limits for floating point types.",
      "Provides information about the `finfo` function and its attributes.",
      "Includes information about the number of bits occupied by a type.",
      "Defines `dtype` to return information, especially for complex input.",
      "Explains `eps` as the difference between 1.0 and the next largest representable float.",
      "Explains `epsneg` as the difference between 1.0 and the next smallest representable float.",
      "Defines the number of bits in the exponent portion.",
      "Defines `machep` as the exponent that yields eps.",
      "Defines `max` as the largest representable number.",
      "Defines `minexp` as the smallest positive power of the base (2) that causes overflow.",
      "Defines `min` as the smallest representable number.",
      "Defines `negep` as the exponent that yields epsneg.",
      "Defines `nexp` as the number of bits in the exponent.",
      "Defines `nmant` as the number of bits in the mantissa.",
      "Defines `precision` as the approximate number of decimal digits to which a float is precise.",
      "Defines `resolution` as the approximate decimal resolution of the type.",
      "Defines `smallest_normal` as the smallest positive floating point number with 1 as leading bit.",
      "Defines `tiny` as an alias for `smallest_normal`.",
      "Describes the usage of subnormal numbers according to IEEE-754."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Examples of numpy.finfo",
    "concepts": [
      "Demonstrates how to use `numpy.finfo` to get the dtype of a float64 number.",
      "Demonstrates how to use `numpy.finfo` to get the dtype of a complex64 number."
    ],
    "code_examples": [
      {
        "description": "Shows how to get the dtype of float64 using numpy.finfo.",
        "code": "import numpy as np\nnp.finfo(np.float64).dtype"
      },
      {
        "description": "Shows how to get the dtype of complex64 using numpy.finfo.",
        "code": "import numpy as np\nnp.finfo(np.complex64).dtype"
      },
      {
        "description": "Shows how to get the dtype of float64 using numpy.finfo.",
        "code": "import numpy as np\nnp.finfo(np.float64).dtype"
      },
      {
        "description": "Shows how to get the dtype of complex64 using numpy.finfo.",
        "code": "import numpy as np\nnp.finfo(np.complex64).dtype"
      }
    ]
  },
  {
    "title": "finfo Attributes",
    "concepts": [
      "Lists the available attributes of the finfo object.",
      "Describes `smallest_normal` as the value for the smallest normal number.",
      "Describes `tiny` as an alias for `smallest_normal`."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.fix()",
    "concepts": [
      "The function rounds input to the nearest integer towards zero.",
      "It is a JAX implementation of numpy.fix().",
      "It takes an ArrayLike as input.",
      "It returns an array with the same shape and dtype as the input, containing the rounded values."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.fix() usage",
    "concepts": [
      "The example demonstrates the usage of jax.numpy.fix() with a JAX array.",
      "It uses jax.random.uniform() to generate a random array.",
      "It utilizes jnp.printoptions() to control the output precision and suppress scientific notation.",
      "The example shows how jax.numpy.fix() rounds the elements of the array towards zero."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how jax.numpy.fix rounds the elements of a JAX array towards zero.",
        "code": "key = jax.random.key(0)\nx = jax.random.uniform(key, (3, 3), minval=-5, maxval=5)\nwith jnp.printoptions(precision=2, suppress=True):\n  print(x)\n\nprint(jnp.fix(x))"
      },
      {
        "description": "Demonstrates how jax.numpy.fix rounds the elements of a JAX array towards zero.",
        "code": "key = jax.random.key(0)\nx = jax.random.uniform(key, (3, 3), minval=-5, maxval=5)\nwith jnp.printoptions(precision=2, suppress=True):\n  print(x)\n\nprint(jnp.fix(x))"
      }
    ]
  },
  {
    "title": "Description of jnp.flatnonzero()",
    "concepts": [
      "jnp.flatnonzero() returns indices of nonzero elements in a flattened array.",
      "It is equivalent to nonzero(ravel(a))[0].",
      "The function mirrors the functionality of numpy.flatnonzero().",
      "The a parameter represents the input N-dimensional array.",
      "The size parameter allows static specification of number of nonzero entries.",
      "The fill_value parameter allows the user to specify a padding value when size is specified."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Examples",
    "concepts": [
      "Example demonstrates finding nonzero indices in a 2D array.",
      "The resulting indices can be used to extract the corresponding values from the flattened array."
    ],
    "code_examples": [
      {
        "description": "Example showing basic usage of jnp.flatnonzero().",
        "code": "x = jnp.array([[0, 5, 0],\n               [6, 0, 8]])\njnp.flatnonzero(x)"
      },
      {
        "description": "Demonstrates the equivalence between jnp.flatnonzero() and jnp.nonzero(x.ravel())[0].",
        "code": "x = jnp.array([[0, 5, 0],\n               [6, 0, 8]])\njnp.nonzero(x.ravel())[0]"
      },
      {
        "description": "Illustrates extracting nonzero entries from a flattened array using the indices returned by jnp.flatnonzero().",
        "code": "x = jnp.array([[0, 5, 0],\n               [6, 0, 8]])\nindices = jnp.flatnonzero(x)\nx.ravel()[indices]"
      }
    ]
  },
  {
    "title": "Overview",
    "concepts": [
      "The document describes an abstract base class for scalar types without predefined length.",
      "The size of these types depends on the numpy.dtype instantiation."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "The class provides various methods that are identical to corresponding array attributes.",
      "Methods include arithmetic, manipulation, and information retrieval operations."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "The class provides various attributes that are identical to corresponding array attributes.",
      "Attributes describe properties of the scalar such as data type, shape, and memory layout."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.numpy.flip",
    "concepts": [
      "jax.numpy.flip reverses the order of elements in an array along a given axis.",
      "The input array can be of any shape.",
      "The axis argument specifies the axis or axes along which to flip.",
      "If axis is None, the array is flipped along all axes.",
      "jax.numpy.fliplr() reverses the order along axis 1 (left/right).",
      "jax.numpy.flipud() reverses the order along axis 0 (up/down)."
    ],
    "code_examples": []
  },
  {
    "title": "Flipping a 2D Array",
    "concepts": [
      "Demonstrates flipping a 2D array along all axes by default."
    ],
    "code_examples": [
      {
        "description": "Flipping a 2D array without specifying the axis reverses the order of elements along both axes.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[1, 2],\n              [3, 4]])\n\njnp.flip(x1)"
      }
    ]
  },
  {
    "title": "Flipping along a Specific Axis",
    "concepts": [
      "Demonstrates flipping a 2D array along a specific axis."
    ],
    "code_examples": [
      {
        "description": "Flipping a 2D array along axis 1 reverses the order of elements in each row.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[1, 2],\n              [3, 4]])\n\njnp.flip(x1, axis=1)"
      }
    ]
  },
  {
    "title": "Flipping a 3D Array",
    "concepts": [
      "Demonstrates flipping a 3D array along all axes by default."
    ],
    "code_examples": [
      {
        "description": "Flipping a 3D array without specifying the axis reverses the order of elements along all axes.",
        "code": "import jax.numpy as jnp\n\nx2 = jnp.arange(1, 9).reshape(2, 2, 2)\n\nx2\n\njnp.flip(x2)"
      }
    ]
  },
  {
    "title": "Flipping along Multiple Axes",
    "concepts": [
      "Demonstrates flipping a 3D array along multiple specified axes."
    ],
    "code_examples": [
      {
        "description": "Flipping a 3D array along axes 1 and 2 reverses the order of elements along those specific axes.",
        "code": "import jax.numpy as jnp\n\nx2 = jnp.arange(1, 9).reshape(2, 2, 2)\n\njnp.flip(x2, axis=[1, 2])"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.fliplr",
    "concepts": [
      "jax.numpy.fliplr reverses the order of elements along axis 1 of an array.",
      "The input array must have at least two dimensions.",
      "The function returns a new array with the elements reversed along axis 1."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.fliplr Usage",
    "concepts": [
      "Demonstrates how to use jax.numpy.fliplr to reverse the elements along axis 1 of a 2D array."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the usage of jnp.fliplr with a 2x2 array.",
        "code": "x = jnp.array([[1, 2],\n              [3, 4]])\njnp.fliplr(x)"
      },
      {
        "description": "Another example demonstrating the usage of jnp.fliplr with a 2x2 array.",
        "code": "x = jnp.array([[1, 2],\n              [3, 4]])\njnp.fliplr(x)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.flipud()",
    "concepts": [
      "Reverses the order of elements in an array along axis 0.",
      "JAX implementation of numpy.flipud().",
      "The input array must have at least one dimension.",
      "Returns an array with elements reversed along axis 0.",
      "See also: jax.numpy.flip(), jax.numpy.fliplr()"
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.flipud() Usage",
    "concepts": [
      "Demonstrates reversing the order of elements in a 2D JAX array along the vertical axis (axis 0)."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of jnp.flipud() to reverse the order of rows in a 2D array.",
        "code": "x = jnp.array([[1, 2],\n              [3, 4]])\njnp.flipud(x)"
      },
      {
        "description": "Demonstrates the use of jnp.flipud() to reverse the order of rows in a 2D array. (Duplicated Example)",
        "code": "x = jnp.array([[1, 2],\n              [3, 4]])\njnp.flipud(x)"
      }
    ]
  },
  {
    "title": "Alias of float64",
    "concepts": [
      "The document defines an alias.",
      "The alias refers to the float64 data type."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jnp.float_power",
    "concepts": [
      "Calculates element-wise base x exponential of y.",
      "JAX implementation of numpy.float_power.",
      "Handles scalar and array inputs for bases (x) and exponents (y).",
      "Inputs should have the same shape or be broadcast compatible.",
      "Returns an array containing the base x exponentials of y, promoting to inexact dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with same shape",
    "concepts": [
      "Demonstrates jnp.float_power with inputs of the same shape."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating jnp.float_power with arrays of the same shape.",
        "code": "x = jnp.array([3, 1, -5])\ny = jnp.array([2, 4, -1])\njnp.float_power(x, y)"
      },
      {
        "description": "Duplicated example demonstrating jnp.float_power with arrays of the same shape.",
        "code": "x = jnp.array([3, 1, -5])\ny = jnp.array([2, 4, -1])\njnp.float_power(x, y)"
      }
    ]
  },
  {
    "title": "Examples with broadcast compatibility",
    "concepts": [
      "Illustrates jnp.float_power with broadcast-compatible inputs.",
      "Demonstrates how jnp.float_power handles broadcasting."
    ],
    "code_examples": [
      {
        "description": "Example showing jnp.float_power with broadcast compatibility.",
        "code": "x1 = jnp.array([[2, -4, 1],\n                [-1, 2, 3]])\ny1 = jnp.array([-2, 1, 4])\njnp.float_power(x1, y1)"
      },
      {
        "description": "Duplicated example showing jnp.float_power with broadcast compatibility.",
        "code": "x1 = jnp.array([[2, -4, 1],\n                [-1, 2, 3]])\ny1 = jnp.array([-2, 1, 4])\njnp.float_power(x1, y1)"
      }
    ]
  },
  {
    "title": "Handling Negative Values and Non-Integer Exponents",
    "concepts": [
      "jnp.float_power produces nan for negative values raised to non-integer values."
    ],
    "code_examples": [
      {
        "description": "Example showing how jnp.float_power returns NaN for negative bases and non-integer exponents.",
        "code": "jnp.float_power(-3, 1.7)"
      },
      {
        "description": "Duplicated example showing how jnp.float_power returns NaN for negative bases and non-integer exponents.",
        "code": "jnp.float_power(-3, 1.7)"
      }
    ]
  },
  {
    "title": "JAX Float16 Scalar Constructor",
    "concepts": [
      "JAX provides scalar constructors for data types like float16.",
      "JAX represents scalars as zero-dimensional arrays, unlike NumPy.",
      "The constructor takes an argument of any type and converts it to a float16 array."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Scalar Constructor",
    "concepts": [
      "JAX scalar constructor is of type float32.",
      "JAX represents scalars as zero-dimensional arrays, unlike NumPy which defines scalar types for each data type."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Scalar Constructor",
    "concepts": [
      "JAX provides a scalar constructor of type float64.",
      "NumPy defines scalar types for each data type.",
      "JAX represents scalars as zero-dimensional arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Overview",
    "concepts": [
      "Describes the abstract base class for floating-point scalar types."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "Lists available methods for the floating-point scalar type.",
      "Many scalar methods are identical to corresponding array attributes."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "Lists available attributes for the floating-point scalar type.",
      "Many scalar attributes are identical to corresponding array attributes.",
      "Describes attributes such as data, dtype, flags, and shape."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.floor",
    "concepts": [
      "The function rounds input to the nearest integer downwards.",
      "It's a JAX implementation of numpy.floor.",
      "The input array must not have a complex dtype.",
      "The output is an array with the same shape and dtype as the input, containing the rounded values.",
      "The rounded values are the nearest integer less than or equal to the original value."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of jax.numpy.floor",
    "concepts": [
      "The example shows how to use jax.numpy.floor to round a JAX array downwards to the nearest integer.",
      "jax.random.uniform is used to generate a sample array.",
      "jnp.printoptions is used to control the printing precision of the array.",
      "The jnp.floor function is applied to the array x."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of jnp.floor with a randomly generated array.",
        "code": "key = jax.random.key(42)\nx = jax.random.uniform(key, (3, 3), minval=-5, maxval=5)\nwith jnp.printoptions(precision=2, suppress=True):\n    print(x)\nprint(jnp.floor(x))"
      },
      {
        "description": "Demonstrates the usage of jnp.floor with a randomly generated array.",
        "code": "key = jax.random.key(42)\nx = jax.random.uniform(key, (3, 3), minval=-5, maxval=5)\nwith jnp.printoptions(precision=2, suppress=True):\n    print(x)\nprint(jnp.floor(x))"
      }
    ]
  },
  {
    "title": "Description of jnp.fmin",
    "concepts": [
      "jnp.fmin returns the element-wise minimum of two input arrays.",
      "It is a JAX implementation of numpy.fmin().",
      "The input arrays must have the same shape or be broadcast compatible.",
      "The function handles finite numbers, NaNs, infinities, and -infinities according to specific rules."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.fmin with scalars and arrays",
    "concepts": [
      "Demonstrates the use of jnp.fmin with scalar and array inputs.",
      "Shows how jnp.fmin performs element-wise comparison.",
      "Illustrates broadcasting when the inputs have different shapes.",
      "Demonstrates use of jnp.fmin with inf and nan."
    ],
    "code_examples": [
      {
        "description": "jnp.fmin with two scalars",
        "code": "jnp.fmin(2, 3)"
      },
      {
        "description": "jnp.fmin with a scalar and an array",
        "code": "jnp.fmin(2, jnp.array([1, 4, 2, -1]))"
      },
      {
        "description": "jnp.fmin with two arrays",
        "code": "x1 = jnp.array([1, 3, 2])\nx2 = jnp.array([2, 1, 4])\njnp.fmin(x1, x2)"
      },
      {
        "description": "jnp.fmin with arrays of different shapes using broadcasting",
        "code": "x3 = jnp.array([1, 5, 3])\nx4 = jnp.array([[2, 3, 1],\n              [5, 6, 7]])\njnp.fmin(x3, x4)"
      },
      {
        "description": "jnp.fmin with inf and nan",
        "code": "nan = jnp.nan\nx5 = jnp.array([jnp.inf, 5, nan])\nx6 = jnp.array([[2, 3, nan],\n              [nan, 6, 7]])\njnp.fmin(x5, x6)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.fmod",
    "concepts": [
      "jnp.fmod calculates the element-wise floating-point modulo operation.",
      "It is a JAX implementation of numpy.fmod.",
      "The function takes two array-like inputs, x1 (dividend) and x2 (divisor).",
      "x1 and x2 should either have the same shape or be broadcast compatible.",
      "The result has the same sign as the elements of x1.",
      "The result is equivalent to x1 - x2 * jnp.fix(x1 / x2)."
    ],
    "code_examples": []
  },
  {
    "title": "jnp.fmod Examples",
    "concepts": [
      "Demonstration of jnp.fmod with sample arrays.",
      "Verification of the equivalence: jnp.fmod(x1, x2) == x1 - x2 * jnp.fix(x1 / x2)"
    ],
    "code_examples": [
      {
        "description": "Example demonstrating jnp.fmod with two jax.numpy arrays and verification of the result using the formula x1 - x2 * jnp.fix(x1 / x2)",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[3, -1, 4],\n              [8, 5, -2]])\nx2 = jnp.array([2, 3, -5])\n\nprint(jnp.fmod(x1, x2))\nprint(x1 - x2 * jnp.fix(x1 / x2))"
      },
      {
        "description": "Repeating the previous example for demonstration.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[3, -1, 4],\n              [8, 5, -2]])\nx2 = jnp.array([2, 3, -5])\n\nprint(jnp.fmod(x1, x2))\nprint(x1 - x2 * jnp.fix(x1 / x2))"
      }
    ]
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.mod() and jax.numpy.remainder(): Returns the element-wise remainder of the division.",
      "jax.numpy.divmod(): Calculates the integer quotient and remainder of x1 by x2, element-wise."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jax.numpy.frexp",
    "concepts": [
      "The function jax.numpy.frexp() splits floating-point values into mantissa and twos exponent.",
      "It is a JAX implementation of numpy.frexp().",
      "The input is a real-valued array.",
      "The output is a tuple containing the mantissa (a floating-point value between -1 and 1) and the exponent (an integer).",
      "The original value can be reconstructed as x == mantissa * 2 ** exponent.",
      "jax.numpy.ldexp() computes the inverse of frexp."
    ],
    "code_examples": [
      {
        "description": "Split values into mantissa and exponent using jnp.frexp.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1., 2., 3., 4., 5.])\nm, e = jnp.frexp(x)\n\nprint(m)\nprint(e)"
      },
      {
        "description": "Reconstruct the original array from mantissa and exponent.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1., 2., 3., 4., 5.])\nm, e = jnp.frexp(x)\n\nprint(m * 2**e)"
      }
    ]
  },
  {
    "title": "Unimplemented jnp.fromfile",
    "concepts": [
      "jnp.fromfile is deliberately unimplemented due to potential non-purity.",
      "Non-purity makes it unsafe for JIT and other JAX transformations.",
      "Consider using jnp.asarray(np.fromfile(...)) as an alternative.",
      "Be cautious about side-effects when using np.fromfile within JAX transformations, specifically the consumption of the file object.",
      "Refer to Common Gotchas: Pure Functions for more information."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.numpy.fromfunction",
    "concepts": [
      "jax.numpy.fromfunction creates an array by applying a function over its indices.",
      "It is a JAX implementation of numpy.fromfunction.",
      "The JAX implementation uses jax.vmap, operating on scalar inputs rather than broadcasted inputs.",
      "The function argument takes N dynamic scalars and outputs a scalar.",
      "The shape argument is a length-N tuple of integers specifying the output shape.",
      "The dtype argument optionally specifies the data type of the inputs, defaulting to floating-point.",
      "Keyword arguments are passed statically to the function.",
      "The output is an array of shape 'shape' if the function returns a scalar, or a pytree of arrays if the function returns a non-scalar."
    ],
    "code_examples": []
  },
  {
    "title": "Multiplication Table Example",
    "concepts": [
      "Demonstrates generating a multiplication table using jnp.fromfunction and jnp.multiply.",
      "The shape parameter determines the dimensions of the table.",
      "The dtype parameter sets the data type of the table elements."
    ],
    "code_examples": [
      {
        "description": "Generates a 3x6 multiplication table with integer elements.",
        "code": "import jax.numpy as jnp\n\njnp.fromfunction(jnp.multiply, shape=(3, 6), dtype=int)"
      },
      {
        "description": "Generates a 3x6 multiplication table with integer elements. (duplicate)",
        "code": "import jax.numpy as jnp\n\njnp.fromfunction(jnp.multiply, shape=(3, 6), dtype=int)"
      }
    ]
  },
  {
    "title": "Non-Scalar Return Value Example",
    "concepts": [
      "Shows how jnp.fromfunction handles functions that return non-scalar values.",
      "The output array will have leading dimensions specified by the shape parameter.",
      "Illustrates a function that multiplies (x+1) by jnp.arange(3)."
    ],
    "code_examples": [
      {
        "description": "Defines a function f(x) that returns (x+1) * jnp.arange(3), then uses jnp.fromfunction to create a 2x3 array.",
        "code": "import jax.numpy as jnp\n\ndef f(x):\n    return (x + 1) * jnp.arange(3)\n\njnp.fromfunction(f, shape=(2,))"
      },
      {
        "description": "Defines a function f(x) that returns (x+1) * jnp.arange(3), then uses jnp.fromfunction to create a 2x3 array. (duplicate)",
        "code": "import jax.numpy as jnp\n\ndef f(x):\n    return (x + 1) * jnp.arange(3)\n\njnp.fromfunction(f, shape=(2,))"
      }
    ]
  },
  {
    "title": "Multiple Return Values Example",
    "concepts": [
      "Demonstrates jnp.fromfunction with functions that return multiple values.",
      "Each returned value is mapped independently to create separate arrays.",
      "The example function calculates x+y and x*y."
    ],
    "code_examples": [
      {
        "description": "Defines a function f(x, y) that returns x+y and x*y, then uses jnp.fromfunction to create two arrays: x_plus_y and x_times_y.",
        "code": "import jax.numpy as jnp\n\ndef f(x, y):\n    return x + y, x * y\n\nx_plus_y, x_times_y = jnp.fromfunction(f, shape=(3, 5))\nprint(x_plus_y)\nprint(x_times_y)"
      },
      {
        "description": "Defines a function f(x, y) that returns x+y and x*y, then uses jnp.fromfunction to create two arrays: x_plus_y and x_times_y. (duplicate)",
        "code": "import jax.numpy as jnp\n\ndef f(x, y):\n    return x + y, x * y\n\nx_plus_y, x_times_y = jnp.fromfunction(f, shape=(3, 5))\nprint(x_plus_y)\nprint(x_times_y)"
      }
    ]
  },
  {
    "title": "Difference from NumPy Implementation",
    "concepts": [
      "Highlights the key difference between jax.numpy.fromfunction and numpy.fromfunction.",
      "numpy.fromfunction expects the function to operate element-wise on the full grid of input values.",
      "jax.numpy.fromfunction vectorizes the function using jax.vmap, so it expects the function to operate on scalar values."
    ],
    "code_examples": []
  },
  {
    "title": "NumPy Example",
    "concepts": [
      "Shows how numpy.fromfunction expects the function to operate element-wise on the full grid of input values.",
      "The function receives arrays with the shape of the desired output."
    ],
    "code_examples": [
      {
        "description": "Defines a function f(x, y) that prints the shapes of x and y, and returns x + y.  It then uses np.fromfunction to create a 2x3 array.  This demonstrates that the function receives arrays of shape (2, 3).",
        "code": "import numpy as np\n\ndef f(x, y):\n    print(f\"{x.shape=}\\n{y.shape=}\")\n    return x + y\n\nnp.fromfunction(f, (2, 3))"
      },
      {
        "description": "Defines a function f(x, y) that prints the shapes of x and y, and returns x + y.  It then uses np.fromfunction to create a 2x3 array.  This demonstrates that the function receives arrays of shape (2, 3). (duplicate)",
        "code": "import numpy as np\n\ndef f(x, y):\n    print(f\"{x.shape=}\\n{y.shape=}\")\n    return x + y\n\nnp.fromfunction(f, (2, 3))"
      }
    ]
  },
  {
    "title": "JAX Example",
    "concepts": [
      "Demonstrates how jax.numpy.fromfunction expects the function to operate on scalar values.",
      "The function receives scalar inputs due to the use of jax.vmap.",
      "Shows that the function receives scalar inputs because x.shape and y.shape are both ()"
    ],
    "code_examples": [
      {
        "description": "Defines a function f(x, y) that prints the shapes of x and y, and returns x + y.  It then uses jnp.fromfunction to create a 2x3 array.  This demonstrates that the function receives scalar inputs.",
        "code": "import jax.numpy as jnp\n\ndef f(x, y):\n    print(f\"{x.shape=}\\n{y.shape=}\")\n    return x + y\n\njnp.fromfunction(f, (2, 3))"
      },
      {
        "description": "Defines a function f(x, y) that prints the shapes of x and y, and returns x + y.  It then uses jnp.fromfunction to create a 2x3 array.  This demonstrates that the function receives scalar inputs. (duplicate)",
        "code": "import jax.numpy as jnp\n\ndef f(x, y):\n    print(f\"{x.shape=}\\n{y.shape=}\")\n    return x + y\n\njnp.fromfunction(f, (2, 3))"
      }
    ]
  },
  {
    "title": "Introduction to `frompyfunc`",
    "concepts": [
      "`frompyfunc` creates a JAX ufunc from a scalar function.",
      "The function takes `nin` scalar inputs and returns `nout` outputs.",
      "The `identity` parameter specifies the identity element of the operation."
    ],
    "code_examples": []
  },
  {
    "title": "Creating a Ufunc Similar to `jax.numpy.add`",
    "concepts": [
      "The example demonstrates how to create a ufunc similar to `jax.numpy.add` using `frompyfunc` and the `operator.add` function."
    ],
    "code_examples": [
      {
        "description": "Creating a ufunc that mimics jax.numpy.add",
        "code": "import operator\n\nadd = frompyfunc(operator.add, nin=2, nout=1, identity=0)"
      },
      {
        "description": "Creating a ufunc that mimics jax.numpy.add",
        "code": "import operator\n\nadd = frompyfunc(operator.add, nin=2, nout=1, identity=0)"
      }
    ]
  },
  {
    "title": "Using the Created Ufunc",
    "concepts": [
      "The created ufunc supports standard jax.numpy.ufunc methods.",
      "Methods such as `outer`, `reduce`, `accumulate`, and `at` can be used with the ufunc."
    ],
    "code_examples": [
      {
        "description": "Demonstrating the use of the custom 'add' ufunc with various methods like outer, reduce, accumulate, and at.",
        "code": "x = jnp.arange(4)\nadd(x, 10)\nadd.outer(x, x)\nadd.reduce(x)\nadd.accumulate(x)\nadd.at(x, 1, 10, inplace=False)"
      },
      {
        "description": "Demonstrating the use of the custom 'add' ufunc with various methods like outer, reduce, accumulate, and at.",
        "code": "x = jnp.arange(4)\nadd(x, 10)\nadd.outer(x, x)\nadd.reduce(x)\nadd.accumulate(x)\nadd.at(x, 1, 10, inplace=False)"
      }
    ]
  },
  {
    "title": "JAX NumPy fromstring Function",
    "concepts": [
      "The function converts a string into a 1-D JAX array.",
      "It is a JAX implementation of numpy.fromstring().",
      "The 'string' parameter is the input string containing the data.",
      "The 'dtype' parameter specifies the desired data type (default is float).",
      "The 'count' parameter specifies the number of items to read (default is -1, meaning all).",
      "The 'sep' parameter specifies the separator between values in the string."
    ],
    "code_examples": [
      {
        "description": "Converts a string of integers separated by spaces into a JAX array of integers.",
        "code": "jnp.fromstring(\"1 2 3\", dtype=int, sep=\" \")"
      },
      {
        "description": "Converts a string of floats separated by commas into a JAX array of floats, reading only the first two values.",
        "code": "jnp.fromstring(\"0.1, 0.2, 0.3\", dtype=float, count=2, sep=\",\")"
      },
      {
        "description": "Converts a string of integers separated by spaces into a JAX array of integers.",
        "code": "jnp.fromstring(\"1 2 3\", dtype=int, sep=\" \")"
      },
      {
        "description": "Converts a string of floats separated by commas into a JAX array of floats, reading only the first two values.",
        "code": "jnp.fromstring(\"0.1, 0.2, 0.3\", dtype=float, count=2, sep=\",\")"
      }
    ]
  },
  {
    "title": "JAX NumPy Full Function",
    "concepts": [
      "Creates an array filled with a specified value.",
      "JAX implementation of numpy.full().",
      "The 'shape' parameter defines the shape of the array.",
      "The 'fill_value' parameter specifies the value to fill the array with (scalar or array).",
      "The 'dtype' parameter specifies the data type of the array (defaults to fill_value's dtype).",
      "The 'device' parameter specifies the device or sharding for the array.",
      "fill_value can be broadcast to the specified shape."
    ],
    "code_examples": [
      {
        "description": "Creates a 1D JAX array of shape (4,) filled with the value 2, with a float32 data type.",
        "code": "jnp.full(4, 2, dtype=float)"
      },
      {
        "description": "Creates a 2D JAX array of shape (2, 3) filled with the boolean value False.",
        "code": "jnp.full((2, 3), 0, dtype=bool)"
      },
      {
        "description": "Creates a 1D JAX array of shape (4,) filled with the value 2, with a float32 data type.",
        "code": "jnp.full(4, 2, dtype=float)"
      },
      {
        "description": "Creates a 2D JAX array of shape (2, 3) filled with the boolean value False.",
        "code": "jnp.full((2, 3), 0, dtype=bool)"
      },
      {
        "description": "Creates a 2D JAX array of shape (2, 3) where each row is filled with the values [0, 1, 2] using broadcasting.",
        "code": "jnp.full((2, 3), fill_value=jnp.arange(3))"
      },
      {
        "description": "Creates a 2D JAX array of shape (2, 3) where each row is filled with the values [0, 1, 2] using broadcasting.",
        "code": "jnp.full((2, 3), fill_value=jnp.arange(3))"
      }
    ]
  },
  {
    "title": "Introduction to NumPy Scalar Types",
    "concepts": [
      "numpy scalar types derive from a base class.",
      "The base class exposes the same API as ndarray.",
      "Users should derive custom scalar types from this base class."
    ],
    "code_examples": []
  },
  {
    "title": "Methods of NumPy Scalar Types",
    "concepts": [
      "Scalar types have methods like all, any, argmax, argmin, etc.",
      "These methods are identical to the corresponding array attributes.",
      "The methods listed include mathematical operations, array manipulation, and data access."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes of NumPy Scalar Types",
    "concepts": [
      "Scalar types have attributes like T, base, data, dtype, flags, etc.",
      "Many attributes are identical to the corresponding array attributes.",
      "Attributes provide information about the scalar's data, shape, and memory layout."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.numpy.geomspace",
    "concepts": [
      "jax.numpy.geomspace generates geometrically-spaced values.",
      "It is a JAX implementation of numpy.geomspace().",
      "It takes start, stop, num, endpoint, dtype, and axis as parameters.",
      "It returns an array containing the geometrically-spaced values."
    ],
    "code_examples": []
  },
  {
    "title": "Basic geomspace Example",
    "concepts": [
      "Generating 5 geometrically-spaced values between 1 and 16.",
      "The endpoint is included by default."
    ],
    "code_examples": [
      {
        "description": "List 5 geometrically-spaced values between 1 and 16.",
        "code": "import jax.numpy as jnp\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.geomspace(1, 16, 5))"
      },
      {
        "description": "List 5 geometrically-spaced values between 1 and 16.",
        "code": "import jax.numpy as jnp\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.geomspace(1, 16, 5))"
      }
    ]
  },
  {
    "title": "geomspace with endpoint=False",
    "concepts": [
      "Generating 4 geometrically-spaced values between 1 and 16.",
      "The endpoint is excluded."
    ],
    "code_examples": [
      {
        "description": "List 4 geometrically-spaced values between 1 and 16, with endpoint=False.",
        "code": "import jax.numpy as jnp\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.geomspace(1, 16, 4, endpoint=False))"
      },
      {
        "description": "List 4 geometrically-spaced values between 1 and 16, with endpoint=False.",
        "code": "import jax.numpy as jnp\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.geomspace(1, 16, 4, endpoint=False))"
      }
    ]
  },
  {
    "title": "Multi-dimensional geomspace",
    "concepts": [
      "Demonstrates the use of geomspace with multi-dimensional arrays.",
      "The start and stop parameters are arrays.",
      "Generates geometrically spaced values along the default axis (axis=0)."
    ],
    "code_examples": [
      {
        "description": "Multi-dimensional geomspace example.",
        "code": "import jax.numpy as jnp\n\nstart = jnp.array([1, 1000])\nstop = jnp.array([27, 1])\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.geomspace(start, stop, 4))"
      },
      {
        "description": "Multi-dimensional geomspace example.",
        "code": "import jax.numpy as jnp\n\nstart = jnp.array([1, 1000])\nstop = jnp.array([27, 1])\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.geomspace(start, stop, 4))"
      }
    ]
  },
  {
    "title": "Overview of JAX Array Printing",
    "concepts": [
      "JAX array printing is handled via NumPy.",
      "NumPy's print options apply to JAX arrays.",
      "Refer to numpy.set_printoptions() for details on available options."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Numerical Gradients with JAX",
    "concepts": [
      "JAX implementation of numpy.gradient().",
      "Computes the gradient using second-order finite differences.",
      "Distinction between jnp.gradient() and jax.grad(): numerical vs. automatic differentiation."
    ],
    "code_examples": []
  },
  {
    "title": "Function and Gradient Definitions",
    "concepts": [
      "Definition of a function f(x) using JAX.",
      "Definition of the exact analytical gradient gradf_exact(x)."
    ],
    "code_examples": [
      {
        "description": "Defines a function f(x) that combines sine and exponential functions.",
        "code": "def f(x):\n    return jnp.sin(x) * jnp.exp(-x / 4)"
      },
      {
        "description": "Defines the exact analytical gradient of f(x).",
        "code": "def gradf_exact(x):\n    # exact analytical gradient of f(x)\n    return -f(x) / 4 + jnp.cos(x) * jnp.exp(-x / 4)"
      }
    ]
  },
  {
    "title": "Comparing Numerical, Automatic, and Exact Gradients",
    "concepts": [
      "Comparison of numerical gradient (jnp.gradient), automatic gradient (jax.grad), and exact gradient.",
      "Demonstrates the approximation error of numerical gradients.",
      "Usage of jax.vmap to vectorize the gradient computation.",
      "Use of jnp.linspace to create an array of x values.",
      "Using jnp.printoptions to format the output."
    ],
    "code_examples": [
      {
        "description": "Compares numerical, automatic, and exact gradients for the defined function f(x).  It uses jnp.linspace to generate x values, then computes the numerical gradient using jnp.gradient, the automatic gradient using jax.grad and jax.vmap, and compares to the exact analytical gradient.",
        "code": "x = jnp.linspace(0, 5, 10)\n\n\ndef f(x):\n    return jnp.sin(x) * jnp.exp(-x / 4)\n\n\ndef gradf_exact(x):\n    # exact analytical gradient of f(x)\n    return -f(x) / 4 + jnp.cos(x) * jnp.exp(-x / 4)\n\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(\"numerical gradient:\", jnp.gradient(f(x), x))\n    print(\"automatic gradient:\", jax.vmap(jax.grad(f))(x))\n    print(\"exact gradient:    \", gradf_exact(x))"
      },
      {
        "description": "Compares numerical, automatic, and exact gradients for the defined function f(x).  It uses jnp.linspace to generate x values, then computes the numerical gradient using jnp.gradient, the automatic gradient using jax.grad and jax.vmap, and compares to the exact analytical gradient.",
        "code": "x = jnp.linspace(0, 5, 10)\n\n\ndef f(x):\n    return jnp.sin(x) * jnp.exp(-x / 4)\n\n\n\ndef gradf_exact(x):\n    # exact analytical gradient of f(x)\n    return -f(x) / 4 + jnp.cos(x) * jnp.exp(-x / 4)\n\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(\"numerical gradient:\", jnp.gradient(f(x), x))\n    print(\"automatic gradient:\", jax.vmap(jax.grad(f))(x))\n    print(\"exact gradient:    \", gradf_exact(x))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.greater",
    "concepts": [
      "Returns element-wise truth value of x > y.",
      "JAX implementation of numpy.greater.",
      "Inputs must have the same shape or be broadcast compatible.",
      "Returns an array containing boolean values (True if x > y, False otherwise)."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Scalar Inputs",
    "concepts": [
      "Demonstrates the use of jnp.greater with scalar inputs.",
      "Compares two scalar values and returns a boolean array."
    ],
    "code_examples": [
      {
        "description": "Compares two scalar values using jnp.greater.",
        "code": "jnp.greater(5, 2)"
      },
      {
        "description": "Compares two scalar values using jnp.greater.",
        "code": "jnp.greater(5, 2)"
      }
    ]
  },
  {
    "title": "Examples with Arrays of the Same Shape",
    "concepts": [
      "Demonstrates the use of jnp.greater with arrays of the same shape.",
      "Performs element-wise comparison between two arrays."
    ],
    "code_examples": [
      {
        "description": "Compares two arrays of the same shape using jnp.greater.",
        "code": "x = jnp.array([5, 9, -2])\ny = jnp.array([4, -1, 6])\njnp.greater(x, y)"
      },
      {
        "description": "Compares two arrays of the same shape using jnp.greater.",
        "code": "x = jnp.array([5, 9, -2])\ny = jnp.array([4, -1, 6])\njnp.greater(x, y)"
      }
    ]
  },
  {
    "title": "Examples with Broadcast Compatibility",
    "concepts": [
      "Demonstrates the use of jnp.greater with arrays that are broadcast compatible.",
      "Shows how jnp.greater handles broadcasting to perform element-wise comparison."
    ],
    "code_examples": [
      {
        "description": "Compares two broadcast compatible arrays using jnp.greater.",
        "code": "x1 = jnp.array([[5, -6, 7],\n              [-2, 5, 9]])\ny1 = jnp.array([-4, 3, 10])\njnp.greater(x1, y1)"
      },
      {
        "description": "Compares two broadcast compatible arrays using jnp.greater.",
        "code": "x1 = jnp.array([[5, -6, 7],\n              [-2, 5, 9]])\ny1 = jnp.array([-4, 3, 10])\njnp.greater(x1, y1)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.greater_equal",
    "concepts": [
      "The function returns element-wise truth value of x >= y.",
      "It is a JAX implementation of numpy.greater_equal.",
      "Inputs x and y must have the same shape or be broadcast compatible.",
      "It returns an array containing boolean values: True if x >= y, False otherwise."
    ],
    "code_examples": []
  },
  {
    "title": "Scalar Inputs Example",
    "concepts": [
      "Demonstrates the use of jnp.greater_equal with scalar inputs."
    ],
    "code_examples": [
      {
        "description": "Compares two scalar values using jnp.greater_equal.",
        "code": "jnp.greater_equal(4, 7)"
      },
      {
        "description": "Compares two scalar values using jnp.greater_equal.",
        "code": "jnp.greater_equal(4, 7)"
      }
    ]
  },
  {
    "title": "Inputs with Same Shape Example",
    "concepts": [
      "Illustrates jnp.greater_equal when inputs have the same shape.",
      "Compares corresponding elements of two arrays."
    ],
    "code_examples": [
      {
        "description": "Compares two arrays of the same shape using jnp.greater_equal.",
        "code": "x = jnp.array([2, 5, -1])\ny = jnp.array([-6, 4, 3])\njnp.greater_equal(x, y)"
      },
      {
        "description": "Compares two arrays of the same shape using jnp.greater_equal.",
        "code": "x = jnp.array([2, 5, -1])\ny = jnp.array([-6, 4, 3])\njnp.greater_equal(x, y)"
      }
    ]
  },
  {
    "title": "Inputs with Broadcast Compatibility Example",
    "concepts": [
      "Shows how jnp.greater_equal works with broadcast compatible inputs.",
      "Compares a 2D array with a 1D array."
    ],
    "code_examples": [
      {
        "description": "Compares a 2D array with a 1D array using broadcasting in jnp.greater_equal.",
        "code": "x1 = jnp.array([[3, -1, 4],\n                [5, 9, -6]])\ny1 = jnp.array([-1, 4, 2])\njnp.greater_equal(x1, y1)"
      },
      {
        "description": "Compares a 2D array with a 1D array using broadcasting in jnp.greater_equal.",
        "code": "x1 = jnp.array([[3, -1, 4],\n                [5, 9, -6]])\ny1 = jnp.array([-1, 4, 2])\njnp.greater_equal(x1, y1)"
      }
    ]
  },
  {
    "title": "Hamming Window Function",
    "concepts": [
      "Returns a Hamming window of a given size.",
      "JAX implementation of numpy.hamming().",
      "The input M represents the window size.",
      "Output is an array of size M representing the Hamming window."
    ],
    "code_examples": [
      {
        "description": "Example showing the usage of jnp.hamming() with a window size of 4.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.hamming(4))"
      },
      {
        "description": "Another example showing the usage of jnp.hamming() with a window size of 4, same as above.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.hamming(4))"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.numpy.bartlett(): return a Bartlett window of size M.",
      "jax.numpy.blackman(): return a Blackman window of size M.",
      "jax.numpy.hanning(): return a Hanning window of size M.",
      "jax.numpy.kaiser(): return a Kaiser window of size M."
    ],
    "code_examples": []
  },
  {
    "title": "Heaviside Step Function Definition",
    "concepts": [
      "The heaviside step function is defined as 0 for x < 0, x2 for x = 0, and 1 for x > 0.",
      "The function takes two arguments: x1 (input array) and x2 (value when x1 is 0).",
      "Complex dtype is not supported for x1 and x2.",
      "x1 and x2 must have the same shape or be broadcast compatible.",
      "The function returns an array containing the heaviside step function of x1, promoting to inexact dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Heaviside Function Examples",
    "concepts": [
      "Demonstrates the usage of jnp.heaviside with different inputs for x1 and x2.",
      "Shows how to use an array for x1 and an array for x2.",
      "Shows how to use an array for x1 and a scalar for x2.",
      "Shows how to use a scalar for x1 and an array for x2."
    ],
    "code_examples": [
      {
        "description": "Example showing jnp.heaviside with x1 as a 2D array and x2 as an array.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[-2, 0, 3],\n                [5, -1, 0],\n                [0, 7, -3]])\nx2 = jnp.array([2, 0.5, 1])\n\nprint(jnp.heaviside(x1, x2))"
      },
      {
        "description": "Example showing jnp.heaviside with x1 as a 2D array and x2 as a scalar.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[-2, 0, 3],\n                [5, -1, 0],\n                [0, 7, -3]])\n\nprint(jnp.heaviside(x1, 0.5))"
      },
      {
        "description": "Example showing jnp.heaviside with x1 as a scalar and x2 as an array.",
        "code": "import jax.numpy as jnp\n\nx2 = jnp.array([2, 0.5, 1])\n\nprint(jnp.heaviside(-3, x2))"
      },
      {
        "description": "Redundant example showing jnp.heaviside with x1 as a 2D array and x2 as an array.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[-2, 0, 3],\n                [5, -1, 0],\n                [0, 7, -3]])\nx2 = jnp.array([2, 0.5, 1])\n\nprint(jnp.heaviside(x1, x2))"
      },
      {
        "description": "Redundant example showing jnp.heaviside with x1 as a 2D array and x2 as a scalar.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[-2, 0, 3],\n                [5, -1, 0],\n                [0, 7, -3]])\n\nprint(jnp.heaviside(x1, 0.5))"
      },
      {
        "description": "Redundant example showing jnp.heaviside with x1 as a scalar and x2 as an array.",
        "code": "import jax.numpy as jnp\n\nx2 = jnp.array([2, 0.5, 1])\n\nprint(jnp.heaviside(-3, x2))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.histogram",
    "concepts": [
      "The function computes a 1-dimensional histogram.",
      "It is a JAX implementation of numpy.histogram().",
      "The input array 'a' contains the values to be binned.",
      "The 'bins' parameter specifies the number of bins or the bin edges.",
      "The 'range' parameter specifies the data range.",
      "The 'weights' parameter allows assigning weights to data points.",
      "The 'density' parameter normalizes the histogram.",
      "The function returns a tuple of (histogram, bin_edges)."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.histogram",
    "concepts": [
      "Demonstrates the basic usage of jnp.histogram with a specified number of bins.",
      "Shows how to retrieve the counts and bin edges."
    ],
    "code_examples": [
      {
        "description": "Calculates and prints the histogram and bin edges for an array 'a' using 8 bins.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ncounts, bin_edges = jnp.histogram(a, bins=8)\nprint(counts)\nprint(bin_edges)"
      }
    ]
  },
  {
    "title": "Specifying the Bin Range",
    "concepts": [
      "Demonstrates how to specify the range of the data using the 'range' parameter.",
      "Illustrates how this affects the resulting histogram."
    ],
    "code_examples": [
      {
        "description": "Calculates and prints the histogram and bin edges for array 'a' using a specified range of (0, 25) and 5 bins.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ncounts, bin_edges = jnp.histogram(a, range=(0, 25), bins=5)\nprint(counts)\nprint(bin_edges)"
      }
    ]
  },
  {
    "title": "Specifying Bin Edges Explicitly",
    "concepts": [
      "Demonstrates how to explicitly define the bin edges using an array.",
      "Illustrates how to pass the pre-defined bin edges to the 'bins' parameter."
    ],
    "code_examples": [
      {
        "description": "Calculates and prints the histogram for an array 'a' using pre-defined bin edges.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\nbin_edges = jnp.array([0, 10, 20, 30])\ncounts, _ = jnp.histogram(a, bins=bin_edges)\nprint(counts)"
      }
    ]
  },
  {
    "title": "Using Density Normalization",
    "concepts": [
      "Demonstrates how to normalize the histogram using the density=True parameter.",
      "Illustrates how to verify that the integral of the normalized histogram is close to 1."
    ],
    "code_examples": [
      {
        "description": "Calculates the normalized histogram for array 'a', calculates the integral, and verifies it is close to 1.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ndensity, bin_edges = jnp.histogram(a, density=True)\ndx = jnp.diff(bin_edges)\nnormed_sum = jnp.sum(density * dx)\nprint(jnp.allclose(normed_sum, 1.0))"
      }
    ]
  },
  {
    "title": "Introduction to histogram_bin_edges",
    "concepts": [
      "The function calculates bin edges for a histogram.",
      "It is a JAX implementation of numpy.histogram_bin_edges().",
      "The function takes an array of values to be binned.",
      "The function takes the number of bins as an argument.",
      "The function takes the range of the data as an argument. If not specified, the range is inferred from the data.",
      "The function returns an array of bin edges for the histogram."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of histogram_bin_edges",
    "concepts": [
      "Demonstrates calculating bin edges with a specified number of bins.",
      "Demonstrates calculating bin edges with a specified number of bins and a specified range."
    ],
    "code_examples": [
      {
        "description": "Calculates bin edges for the array 'a' with 5 bins, inferring the range from the data.",
        "code": "a = jnp.array([2, 5, 3, 6, 4, 1])\njnp.histogram_bin_edges(a, bins=5)"
      },
      {
        "description": "Calculates bin edges for the array 'a' with 5 bins and a specified range of (-10, 10).",
        "code": "a = jnp.array([2, 5, 3, 6, 4, 1])\njnp.histogram_bin_edges(a, bins=5, range=(-10, 10))"
      }
    ]
  },
  {
    "title": "Repeated Examples of histogram_bin_edges",
    "concepts": [
      "Redundant examples demonstrating calculating bin edges with a specified number of bins.",
      "Redundant examples demonstrating calculating bin edges with a specified number of bins and a specified range."
    ],
    "code_examples": [
      {
        "description": "Repeats the calculation of bin edges for the array 'a' with 5 bins, inferring the range from the data.",
        "code": "a = jnp.array([2, 5, 3, 6, 4, 1])\njnp.histogram_bin_edges(a, bins=5)"
      },
      {
        "description": "Repeats the calculation of bin edges for the array 'a' with 5 bins and a specified range of (-10, 10).",
        "code": "a = jnp.array([2, 5, 3, 6, 4, 1])\njnp.histogram_bin_edges(a, bins=5, range=(-10, 10))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.histogram2d",
    "concepts": [
      "Computes a 2-dimensional histogram from sample data.",
      "JAX implementation of numpy.histogram2d().",
      "It takes x and y coordinates as input, along with optional binning parameters.",
      "It returns the histogram counts, x-axis bin edges, and y-axis bin edges."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.histogram2d with Default Bins",
    "concepts": [
      "Demonstrates creating a 2D histogram with the default number of bins (10).",
      "Illustrates how to obtain the counts, x_edges, and y_edges from the function's output.",
      "Shows the shapes of the resulting arrays."
    ],
    "code_examples": [
      {
        "description": "Computes a 2D histogram of x and y values with default binning, then prints the shape of the counts array and the x and y edge arrays.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ny = jnp.array([2, 5, 6, 8, 13, 16, 17, 18])\n\ncounts, x_edges, y_edges = jnp.histogram2d(x, y, bins=8)\n\nprint(counts.shape)\nprint(x_edges)\nprint(y_edges)"
      },
      {
        "description": "Computes a 2D histogram of x and y values with default binning, then prints the shape of the counts array and the x and y edge arrays.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ny = jnp.array([2, 5, 6, 8, 13, 16, 17, 18])\n\ncounts, x_edges, y_edges = jnp.histogram2d(x, y, bins=8)\n\nprint(counts.shape)\nprint(x_edges)\nprint(y_edges)"
      }
    ]
  },
  {
    "title": "Specifying the Bin Range in jnp.histogram2d",
    "concepts": [
      "Demonstrates specifying the range of data for the histogram.",
      "Illustrates how the `range` parameter can be used to define the boundaries of the histogram in each dimension.",
      "Shows how the bin edges and counts are affected by the specified range."
    ],
    "code_examples": [
      {
        "description": "Computes a 2D histogram with a specified range for both x and y dimensions, and a given number of bins.  Prints the shape of the resulting `counts` array and the `x_edges` and `y_edges`.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ny = jnp.array([2, 5, 6, 8, 13, 16, 17, 18])\n\ncounts, x_edges, y_edges = jnp.histogram2d(x, y, range=[(0, 25), (0, 25)], bins=5)\n\nprint(counts.shape)\nprint(x_edges)\nprint(y_edges)"
      },
      {
        "description": "Computes a 2D histogram with a specified range for both x and y dimensions, and a given number of bins.  Prints the shape of the resulting `counts` array and the `x_edges` and `y_edges`.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ny = jnp.array([2, 5, 6, 8, 13, 16, 17, 18])\n\ncounts, x_edges, y_edges = jnp.histogram2d(x, y, range=[(0, 25), (0, 25)], bins=5)\n\nprint(counts.shape)\nprint(x_edges)\nprint(y_edges)"
      }
    ]
  },
  {
    "title": "Specifying Bin Edges Explicitly in jnp.histogram2d",
    "concepts": [
      "Demonstrates how to define the bin edges directly using arrays.",
      "Shows how to use pre-defined `x_edges` and `y_edges` to create the histogram.",
      "Illustrates the resulting count matrix based on the specified bin edges."
    ],
    "code_examples": [
      {
        "description": "Computes a 2D histogram with explicitly defined bin edges for both x and y dimensions. The resulting `counts` array represents the histogram.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ny = jnp.array([2, 5, 6, 8, 13, 16, 17, 18])\n\nx_edges = jnp.array([0, 10, 20, 30])\ny_edges = jnp.array([0, 10, 20, 30])\n\ncounts, _, _ = jnp.histogram2d(x, y, bins=[x_edges, y_edges])\n\nprint(counts)"
      },
      {
        "description": "Computes a 2D histogram with explicitly defined bin edges for both x and y dimensions. The resulting `counts` array represents the histogram.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ny = jnp.array([2, 5, 6, 8, 13, 16, 17, 18])\n\nx_edges = jnp.array([0, 10, 20, 30])\ny_edges = jnp.array([0, 10, 20, 30])\n\ncounts, _, _ = jnp.histogram2d(x, y, bins=[x_edges, y_edges])\n\nprint(counts)"
      }
    ]
  },
  {
    "title": "Using Density Normalization in jnp.histogram2d",
    "concepts": [
      "Demonstrates how to normalize the histogram to represent counts per unit area by setting `density=True`.",
      "Shows how to calculate the normalized sum to verify that the histogram is properly normalized.",
      "Illustrates the use of `jnp.diff` to compute the bin widths."
    ],
    "code_examples": [
      {
        "description": "Computes a normalized 2D histogram (density=True), calculates the sum of the density multiplied by the bin area, and verifies that the result is close to 1.0.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ny = jnp.array([2, 5, 6, 8, 13, 16, 17, 18])\n\ndensity, x_edges, y_edges = jnp.histogram2d(x, y, density=True)\n\ndx = jnp.diff(x_edges)\ndy = jnp.diff(y_edges)\n\nnormed_sum = jnp.sum(density * dx[:, None] * dy[None, :])\n\nprint(jnp.allclose(normed_sum, 1.0))"
      },
      {
        "description": "Computes a normalized 2D histogram (density=True), calculates the sum of the density multiplied by the bin area, and verifies that the result is close to 1.0.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 10, 11, 15, 19, 25])\ny = jnp.array([2, 5, 6, 8, 13, 16, 17, 18])\n\ndensity, x_edges, y_edges = jnp.histogram2d(x, y, density=True)\n\ndx = jnp.diff(x_edges)\ndy = jnp.diff(y_edges)\n\nnormed_sum = jnp.sum(density * dx[:, None] * dy[None, :])\n\nprint(jnp.allclose(normed_sum, 1.0))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.histogramdd",
    "concepts": [
      "jnp.histogramdd computes an N-dimensional histogram.",
      "The input array 'sample' has a shape of (N, D), representing N points in D dimensions.",
      "The 'bins' argument specifies the number of bins in each dimension or can be a sequence of bin edges.",
      "The 'range' argument specifies the range for each dimension; if not specified, it's inferred from the data.",
      "The 'weights' argument is an optional array specifying the weights of the data points.",
      "The 'density' argument, if True, returns a normalized histogram.",
      "The function returns a tuple of (histogram, bin_edges)."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.histogramdd",
    "concepts": [
      "Demonstrates how to compute a 3D histogram using jnp.histogramdd.",
      "Shows how to specify the number of bins and the range for each dimension.",
      "Illustrates the shape of the resulting 'counts' array and the structure of 'bin_edges'."
    ],
    "code_examples": [
      {
        "description": "Computes a 3D histogram with specified bins and range using jnp.histogramdd and prints the shape of the result and the bin edges.",
        "code": "key = jax.random.key(42)\na = jax.random.normal(key, (100, 3))\ncounts, bin_edges = jnp.histogramdd(a, bins=6, range=[(-3, 3), (-3, 3), (-3, 3)])\nprint(counts.shape)\nprint(bin_edges)"
      },
      {
        "description": "Computes a 3D histogram with specified bins and range using jnp.histogramdd and prints the shape of the result and the bin edges.",
        "code": "key = jax.random.key(42)\na = jax.random.normal(key, (100, 3))\ncounts, bin_edges = jnp.histogramdd(a, bins=6, range=[(-3, 3), (-3, 3), (-3, 3)])\nprint(counts.shape)\nprint(bin_edges)"
      }
    ]
  },
  {
    "title": "Using density=True for Normalized Histograms",
    "concepts": [
      "Demonstrates how to obtain a normalized histogram by setting density=True.",
      "Shows how to calculate bin widths and normalize the histogram to verify that it sums to 1."
    ],
    "code_examples": [
      {
        "description": "Computes a normalized 3D histogram (density=True) and verifies that the integral of the density is approximately 1.",
        "code": "density, bin_edges = jnp.histogramdd(a, density=True)\nbin_widths = map(jnp.diff, bin_edges)\ndx, dy, dz = jnp.meshgrid(*bin_widths, indexing='ij')\nnormed = jnp.sum(density * dx * dy * dz)\nprint(jnp.allclose(normed, 1.0))"
      },
      {
        "description": "Computes a normalized 3D histogram (density=True) and verifies that the integral of the density is approximately 1.",
        "code": "density, bin_edges = jnp.histogramdd(a, density=True)\nbin_widths = map(jnp.diff, bin_edges)\ndx, dy, dz = jnp.meshgrid(*bin_widths, indexing='ij')\nnormed = jnp.sum(density * dx * dy * dz)\nprint(jnp.allclose(normed, 1.0))"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.hsplit",
    "concepts": [
      "jax.numpy.hsplit splits an array into sub-arrays horizontally.",
      "It is a JAX implementation of numpy.hsplit().",
      "It's equivalent to jax.numpy.split() with axis=1, or axis=0 for 1D arrays.",
      "Refer to jax.numpy.split() documentation for details."
    ],
    "code_examples": []
  },
  {
    "title": "1D Array Splitting with jax.numpy.hsplit",
    "concepts": [
      "Demonstrates the usage of jax.numpy.hsplit with a one-dimensional array.",
      "The array is split into two sub-arrays."
    ],
    "code_examples": [
      {
        "description": "Splitting a 1D array into two sub-arrays using jax.numpy.hsplit.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5, 6])\nx1, x2 = jnp.hsplit(x, 2)\nprint(x1, x2)"
      },
      {
        "description": "Splitting a 1D array into two sub-arrays using jax.numpy.hsplit.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5, 6])\nx1, x2 = jnp.hsplit(x, 2)\nprint(x1, x2)"
      }
    ]
  },
  {
    "title": "2D Array Splitting with jax.numpy.hsplit",
    "concepts": [
      "Demonstrates the usage of jax.numpy.hsplit with a two-dimensional array.",
      "The array is split into two sub-arrays horizontally."
    ],
    "code_examples": [
      {
        "description": "Splitting a 2D array into two sub-arrays horizontally using jax.numpy.hsplit.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3, 4],\n              [5, 6, 7, 8]])\nx1, x2 = jnp.hsplit(x, 2)\nprint(x1)\nprint(x2)"
      },
      {
        "description": "Splitting a 2D array into two sub-arrays horizontally using jax.numpy.hsplit.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3, 4],\n              [5, 6, 7, 8]])\nx1, x2 = jnp.hsplit(x, 2)\nprint(x1)\nprint(x2)"
      }
    ]
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.split(): split an array along any axis.",
      "jax.numpy.vsplit(): split vertically, i.e. along axis=0",
      "jax.numpy.dsplit(): split depth-wise, i.e. along axis=2",
      "jax.numpy.array_split(): like split, but allows indices_or_sections to be an integer that does not evenly divide the size of the array."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.numpy.hstack",
    "concepts": [
      "jax.numpy.hstack() is a JAX implementation of numpy.hstack().",
      "For arrays of one or more dimensions, it's equivalent to jax.numpy.concatenate() with axis=1.",
      "It stacks arrays horizontally.",
      "Input arrays are promoted to at least rank 1."
    ],
    "code_examples": []
  },
  {
    "title": "Usage with Scalar Values",
    "concepts": [
      "hstack can be used to combine scalar values into a 1D array.",
      "Scalar inputs are treated as a sequence."
    ],
    "code_examples": [
      {
        "description": "Demonstrates hstack with scalar values.",
        "code": "jnp.hstack([1, 2, 3])\n# Output: Array([1, 2, 3], dtype=int32, weak_type=True)"
      },
      {
        "description": "Demonstrates hstack with scalar values.",
        "code": "jnp.hstack([1, 2, 3])\n# Output: Array([1, 2, 3], dtype=int32, weak_type=True)"
      }
    ]
  },
  {
    "title": "Usage with 1D Arrays",
    "concepts": [
      "hstack can combine 1D arrays into a single 1D array.",
      "The arrays are concatenated along the horizontal axis (axis=1, implicitly)."
    ],
    "code_examples": [
      {
        "description": "Demonstrates hstack with 1D arrays.",
        "code": "x = jnp.arange(3)\ny = jnp.ones(3)\njnp.hstack([x, y])\n# Output: Array([0., 1., 2., 1., 1., 1.], dtype=float32)"
      },
      {
        "description": "Demonstrates hstack with 1D arrays.",
        "code": "x = jnp.arange(3)\ny = jnp.ones(3)\njnp.hstack([x, y])\n# Output: Array([0., 1., 2., 1., 1., 1.], dtype=float32)"
      }
    ]
  },
  {
    "title": "Usage with 2D Arrays",
    "concepts": [
      "hstack can combine 2D arrays horizontally.",
      "Arrays must have compatible shapes along the non-horizontal axis."
    ],
    "code_examples": [
      {
        "description": "Demonstrates hstack with 2D arrays.",
        "code": "x = jnp.arange(3)\ny = jnp.ones(3)\nx = x.reshape(3, 1)\ny = y.reshape(3, 1)\njnp.hstack([x, y])\n# Output: Array([[0., 1.],\n#               [1., 1.],\n#               [2., 1.]], dtype=float32)"
      },
      {
        "description": "Demonstrates hstack with 2D arrays.",
        "code": "x = jnp.arange(3)\ny = jnp.ones(3)\nx = x.reshape(3, 1)\ny = y.reshape(3, 1)\njnp.hstack([x, y])\n# Output: Array([[0., 1.],\n#               [1., 1.],\n#               [2., 1.]], dtype=float32)"
      }
    ]
  },
  {
    "title": "Hypotenuse Calculation with jnp.hypot",
    "concepts": [
      "Calculates the hypotenuse of a right-angled triangle element-wise.",
      "JAX implementation of numpy.hypot.",
      "Accepts two ArrayLike inputs (x1 and x2) representing the legs of the triangle.",
      "Complex dtypes are not supported.",
      "x1 and x2 must have the same shape or be broadcast compatible.",
      "Returns an array containing the hypotenuse, promoting to inexact dtype.",
      "jnp.hypot is more numerically stable than jnp.sqrt(x1 ** 2 + x2 ** 2)."
    ],
    "code_examples": [
      {
        "description": "Demonstrates basic usage of jnp.hypot with scalar inputs.",
        "code": "jnp.hypot(3, 4)"
      },
      {
        "description": "Demonstrates usage of jnp.hypot with array inputs and jnp.printoptions for formatting the output.",
        "code": "x1 = jnp.array([[3, -2, 5],\n                [9, 1, -4]])\nx2 = jnp.array([-5, 6, 8])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.hypot(x1, x2)"
      },
      {
        "description": "Demonstrates basic usage of jnp.hypot with scalar inputs.",
        "code": "jnp.hypot(3, 4)"
      },
      {
        "description": "Demonstrates usage of jnp.hypot with array inputs and jnp.printoptions for formatting the output.",
        "code": "x1 = jnp.array([[3, -2, 5],\n                [9, 1, -4]])\nx2 = jnp.array([-5, 6, 8])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.hypot(x1, x2)"
      }
    ]
  },
  {
    "title": "Description of Modified Bessel Function of the First Kind, Zeroth Order",
    "concepts": [
      "The document describes the JAX implementation of the numpy.i0() function.",
      "The function calculates the modified Bessel function of the first kind, zeroth order.",
      "The function is defined by the series expansion: i0(x) = I_0(x) = sum_{k=0}^{infty} (x^2/4)^k / (k!)^2.",
      "The input 'x' can be a scalar or an array, but complex inputs are not supported.",
      "The output is an array containing the corresponding values of the modified Bessel function of x."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.i0() Usage",
    "concepts": [
      "The example demonstrates the usage of jnp.i0() with a JAX array.",
      "It calculates the modified Bessel function for the input array [-2, -1, 0, 1, 2]."
    ],
    "code_examples": [
      {
        "description": "Example usage of jnp.i0() with a JAX array to calculate the modified Bessel function of the first kind, zeroth order.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([-2, -1, 0, 1, 2])\n\njnp.i0(x)"
      },
      {
        "description": "Repeated example usage of jnp.i0() with the same JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([-2, -1, 0, 1, 2])\n\njnp.i0(x)"
      }
    ]
  },
  {
    "title": "Identity Matrix Creation with JAX",
    "concepts": [
      "JAX implementation of numpy.identity().",
      "Creates a square identity matrix of a specified size.",
      "The function takes the size 'n' as input.",
      "The optional dtype parameter allows specifying the data type of the matrix elements.",
      "Returns an identity array of shape (n, n)."
    ],
    "code_examples": [
      {
        "description": "A simple 3x3 identity matrix with default float32 dtype.",
        "code": "jnp.identity(3)"
      },
      {
        "description": "A 2x2 integer identity matrix.",
        "code": "jnp.identity(2, dtype=int)"
      }
    ]
  },
  {
    "title": "Methods",
    "concepts": [
      "The document mentions a method named `__init__`.",
      "The `__init__` method takes an argument `int_type`."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "The document lists several attributes.",
      "The attributes are: kind, bits, min, max, and dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.imag",
    "concepts": [
      "The function returns the element-wise imaginary part of a complex number or array.",
      "It is a JAX implementation of numpy.imag.",
      "The input can be an array-like object or a scalar.",
      "The output is an array containing the imaginary parts of the input elements."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.imag Usage",
    "concepts": [
      "Demonstrates how to use jnp.imag with different inputs.",
      "Shows the imaginary part of a real number is zero.",
      "Illustrates how to extract the imaginary part of a complex number.",
      "Demonstrates jnp.imag on an array of complex and real numbers."
    ],
    "code_examples": [
      {
        "description": "Example showing the imaginary part of the integer 4 is 0.",
        "code": "jnp.imag(4)\nArray(0, dtype=int32, weak_type=True)"
      },
      {
        "description": "Example showing the imaginary part of 5j is 5.0.",
        "code": "jnp.imag(5j)\nArray(5., dtype=float32, weak_type=True)"
      },
      {
        "description": "Example showing how to extract the imaginary part of each element in an array containing complex and real numbers.",
        "code": "x = jnp.array([2 + 3j, 5 - 1j, -3])\njnp.imag(x)\nArray([ 3., -1.,  0.], dtype=float32)"
      },
      {
        "description": "Example showing the imaginary part of the integer 4 is 0.",
        "code": "jnp.imag(4)\nArray(0, dtype=int32, weak_type=True)"
      },
      {
        "description": "Example showing the imaginary part of 5j is 5.0.",
        "code": "jnp.imag(5j)\nArray(5., dtype=float32, weak_type=True)"
      },
      {
        "description": "Example showing how to extract the imaginary part of each element in an array containing complex and real numbers.",
        "code": "x = jnp.array([2 + 3j, 5 - 1j, -3])\njnp.imag(x)\nArray([ 3., -1.,  0.], dtype=float32)"
      }
    ]
  },
  {
    "title": "Introduction to IndexExpression",
    "concepts": [
      "IndexExpression is used to build index tuples for arrays in a more convenient way.",
      "It provides a way to use array indexing syntax to create slice objects.",
      "Using `a[indices]` is equivalent to `a[np.index_exp[indices]]` for any array `a`.",
      "`np.index_exp[indices]` returns a tuple of slice objects.",
      "There are two predefined instances: `index_exp` which always returns a tuple, and `s_` which does not always return a tuple.",
      "The `maketuple` parameter determines whether a tuple is always returned.",
      "Using IndexExpression simplifies complex index expressions compared to using slice objects directly."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Examples of s_ and index_exp",
    "concepts": [
      "`s_` and `index_exp` are instances of IndexExpression.",
      "`s_[indices]` returns a slice object.",
      "`index_exp[indices]` returns a tuple containing the slice object."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of np.s_ to create a slice object.",
        "code": "import numpy as np\n\nnp.s_[2::2]"
      },
      {
        "description": "Demonstrates the use of np.index_exp to create a tuple containing a slice object.",
        "code": "import numpy as np\n\nnp.index_exp[2::2]"
      },
      {
        "description": "Shows how to use np.s_ to slice an array.",
        "code": "import numpy as np\n\nnp.array([0, 1, 2, 3, 4])[np.s_[2::2]]"
      }
    ]
  },
  {
    "title": "JAX Indices Function",
    "concepts": [
      "The function generates arrays of grid indices similar to NumPy's indices function.",
      "It takes dimensions, dtype, and sparse as input parameters.",
      "The dimensions parameter defines the shape of the grid.",
      "The dtype parameter specifies the data type of the indices.",
      "The sparse parameter determines whether to return sparse or dense indices.",
      "Returns a dense array or a tuple of sparse arrays."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the usage of jnp.indices() with a shape of (2, 3) and default sparse=False.",
        "code": "jnp.indices((2, 3))\nArray([[[0, 0, 0],\n        [1, 1, 1]],\n\n       [[0, 1, 2],\n        [0, 1, 2]]], dtype=int32)"
      },
      {
        "description": "Example demonstrating the usage of jnp.indices() with a shape of (2, 3) and sparse=True.",
        "code": "jnp.indices((2, 3), sparse=True)\n(Array([[0],\n        [1]], dtype=int32), Array([[0, 1, 2]], dtype=int32))"
      },
      {
        "description": "Example demonstrating the usage of jnp.indices() with a shape of (2, 3) and default sparse=False (repetition of the first example).",
        "code": "jnp.indices((2, 3))\nArray([[[0, 0, 0],\n        [1, 1, 1]],\n\n       [[0, 1, 2],\n        [0, 1, 2]]], dtype=int32)"
      },
      {
        "description": "Example demonstrating the usage of jnp.indices() with a shape of (2, 3) and sparse=True (repetition of the second example).",
        "code": "jnp.indices((2, 3), sparse=True)\n(Array([[0],\n        [1]], dtype=int32), Array([[0, 1, 2]], dtype=int32))"
      }
    ]
  },
  {
    "title": "Introduction to Numeric Scalar Types",
    "concepts": [
      "Numeric scalar types are abstract base classes.",
      "They represent numeric values with a potentially inexact representation.",
      "Floating-point numbers are examples of numeric scalar types."
    ],
    "code_examples": []
  },
  {
    "title": "Methods of Numeric Scalar Types",
    "concepts": [
      "Scalar methods are often identical to corresponding array attributes.",
      "Methods include: all, any, argmax, argmin, argsort, astype, byteswap, choose, clip, compress, conj, conjugate, copy, cumprod, cumsum, diagonal, dump, dumps, fill, flatten, getfield, item, max, mean, min, nonzero, prod, put, ravel, repeat, reshape, resize, round, searchsorted, setfield, setflags, sort, squeeze, std, sum, swapaxes, take, to_device, tobytes, tofile, tolist, tostring, trace, transpose, var, view"
    ],
    "code_examples": []
  },
  {
    "title": "Attributes of Numeric Scalar Types",
    "concepts": [
      "Scalar attributes are often identical to corresponding array attributes.",
      "Attributes include: T, base, data, device, dtype, flags, flat, imag, itemset, itemsize, nbytes, ndim, newbyteorder, ptp, real, shape, size, strides"
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.numpy.inner",
    "concepts": [
      "Computes the inner product of two arrays.",
      "JAX implementation of numpy.inner().",
      "Performs contraction along the last dimension of each input.",
      "Unlike jax.numpy.matmul() or jax.numpy.dot(), it always contracts the last dimension."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of jax.numpy.inner",
    "concepts": [
      "a: Input array of shape (..., N).",
      "b: Input array of shape (..., N).",
      "precision: Specifies the precision to use for the computation.",
      "preferred_element_type: Specifies the preferred data type for accumulation.",
      "Returns an array of shape (*a.shape[:-1], *b.shape[:-1]) containing the batched vector product of the inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.vecdot(): Conjugate multiplication along a specified axis.",
      "jax.numpy.tensordot(): General tensor multiplication.",
      "jax.numpy.matmul(): General batched matrix & vector multiplication."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.inner Usage",
    "concepts": [
      "Demonstrates the usage of jax.numpy.inner with 1D arrays.",
      "Demonstrates the usage of jax.numpy.inner with multi-dimensional arrays."
    ],
    "code_examples": [
      {
        "description": "Inner product of two 1D complex arrays.",
        "code": "a = jnp.array([1j, 3j, 4j])\nb = jnp.array([4., 2., 5.])\njnp.inner(a, b)"
      },
      {
        "description": "Inner product of two 1D complex arrays. (Duplicated from documentation)",
        "code": "a = jnp.array([1j, 3j, 4j])\nb = jnp.array([4., 2., 5.])\njnp.inner(a, b)"
      },
      {
        "description": "Inner product of two multi-dimensional arrays. Shows that batch dimensions are stacked.",
        "code": "a = jnp.ones((2, 3))\nb = jnp.ones((5, 3))\njnp.inner(a, b).shape"
      },
      {
        "description": "Inner product of two multi-dimensional arrays. Shows that batch dimensions are stacked. (Duplicated from documentation)",
        "code": "a = jnp.ones((2, 3))\nb = jnp.ones((5, 3))\njnp.inner(a, b).shape"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.insert",
    "concepts": [
      "The function jax.numpy.insert() inserts values into an array at specified indices.",
      "It is a JAX implementation of numpy.insert().",
      "The function returns a copy of the array with values inserted.",
      "The axis argument specifies the insertion axis for multi-dimensional arrays; if unspecified, the array is flattened.",
      "It uses ArrayLike objects for the input array, indices, and values to be inserted."
    ],
    "code_examples": []
  },
  {
    "title": "Inserting a single value",
    "concepts": [
      "A single value can be inserted into a 1D array at a specific index.",
      "The array is created using jnp.arange().",
      "The jnp.insert() function is used to insert the value at the specified index."
    ],
    "code_examples": [
      {
        "description": "Demonstrates inserting a single value into a 1D array at index 2.",
        "code": "x = jnp.arange(5)\njnp.insert(x, 2, 99)"
      },
      {
        "description": "Demonstrates inserting a single value into a 1D array at index 2. Repeated example.",
        "code": "x = jnp.arange(5)\njnp.insert(x, 2, 99)"
      }
    ]
  },
  {
    "title": "Inserting multiple identical values using a slice",
    "concepts": [
      "Multiple identical values can be inserted using a slice.",
      "A slice with a step is used to specify the indices for insertion.",
      "The value is inserted at every index specified by the slice."
    ],
    "code_examples": [
      {
        "description": "Demonstrates inserting the value -1 at every other index in the array.",
        "code": "jnp.insert(x, slice(None, None, 2), -1)"
      },
      {
        "description": "Demonstrates inserting the value -1 at every other index in the array. Repeated example.",
        "code": "jnp.insert(x, slice(None, None, 2), -1)"
      }
    ]
  },
  {
    "title": "Inserting multiple values using an index array",
    "concepts": [
      "Multiple distinct values can be inserted at different indices.",
      "An array of indices is used to specify the insertion locations.",
      "An array of values is used to specify the values to be inserted.",
      "The length of indices and values should match."
    ],
    "code_examples": [
      {
        "description": "Demonstrates inserting multiple values at different indices using numpy arrays.",
        "code": "indices = jnp.array([4, 2, 5])\nvalues = jnp.array([10, 11, 12])\njnp.insert(x, indices, values)"
      },
      {
        "description": "Demonstrates inserting multiple values at different indices using numpy arrays. Repeated example.",
        "code": "indices = jnp.array([4, 2, 5])\nvalues = jnp.array([10, 11, 12])\njnp.insert(x, indices, values)"
      }
    ]
  },
  {
    "title": "Inserting columns into a 2D array",
    "concepts": [
      "Columns can be inserted into a 2D array along a specified axis.",
      "The axis argument is used to specify the insertion axis (1 for columns).",
      "The indices and values arrays should be compatible with the axis of insertion."
    ],
    "code_examples": [
      {
        "description": "Demonstrates inserting columns into a 2D array at specified indices using the axis argument.",
        "code": "x = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\nindices = jnp.array([1, 3])\nvalues = jnp.array([[10, 11],\n                [12, 13]])\njnp.insert(x, indices, values, axis=1)"
      },
      {
        "description": "Demonstrates inserting columns into a 2D array at specified indices using the axis argument. Repeated example.",
        "code": "x = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\nindices = jnp.array([1, 3])\nvalues = jnp.array([[10, 11],\n                [12, 13]])\njnp.insert(x, indices, values, axis=1)"
      }
    ]
  },
  {
    "title": "Alias of int64",
    "concepts": [
      "int64 is aliased, meaning it's another name for the int64 type.",
      "The alias provides an alternative way to refer to the int64 type."
    ],
    "code_examples": []
  },
  {
    "title": "JAX int16 Scalar Constructor",
    "concepts": [
      "JAX scalar constructor of type int16.",
      "NumPy defines scalar types for each data type.",
      "JAX represents scalars as zero-dimensional arrays."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Scalar Constructor",
    "concepts": [
      "JAX has a scalar constructor of type int32.",
      "NumPy defines scalar types for each data type.",
      "JAX represents scalars as zero-dimensional arrays."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Scalar Constructor",
    "concepts": [
      "JAX scalar constructor is of type int64.",
      "NumPy defines scalar types for each data type.",
      "JAX represents scalars as zero-dimensional arrays."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Scalar Constructor (int8)",
    "concepts": [
      "JAX provides scalar constructors for primitive data types like int8.",
      "Unlike NumPy, JAX represents scalars as zero-dimensional arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Overview",
    "concepts": [
      "The document describes an abstract base class for integer scalar types.",
      "It outlines methods and attributes associated with this class.",
      "Many methods are scalar equivalents of array attributes."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "The class includes an initialization method: __init__().",
      "Methods like all, any, argmax, argmin, argsort, astype, byteswap, choose, clip, compress, conj, conjugate, copy, cumprod, cumsum, diagonal, dump, dumps, fill, flatten, getfield, item, max, mean, min, nonzero, prod, put, ravel, repeat, reshape, resize, round, searchsorted, setfield, setflags, sort, squeeze, std, sum, swapaxes, take, to_device, tobytes, tofile, tolist, tostring, trace, transpose, var, and view are included.",
      "The method is_integer() returns True if the number is finite with integral value.",
      "Many of these methods are scalar equivalents of corresponding array attributes."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "The class has attributes like T, base, data, denominator, device, dtype, flags, flat, imag, itemsize, nbytes, ndim, newbyteorder, numerator, ptp, real, shape, size, and strides.",
      "T and base are scalar attributes identical to corresponding array attributes.",
      "Data is a pointer to the start of the data.",
      "Denominator defaults to 1.",
      "Numerator represents the value itself.",
      "Flat provides a 1-D view of the scalar.",
      "Imag and real represent the imaginary and real parts of the scalar respectively."
    ],
    "code_examples": []
  },
  {
    "title": "One-Dimensional Linear Interpolation with JAX",
    "concepts": [
      "JAX implementation of numpy.interp().",
      "Performs one-dimensional linear interpolation.",
      "x: N-dimensional array of x coordinates for interpolation.",
      "xp: one-dimensional sorted array of points to be interpolated.",
      "fp: array of function values associated with xp.",
      "left: specifies handling for x < xp[0] (default: fp[0]).",
      "right: specifies handling for x > xp[-1] (default: fp[-1]).",
      "period: optionally specify the period for x coordinates."
    ],
    "code_examples": [
      {
        "description": "Basic linear interpolation example.",
        "code": "xp = jnp.arange(10)\nfp = 2 * xp\nx = jnp.array([0.5, 2.0, 3.5])\ninterp(x, xp, fp)"
      },
      {
        "description": "Demonstrates default constant extrapolation.",
        "code": "x = jnp.array([-10., 10.])\ninterp(x, xp, fp)"
      },
      {
        "description": "Linear extrapolation using 'extrapolate' mode.",
        "code": "interp(x, xp, fp, left='extrapolate', right='extrapolate')"
      },
      {
        "description": "Periodic interpolation example.",
        "code": "xp = jnp.array([0, jnp.pi/2, jnp.pi, 3*jnp.pi/2])\nfp = jnp.sin(xp)\nx = 2 * jnp.pi  # note: not in input array\njnp.interp(x, xp, fp, period=2*jnp.pi)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.intersect1d",
    "concepts": [
      "Computes the set intersection of two 1D arrays.",
      "JAX implementation of numpy.intersect1d().",
      "Requires static size specification for JIT compatibility due to data-dependent output size.",
      "The function may not be compatible with jit() and other JAX transformations if size is not specified."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of jax.numpy.intersect1d",
    "concepts": [
      "ar1: The first input array.",
      "ar2: The second input array.",
      "assume_unique: If True, assumes input arrays contain unique values for efficiency.",
      "return_indices: If True, returns indices of intersected values in the input arrays.",
      "size: If specified, returns only the first 'size' sorted elements, padding with fill_value if necessary.",
      "fill_value: Value to pad with when size is specified and there are fewer elements than size indicates. Defaults to the smallest value in the intersection."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value of jax.numpy.intersect1d",
    "concepts": [
      "Returns an array containing the intersection of the two input arrays.",
      "If return_indices is True, returns a tuple containing the intersection, the indices in ar1, and the indices in ar2.",
      "intersection: A 1D array containing each value that appears in both ar1 and ar2.",
      "ar1_indices: Indices of values in the intersection within the flattened ar1 array.",
      "ar2_indices: Indices of values in the intersection within the flattened ar2 array."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.intersect1d Usage",
    "concepts": [
      "Demonstrates basic usage of jnp.intersect1d to find the intersection of two arrays.",
      "Shows how to obtain the indices of the intersecting elements within the original arrays using the return_indices parameter.",
      "Illustrates how to verify that the intersected values correspond to the values at the returned indices in the original arrays."
    ],
    "code_examples": [
      {
        "description": "Find the intersection of two arrays.",
        "code": "import jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njnp.intersect1d(ar1, ar2)"
      },
      {
        "description": "Compute intersection with indices.",
        "code": "import jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\nintersection, ar1_indices, ar2_indices = jnp.intersect1d(ar1, ar2, return_indices=True)\nprint(intersection)\nprint(ar1_indices)\nprint(ar2_indices)"
      },
      {
        "description": "Verify that the intersected values correspond to the values at the returned indices in the original arrays.",
        "code": "import jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\nintersection, ar1_indices, ar2_indices = jnp.intersect1d(ar1, ar2, return_indices=True)\n\nprint(jnp.all(intersection == ar1[ar1_indices]))\nprint(jnp.all(intersection == ar2[ar2_indices]))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.invert",
    "concepts": [
      "Computes the bitwise inversion of an input array.",
      "It is the JAX implementation of numpy.invert().",
      "It is equivalent to the ~ operator for JAX arrays.",
      "Input array must be boolean or integer typed.",
      "Returns an array of the same shape and dtype as the input, with bits inverted."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage with Integer Arrays",
    "concepts": [
      "Demonstrates the bitwise inversion of a uint8 array using jnp.invert().",
      "The example showcases how integer values are inverted bit by bit."
    ],
    "code_examples": [
      {
        "description": "Demonstrates bitwise inversion on a jax array of unsigned 8 bit integers.",
        "code": "x = jnp.arange(5, dtype='uint8')\nprint(x)\nprint(jnp.invert(x))"
      },
      {
        "description": "Demonstrates bitwise inversion on a jax array of unsigned 8 bit integers.",
        "code": "x = jnp.arange(5, dtype='uint8')\nprint(x)\nprint(jnp.invert(x))"
      }
    ]
  },
  {
    "title": "Unary ~ operator",
    "concepts": [
      "Demonstrates using the ~ operator for bitwise inversion.",
      "The ~ operator is equivalent to jnp.invert()."
    ],
    "code_examples": [
      {
        "description": "Shows that the ~ operator is equivalent to jnp.invert",
        "code": "x = jnp.arange(5, dtype='uint8')\nprint(~x)"
      },
      {
        "description": "Shows that the ~ operator is equivalent to jnp.invert",
        "code": "x = jnp.arange(5, dtype='uint8')\nprint(~x)"
      }
    ]
  },
  {
    "title": "Bitwise Representation",
    "concepts": [
      "Illustrates the bitwise representation of the input and output after inversion.",
      "Uses jnp.printoptions to display integers in binary format."
    ],
    "code_examples": [
      {
        "description": "Shows the bitwise representation of the input and output of jnp.invert()",
        "code": "with jnp.printoptions(formatter={'int': lambda x: format(x, '#010b')}):\n  x = jnp.arange(5, dtype='uint8')\n  print(f\"{x=}\")\n  print(f\"{~x=}\")"
      },
      {
        "description": "Shows the bitwise representation of the input and output of jnp.invert()",
        "code": "with jnp.printoptions(formatter={'int': lambda x: format(x, '#010b')}):\n  x = jnp.arange(5, dtype='uint8')\n  print(f\"{x=}\")\n  print(f\"{~x=}\")"
      }
    ]
  },
  {
    "title": "Boolean Inversion",
    "concepts": [
      "Shows the behavior of invert() on boolean arrays.",
      "It is equivalent to jax.numpy.logical_not() when the input is boolean."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of jnp.invert() on boolean arrays.",
        "code": "x = jnp.array([True, False, True, True, False])\njnp.invert(x)"
      },
      {
        "description": "Demonstrates the usage of jnp.invert() on boolean arrays.",
        "code": "x = jnp.array([True, False, True, True, False])\njnp.invert(x)"
      }
    ]
  },
  {
    "title": "Description of jnp.isclose",
    "concepts": [
      "Checks if elements of two arrays are approximately equal within a tolerance.",
      "JAX implementation of numpy.allclose().",
      "Evaluates |a - b| <= atol + rtol * |b|.",
      "jnp.inf in a is considered equal to jnp.inf in b.",
      "The function accepts two arrays (a, b), relative tolerance (rtol), absolute tolerance (atol), and a boolean equal_nan.",
      "Returns a boolean array indicating element-wise approximate equality."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.isclose Usage",
    "concepts": [
      "Demonstrates the usage of jnp.isclose with various examples.",
      "Shows the effect of rtol and atol on the result.",
      "Demonstrates the behavior when equal_nan is set to True."
    ],
    "code_examples": [
      {
        "description": "Compares two arrays with default tolerances.",
        "code": "jnp.isclose(\n    jnp.array([1e6, 2e6, jnp.inf]),\n    jnp.array([1e6, 2e7, jnp.inf]))"
      },
      {
        "description": "Compares two arrays with a relative tolerance of 1e3.",
        "code": "jnp.isclose(\n    jnp.array([1e6, 2e6, 3e6]),\n    jnp.array([1.00008e6, 2.00008e7, 3.00008e8]),\n    rtol=1e3)"
      },
      {
        "description": "Compares two arrays with an absolute tolerance of 1e3.",
        "code": "jnp.isclose(\n    jnp.array([1e6, 2e6, 3e6]),\n    jnp.array([1.00001e6, 2.00002e6, 3.00009e6]),\n    atol=1e3)"
      },
      {
        "description": "Compares two arrays containing NaNs with equal_nan set to True.",
        "code": "jnp.isclose(\n    jnp.array([jnp.nan, 1, 2]),\n    jnp.array([jnp.nan, 1, 2]),\n    equal_nan=True)"
      },
      {
        "description": "Compares two arrays with default tolerances.",
        "code": "jnp.isclose(\n    jnp.array([1e6, 2e6, jnp.inf]),\n    jnp.array([1e6, 2e7, jnp.inf]))"
      },
      {
        "description": "Compares two arrays with a relative tolerance of 1e3.",
        "code": "jnp.isclose(\n    jnp.array([1e6, 2e6, 3e6]),\n    jnp.array([1.00008e6, 2.00008e7, 3.00008e8]),\n    rtol=1e3)"
      },
      {
        "description": "Compares two arrays with an absolute tolerance of 1e3.",
        "code": "jnp.isclose(\n    jnp.array([1e6, 2e6, 3e6]),\n    jnp.array([1.00001e6, 2.00002e6, 3.00009e6]),\n    atol=1e3)"
      },
      {
        "description": "Compares two arrays containing NaNs with equal_nan set to True.",
        "code": "jnp.isclose(\n    jnp.array([jnp.nan, 1, 2]),\n    jnp.array([jnp.nan, 1, 2]),\n    equal_nan=True)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.iscomplex()",
    "concepts": [
      "The function returns a boolean array.",
      "The function determines if the input array elements are complex numbers.",
      "It mirrors the functionality of numpy.iscomplex()."
    ],
    "code_examples": [
      {
        "description": "Example usage of jnp.iscomplex() with a JAX array containing both real and complex numbers.",
        "code": "import jax.numpy as jnp\n\njnp.iscomplex(jnp.array([True, 0, 1, 2j, 1 + 2j]))"
      },
      {
        "description": "Another example usage of jnp.iscomplex() with a JAX array containing both real and complex numbers. It outputs a boolean array indicating which elements are complex.",
        "code": "import jax.numpy as jnp\n\njnp.iscomplex(jnp.array([True, 0, 1, 2j, 1 + 2j]))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.iscomplexobj",
    "concepts": [
      "The function checks if the input is a complex number or an array containing complex elements.",
      "It's a JAX implementation of numpy.iscomplexobj().",
      "The function evaluates based on input type, not value.",
      "Inputs with zero imaginary parts are still considered complex.",
      "It returns True if the input is complex or an array containing at least one complex element, False otherwise."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.iscomplexobj usage",
    "concepts": [
      "Demonstrates how to use jnp.iscomplexobj with different data types and arrays."
    ],
    "code_examples": [
      {
        "description": "Check if a boolean is a complex object.",
        "code": "jnp.iscomplexobj(True)\nFalse"
      },
      {
        "description": "Check if an integer is a complex object.",
        "code": "jnp.iscomplexobj(0)\nFalse"
      },
      {
        "description": "Check if an integer array is a complex object.",
        "code": "jnp.iscomplexobj(jnp.array([1, 2]))\nFalse"
      },
      {
        "description": "Check if a complex number is a complex object.",
        "code": "jnp.iscomplexobj(1 + 2j)\nTrue"
      },
      {
        "description": "Check if an array containing a complex number is a complex object.",
        "code": "jnp.iscomplexobj(jnp.array([0, 1 + 2j]))\nTrue"
      },
      {
        "description": "Check if a boolean is a complex object.",
        "code": "jnp.iscomplexobj(True)\nFalse"
      },
      {
        "description": "Check if an integer is a complex object.",
        "code": "jnp.iscomplexobj(0)\nFalse"
      },
      {
        "description": "Check if an integer array is a complex object.",
        "code": "jnp.iscomplexobj(jnp.array([1, 2]))\nFalse"
      },
      {
        "description": "Check if a complex number is a complex object.",
        "code": "jnp.iscomplexobj(1 + 2j)\nTrue"
      },
      {
        "description": "Check if an array containing a complex number is a complex object.",
        "code": "jnp.iscomplexobj(jnp.array([0, 1 + 2j]))\nTrue"
      }
    ]
  },
  {
    "title": "Dtype Kind Checking",
    "concepts": [
      "Determine if a dtype is of a specified kind.",
      "The `dtype` parameter is the input data type.",
      "The `kind` parameter specifies the data type kind to check against.",
      "If `kind` is dtype-like, the function returns `True` if `dtype == kind`.",
      "If `kind` is a string, it checks if `dtype` belongs to a predefined category (e.g., 'bool', 'signed integer', 'real floating').",
      "If `kind` is a tuple, it returns `True` if `dtype` matches any entry in the tuple.",
      "The function returns a boolean value indicating whether the dtype is of the specified kind."
    ],
    "code_examples": []
  },
  {
    "title": "Data Type Categories",
    "concepts": [
      "The document defines several categories of data types.",
      "'bool' category includes: {bool}",
      "'signed integer' category includes: {int4, int8, int16, int32, int64}",
      "'unsigned integer' category includes: {uint4, uint8, uint16, uint32, uint64}",
      "'integral' category is a shorthand for ('signed integer', 'unsigned integer').",
      "'real floating' category includes: {float8_*, float16, bfloat16, float32, float64}",
      "'complex floating' category includes: {complex64, complex128}",
      "'numeric' category is a shorthand for ('integral', 'real floating', 'complex floating')."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.isfinite",
    "concepts": [
      "Returns a boolean array indicating whether each element of input is finite.",
      "JAX implementation of numpy.isfinite.",
      "The input 'x' can be an array or a scalar.",
      "Returns a boolean array of the same shape as 'x'.",
      "Result is True where 'x' is not inf, -inf, or NaN, and False otherwise."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.isfinite Usage",
    "concepts": [
      "Demonstrates the usage of jnp.isfinite with a JAX array containing numbers, infinity, and NaN.",
      "Demonstrates the usage of jnp.isfinite with a complex number."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of jnp.isfinite with an array containing regular numbers, infinity, and NaN values.",
        "code": "x = jnp.array([-1, 3, jnp.inf, jnp.nan])\njnp.isfinite(x)"
      },
      {
        "description": "Demonstrates the use of jnp.isfinite with a complex number.",
        "code": "jnp.isfinite(3 - 4j)"
      },
      {
        "description": "Demonstrates the use of jnp.isfinite with an array containing regular numbers, infinity, and NaN values.",
        "code": "x = jnp.array([-1, 3, jnp.inf, jnp.nan])\njnp.isfinite(x)"
      },
      {
        "description": "Demonstrates the use of jnp.isfinite with a complex number.",
        "code": "jnp.isfinite(3 - 4j)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.isin()",
    "concepts": [
      "The function determines if elements in 'element' array are present in 'test_elements' array.",
      "It provides a JAX implementation of numpy.isin().",
      "The 'element' parameter represents the input array of elements to check.",
      "The 'test_elements' parameter is the array to test for the presence of each element from 'element'.",
      "The 'invert' parameter inverts the result, returning ~isin(element, test_elements) if True.",
      "The 'assume_unique' parameter optimizes computation if input arrays are unique. Using True with non-unique arrays leads to undefined behavior.",
      "The 'method' parameter specifies the computation method: 'compare_all', 'binary_search', 'sort', or 'auto'."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.isin() usage",
    "concepts": [
      "Demonstrates how to use jax.numpy.isin() to check for element membership in an array.",
      "Shows the expected boolean array output indicating presence or absence."
    ],
    "code_examples": [
      {
        "description": "Example usage of jax.numpy.isin() with two arrays, elements and test_elements. It checks which elements of 'elements' are present in 'test_elements'.",
        "code": "elements = jnp.array([1, 2, 3, 4])\ntest_elements = jnp.array([[1, 5, 6, 3, 7, 1]])\njnp.isin(elements, test_elements)"
      },
      {
        "description": "Another example usage of jax.numpy.isin() with two arrays, elements and test_elements. It checks which elements of 'elements' are present in 'test_elements'.",
        "code": "elements = jnp.array([1, 2, 3, 4])\ntest_elements = jnp.array([[1, 5, 6, 3, 7, 1]])\njnp.isin(elements, test_elements)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.isinf",
    "concepts": [
      "Returns a boolean array indicating whether each element of the input is infinite.",
      "JAX implementation of numpy.isinf.",
      "The input can be an array or a scalar.",
      "The output is a boolean array of the same shape as the input, containing True where the input is inf or -inf, and False otherwise."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.isinf Usage",
    "concepts": [
      "Demonstrates how to use jnp.isinf to check for infinite values.",
      "Shows the result of checking jnp.inf with jnp.isinf",
      "Shows the result of checking an array containing complex numbers, infinity, and NaN values with jnp.isinf."
    ],
    "code_examples": [
      {
        "description": "Checking if jnp.inf is infinite.",
        "code": "jnp.isinf(jnp.inf)"
      },
      {
        "description": "Checking an array containing complex numbers, infinity, and NaN values for infinite values.",
        "code": "x = jnp.array([2 + 3j, -jnp.inf, 6, jnp.inf, jnp.nan])\njnp.isinf(x)"
      },
      {
        "description": "Checking if jnp.inf is infinite (repeated).",
        "code": "jnp.isinf(jnp.inf)"
      },
      {
        "description": "Checking an array containing complex numbers, infinity, and NaN values for infinite values (repeated).",
        "code": "x = jnp.array([2 + 3j, -jnp.inf, 6, jnp.inf, jnp.nan])\njnp.isinf(x)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.isnan",
    "concepts": [
      "The function checks for NaN (Not a Number) values in an array.",
      "It returns a boolean array with the same shape as the input.",
      "True indicates NaN values, False indicates non-NaN values."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.isnan Usage",
    "concepts": [
      "Demonstrates usage with a single number.",
      "Demonstrates usage with an array including complex numbers, infinity, and NaN."
    ],
    "code_examples": [
      {
        "description": "Checks if the number 6 is NaN.",
        "code": "jnp.isnan(6)"
      },
      {
        "description": "Creates an array containing a real number, a complex number, infinity, and NaN, and then checks for NaN values.",
        "code": "x = jnp.array([2, 1 + 4j, jnp.inf, jnp.nan])\njnp.isnan(x)"
      },
      {
        "description": "Checks if the number 6 is NaN.",
        "code": "jnp.isnan(6)"
      },
      {
        "description": "Creates an array containing a real number, a complex number, infinity, and NaN, and then checks for NaN values.",
        "code": "x = jnp.array([2, 1 + 4j, jnp.inf, jnp.nan])\njnp.isnan(x)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.isneginf",
    "concepts": [
      "The function returns a boolean array.",
      "The function identifies negative infinite elements in an array or scalar.",
      "The function does not support complex data types.",
      "jax.numpy.isinf() returns True if the element is positive or negative infinity.",
      "jax.numpy.isposinf() returns True if the element is positive infinity.",
      "jax.numpy.isfinite() returns True if the element is a finite number.",
      "jax.numpy.isnan() returns True if the element is not a number (NaN)."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.isneginf usage",
    "concepts": [
      "Demonstrates how to use jnp.isneginf() with jnp.inf",
      "Demonstrates how to use jnp.isneginf() with an array containing -inf, inf, nan and regular numbers."
    ],
    "code_examples": [
      {
        "description": "Checks if positive infinity is negative infinity.",
        "code": "jnp.isneginf(jnp.inf)"
      },
      {
        "description": "Checks for negative infinity in an array.",
        "code": "x = jnp.array([-jnp.inf, 5, jnp.inf, jnp.nan, 1])\njnp.isneginf(x)"
      },
      {
        "description": "Checks if positive infinity is negative infinity (repeated example).",
        "code": "jnp.isneginf(jnp.inf)"
      },
      {
        "description": "Checks for negative infinity in an array (repeated example).",
        "code": "x = jnp.array([-jnp.inf, 5, jnp.inf, jnp.nan, 1])\njnp.isneginf(x)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.isposinf",
    "concepts": [
      "The function returns a boolean array.",
      "It checks if each element of the input is positive infinity.",
      "Complex dtype inputs are not supported.",
      "The output array has the same shape as the input.",
      "It uses JAX implementation of numpy.isposinf."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.isposinf usage",
    "concepts": [
      "Demonstrates the usage of jnp.isposinf with a scalar value.",
      "Demonstrates the usage of jnp.isposinf with an array containing positive infinity, negative infinity, NaN, and finite numbers."
    ],
    "code_examples": [
      {
        "description": "Check if 5 is positive infinity.",
        "code": "jnp.isposinf(5)"
      },
      {
        "description": "Check an array for positive infinity.",
        "code": "x = jnp.array([-jnp.inf, 5, jnp.inf, jnp.nan, 1])\njnp.isposinf(x)"
      },
      {
        "description": "Check if 5 is positive infinity (repeated example).",
        "code": "jnp.isposinf(5)"
      },
      {
        "description": "Check an array for positive infinity (repeated example).",
        "code": "x = jnp.array([-jnp.inf, 5, jnp.inf, jnp.nan, 1])\njnp.isposinf(x)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.isreal()",
    "concepts": [
      "The function returns a boolean array.",
      "The function checks if the elements of the input array are real numbers.",
      "The input array is named 'x'."
    ],
    "code_examples": [
      {
        "description": "Example usage of jnp.isreal() to check for real numbers in an array containing complex numbers.",
        "code": "jnp.isreal(jnp.array([False, 0j, 1, 2.1, 1 + 2j]))"
      },
      {
        "description": "Another example usage of jnp.isreal() with the same input array.",
        "code": "jnp.isreal(jnp.array([False, 0j, 1, 2.1, 1 + 2j]))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.isrealobj()",
    "concepts": [
      "Checks if the input is not a complex number or an array containing complex elements.",
      "JAX implementation of numpy.isrealobj().",
      "The function evaluates based on input type rather than value.",
      "Inputs with zero imaginary parts are still considered complex."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Examples",
    "concepts": [
      "Demonstrates the use of jax.numpy.isrealobj() with various inputs.",
      "Shows how the function returns True for real numbers and arrays of real numbers.",
      "Shows how the function returns False for complex numbers and arrays containing complex numbers."
    ],
    "code_examples": [
      {
        "description": "Check if 0 is a real object.",
        "code": "jnp.isrealobj(0)\n# Output: True"
      },
      {
        "description": "Check if 1.2 is a real object.",
        "code": "jnp.isrealobj(1.2)\n# Output: True"
      },
      {
        "description": "Check if an array of real numbers is a real object.",
        "code": "jnp.isrealobj(jnp.array([1, 2]))\n# Output: True"
      },
      {
        "description": "Check if a complex number is a real object.",
        "code": "jnp.isrealobj(1 + 2j)\n# Output: False"
      },
      {
        "description": "Check if an array containing a complex number is a real object.",
        "code": "jnp.isrealobj(jnp.array([0, 1 + 2j]))\n# Output: False"
      },
      {
        "description": "Check if 0 is a real object.",
        "code": "jnp.isrealobj(0)\n# Output: True"
      },
      {
        "description": "Check if 1.2 is a real object.",
        "code": "jnp.isrealobj(1.2)\n# Output: True"
      },
      {
        "description": "Check if an array of real numbers is a real object.",
        "code": "jnp.isrealobj(jnp.array([1, 2]))\n# Output: True"
      },
      {
        "description": "Check if a complex number is a real object.",
        "code": "jnp.isrealobj(1 + 2j)\n# Output: False"
      },
      {
        "description": "Check if an array containing a complex number is a real object.",
        "code": "jnp.isrealobj(jnp.array([0, 1 + 2j]))\n# Output: False"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.isscalar()",
    "concepts": [
      "jax.numpy.isscalar() checks if an input is a scalar.",
      "JAX considers zero-dimensional arrays as scalars, unlike NumPy.",
      "The function returns True for scalar values or zero-dimensional array-like objects.",
      "It returns False otherwise.",
      "JAX's implementation aims to maintain JIT-invariance."
    ],
    "code_examples": []
  },
  {
    "title": "JIT-Invariance in JAX vs NumPy",
    "concepts": [
      "NumPy's `numpy.isscalar()` behavior changes under JIT compilation due to scalar inputs being cast to zero-dimensional JAX arrays.",
      "JAX's `jax.numpy.isscalar()` avoids this issue by treating zero-dimensional arrays as scalars.",
      "JIT-invariance means the result of a function should not change when it is JIT-compiled."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the change in numpy.isscalar() behavior when using JIT.",
        "code": ">>> import numpy as np\n>>> import jax\n\n>>> np.isscalar(1.0)\nTrue\n>>> jax.jit(np.isscalar)(1.0)\nArray(False, dtype=bool)\n>>> np.isscalar(1.0)\nTrue\n>>> jax.jit(np.isscalar)(1.0)\nArray(False, dtype=bool)"
      },
      {
        "description": "Demonstrates that jax.numpy.isscalar() maintains consistent behavior with JIT.",
        "code": ">>> import jax.numpy as jnp\n>>> import jax\n\n>>> jnp.isscalar(1.0)\nTrue\n>>> jax.jit(jnp.isscalar)(1.0)\nArray(True, dtype=bool)\n>>> jnp.isscalar(1.0)\nTrue\n>>> jax.jit(jnp.isscalar)(1.0)\nArray(True, dtype=bool)"
      }
    ]
  },
  {
    "title": "Examples of jax.numpy.isscalar() Usage",
    "concepts": [
      "Scalars and zero-dimensional array-like objects are considered scalars in JAX.",
      "Arrays with one or more dimensions are not scalars.",
      "Non-array-like objects are not scalars.",
      "This section provides comprehensive examples of how `jnp.isscalar()` works with different input types, including regular scalars, zero-dimensional JAX and NumPy arrays, and objects that are not scalars."
    ],
    "code_examples": [
      {
        "description": "Examples of inputs that jax.numpy.isscalar() considers to be scalars.",
        "code": ">>> import jax.numpy as jnp\n>>> import numpy as np\n\n>>> jnp.isscalar(1.0)\nTrue\n>>> jnp.isscalar(1 + 1j)\nTrue\n>>> jnp.isscalar(jnp.array(1))  # zero-dimensional JAX array\nTrue\n>>> jnp.isscalar(jnp.int32(1))  # JAX scalar constructor\nTrue\n>>> jnp.isscalar(np.array(1.0))  # zero-dimensional NumPy array\nTrue\n>>> jnp.isscalar(np.int32(1))  # NumPy scalar type\nTrue\n>>> jnp.isscalar(1.0)\nTrue\n>>> jnp.isscalar(1 + 1j)\nTrue\n>>> jnp.isscalar(jnp.array(1))  # zero-dimensional JAX array\nTrue\n>>> jnp.isscalar(jnp.int32(1))  # JAX scalar constructor\nTrue\n>>> jnp.isscalar(np.array(1.0))  # zero-dimensional NumPy array\nTrue\n>>> jnp.isscalar(np.int32(1))  # NumPy scalar type\nTrue"
      },
      {
        "description": "Examples of arrays that jax.numpy.isscalar() does NOT consider to be scalars.",
        "code": ">>> import jax.numpy as jnp\n>>> import numpy as np\n\n>>> jnp.isscalar(jnp.array([1]))\nFalse\n>>> jnp.isscalar(np.array([1]))\nFalse\n>>> jnp.isscalar(jnp.array([1]))\nFalse\n>>> jnp.isscalar(np.array([1]))\nFalse"
      },
      {
        "description": "Comparison between jax.numpy.isscalar() and numpy.isscalar() for scalar-typed objects and zero-dimensional arrays.",
        "code": ">>> import jax.numpy as jnp\n>>> import numpy as np\n\n>>> np.isscalar(np.int32(1))  # scalar object\nTrue\n>>> np.isscalar(np.array(1))  # zero-dimensional array\nFalse\n>>> np.isscalar(np.int32(1))  # scalar object\nTrue\n>>> np.isscalar(np.array(1))  # zero-dimensional array\nFalse"
      },
      {
        "description": "Examples of objects that are not array-like, and thus not considered scalars.",
        "code": ">>> import jax.numpy as jnp\n\n>>> jnp.isscalar(None)\nFalse\n>>> jnp.isscalar([1])\nFalse\n>>> jnp.isscalar(tuple())\nFalse\n>>> jnp.isscalar(slice(10))\nFalse\n>>> jnp.isscalar(None)\nFalse\n>>> jnp.isscalar([1])\nFalse\n>>> jnp.isscalar(tuple())\nFalse\n>>> jnp.isscalar(slice(10))\nFalse"
      }
    ]
  },
  {
    "title": "Iterable Check with NumPy",
    "concepts": [
      "Determines if an object can be iterated over.",
      "Returns True if the object has an iterator method or is a sequence.",
      "Returns False otherwise."
    ],
    "code_examples": [
      {
        "description": "Checks if a list is iterable.",
        "code": "import numpy as np\n\nnp.iterable([1, 2, 3])"
      },
      {
        "description": "Checks if an integer is iterable.",
        "code": "import numpy as np\n\nnp.iterable(2)"
      },
      {
        "description": "Demonstrates the difference between np.iterable and collections.abc.Iterable for 0-dimensional arrays.",
        "code": "import numpy as np\nfrom collections.abc import Iterable\n\na = np.array(1.0)  # 0-dimensional numpy array\n\nisinstance(a, Iterable)\n\nnp.iterable(a)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.ix_",
    "concepts": [
      "jnp.ix_ creates an open mesh from N one-dimensional sequences.",
      "It is a JAX implementation of numpy.ix_().",
      "The function accepts N one-dimensional arrays as input.",
      "The function returns a tuple of Jax arrays forming an open mesh, each with N dimensions.",
      "It is related to jax.numpy.ogrid, jax.numpy.mgrid and jax.numpy.meshgrid()."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Example of jnp.ix_",
    "concepts": [
      "Demonstrates creating an open mesh using jnp.ix_ with row and column indices.",
      "Shows how to access elements of a multi-dimensional array using the open mesh.",
      "Illustrates how the open mesh is used to index a 2D array.",
      "Demonstrates how to get the shape of the resulting mesh."
    ],
    "code_examples": [
      {
        "description": "Creating an open mesh and using it to index a 2D array.",
        "code": "rows = jnp.array([0, 2])\ncols = jnp.array([1, 3])\nopen_mesh = jnp.ix_(rows, cols)\nprint(open_mesh)\nprint([grid.shape for grid in open_mesh])\nx = jnp.array([[10, 20, 30, 40],\n               [50, 60, 70, 80],\n               [90, 100, 110, 120],\n               [130, 140, 150, 160]])\nprint(x[open_mesh])"
      },
      {
        "description": "Creating an open mesh and using it to index a 2D array. (Repetition of the previous example)",
        "code": "rows = jnp.array([0, 2])\ncols = jnp.array([1, 3])\nopen_mesh = jnp.ix_(rows, cols)\nprint(open_mesh)\nprint([grid.shape for grid in open_mesh])\nx = jnp.array([[10, 20, 30, 40],\n               [50, 60, 70, 80],\n               [90, 100, 110, 120],\n               [130, 140, 150, 160]])\nprint(x[open_mesh])"
      }
    ]
  },
  {
    "title": "Kaiser Window Implementation in JAX",
    "concepts": [
      "Returns a Kaiser window of size M.",
      "JAX implementation of numpy.kaiser().",
      "M is the window size (int).",
      "beta is the Kaiser window parameter (Array | ndarray | bool | number | bool | int | float | complex).",
      "Returns an array of size M containing the Kaiser window."
    ],
    "code_examples": [
      {
        "description": "Example usage of jnp.kaiser() with M=4 and beta=1.5, demonstrating the output with specified precision.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.kaiser(4, 1.5))"
      },
      {
        "description": "Another example usage of jnp.kaiser() with M=4 and beta=1.5, demonstrating the output with specified precision.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.kaiser(4, 1.5))"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.numpy.bartlett(): return a Bartlett window of size M.",
      "jax.numpy.blackman(): return a Blackman window of size M.",
      "jax.numpy.hamming(): return a Hamming window of size M.",
      "jax.numpy.hanning(): return a Hanning window of size M."
    ],
    "code_examples": []
  },
  {
    "title": "Kronecker Product Overview",
    "concepts": [
      "The Kronecker product is an operation on two matrices.",
      "It produces a block matrix.",
      "Each element of the first matrix is multiplied by the second matrix.",
      "If matrix 'a' has shape (m, n) and matrix 'b' has shape (p, q), the resulting matrix has shape (m * p, n * q)."
    ],
    "code_examples": []
  },
  {
    "title": "Kronecker Product Example",
    "concepts": [
      "Demonstrates the computation of the Kronecker product using jax.numpy.kron().",
      "The output is an array representing the Kronecker product of the two input arrays."
    ],
    "code_examples": [
      {
        "description": "Compute the kronecker product of two 2x2 matrices using jax.numpy.kron.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 2],\n               [3, 4]])\nb = jnp.array([[5, 6],\n               [7, 8]])\n\nprint(jnp.kron(a, b))"
      },
      {
        "description": "Compute the kronecker product of two 2x2 matrices using jax.numpy.kron.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 2],\n               [3, 4]])\nb = jnp.array([[5, 6],\n               [7, 8]])\n\nprint(jnp.kron(a, b))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.ldexp()",
    "concepts": [
      "Computes x1 * 2 ** x2 element-wise.",
      "JAX implementation of numpy.ldexp().",
      "Implemented via multiplication and exponentiation due to XLA limitations.",
      "x1 is a real-valued input array.",
      "x2 is an integer input array, broadcast-compatible with x1."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.ldexp() usage",
    "concepts": [
      "Demonstrates jnp.ldexp() with a simple array and scalar exponent.",
      "Illustrates reconstruction of frexp() input using ldexp()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of jnp.ldexp with a jax array and an integer. This example shows the result of multiplying each element of the array by 2 raised to the power of 10.",
        "code": "x1 = jnp.arange(5.0)\nx2 = 10\njnp.ldexp(x1, x2)"
      },
      {
        "description": "Demonstrates the use of jnp.ldexp with a jax array and an integer. This example shows the result of multiplying each element of the array by 2 raised to the power of 10.",
        "code": "x1 = jnp.arange(5.0)\nx2 = 10\njnp.ldexp(x1, x2)"
      },
      {
        "description": "Demonstrates how to reconstruct the input of jnp.frexp using jnp.ldexp. frexp decomposes a number into a mantissa and an exponent. ldexp combines them back.",
        "code": "x = jnp.array([2., 3., 5., 11.])\nm, e = jnp.frexp(x)\nm\ne\njnp.ldexp(m, e)"
      },
      {
        "description": "Demonstrates how to reconstruct the input of jnp.frexp using jnp.ldexp. frexp decomposes a number into a mantissa and an exponent. ldexp combines them back.",
        "code": "x = jnp.array([2., 3., 5., 11.])\nm, e = jnp.frexp(x)\nm\ne\njnp.ldexp(m, e)"
      }
    ]
  },
  {
    "title": "JAX Left Shift Function",
    "concepts": [
      "The function shifts bits of an integer array to the left.",
      "It's a JAX implementation of NumPy's left_shift.",
      "The first argument is the input array (x), which must be integer-typed.",
      "The second argument (y) specifies the amount of bits to shift.",
      "x and y must have the same shape or be broadcast compatible.",
      "The return value is an array containing the left-shifted elements.",
      "Left shifting x by y is equivalent to x * (2**y).",
      "jax.numpy.bitwise_left_shift() is an alias of jax.left_shift()."
    ],
    "code_examples": []
  },
  {
    "title": "Binary Printing Helper Function",
    "concepts": [
      "A helper function to print the binary representation of numbers in an array."
    ],
    "code_examples": [
      {
        "description": "Defines a function to convert integers in an array to their binary string representation.",
        "code": "def print_binary(x):\n  return [bin(int(val)) for val in x]"
      }
    ]
  },
  {
    "title": "Left Shift with Scalar Shift Amount",
    "concepts": [
      "Demonstrates left-shifting an array by a scalar value."
    ],
    "code_examples": [
      {
        "description": "Shifts the bits of elements in the jnp.arange(5) array to the left by 1.",
        "code": "import jax.numpy as jnp\n\ndef print_binary(x):\n  return [bin(int(val)) for val in x]\n\nx1 = jnp.arange(5)\nprint(\"x1\", x1)\nprint(\"Binary x1\", print_binary(x1))\n\nx2 = 1\n\nresult = jnp.left_shift(x1, x2)\nprint(\"result\", result)\nprint(\"Binary result\", print_binary(result))"
      }
    ]
  },
  {
    "title": "Left Shift with Array Shift Amount",
    "concepts": [
      "Demonstrates left-shifting a scalar by an array of shift amounts."
    ],
    "code_examples": [
      {
        "description": "Shifts the bits of x3 (scalar) to the left by amounts specified in the x4 (array).",
        "code": "import jax.numpy as jnp\n\ndef print_binary(x):\n  return [bin(int(val)) for val in x]\n\nx3 = 4\nprint(\"x3\", print_binary([x3]))\n\nx4 = jnp.array([1, 2, 3, 4])\n\nresult1 = jnp.left_shift(x3, x4)\nprint(\"result1\", result1)\nprint(\"Binary result1\", print_binary(result1))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.less",
    "concepts": [
      "The function returns element-wise truth value of x < y.",
      "It's a JAX implementation of numpy.less.",
      "Inputs x and y can be arrays or scalars.",
      "x and y must have the same shape or be broadcast compatible.",
      "The output is an array containing boolean values, True if x < y, False otherwise."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Scalar Inputs",
    "concepts": [
      "Demonstrates the use of jnp.less with scalar inputs.",
      "The output is a boolean array."
    ],
    "code_examples": [
      {
        "description": "Demonstrates jnp.less with two scalar inputs.",
        "code": "jnp.less(3, 7)"
      },
      {
        "description": "Demonstrates jnp.less with two scalar inputs.",
        "code": "jnp.less(3, 7)"
      }
    ]
  },
  {
    "title": "Examples with Arrays of the Same Shape",
    "concepts": [
      "Demonstrates the use of jnp.less with arrays of the same shape.",
      "The output is a boolean array, element-wise comparison of the input arrays."
    ],
    "code_examples": [
      {
        "description": "Demonstrates jnp.less with two arrays of the same shape.",
        "code": "x = jnp.array([5, 9, -3])\ny = jnp.array([1, 6, 4])\njnp.less(x, y)"
      },
      {
        "description": "Demonstrates jnp.less with two arrays of the same shape.",
        "code": "x = jnp.array([5, 9, -3])\ny = jnp.array([1, 6, 4])\njnp.less(x, y)"
      }
    ]
  },
  {
    "title": "Examples with Broadcast Compatibility",
    "concepts": [
      "Demonstrates the use of jnp.less with arrays that are broadcast compatible.",
      "The output is a boolean array representing element-wise comparison after broadcasting."
    ],
    "code_examples": [
      {
        "description": "Demonstrates jnp.less with two arrays that can be broadcast.",
        "code": "x1 = jnp.array([[2, -4, 6, -8],\n                [-1, 5, -3, 7]])\ny1 = jnp.array([0, 3, -5, 9])\njnp.less(x1, y1)"
      },
      {
        "description": "Demonstrates jnp.less with two arrays that can be broadcast.",
        "code": "x1 = jnp.array([[2, -4, 6, -8],\n                [-1, 5, -3, 7]])\ny1 = jnp.array([0, 3, -5, 9])\njnp.less(x1, y1)"
      }
    ]
  },
  {
    "title": "Description and Usage of jax.numpy.less_equal",
    "concepts": [
      "The function `jax.numpy.less_equal` returns element-wise truth value of x <= y.",
      "It is a JAX implementation of numpy.less_equal.",
      "The inputs x and y can be arrays or scalars.",
      "x and y must have either the same shape or be broadcast compatible.",
      "The function returns an array of boolean values indicating if x <= y for each element.",
      "Related functions include: jax.numpy.greater_equal(), jax.numpy.greater(), and jax.numpy.less()."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Scalar Inputs",
    "concepts": [
      "Demonstrates the usage of jnp.less_equal with scalar inputs."
    ],
    "code_examples": [
      {
        "description": "Compares two scalar values using jnp.less_equal.",
        "code": "jnp.less_equal(6, -2)"
      },
      {
        "description": "Compares two scalar values using jnp.less_equal.",
        "code": "jnp.less_equal(6, -2)"
      }
    ]
  },
  {
    "title": "Examples with Arrays of the Same Shape",
    "concepts": [
      "Demonstrates the usage of jnp.less_equal with arrays that have the same shape."
    ],
    "code_examples": [
      {
        "description": "Compares two arrays of the same shape element-wise using jnp.less_equal.",
        "code": "x = jnp.array([-4, 1, 7])\ny = jnp.array([2, -3, 8])\njnp.less_equal(x, y)"
      },
      {
        "description": "Compares two arrays of the same shape element-wise using jnp.less_equal.",
        "code": "x = jnp.array([-4, 1, 7])\ny = jnp.array([2, -3, 8])\njnp.less_equal(x, y)"
      }
    ]
  },
  {
    "title": "Examples with Broadcast Compatible Inputs",
    "concepts": [
      "Demonstrates the usage of jnp.less_equal with arrays that are broadcast compatible."
    ],
    "code_examples": [
      {
        "description": "Compares two broadcast compatible arrays element-wise using jnp.less_equal.",
        "code": "x1 = jnp.array([2, -5, 9])\ny1 = jnp.array([[1, -6, 5],\n                [-2, 4, -6]])\njnp.less_equal(x1, y1)"
      },
      {
        "description": "Compares two broadcast compatible arrays element-wise using jnp.less_equal.",
        "code": "x1 = jnp.array([2, -5, 9])\ny1 = jnp.array([[1, -6, 5],\n                [-2, 4, -6]])\njnp.less_equal(x1, y1)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.linspace",
    "concepts": [
      "The function returns evenly-spaced numbers within a specified interval.",
      "It is a JAX implementation of numpy.linspace().",
      "It accepts start and stop values, number of values to generate, endpoint inclusion, retstep option, axis, device and dtype.",
      "If retstep is True, it returns a tuple of (values, step), where step is the interval between adjacent values.",
      "The function is similar to jax.numpy.arange(), jax.numpy.logspace(), and jax.numpy.geomspace()."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage: List of 5 values between 0 and 10",
    "concepts": [
      "Demonstrates generating a list of evenly spaced values using jnp.linspace.",
      "The example includes both the start and stop values in the resulting array."
    ],
    "code_examples": [
      {
        "description": "Generates 5 evenly spaced values between 0 and 10, inclusive.",
        "code": "jnp.linspace(0, 10, 5)"
      }
    ]
  },
  {
    "title": "Excluding the Endpoint",
    "concepts": [
      "Demonstrates excluding the stop value from the generated sequence.",
      "The `endpoint` parameter is set to `False`."
    ],
    "code_examples": [
      {
        "description": "Generates 8 evenly spaced values between 0 and 10, excluding 10.",
        "code": "jnp.linspace(0, 10, 8, endpoint=False)"
      }
    ]
  },
  {
    "title": "Returning the Step Size",
    "concepts": [
      "Demonstrates how to retrieve the step size between adjacent values using `retstep=True`.",
      "The function returns a tuple containing the array of values and the step size."
    ],
    "code_examples": [
      {
        "description": "Generates 9 evenly spaced values between 0 and 10, returning both the values and the step size.",
        "code": "vals, step = jnp.linspace(0, 10, 9, retstep=True)"
      },
      {
        "description": "Prints the array of values.",
        "code": "vals"
      },
      {
        "description": "Prints the step size.",
        "code": "step"
      }
    ]
  },
  {
    "title": "Multi-dimensional linspace",
    "concepts": [
      "Demonstrates the use of jnp.linspace with multi-dimensional arrays for start and stop values.",
      "linspace is applied along the first axis (axis=0) by default"
    ],
    "code_examples": [
      {
        "description": "Defines the start array.",
        "code": "start = jnp.array([0, 5])"
      },
      {
        "description": "Defines the stop array.",
        "code": "stop = jnp.array([5, 10])"
      },
      {
        "description": "Generates 5 evenly spaced values between [0, 5] and [5, 10].",
        "code": "jnp.linspace(start, stop, 5)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.load",
    "concepts": [
      "jax.numpy.load() is a JAX wrapper around numpy.load().",
      "It loads JAX arrays from .npy files.",
      "For .npy files, the output will be a jax.Array, and bfloat16 data types are restored.",
      "For .npz files, results are returned as NumPy arrays.",
      "This function requires concrete array inputs and is not compatible with jax.jit() or jax.vmap()."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Example with bfloat16 data type",
    "concepts": [
      "This example demonstrates loading a bfloat16 array from a file-like object.",
      "It shows the usage of io.BytesIO to create an in-memory file.",
      "The jnp.save() function saves the array to the file-like object.",
      "The file's seek position is reset to 0 before loading.",
      "jnp.load() is used to load the array, preserving the bfloat16 data type."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating loading a bfloat16 array from an in-memory file-like object.",
        "code": "import io\n\nf = io.BytesIO()\n# use an in-memory file-like object.\nx = jnp.array([2, 4, 6, 8], dtype='bfloat16')\njnp.save(f, x)\nf.seek(0)\n0\njnp.load(f)\n"
      },
      {
        "description": "Example demonstrating loading a bfloat16 array from an in-memory file-like object.",
        "code": "import io\n\nf = io.BytesIO()\n# use an in-memory file-like object.\nx = jnp.array([2, 4, 6, 8], dtype='bfloat16')\njnp.save(f, x)\nf.seek(0)\n0\njnp.load(f)\n"
      }
    ]
  },
  {
    "title": "Description of jnp.log",
    "concepts": [
      "Calculates the element-wise natural logarithm of the input.",
      "JAX implementation of numpy.log.",
      "Input can be an array or scalar.",
      "Returns an array containing the logarithm of each element, promoting to inexact dtype.",
      "Related functions: jax.numpy.exp(), jax.numpy.log2(), jax.numpy.log1p()."
    ],
    "code_examples": []
  },
  {
    "title": "Inverse Relationship with jnp.exp",
    "concepts": [
      "jnp.log and jnp.exp are inverse functions."
    ],
    "code_examples": [
      {
        "description": "Demonstrates that applying jnp.log on the result of jnp.exp(x) yields the original input x.",
        "code": "x = jnp.array([2, 3, 4, 5])\njnp.log(jnp.exp(x))"
      },
      {
        "description": "Demonstrates that applying jnp.log on the result of jnp.exp(x) yields the original input x.",
        "code": "x = jnp.array([2, 3, 4, 5])\njnp.log(jnp.exp(x))"
      }
    ]
  },
  {
    "title": "Logarithmic Property Demonstration",
    "concepts": [
      "Demonstrates the logarithmic property log(a*b) = log(a) + log(b)."
    ],
    "code_examples": [
      {
        "description": "Verifies the property log(a*b) = log(a) + log(b) using jnp.allclose.",
        "code": "x1 = jnp.array([2, 1, 3, 1])\nx2 = jnp.array([1, 3, 2, 4])\njnp.allclose(jnp.log(x1 * x2), jnp.log(x1) + jnp.log(x2))"
      },
      {
        "description": "Verifies the property log(a*b) = log(a) + log(b) using jnp.allclose.",
        "code": "x1 = jnp.array([2, 1, 3, 1])\nx2 = jnp.array([1, 3, 2, 4])\njnp.allclose(jnp.log(x1 * x2), jnp.log(x1) + jnp.log(x2))"
      }
    ]
  },
  {
    "title": "Base-10 Logarithm Calculation with JAX",
    "concepts": [
      "Calculates the base-10 logarithm of elements in an array.",
      "JAX implementation of numpy.log10.",
      "Input is an ArrayLike object.",
      "Output is an array containing the base-10 logarithm of each element.",
      "Output array promotes to inexact dtype."
    ],
    "code_examples": [
      {
        "description": "Example usage of jnp.log10 with a JAX array.",
        "code": "x1 = jnp.array([0.01, 0.1, 1, 10, 100, 1000])\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.log10(x1))"
      },
      {
        "description": "Example usage of jnp.log10 with a JAX array.",
        "code": "x1 = jnp.array([0.01, 0.1, 1, 10, 100, 1000])\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.log10(x1))"
      }
    ]
  },
  {
    "title": "Description of jnp.log1p",
    "concepts": [
      "Calculates element-wise logarithm of one plus the input, log(x+1).",
      "JAX implementation of numpy.log1p.",
      "More accurate than the naive computation of log(x+1) for small values of x."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Example",
    "concepts": [
      "Demonstrates the basic usage of jnp.log1p with an array.",
      "Compares the result of jnp.log1p(x) with jnp.log(x+1) using jnp.allclose.",
      "Promotes to inexact dtype."
    ],
    "code_examples": [
      {
        "description": "Illustrates the usage of jnp.log1p and compares it with jnp.log(x+1)",
        "code": "x = jnp.array([2, 5, 9, 4])\njnp.allclose(jnp.log1p(x), jnp.log(x + 1))"
      },
      {
        "description": "Illustrates the usage of jnp.log1p and compares it with jnp.log(x+1)",
        "code": "x = jnp.array([2, 5, 9, 4])\njnp.allclose(jnp.log1p(x), jnp.log(x + 1))"
      }
    ]
  },
  {
    "title": "Accuracy Comparison for Small Values",
    "concepts": [
      "Demonstrates the increased accuracy of jnp.log1p(x) compared to jnp.log(x+1) when x is close to 0.",
      "Uses jnp.expm1() to revert the log operation and highlights the difference in accuracy.",
      "Illustrates that jnp.log1p maintains better precision for very small values."
    ],
    "code_examples": [
      {
        "description": "Compares the accuracy of jnp.log1p(x1) and jnp.log(x1+1) for small values of x1 using jnp.expm1.",
        "code": "x1 = jnp.array([1e-4, 1e-6, 2e-10])\njnp.expm1(jnp.log1p(x1))\n"
      },
      {
        "description": "Compares the accuracy of jnp.log1p(x1) and jnp.log(x1+1) for small values of x1 using jnp.expm1.",
        "code": "jnp.expm1(jnp.log(x1 + 1))"
      },
      {
        "description": "Compares the accuracy of jnp.log1p(x1) and jnp.log(x1+1) for small values of x1 using jnp.expm1.",
        "code": "x1 = jnp.array([1e-4, 1e-6, 2e-10])\njnp.expm1(jnp.log1p(x1))"
      },
      {
        "description": "Compares the accuracy of jnp.log1p(x1) and jnp.log(x1+1) for small values of x1 using jnp.expm1.",
        "code": "jnp.expm1(jnp.log(x1 + 1))"
      }
    ]
  },
  {
    "title": "Base-2 Logarithm Calculation with JAX",
    "concepts": [
      "Calculates the base-2 logarithm of x element-wise.",
      "JAX implementation of numpy.log2.",
      "Input array (x) is of type ArrayLike.",
      "Output is an array containing the base-2 logarithm of each element in x.",
      "Output promotes to inexact dtype."
    ],
    "code_examples": [
      {
        "description": "Calculating the base-2 logarithm of a JAX array.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([0.25, 0.5, 1, 2, 4, 8])\n\njnp.log2(x1)"
      },
      {
        "description": "Calculating the base-2 logarithm of a JAX array (duplicate example).",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([0.25, 0.5, 1, 2, 4, 8])\n\njnp.log2(x1)"
      }
    ]
  },
  {
    "title": "Description of logaddexp Function",
    "concepts": [
      "Computes log(exp(x1) + exp(x2)) to avoid overflow.",
      "JAX implementation of numpy.logaddexp.",
      "Takes two input arrays, x1 and x2.",
      "Returns an array containing the result of the computation."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of logaddexp",
    "concepts": [
      "Demonstrates the usage of jnp.logaddexp with two JAX arrays.",
      "Compares the result of jnp.logaddexp with the direct computation using jnp.log, jnp.exp.",
      "Verifies that the results are close using jnp.allclose."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of jnp.logaddexp with two JAX arrays and compares the result with the direct computation using jnp.log, jnp.exp.",
        "code": "x1 = jnp.array([1, 2, 3])\nx2 = jnp.array([4, 5, 6])\nresult1 = jnp.logaddexp(x1, x2)\nresult2 = jnp.log(jnp.exp(x1) + jnp.exp(x2))\nprint(jnp.allclose(result1, result2))"
      },
      {
        "description": "Demonstrates the usage of jnp.logaddexp with two JAX arrays and compares the result with the direct computation using jnp.log, jnp.exp.",
        "code": "x1 = jnp.array([1, 2, 3])\nx2 = jnp.array([4, 5, 6])\nresult1 = jnp.logaddexp(x1, x2)\nresult2 = jnp.log(jnp.exp(x1) + jnp.exp(x2))\nprint(jnp.allclose(result1, result2))"
      }
    ]
  },
  {
    "title": "Logical AND Operation with JAX",
    "concepts": [
      "Computes the logical AND operation elementwise.",
      "JAX implementation of numpy.logical_and.",
      "Supports additional APIs described at jax.numpy.ufunc.",
      "Input arrays must be broadcastable to a common shape."
    ],
    "code_examples": [
      {
        "description": "Example of using jnp.logical_and with an array and a scalar value.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(4)\n\njnp.logical_and(x, 1)"
      },
      {
        "description": "Another example of using jnp.logical_and with an array and a scalar value.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(4)\n\njnp.logical_and(x, 1)"
      }
    ]
  },
  {
    "title": "Description",
    "concepts": [
      "Computes the logical NOT of boolean array elements.",
      "JAX implementation of numpy.logical_not().",
      "Input array can be of any dtype.",
      "Returns a boolean array.",
      "For non-boolean input, the input is implicitly cast to boolean."
    ],
    "code_examples": []
  },
  {
    "title": "Boolean Array Example",
    "concepts": [
      "Demonstrates logical_not on a boolean array.",
      "Shows the result of inverting boolean values."
    ],
    "code_examples": [
      {
        "description": "Compute NOT x element-wise on a boolean array",
        "code": "x = jnp.array([True, False, True])\njnp.logical_not(x)"
      },
      {
        "description": "Compute NOT x element-wise on a boolean array (repeated)",
        "code": "x = jnp.array([True, False, True])\njnp.logical_not(x)"
      }
    ]
  },
  {
    "title": "Boolean Array Comparison with Invert",
    "concepts": [
      "Shows that logical_not is equivalent to invert (~) for boolean arrays.",
      "Illustrates the use of the ~ operator."
    ],
    "code_examples": [
      {
        "description": "Demonstrates equivalence to invert() for boolean arrays",
        "code": "~ x"
      },
      {
        "description": "Demonstrates equivalence to invert() for boolean arrays (repeated)",
        "code": "~ x"
      }
    ]
  },
  {
    "title": "Non-Boolean Array Example",
    "concepts": [
      "Demonstrates logical_not on a non-boolean array.",
      "Shows implicit casting to boolean.",
      "Any non-zero number will be evaluated to True. Thus, its NOT is False."
    ],
    "code_examples": [
      {
        "description": "Compute NOT x element-wise on a non-boolean array",
        "code": "x = jnp.array([-1, 0, 1])\njnp.logical_not(x)"
      },
      {
        "description": "Compute NOT x element-wise on a non-boolean array (repeated)",
        "code": "x = jnp.array([-1, 0, 1])\njnp.logical_not(x)"
      }
    ]
  },
  {
    "title": "Logical OR Operation with JAX",
    "concepts": [
      "Computes the logical OR operation elementwise.",
      "JAX implementation of numpy.logical_or.",
      "Supports jax.numpy.ufunc APIs.",
      "Input arrays must be broadcastable to a common shape.",
      "Returns an array containing the result of the element-wise logical OR."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the logical_or function with jnp.arange(4) and 1 as inputs.",
        "code": "x = jnp.arange(4)\njnp.logical_or(x, 1)\n"
      },
      {
        "description": "Demonstrates the logical_or function with jnp.arange(4) and 1 as inputs.",
        "code": "x = jnp.arange(4)\njnp.logical_or(x, 1)"
      }
    ]
  },
  {
    "title": "Description of logical_xor function",
    "concepts": [
      "Computes the element-wise logical XOR operation.",
      "JAX implementation of numpy.logical_xor.",
      "It is a universal function (ufunc).",
      "Takes two input arrays, x and y, that must be broadcastable to a common shape.",
      "Returns an array containing the element-wise logical XOR result."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of logical_xor function",
    "concepts": [
      "Demonstrates the usage of jnp.logical_xor with a JAX array and a scalar value."
    ],
    "code_examples": [
      {
        "description": "Computes the element-wise logical XOR of a JAX array with the scalar value 1.",
        "code": "x = jnp.arange(4)\njnp.logical_xor(x, 1)"
      },
      {
        "description": "Computes the element-wise logical XOR of a JAX array with the scalar value 1.",
        "code": "x = jnp.arange(4)\njnp.logical_xor(x, 1)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.logspace",
    "concepts": [
      "The function generates logarithmically-spaced values.",
      "It is a JAX implementation of numpy.logspace().",
      "The 'start' parameter specifies the start value as base ** start.",
      "The 'stop' parameter specifies the end value as base ** stop.",
      "The 'num' parameter determines the number of values to generate.",
      "The 'endpoint' parameter includes or excludes the stop value.",
      "The 'base' parameter specifies the base of the logarithm.",
      "The 'dtype' parameter specifies the data type of the output.",
      "The 'axis' parameter specifies the axis along which to generate the logspace."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage: Generating Logarithmically Spaced Values",
    "concepts": [
      "Demonstrates generating 5 logarithmically spaced values between 1 (10 ** 0) and 100 (10 ** 2) using jnp.logspace.",
      "The jnp.printoptions context manager is used to format the output for better readability.",
      "The default base is 10 and endpoint is True."
    ],
    "code_examples": [
      {
        "description": "Generating 5 logarithmically spaced values between 1 and 100.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    print(jnp.logspace(0, 2, 5))"
      },
      {
        "description": "Generating 5 logarithmically spaced values between 1 and 100.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    print(jnp.logspace(0, 2, 5))"
      }
    ]
  },
  {
    "title": "Excluding the Endpoint",
    "concepts": [
      "Demonstrates generating logarithmically spaced values while excluding the endpoint using the 'endpoint' parameter.",
      "The 'endpoint' parameter is set to False to exclude the stop value from the result."
    ],
    "code_examples": [
      {
        "description": "Generating 5 logarithmically spaced values between 1 and 100, excluding the endpoint.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    print(jnp.logspace(0, 2, 5, endpoint=False))"
      },
      {
        "description": "Generating 5 logarithmically spaced values between 1 and 100, excluding the endpoint.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    print(jnp.logspace(0, 2, 5, endpoint=False))"
      }
    ]
  },
  {
    "title": "Changing the Base",
    "concepts": [
      "Demonstrates generating logarithmically spaced values with a custom base using the 'base' parameter.",
      "The 'base' parameter is set to 2 to generate values logarithmically spaced between 1 (2 ** 0) and 4 (2 ** 2) with base 2."
    ],
    "code_examples": [
      {
        "description": "Generating 7 logarithmically spaced values between 1 and 4 with base 2.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    print(jnp.logspace(0, 2, 7, base=2))"
      },
      {
        "description": "Generating 7 logarithmically spaced values between 1 and 4 with base 2.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    print(jnp.logspace(0, 2, 7, base=2))"
      }
    ]
  },
  {
    "title": "Multi-Dimensional Logspace",
    "concepts": [
      "Demonstrates how to generate multi-dimensional logspace using array-like 'start', 'stop', and 'base' parameters.",
      "The 'start', 'stop', and 'base' parameters are defined as JAX arrays to create a multi-dimensional logspace."
    ],
    "code_examples": [
      {
        "description": "Generating a multi-dimensional logspace with array-like start, stop, and base.",
        "code": "start = jnp.array([0, 5])\nstop = jnp.array([5, 0])\nbase = jnp.array([2, 3])\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.logspace(start, stop, 5, base=base))"
      },
      {
        "description": "Generating a multi-dimensional logspace with array-like start, stop, and base.",
        "code": "start = jnp.array([0, 5])\nstop = jnp.array([5, 0])\nbase = jnp.array([2, 3])\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.logspace(start, stop, 5, base=base))"
      }
    ]
  },
  {
    "title": "Introduction to mask_indices",
    "concepts": [
      "The function returns indices of a mask of an (n, n) array.",
      "It takes an integer 'n' as the dimension of the array.",
      "It takes a callable 'mask_func' that returns a boolean mask of shape (n, n).",
      "An optional 'k' value can be passed to the mask_func.",
      "An optional 'size' argument specifies the static size of the output arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Usage with built-in masking functions",
    "concepts": [
      "Demonstrates the use of mask_indices with jnp.triu and jnp.tril.",
      "jnp.triu creates an upper triangular matrix.",
      "jnp.tril creates a lower triangular matrix.",
      "mask_indices returns the indices where the triangular matrix is non-zero."
    ],
    "code_examples": [
      {
        "description": "Example using jnp.mask_indices with jnp.triu",
        "code": "jnp.mask_indices(3, jnp.triu)"
      },
      {
        "description": "Example using jnp.mask_indices with jnp.tril",
        "code": "jnp.mask_indices(3, jnp.tril)"
      }
    ]
  },
  {
    "title": "Usage with a custom masking function",
    "concepts": [
      "Demonstrates how to use mask_indices with a custom masking function.",
      "The custom mask function must accept an array and an optional offset k.",
      "The custom function must return a boolean mask of the same shape as the input array.",
      "The example uses modulo operation to create the mask."
    ],
    "code_examples": [
      {
        "description": "Define a custom masking function",
        "code": "def mask_func(x, k=0):\n    i = jnp.arange(x.shape[0])[:, None]\n    j = jnp.arange(x.shape[1])\n    return (i + 1) % (j + 1 + k) == 0"
      },
      {
        "description": "Apply the custom masking function to an array of ones",
        "code": "mask_func(jnp.ones((3, 3)))"
      },
      {
        "description": "Apply mask_indices with the custom masking function",
        "code": "jnp.mask_indices(3, mask_func)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.matmul",
    "concepts": [
      "jax.numpy.matmul() is a JAX implementation of numpy.matmul().",
      "It performs matrix multiplication.",
      "The inputs a and b can be arrays or scalars.",
      "It supports specifying precision for the computation.",
      "It supports specifying the preferred element type for accumulation.",
      "The function returns the matrix product of the inputs.",
      "Leading dimensions of multidimensional arrays must be broadcast-compatible."
    ],
    "code_examples": []
  },
  {
    "title": "Vector Dot Products",
    "concepts": [
      "Demonstrates vector dot product using jnp.matmul().",
      "Uses jnp.array to define vectors.",
      "Calculates the dot product of two vectors."
    ],
    "code_examples": [
      {
        "description": "Calculates the dot product of two vectors using jnp.matmul().",
        "code": "a = jnp.array([1, 2, 3])\nb = jnp.array([4, 5, 6])\njnp.matmul(a, b)"
      },
      {
        "description": "Calculates the dot product of two vectors using jnp.matmul().",
        "code": "a = jnp.array([1, 2, 3])\nb = jnp.array([4, 5, 6])\njnp.matmul(a, b)"
      }
    ]
  },
  {
    "title": "Matrix Dot Product",
    "concepts": [
      "Illustrates matrix multiplication using jnp.matmul().",
      "Uses jnp.array to define matrices.",
      "Computes the matrix product of two matrices."
    ],
    "code_examples": [
      {
        "description": "Calculates the matrix product of two matrices using jnp.matmul().",
        "code": "a = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\nb = jnp.array([[1, 2],\n               [3, 4],\n               [5, 6]])\njnp.matmul(a, b)"
      },
      {
        "description": "Calculates the matrix product of two matrices using jnp.matmul().",
        "code": "a = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\nb = jnp.array([[1, 2],\n               [3, 4],\n               [5, 6]])\njnp.matmul(a, b)"
      }
    ]
  },
  {
    "title": "Using the @ operator",
    "concepts": [
      "Shows how to use the @ operator as a shorthand for jnp.matmul().",
      "Demonstrates matrix multiplication using the @ operator."
    ],
    "code_examples": [
      {
        "description": "Calculates the matrix product using the @ operator.",
        "code": "a @ b"
      },
      {
        "description": "Calculates the matrix product using the @ operator.",
        "code": "a @ b"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.matrix_transpose",
    "concepts": [
      "Transposes the last two dimensions of an array.",
      "JAX implementation of numpy.matrix_transpose().",
      "Implemented using jax.lax.transpose().",
      "Input array must have at least two dimensions.",
      "Returns a matrix-transposed copy of the array.",
      "The function returns a copy, not a view.",
      "The compiler can optimize away copies under JIT."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.matrix_transpose Usage",
    "concepts": [
      "Demonstrates matrix transposition of a 3D array using jax.numpy.matrix_transpose().",
      "Demonstrates matrix transposition using the .mT property of jax.Array."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating matrix transposition of a 3D array.",
        "code": "x = jnp.array([[[1, 2],\n[3, 4]],\n[[5, 6],\n[7, 8]]])\n\njnp.matrix_transpose(x)"
      },
      {
        "description": "Example demonstrating matrix transposition using the .mT property of jax.Array.",
        "code": "x = jnp.array([[[1, 2],\n[3, 4]],\n[[5, 6],\n[7, 8]]])\n\nx.mT"
      }
    ]
  },
  {
    "title": "Batched Matrix-Vector Product",
    "concepts": [
      "JAX implementation of numpy.matvec().",
      "x1 is an array of shape (..., M, N).",
      "x2 is an array of shape (..., N).",
      "Leading dimensions of x1 and x2 must be broadcast-compatible.",
      "The result is an array of shape (..., M) containing the batched matrix-vector product."
    ],
    "code_examples": [
      {
        "description": "Simple matrix-vector product using jnp.matvec.",
        "code": "x1 = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\nx2 = jnp.array([7, 8, 9])\njnp.matvec(x1, x2)"
      },
      {
        "description": "Batched matrix-vector product using jnp.matvec.",
        "code": "x1 = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\nx2 = jnp.array([[7, 8, 9],\n                [5, 6, 7]])\njnp.matvec(x1, x2)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.max",
    "concepts": [
      "Calculates the maximum of array elements along a specified axis.",
      "It is a JAX implementation of numpy.max().",
      "Accepts an array, axis, keepdims, initial and where parameters.",
      "Returns an array of maximum values."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jax.numpy.max",
    "concepts": [
      "Demonstrates finding the maximum element in an array.",
      "Shows default behavior of finding the maximum across all axes."
    ],
    "code_examples": [
      {
        "description": "Finds the maximum element in a 2D JAX array.",
        "code": "x = jnp.array([[9, 3, 4, 5],\n               [5, 2, 7, 4],\n               [8, 1, 3, 6]])\njnp.max(x)"
      },
      {
        "description": "Finds the maximum element in a 2D JAX array.",
        "code": "x = jnp.array([[9, 3, 4, 5],\n               [5, 2, 7, 4],\n               [8, 1, 3, 6]])\njnp.max(x)"
      }
    ]
  },
  {
    "title": "Specifying the Axis",
    "concepts": [
      "Demonstrates finding the maximum along a specific axis (axis=1).",
      "axis=1 computes the maximum for each row."
    ],
    "code_examples": [
      {
        "description": "Calculates the maximum value along axis 1 (rows) of the array.",
        "code": "jnp.max(x, axis=1)"
      },
      {
        "description": "Calculates the maximum value along axis 1 (rows) of the array.",
        "code": "jnp.max(x, axis=1)"
      }
    ]
  },
  {
    "title": "Keeping Dimensions with keepdims",
    "concepts": [
      "Illustrates the use of `keepdims=True` to maintain the original number of dimensions in the output.",
      "The reduced axis will have a size of 1."
    ],
    "code_examples": [
      {
        "description": "Calculates the maximum along axis 1 while keeping the dimensions of the original array.",
        "code": "jnp.max(x, axis=1, keepdims=True)"
      },
      {
        "description": "Calculates the maximum along axis 1 while keeping the dimensions of the original array.",
        "code": "jnp.max(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Conditional Maximum with 'where'",
    "concepts": [
      "Demonstrates using the `where` parameter to conditionally include elements in the maximum calculation.",
      "`initial` value must be set when `where` is specified.",
      "`where` array can have the same dimension as the input array.",
      "`where` array can be broadcast compatible with the input array."
    ],
    "code_examples": [
      {
        "description": "Calculates the maximum along axis 1, including elements based on the 'where' condition, which has the same dimensions as the input.",
        "code": "where = jnp.array([[0, 0, 1, 0],\n                   [0, 0, 1, 1],\n                   [1, 1, 1, 0]], dtype=bool)\njnp.max(x, axis=1, keepdims=True, initial=0, where=where)"
      },
      {
        "description": "Calculates the maximum along axis 1, including elements based on the 'where' condition, which has the same dimensions as the input.",
        "code": "where = jnp.array([[0, 0, 1, 0],\n                   [0, 0, 1, 1],\n                   [1, 1, 1, 0]], dtype=bool)\njnp.max(x, axis=1, keepdims=True, initial=0, where=where)"
      },
      {
        "description": "Calculates the maximum along axis 0, including elements based on the broadcast-compatible 'where' condition.",
        "code": "where = jnp.array([[False],\n                   [False],\n                   [False]])\njnp.max(x, axis=0, keepdims=True, initial=0, where=where)"
      },
      {
        "description": "Calculates the maximum along axis 0, including elements based on the broadcast-compatible 'where' condition.",
        "code": "where = jnp.array([[False],\n                   [False],\n                   [False]])\njnp.max(x, axis=0, keepdims=True, initial=0, where=where)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.mean()",
    "concepts": [
      "Calculates the mean of array elements along a specified axis.",
      "JAX implementation of numpy.mean().",
      "Accepts an array as input.",
      "The 'axis' parameter specifies the axis or axes along which to compute the mean.",
      "The 'dtype' parameter specifies the output data type.",
      "The 'keepdims' parameter keeps the reduced axes in the result with size 1.",
      "The 'where' parameter allows you to specify elements to include in the calculation.",
      "For float16 or bfloat16 inputs, reductions are performed at float32 precision."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage: Mean Along All Axes",
    "concepts": [
      "The default behavior computes the mean across all elements of the array."
    ],
    "code_examples": [
      {
        "description": "This example demonstrates calculating the mean of all elements in a 2D array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 1, 2, 9]])\n\nprint(jnp.mean(x))"
      },
      {
        "description": "This example demonstrates calculating the mean of all elements in a 2D array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 1, 2, 9]])\n\nprint(jnp.mean(x))"
      }
    ]
  },
  {
    "title": "Mean Along a Specific Axis",
    "concepts": [
      "The axis parameter allows you to calculate the mean along a specific axis of the array.",
      "Specifying axis=1 calculates the mean along the rows (horizontally)."
    ],
    "code_examples": [
      {
        "description": "This example calculates the mean along axis 1 (rows) of the array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 1, 2, 9]])\n\nprint(jnp.mean(x, axis=1))"
      },
      {
        "description": "This example calculates the mean along axis 1 (rows) of the array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 1, 2, 9]])\n\nprint(jnp.mean(x, axis=1))"
      }
    ]
  },
  {
    "title": "Keeping Dimensions with keepdims",
    "concepts": [
      "The keepdims parameter preserves the dimensions of the input array in the output.",
      "When keepdims=True, the reduced axes are left in the result with size 1."
    ],
    "code_examples": [
      {
        "description": "This example calculates the mean along axis 1 and keeps the dimensions of the input array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 1, 2, 9]])\n\nprint(jnp.mean(x, axis=1, keepdims=True))"
      },
      {
        "description": "This example calculates the mean along axis 1 and keeps the dimensions of the input array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 1, 2, 9]])\n\nprint(jnp.mean(x, axis=1, keepdims=True))"
      }
    ]
  },
  {
    "title": "Using the where Parameter for Conditional Mean",
    "concepts": [
      "The where parameter allows you to compute the mean based on a boolean mask.",
      "Only elements where the corresponding value in the where array is True are included in the mean calculation."
    ],
    "code_examples": [
      {
        "description": "This example calculates the mean along axis 1 using a where condition to include only specific elements.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 1, 2, 9]])\n\nwhere = jnp.array([[1, 0, 1, 0],\n                  [0, 1, 0, 1],\n                  [1, 1, 0, 1]], dtype=bool)\n\nprint(jnp.mean(x, axis=1, keepdims=True, where=where))"
      },
      {
        "description": "This example calculates the mean along axis 1 using a where condition to include only specific elements.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 1, 2, 9]])\n\nwhere = jnp.array([[1, 0, 1, 0],\n                  [0, 1, 0, 1],\n                  [1, 1, 0, 1]], dtype=bool)\n\nprint(jnp.mean(x, axis=1, keepdims=True, where=where))"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.median",
    "concepts": [
      "The function calculates the median of array elements along a specified axis.",
      "It is a JAX implementation of numpy.median().",
      "It accepts an array as input.",
      "The axis parameter specifies the axis along which to compute the median.",
      "The keepdims parameter determines if reduced axes are kept with size 1.",
      "The out and overwrite_input parameters are unused by JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Median of a Flattened Array",
    "concepts": [
      "By default, the median is computed for the flattened array if no axis is specified."
    ],
    "code_examples": [
      {
        "description": "Calculates the median of a flattened JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[2, 4, 7, 1],\n               [3, 5, 9, 2],\n               [6, 1, 8, 3]])\n\njnp.median(x)"
      }
    ]
  },
  {
    "title": "Median Along a Specific Axis",
    "concepts": [
      "The median can be calculated along a specific axis using the axis parameter."
    ],
    "code_examples": [
      {
        "description": "Calculates the median along axis 1 of a JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[2, 4, 7, 1],\n               [3, 5, 9, 2],\n               [6, 1, 8, 3]])\n\njnp.median(x, axis=1)"
      }
    ]
  },
  {
    "title": "Keeping Dimensions",
    "concepts": [
      "The keepdims parameter can be used to maintain the original number of dimensions in the output."
    ],
    "code_examples": [
      {
        "description": "Calculates the median along axis 1 and keeps the dimensions of the input array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[2, 4, 7, 1],\n               [3, 5, 9, 2],\n               [6, 1, 8, 3]])\n\njnp.median(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.meshgrid",
    "concepts": [
      "Constructs N-dimensional grid arrays from N 1-dimensional arrays.",
      "It is a JAX implementation of NumPy's meshgrid.",
      "It takes N arrays as input.",
      "The `copy` argument is only supported as `True` in JAX.",
      "The `sparse` argument controls the shape of the output arrays.",
      "The `indexing` argument controls the style of indexing ('xy' for cartesian, 'ij' for matrix)."
    ],
    "code_examples": []
  },
  {
    "title": "Example Setup",
    "concepts": [
      "Defining sample 1D arrays `x` and `y` for use in subsequent examples."
    ],
    "code_examples": [
      {
        "description": "Define 1D arrays x and y using jnp.array.",
        "code": "x = jnp.array([1, 2])\ny = jnp.array([10, 20, 30])"
      }
    ]
  },
  {
    "title": "2D Cartesian Mesh Grid",
    "concepts": [
      "Creating a 2D Cartesian mesh grid using the default 'xy' indexing."
    ],
    "code_examples": [
      {
        "description": "Create a 2D cartesian meshgrid using jnp.meshgrid and print the resulting x and y grids.",
        "code": "x_grid, y_grid = jnp.meshgrid(x, y)\nprint(x_grid)\nprint(y_grid)"
      }
    ]
  },
  {
    "title": "2D Sparse Cartesian Mesh Grid",
    "concepts": [
      "Creating a 2D sparse Cartesian mesh grid using the `sparse=True` option."
    ],
    "code_examples": [
      {
        "description": "Create a 2D sparse cartesian meshgrid using jnp.meshgrid with sparse=True and print the resulting x and y grids.",
        "code": "x_grid, y_grid = jnp.meshgrid(x, y, sparse=True)\nprint(x_grid)\nprint(y_grid)"
      }
    ]
  },
  {
    "title": "2D Matrix-Index Mesh Grid",
    "concepts": [
      "Creating a 2D matrix-index mesh grid using the `indexing='ij'` option."
    ],
    "code_examples": [
      {
        "description": "Create a 2D matrix-index meshgrid using jnp.meshgrid with indexing='ij' and print the resulting x and y grids.",
        "code": "x_grid, y_grid = jnp.meshgrid(x, y, indexing='ij')\nprint(x_grid)\nprint(y_grid)"
      }
    ]
  },
  {
    "title": "Description and Purpose of jnp.mgrid",
    "concepts": [
      "jnp.mgrid returns a dense multi-dimensional meshgrid.",
      "It's a LAX-backend implementation of numpy.mgrid.",
      "It serves as a convenience wrapper for jax.numpy.meshgrid() with sparse=False.",
      "jnp.ogrid is the open/sparse version of jnp.mgrid."
    ],
    "code_examples": []
  },
  {
    "title": "Generating Values Similar to jax.numpy.arange()",
    "concepts": [
      "Passing [start:stop:step] to jnp.mgrid generates values similar to jax.numpy.arange()."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating jnp.mgrid with integer steps.",
        "code": "jnp.mgrid[0:4:1]\n# Output:\n# Array([0, 1, 2, 3], dtype=int32)"
      },
      {
        "description": "Duplicated Example demonstrating jnp.mgrid with integer steps.",
        "code": "jnp.mgrid[0:4:1]\n# Output:\n# Array([0, 1, 2, 3], dtype=int32)"
      }
    ]
  },
  {
    "title": "Generating Values Similar to jax.numpy.linspace()",
    "concepts": [
      "Passing an imaginary step generates values similar to jax.numpy.linspace()."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating jnp.mgrid with an imaginary step.",
        "code": "jnp.mgrid[0:1:4j]\n# Output:\n# Array([0.        , 0.33333334, 0.6666667 , 1.        ], dtype=float32)"
      },
      {
        "description": "Duplicated Example demonstrating jnp.mgrid with an imaginary step.",
        "code": "jnp.mgrid[0:1:4j]\n# Output:\n# Array([0.        , 0.33333334, 0.6666667 , 1.        ], dtype=float32)"
      }
    ]
  },
  {
    "title": "Creating Broadcasted Grids of Indices",
    "concepts": [
      "Multiple slices can be used to create broadcasted grids of indices."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the use of multiple slices with jnp.mgrid.",
        "code": "jnp.mgrid[:2,:3]\n# Output:\n# Array([[[0, 0, 0],\n#         [1, 1, 1]],\n#\n#        [[0, 1, 2],\n#         [0, 1, 2]]], dtype=int32)"
      },
      {
        "description": "Duplicated Example demonstrating the use of multiple slices with jnp.mgrid.",
        "code": "jnp.mgrid[:2,:3]\n# Output:\n# Array([[[0, 0, 0],\n#         [1, 1, 1]],\n#\n#        [[0, 1, 2],\n#         [0, 1, 2]]], dtype=int32)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.minimum",
    "concepts": [
      "The function returns the element-wise minimum of two input arrays.",
      "It's a JAX implementation of numpy.minimum.",
      "Input arrays must have the same shape or be broadcast compatible.",
      "If both elements are finite numbers, it returns the smaller of the two.",
      "It returns nan if one element is nan."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Inputs Having the Same Shape",
    "concepts": [
      "Demonstrates the use of jnp.minimum with arrays of the same shape.",
      "Shows element-wise comparison and selection of the minimum value."
    ],
    "code_examples": [
      {
        "description": "Example showcasing jnp.minimum with two arrays of the same shape.  The result is an array containing the element-wise minimum of the two input arrays.",
        "code": "x = jnp.array([2, 3, 5, 1])\ny = jnp.array([-3, 6, -4, 7])\njnp.minimum(x, y)"
      },
      {
        "description": "Redundant example showcasing jnp.minimum with two arrays of the same shape.",
        "code": "x = jnp.array([2, 3, 5, 1])\ny = jnp.array([-3, 6, -4, 7])\njnp.minimum(x, y)"
      }
    ]
  },
  {
    "title": "Examples with Broadcast Compatibility",
    "concepts": [
      "Illustrates the behavior of jnp.minimum when input arrays are broadcast compatible.",
      "Shows how the smaller array is broadcasted to match the shape of the larger array before element-wise comparison."
    ],
    "code_examples": [
      {
        "description": "Example showcasing jnp.minimum with arrays that are broadcast compatible.  The array `y1` is broadcast across the rows of `x1`.",
        "code": "x1 = jnp.array([[1, 5, 2],\n                [-3, 4, 7]])\ny1 = jnp.array([-2, 3, 6])\njnp.minimum(x1, y1)"
      },
      {
        "description": "Redundant example showcasing jnp.minimum with arrays that are broadcast compatible.",
        "code": "x1 = jnp.array([[1, 5, 2],\n                [-3, 4, 7]])\ny1 = jnp.array([-2, 3, 6])\njnp.minimum(x1, y1)"
      }
    ]
  },
  {
    "title": "Examples with NaN Values",
    "concepts": [
      "Demonstrates how jnp.minimum handles NaN (Not a Number) values in the input arrays.",
      "Explains that if either of the elements being compared is NaN, the result is NaN."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating jnp.minimum's behavior when NaN values are present. If either input is NaN, the output is NaN.",
        "code": "nan = jnp.nan\nx2 = jnp.array([[2.5, nan, -2],\n                [nan, 5, 6],\n                [-4, 3, 7]])\ny2 = jnp.array([1, nan, 5])\njnp.minimum(x2, y2)"
      },
      {
        "description": "Redundant example demonstrating jnp.minimum's behavior when NaN values are present.",
        "code": "nan = jnp.nan\nx2 = jnp.array([[2.5, nan, -2],\n                [nan, 5, 6],\n                [-4, 3, 7]])\ny2 = jnp.array([1, nan, 5])\njnp.minimum(x2, y2)"
      }
    ]
  },
  {
    "title": "Alias of jax.numpy.remainder()",
    "concepts": [
      "This entry describes an alias for the jax.numpy.remainder() function.",
      "x1 is an input array-like object.",
      "x2 is an input array-like object.",
      "The output is an array."
    ],
    "code_examples": []
  },
  {
    "title": "JAX.NumPy.modf Function",
    "concepts": [
      "The function returns the fractional and integral parts of the input array element-wise.",
      "It is a JAX implementation of NumPy's modf function.",
      "The input can be an array or a scalar.",
      "The function promotes dtypes to inexact types.",
      "It returns a tuple containing the fractional and integral parts as JAX arrays."
    ],
    "code_examples": [
      {
        "description": "Example of using jnp.modf with a scalar input.",
        "code": "jnp.modf(4.8)\n(Array(0.8000002, dtype=float32, weak_type=True), Array(4., dtype=float32, weak_type=True))"
      },
      {
        "description": "Example of using jnp.modf with an array input.",
        "code": "x = jnp.array([-3.4, -5.7, 0.6, 1.5, 2.3])\njnp.modf(x)\n(Array([-0.4000001 , -0.6999998 ,  0.6       ,  0.5       ,  0.29999995],\n      dtype=float32), Array([-3., -5.,  0.,  1.,  2.], dtype=float32))"
      },
      {
        "description": "Example of using jnp.modf with a scalar input.",
        "code": "jnp.modf(4.8)\n(Array(0.8000002, dtype=float32, weak_type=True), Array(4., dtype=float32, weak_type=True))"
      },
      {
        "description": "Example of using jnp.modf with an array input.",
        "code": "x = jnp.array([-3.4, -5.7, 0.6, 1.5, 2.3])\njnp.modf(x)\n(Array([-0.4000001 , -0.6999998 ,  0.6       ,  0.5       ,  0.29999995],\n      dtype=float32), Array([-3., -5.,  0.,  1.,  2.], dtype=float32))"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.moveaxis",
    "concepts": [
      "The function moves array axes to new positions.",
      "It is a JAX implementation of numpy.moveaxis.",
      "It is implemented using jax.lax.transpose().",
      "The function takes an array, source indices, and destination indices as input.",
      "It returns a copy of the array with the axes moved.",
      "Under JIT compilation, copies may be optimized away."
    ],
    "code_examples": []
  },
  {
    "title": "Moving a Single Axis",
    "concepts": [
      "Moving axis 1 to the end of the array.",
      "Moving the last axis to position 1."
    ],
    "code_examples": [
      {
        "description": "Move axis 1 to the end of the array:",
        "code": "a = jnp.ones((2, 3, 4, 5))\njnp.moveaxis(a, 1, -1).shape"
      },
      {
        "description": "Move the last axis to position 1:",
        "code": "a = jnp.ones((2, 3, 4, 5))\njnp.moveaxis(a, -1, 1).shape"
      }
    ]
  },
  {
    "title": "Moving Multiple Axes",
    "concepts": [
      "Moving multiple axes at once.",
      "Using tuples to specify multiple source and destination axes."
    ],
    "code_examples": [
      {
        "description": "Move multiple axes:",
        "code": "a = jnp.ones((2, 3, 4, 5))\njnp.moveaxis(a, (0, 1), (-1, -2)).shape"
      }
    ]
  },
  {
    "title": "Equivalence with transpose()",
    "concepts": [
      "moveaxis() can be achieved using transpose().",
      "transpose() provides more general axes permutation."
    ],
    "code_examples": [
      {
        "description": "Achieving the same result with transpose():",
        "code": "a = jnp.ones((2, 3, 4, 5))\na.transpose(2, 3, 1, 0).shape"
      }
    ]
  },
  {
    "title": "Introduction to jnp.multiply",
    "concepts": [
      "jnp.multiply is a JAX implementation of numpy.multiply.",
      "It's a universal function (ufunc) in JAX.",
      "It implements the * operator for JAX arrays.",
      "It performs element-wise multiplication of two arrays.",
      "Arrays must be broadcastable to a common shape."
    ],
    "code_examples": []
  },
  {
    "title": "Explicitly calling jnp.multiply",
    "concepts": [
      "Demonstrates how to use jnp.multiply function directly."
    ],
    "code_examples": [
      {
        "description": "Example of multiplying a JAX array by a scalar using jnp.multiply.",
        "code": "x = jnp.arange(4)\njnp.multiply(x, 10)"
      },
      {
        "description": "Example of multiplying a JAX array by a scalar using jnp.multiply.",
        "code": "x = jnp.arange(4)\njnp.multiply(x, 10)"
      }
    ]
  },
  {
    "title": "Using the * operator for multiplication",
    "concepts": [
      "Shows how to use the * operator as a shorthand for jnp.multiply."
    ],
    "code_examples": [
      {
        "description": "Example of multiplying a JAX array by a scalar using the * operator.",
        "code": "x * 10"
      },
      {
        "description": "Example of multiplying a JAX array by a scalar using the * operator.",
        "code": "x * 10"
      }
    ]
  },
  {
    "title": "Introduction to jnp.nan_to_num",
    "concepts": [
      "The function replaces NaN and infinite values in a JAX array.",
      "It is a JAX implementation of numpy.nan_to_num().",
      "The function returns a copy of the array with substitutions.",
      "The nan argument specifies the value to substitute for NaN entries (default is 0.0).",
      "The posinf argument specifies the value to substitute for positive infinite entries (default is the maximum representable value).",
      "The neginf argument specifies the value to substitute for negative infinite entries (default is the minimum representable value)."
    ],
    "code_examples": []
  },
  {
    "title": "Default Substitution Values",
    "concepts": [
      "Demonstrates the default behavior of jnp.nan_to_num.",
      "NaN values are replaced by 0.0.",
      "Positive infinity is replaced by the maximum representable float value.",
      "Negative infinity is replaced by the minimum representable float value."
    ],
    "code_examples": [
      {
        "description": "Example array containing NaN, positive infinity, and negative infinity.",
        "code": "x = jnp.array([0, jnp.nan, 1, jnp.inf, 2, -jnp.inf])"
      },
      {
        "description": "Applies nan_to_num with default parameters, replacing NaN, inf, and -inf with default values.",
        "code": "jnp.nan_to_num(x)"
      }
    ]
  },
  {
    "title": "Overriding Substitution Values",
    "concepts": [
      "Shows how to override the default substitution values for positive and negative infinity using the posinf and neginf arguments.",
      "This allows for custom replacements for infinite values."
    ],
    "code_examples": [
      {
        "description": "Replaces NaN, positive infinity, and negative infinity with specified values.",
        "code": "jnp.nan_to_num(x, posinf=999, neginf=-999)"
      }
    ]
  },
  {
    "title": "Substituting Only NaN Values",
    "concepts": [
      "Demonstrates how to replace only NaN values, leaving infinite values untouched.",
      "Uses jnp.where() and jnp.isnan() for selective replacement."
    ],
    "code_examples": [
      {
        "description": "Replaces NaN values with 0, leaving positive and negative infinity untouched using jnp.where and jnp.isnan.",
        "code": "jnp.where(jnp.isnan(x), 0, x)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.nanargmax()",
    "concepts": [
      "Returns the index of the maximum value of an array, ignoring NaNs.",
      "JAX implementation of numpy.nanargmax().",
      "If an axis contains all NaN values, the returned index is -1.",
      "Differs from numpy.nanargmax() which raises an error in the case of an axis with all-NaN values."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jax.numpy.nanargmax()",
    "concepts": [
      "Demonstrates how nanargmax() finds the index of the maximum non-NaN value in an array.",
      "Shows the difference between argmax() and nanargmax() when NaNs are present."
    ],
    "code_examples": [
      {
        "description": "Illustrates the difference between jnp.argmax() and jnp.nanargmax() when dealing with NaN values in an array.",
        "code": "x = jnp.array([1, 3, 5, 4, jnp.nan])\nprint(jnp.argmax(x))\nprint(jnp.nanargmax(x))"
      }
    ]
  },
  {
    "title": "Usage with Axis and keepdims",
    "concepts": [
      "Demonstrates how to use the `axis` parameter to find the index of the maximum non-NaN value along a specific axis.",
      "Shows the use of `keepdims` parameter to maintain the original number of dimensions in the output array."
    ],
    "code_examples": [
      {
        "description": "Demonstrates nanargmax with axis=1.",
        "code": "x = jnp.array([[1, 3, jnp.nan],\n              [5, 4, jnp.nan]])\nprint(jnp.nanargmax(x, axis=1))"
      },
      {
        "description": "Demonstrates nanargmax with axis=1 and keepdims=True.",
        "code": "x = jnp.array([[1, 3, jnp.nan],\n              [5, 4, jnp.nan]])\nprint(jnp.nanargmax(x, axis=1, keepdims=True))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.nanargmin()",
    "concepts": [
      "The function returns the index of the minimum value of an array, ignoring NaN values.",
      "It is a JAX implementation of numpy.nanargmin().",
      "If an axis contains all NaN values, the function returns -1.",
      "This behavior differs from numpy.nanargmin(), which raises an error in the same situation."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.nanargmin()",
    "concepts": [
      "Demonstrates how to use jnp.nanargmin() on a 1D array containing NaN values.",
      "The function returns the index of the minimum non-NaN value."
    ],
    "code_examples": [
      {
        "description": "Finds the index of the minimum value in a 1D array, ignoring NaN.",
        "code": "x = jnp.array([jnp.nan, 3, 5, 4, 2])\njnp.nanargmin(x)"
      },
      {
        "description": "Finds the index of the minimum value in a 1D array, ignoring NaN. (Duplicate)",
        "code": "x = jnp.array([jnp.nan, 3, 5, 4, 2])\njnp.nanargmin(x)"
      }
    ]
  },
  {
    "title": "Usage with axis argument",
    "concepts": [
      "Demonstrates how to use jnp.nanargmin() on a 2D array with the axis argument.",
      "The function returns the index of the minimum non-NaN value along the specified axis."
    ],
    "code_examples": [
      {
        "description": "Finds the index of the minimum value along axis 1, ignoring NaN.",
        "code": "x = jnp.array([[1, 3, jnp.nan],\n                [5, 4, jnp.nan]])\njnp.nanargmin(x, axis=1)"
      },
      {
        "description": "Finds the index of the minimum value along axis 1, ignoring NaN. (Duplicate)",
        "code": "x = jnp.array([[1, 3, jnp.nan],\n                [5, 4, jnp.nan]])\njnp.nanargmin(x, axis=1)"
      }
    ]
  },
  {
    "title": "Usage with keepdims argument",
    "concepts": [
      "Demonstrates how to use jnp.nanargmin() on a 2D array with the axis and keepdims arguments.",
      "The function returns an array with the same number of dimensions as the input array."
    ],
    "code_examples": [
      {
        "description": "Finds the index of the minimum value along axis 1, ignoring NaN, and keeps the dimensions.",
        "code": "x = jnp.array([[1, 3, jnp.nan],\n                [5, 4, jnp.nan]])\njnp.nanargmin(x, axis=1, keepdims=True)"
      },
      {
        "description": "Finds the index of the minimum value along axis 1, ignoring NaN, and keeps the dimensions. (Duplicate)",
        "code": "x = jnp.array([[1, 3, jnp.nan],\n                [5, 4, jnp.nan]])\njnp.nanargmin(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.nancumprod",
    "concepts": [
      "Calculates the cumulative product of elements along a given axis, ignoring NaN values.",
      "It is a JAX implementation of NumPy's nancumprod function.",
      "NaN values are effectively treated as ones during the cumulative product calculation.",
      "The output is an array containing the accumulated product along the specified axis."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage and Comparison with jax.numpy.cumprod",
    "concepts": [
      "Demonstrates the difference between cumprod and nancumprod when NaN values are present.",
      "cumprod propagates NaN values throughout the result.",
      "nancumprod treats NaN values as 1, thus not propagating them."
    ],
    "code_examples": [
      {
        "description": "Illustrates the propagation of NaN values with jnp.cumprod and the handling of NaN values with jnp.nancumprod.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., jnp.nan],\n               [4., jnp.nan, 6.]])\n\nprint(jnp.cumprod(x))\nprint(jnp.nancumprod(x))"
      }
    ]
  },
  {
    "title": "Cumulative Product Along a Specific Axis",
    "concepts": [
      "Calculates the cumulative product along a specified axis (axis=1 in this example).",
      "NaN values are ignored during the cumulative product calculation along the specified axis."
    ],
    "code_examples": [
      {
        "description": "Calculates the cumulative product along axis 1, ignoring NaN values.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., jnp.nan],\n               [4., jnp.nan, 6.]])\n\nprint(jnp.nancumprod(x, axis=1))"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.nancumsum",
    "concepts": [
      "Calculates the cumulative sum of array elements along a specified axis, ignoring NaN values.",
      "It's a JAX implementation of NumPy's nancumsum function.",
      "The 'axis' parameter determines the axis along which the cumulative sum is computed; if None, the array is flattened.",
      "The 'dtype' parameter allows specifying the output array's data type.",
      "NaN values are treated as zero during the cumulative sum calculation."
    ],
    "code_examples": []
  },
  {
    "title": "Demonstration of nancumsum vs cumsum",
    "concepts": [
      "Illustrates the difference between jnp.cumsum and jnp.nancumsum when dealing with NaN values.",
      "jnp.cumsum propagates NaN values through the cumulative sum.",
      "jnp.nancumsum treats NaN values as zero, preventing their propagation."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how standard cumulative sum propagates NaN values.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., jnp.nan],\n               [4., jnp.nan, 6.]])\n\njnp.cumsum(x)"
      },
      {
        "description": "Shows how nancumsum ignores NaN values.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., jnp.nan],\n               [4., jnp.nan, 6.]])\n\njnp.nancumsum(x)"
      }
    ]
  },
  {
    "title": "Cumulative Sum Along a Specific Axis",
    "concepts": [
      "Demonstrates calculating the cumulative sum along a specified axis using the 'axis' parameter.",
      "Specifying axis=1 calculates the cumulative sum for each row."
    ],
    "code_examples": [
      {
        "description": "Calculates the cumulative sum along axis 1, ignoring NaN values.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., jnp.nan],\n               [4., jnp.nan, 6.]])\n\njnp.nancumsum(x, axis=1)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.nanmax",
    "concepts": [
      "Computes the maximum of array elements along a given axis, ignoring NaNs.",
      "It is a JAX implementation of numpy.nanmax().",
      "The function returns nan if all values are NaNs along the given axis."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage",
    "concepts": [
      "Demonstrates the basic usage of jnp.nanmax without specifying any axis, resulting in the maximum of the flattened array.",
      "Illustrates how NaN values are ignored when calculating the maximum."
    ],
    "code_examples": [
      {
        "description": "Computes the maximum of elements along the flattened array.",
        "code": "nan = jnp.nan\nx = jnp.array([[8, nan, 4, 6],\n               [nan, -2, nan, -4],\n               [-2, 1, 7, nan]])\njnp.nanmax(x)"
      },
      {
        "description": "Computes the maximum of elements along the flattened array.",
        "code": "nan = jnp.nan\nx = jnp.array([[8, nan, 4, 6],\n               [nan, -2, nan, -4],\n               [-2, 1, 7, nan]])\njnp.nanmax(x)"
      }
    ]
  },
  {
    "title": "Specifying the Axis",
    "concepts": [
      "Shows how to compute the maximum along a specific axis (axis=1).",
      "The output array contains the maximum value for each row."
    ],
    "code_examples": [
      {
        "description": "Computes the maximum along axis 1.",
        "code": "jnp.nanmax(x, axis=1)"
      },
      {
        "description": "Computes the maximum along axis 1.",
        "code": "jnp.nanmax(x, axis=1)"
      }
    ]
  },
  {
    "title": "Keeping Dimensions",
    "concepts": [
      "Demonstrates the use of the keepdims parameter to maintain the same number of dimensions in the output as the input.",
      "The reduced axis is left in the result with size 1."
    ],
    "code_examples": [
      {
        "description": "Computes the maximum along axis 1, keeping the dimensions.",
        "code": "jnp.nanmax(x, axis=1, keepdims=True)"
      },
      {
        "description": "Computes the maximum along axis 1, keeping the dimensions.",
        "code": "jnp.nanmax(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Using the where parameter",
    "concepts": [
      "Explains how to use the where parameter to include only specific elements in the maximum computation.",
      "The where array can have the same dimension as the input or be broadcast compatible.",
      "The initial value must be specified when where is used."
    ],
    "code_examples": [
      {
        "description": "Computes the maximum along axis 1, keeping the dimensions, with an initial value of 0, using a where array with the same dimension as the input.",
        "code": "where = jnp.array([[0, 0, 1, 0],\n               [0, 0, 1, 1],\n               [1, 1, 1, 0]], dtype=bool)\njnp.nanmax(x, axis=1, keepdims=True, initial=0, where=where)"
      },
      {
        "description": "Computes the maximum along axis 1, keeping the dimensions, with an initial value of 0, using a where array with the same dimension as the input.",
        "code": "where = jnp.array([[0, 0, 1, 0],\n               [0, 0, 1, 1],\n               [1, 1, 1, 0]], dtype=bool)\njnp.nanmax(x, axis=1, keepdims=True, initial=0, where=where)"
      },
      {
        "description": "Computes the maximum along axis 0, keeping the dimensions, with an initial value of 0, using a where array that is broadcast compatible with the input.",
        "code": "where = jnp.array([[True],\n               [False],\n               [False]])\njnp.nanmax(x, axis=0, keepdims=True, initial=0, where=where)"
      },
      {
        "description": "Computes the maximum along axis 0, keeping the dimensions, with an initial value of 0, using a where array that is broadcast compatible with the input.",
        "code": "where = jnp.array([[True],\n               [False],\n               [False]])\njnp.nanmax(x, axis=0, keepdims=True, initial=0, where=where)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.nanmean",
    "concepts": [
      "Calculates the mean of array elements along a specified axis, ignoring NaN values.",
      "It is a JAX implementation of numpy.nanmean().",
      "If all elements along the specified axis are NaNs, it returns NaN."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.nanmean",
    "concepts": [
      "Demonstrates calculating the mean of a flattened array containing NaN values.",
      "The function ignores NaN values when computing the mean."
    ],
    "code_examples": [
      {
        "description": "Calculating the mean of a flattened array, ignoring NaN values.",
        "code": "nan = jnp.nan\nx = jnp.array([[2, nan, 4, 3],\n               [nan, -2, nan, 9],\n               [4, -7, 6, nan]])\njnp.nanmean(x)"
      },
      {
        "description": "Calculating the mean of a flattened array, ignoring NaN values.",
        "code": "nan = jnp.nan\nx = jnp.array([[2, nan, 4, 3],\n               [nan, -2, nan, 9],\n               [4, -7, 6, nan]])\njnp.nanmean(x)"
      }
    ]
  },
  {
    "title": "Using the axis Parameter",
    "concepts": [
      "Demonstrates calculating the mean along a specific axis (axis=1) while ignoring NaN values."
    ],
    "code_examples": [
      {
        "description": "Calculating the mean along axis 1, ignoring NaN values.",
        "code": "x = jnp.array([[2, nan, 4, 3],\n               [nan, -2, nan, 9],\n               [4, -7, 6, nan]])\njnp.nanmean(x, axis=1)"
      },
      {
        "description": "Calculating the mean along axis 1, ignoring NaN values.",
        "code": "x = jnp.array([[2, nan, 4, 3],\n               [nan, -2, nan, 9],\n               [4, -7, 6, nan]])\njnp.nanmean(x, axis=1)"
      }
    ]
  },
  {
    "title": "Using the keepdims Parameter",
    "concepts": [
      "Demonstrates preserving the number of dimensions in the output using keepdims=True.",
      "The reduced axis will have a size of 1."
    ],
    "code_examples": [
      {
        "description": "Calculating the mean along axis 1, keeping the dimensions of the original array.",
        "code": "x = jnp.array([[2, nan, 4, 3],\n               [nan, -2, nan, 9],\n               [4, -7, 6, nan]])\njnp.nanmean(x, axis=1, keepdims=True)"
      },
      {
        "description": "Calculating the mean along axis 1, keeping the dimensions of the original array.",
        "code": "x = jnp.array([[2, nan, 4, 3],\n               [nan, -2, nan, 9],\n               [4, -7, 6, nan]])\njnp.nanmean(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Using the where Parameter",
    "concepts": [
      "Demonstrates using the where parameter to conditionally include elements in the mean calculation.",
      "The where array must be broadcast-compatible with the input array."
    ],
    "code_examples": [
      {
        "description": "Calculating the mean along axis 1 using a where array to select elements.",
        "code": "x = jnp.array([[2, nan, 4, 3],\n               [nan, -2, nan, 9],\n               [4, -7, 6, nan]])\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 0, 1, 1],\n                   [1, 1, 0, 1]], dtype=bool)\njnp.nanmean(x, axis=1, keepdims=True, where=where)"
      },
      {
        "description": "Calculating the mean along axis 1 using a where array to select elements.",
        "code": "x = jnp.array([[2, nan, 4, 3],\n               [nan, -2, nan, 9],\n               [4, -7, 6, nan]])\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 0, 1, 1],\n                   [1, 1, 0, 1]], dtype=bool)\njnp.nanmean(x, axis=1, keepdims=True, where=where)"
      }
    ]
  },
  {
    "title": "Handling All-False where Condition",
    "concepts": [
      "Illustrates the behavior when the where array is False for all elements along a given axis.",
      "In this case, jnp.nanmean returns NaN along that axis."
    ],
    "code_examples": [
      {
        "description": "Calculating the mean along axis 0 with a where array that is always False, resulting in NaN.",
        "code": "x = jnp.array([[2, nan, 4, 3],\n               [nan, -2, nan, 9],\n               [4, -7, 6, nan]])\nwhere = jnp.array([[False],\n                   [False],\n                   [False]])\njnp.nanmean(x, axis=0, keepdims=True, where=where)"
      },
      {
        "description": "Calculating the mean along axis 0 with a where array that is always False, resulting in NaN.",
        "code": "x = jnp.array([[2, nan, 4, 3],\n               [nan, -2, nan, 9],\n               [4, -7, 6, nan]])\nwhere = jnp.array([[False],\n                   [False],\n                   [False]])\njnp.nanmean(x, axis=0, keepdims=True, where=where)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.nanmedian",
    "concepts": [
      "Calculates the median of array elements along a specified axis, while ignoring NaN values.",
      "It is a JAX implementation of NumPy's nanmedian function.",
      "The function can handle multi-dimensional arrays.",
      "The axis along which the median is computed can be specified.",
      "The function can maintain the dimensions of the input array with the keepdims parameter.",
      "Returns NaN if all elements along the given axis are NaNs."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage",
    "concepts": [
      "Demonstrates calculating the nanmedian of a flattened array.",
      "Illustrates the default behavior of calculating the median across all elements when no axis is specified."
    ],
    "code_examples": [
      {
        "description": "Calculates the nanmedian of a 2D array, treating it as a flattened array.",
        "code": "nan = jnp.nan\nx = jnp.array([[2, nan, 7, nan],\n               [nan, 5, 9, 2],\n               [6, 1, nan, 3]])\njnp.nanmedian(x)"
      },
      {
        "description": "Calculates the nanmedian of a 2D array, treating it as a flattened array.",
        "code": "nan = jnp.nan\nx = jnp.array([[2, nan, 7, nan],\n               [nan, 5, 9, 2],\n               [6, 1, nan, 3]])\njnp.nanmedian(x)"
      }
    ]
  },
  {
    "title": "Specifying the Axis",
    "concepts": [
      "Shows how to calculate the nanmedian along a specific axis (axis=1).",
      "Demonstrates calculating the median for each row in the array."
    ],
    "code_examples": [
      {
        "description": "Calculates the nanmedian along axis 1 (rows) of the array.",
        "code": "jnp.nanmedian(x, axis=1)"
      },
      {
        "description": "Calculates the nanmedian along axis 1 (rows) of the array.",
        "code": "jnp.nanmedian(x, axis=1)"
      }
    ]
  },
  {
    "title": "Keeping Dimensions",
    "concepts": [
      "Explains the use of the keepdims parameter to maintain the original dimensions of the input array.",
      "Illustrates how the output shape changes when keepdims=True."
    ],
    "code_examples": [
      {
        "description": "Calculates the nanmedian along axis 1 (rows) and keeps the dimensions of the original array using keepdims=True.",
        "code": "jnp.nanmedian(x, axis=1, keepdims=True)"
      },
      {
        "description": "Calculates the nanmedian along axis 1 (rows) and keeps the dimensions of the original array using keepdims=True.",
        "code": "jnp.nanmedian(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.nanmin",
    "concepts": [
      "Calculates the minimum of array elements along a specified axis, ignoring NaN values.",
      "It is a JAX implementation of numpy.nanmin().",
      "If all values along the given axis are NaNs, the function returns NaN.",
      "The 'axis' parameter specifies the axis along which the minimum is computed; if None, the flattened array is used.",
      "The 'keepdims' parameter, if True, retains the reduced axes in the output with size 1.",
      "The 'initial' parameter sets an initial value for the minimum calculation, which is required when using the 'where' parameter.",
      "The 'where' parameter allows specifying elements to be included in the minimum computation, requiring an array of boolean dtype broadcast compatible with the input and initial must be specified when where is used."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jax.numpy.nanmin",
    "concepts": [
      "Demonstrates finding the minimum of a flattened array containing NaN values.",
      "Shows that jnp.nanmin ignores NaN values when determining the minimum."
    ],
    "code_examples": [
      {
        "description": "Calculates the minimum value of a 2D JAX array, ignoring NaN values.  First defines `nan = jnp.nan` which may be an implicit requirement for the following examples.",
        "code": "nan = jnp.nan\nx = jnp.array([[1, nan, 4, 5],\n               [nan, -2, nan, -4],\n               [2, 1, 3, nan]])\njnp.nanmin(x)"
      },
      {
        "description": "Calculates the minimum value of a 2D JAX array, ignoring NaN values.  Repeats previous example. First defines `nan = jnp.nan` which may be an implicit requirement for the following examples.",
        "code": "nan = jnp.nan\nx = jnp.array([[1, nan, 4, 5],\n               [nan, -2, nan, -4],\n               [2, 1, 3, nan]])\njnp.nanmin(x)"
      }
    ]
  },
  {
    "title": "Specifying the Axis",
    "concepts": [
      "Illustrates finding the minimum along a specific axis (axis=1).",
      "Shows how to compute the minimum for each row of the array, ignoring NaNs."
    ],
    "code_examples": [
      {
        "description": "Calculates the minimum value along axis 1 (rows) of a 2D JAX array, ignoring NaN values.",
        "code": "jnp.nanmin(x, axis=1)"
      },
      {
        "description": "Calculates the minimum value along axis 1 (rows) of a 2D JAX array, ignoring NaN values. Repeats previous example.",
        "code": "jnp.nanmin(x, axis=1)"
      }
    ]
  },
  {
    "title": "Keeping Dimensions with keepdims",
    "concepts": [
      "Demonstrates the use of the 'keepdims' parameter to maintain the original number of dimensions in the output.",
      "Shows how the output shape changes when keepdims=True."
    ],
    "code_examples": [
      {
        "description": "Calculates the minimum value along axis 1 (rows) of a 2D JAX array, ignoring NaN values, and keeps the dimensions of the original array.",
        "code": "jnp.nanmin(x, axis=1, keepdims=True)"
      },
      {
        "description": "Calculates the minimum value along axis 1 (rows) of a 2D JAX array, ignoring NaN values, and keeps the dimensions of the original array. Repeats previous example.",
        "code": "jnp.nanmin(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Using the 'where' Parameter",
    "concepts": [
      "Demonstrates using the 'where' parameter to include only specific elements in the minimum computation.",
      "Illustrates how to use a boolean array with the same dimensions as the input array to select elements.",
      "Shows how to use a broadcast compatible boolean array to select elements.",
      "Explains that initial must be specified when where is used."
    ],
    "code_examples": [
      {
        "description": "Calculates the minimum value along axis 1 (rows) of a 2D JAX array, ignoring NaN values, only considering elements where the corresponding 'where' array is True. An initial value must be given when using the 'where' parameter.",
        "code": "where = jnp.array([[0, 0, 1, 0],\n                   [0, 0, 1, 1],\n                   [1, 1, 1, 0]], dtype=bool)\njnp.nanmin(x, axis=1, keepdims=True, initial=0, where=where)"
      },
      {
        "description": "Calculates the minimum value along axis 1 (rows) of a 2D JAX array, ignoring NaN values, only considering elements where the corresponding 'where' array is True. An initial value must be given when using the 'where' parameter. Repeats previous example.",
        "code": "where = jnp.array([[0, 0, 1, 0],\n                   [0, 0, 1, 1],\n                   [1, 1, 1, 0]], dtype=bool)\njnp.nanmin(x, axis=1, keepdims=True, initial=0, where=where)"
      },
      {
        "description": "Calculates the minimum value along axis 0 (columns) of a 2D JAX array, ignoring NaN values, only considering elements where the corresponding 'where' array is True. The 'where' array is broadcast compatible. An initial value must be given when using the 'where' parameter.",
        "code": "where = jnp.array([[False],\n                   [True],\n                   [False]])\njnp.nanmin(x, axis=0, keepdims=True, initial=0, where=where)"
      },
      {
        "description": "Calculates the minimum value along axis 0 (columns) of a 2D JAX array, ignoring NaN values, only considering elements where the corresponding 'where' array is True. The 'where' array is broadcast compatible. An initial value must be given when using the 'where' parameter. Repeats previous example.",
        "code": "where = jnp.array([[False],\n                   [True],\n                   [False]])\njnp.nanmin(x, axis=0, keepdims=True, initial=0, where=where)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.nanpercentile",
    "concepts": [
      "Computes the percentile of data along a specified axis, ignoring NaN values.",
      "JAX implementation of numpy.nanpercentile().",
      "Input array (a), quantiles (q), axis, interpolation method, and keepdims can be specified.",
      "It returns an array containing the specified percentiles along the specified axes."
    ],
    "code_examples": []
  },
  {
    "title": "Comparison with jax.numpy.percentile",
    "concepts": [
      "jax.numpy.nanpercentile() ignores NaN values, while jax.numpy.percentile() does not.",
      "jax.numpy.nanquantile() computes the nan-aware quantile (0.0-1.0)."
    ],
    "code_examples": [
      {
        "description": "Computing the median and quartiles of a 1D array using jax.numpy.nanpercentile() and comparing it to jax.numpy.percentile() when NaN values are present.",
        "code": "x = jnp.array([0, 1, 2, jnp.nan, 3, 4, 5, 6])\nq = jnp.array([25, 50, 75])\n\nprint(jnp.percentile(x, q))\nprint(jnp.nanpercentile(x, q))"
      },
      {
        "description": "Computing the median and quartiles of a 1D array using jax.numpy.nanpercentile() and comparing it to jax.numpy.percentile() when NaN values are present.",
        "code": "x = jnp.array([0, 1, 2, jnp.nan, 3, 4, 5, 6])\nq = jnp.array([25, 50, 75])\nprint(jnp.percentile(x, q))\nprint(jnp.nanpercentile(x, q))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.nanprod",
    "concepts": [
      "Calculates the product of array elements along a specified axis, ignoring NaN values.",
      "It is a JAX implementation of numpy.nanprod().",
      "If all elements along the given axis are NaNs, it returns 1."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.nanprod",
    "concepts": [
      "By default, jnp.nanprod computes the product of elements along the flattened array.",
      "NaN values are ignored during the product calculation."
    ],
    "code_examples": [
      {
        "description": "Calculates the product of all elements in the array, ignoring NaN values.",
        "code": "import jax.numpy as jnp\n\nnan = jnp.nan\nx = jnp.array([[nan, 3, 4, nan],\n               [5, nan, 1, 3],\n               [2, 1, nan, 1]])\n\njnp.nanprod(x)"
      }
    ]
  },
  {
    "title": "Specifying the Axis",
    "concepts": [
      "The `axis` parameter specifies the axis along which the product is computed.",
      "Using `axis=1` calculates the product along each row."
    ],
    "code_examples": [
      {
        "description": "Calculates the product of elements along axis 1 (rows), ignoring NaN values.",
        "code": "import jax.numpy as jnp\n\nnan = jnp.nan\nx = jnp.array([[nan, 3, 4, nan],\n               [5, nan, 1, 3],\n               [2, 1, nan, 1]])\n\njnp.nanprod(x, axis=1)"
      }
    ]
  },
  {
    "title": "Keeping Dimensions",
    "concepts": [
      "The `keepdims` parameter preserves the dimensions of the input array in the output.",
      "Setting `keepdims=True` ensures the output has the same number of dimensions as the input."
    ],
    "code_examples": [
      {
        "description": "Calculates the product along axis 1, keeping the dimensions of the input array.",
        "code": "import jax.numpy as jnp\n\nnan = jnp.nan\nx = jnp.array([[nan, 3, 4, nan],\n               [5, nan, 1, 3],\n               [2, 1, nan, 1]])\n\njnp.nanprod(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Using the `where` Parameter",
    "concepts": [
      "The `where` parameter allows specifying which elements to include in the product calculation.",
      "It takes a boolean array that is broadcast-compatible with the input array.",
      "If `where` is False for all elements along a given axis, jnp.nanprod returns 1."
    ],
    "code_examples": [
      {
        "description": "Calculates the product along axis 1, including only elements where the corresponding `where` array element is True.",
        "code": "import jax.numpy as jnp\n\nnan = jnp.nan\nx = jnp.array([[nan, 3, 4, nan],\n               [5, nan, 1, 3],\n               [2, 1, nan, 1]])\n\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 0, 1, 1],\n                   [1, 1, 1, 0]], dtype=bool)\n\njnp.nanprod(x, axis=1, keepdims=True, where=where)"
      },
      {
        "description": "Demonstrates the behavior of nanprod when 'where' is False for all elements along a given axis.",
        "code": "import jax.numpy as jnp\n\nnan = jnp.nan\nx = jnp.array([[nan, 3, 4, nan],\n               [5, nan, 1, 3],\n               [2, 1, nan, 1]])\n\nwhere = jnp.array([[False],\n                   [False],\n                   [False]])\n\njnp.nanprod(x, axis=0, keepdims=True, where=where)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.nanquantile",
    "concepts": [
      "Computes quantiles of data along a specified axis, ignoring NaNs.",
      "JAX implementation of numpy.nanquantile().",
      "q should contain floating-point values between 0.0 and 1.0.",
      "The 'out' and 'overwrite_input' arguments are not implemented by JAX and will raise errors if used.",
      "The 'method' argument specifies the interpolation method (linear, lower, higher, midpoint, nearest).",
      "The 'interpolation' argument is a deprecated alias of the 'method' argument."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of nanquantile",
    "concepts": [
      "Demonstrates calculating the median and quartiles of a 1D array with NaN values.",
      "Compares the output of jnp.quantile (which returns NaN values) with jnp.nanquantile (which ignores NaN values).",
      "Illustrates how nanquantile can provide meaningful quantile values even when the input array contains NaNs."
    ],
    "code_examples": [
      {
        "description": "Computing the median and quartiles of a 1D array using jnp.nanquantile and comparing it with jnp.quantile.",
        "code": "x = jnp.array([0, 1, 2, jnp.nan, 3, 4, 5, 6])\nq = jnp.array([0.25, 0.5, 0.75])\n\nprint(jnp.quantile(x, q))\nprint(jnp.nanquantile(x, q))\n\nx = jnp.array([0, 1, 2, jnp.nan, 3, 4, 5, 6])\nq = jnp.array([0.25, 0.5, 0.75])\n\nprint(jnp.quantile(x, q))\nprint(jnp.nanquantile(x, q))"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.nansum",
    "concepts": [
      "Calculates the sum of array elements along a specified axis, while ignoring NaN values.",
      "It is a JAX implementation of NumPy's nansum function.",
      "If all elements along the given axis are NaNs, the function returns 0."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage",
    "concepts": [
      "Demonstrates the basic usage of jnp.nansum to compute the sum of all elements in an array, ignoring NaNs."
    ],
    "code_examples": [
      {
        "description": "Compute the sum of elements along the flattened array, ignoring NaN values.",
        "code": "nan = jnp.nan\nx = jnp.array([[3, nan, 4, 5],\n               [nan, -2, nan, 7],\n               [2, 1, 6, nan]])\njnp.nansum(x)"
      }
    ]
  },
  {
    "title": "Specifying Axis",
    "concepts": [
      "Shows how to specify the axis along which the sum is computed."
    ],
    "code_examples": [
      {
        "description": "Compute the sum along axis 1, ignoring NaN values.",
        "code": "nan = jnp.nan\nx = jnp.array([[3, nan, 4, 5],\n               [nan, -2, nan, 7],\n               [2, 1, 6, nan]])\njnp.nansum(x, axis=1)"
      }
    ]
  },
  {
    "title": "Using keepdims",
    "concepts": [
      "Explains the usage of the keepdims parameter to maintain the original number of dimensions in the output."
    ],
    "code_examples": [
      {
        "description": "Compute the sum along axis 1, keeping the dimensions of the original array.",
        "code": "nan = jnp.nan\nx = jnp.array([[3, nan, 4, 5],\n               [nan, -2, nan, 7],\n               [2, 1, 6, nan]])\njnp.nansum(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Using the where Parameter",
    "concepts": [
      "Demonstrates how to use the where parameter to include only specific elements in the sum."
    ],
    "code_examples": [
      {
        "description": "Compute the sum along axis 1, including only elements where the corresponding value in the 'where' array is True.",
        "code": "nan = jnp.nan\nx = jnp.array([[3, nan, 4, 5],\n               [nan, -2, nan, 7],\n               [2, 1, 6, nan]])\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 0, 1, 1],\n                   [1, 1, 1, 0]], dtype=bool)\njnp.nansum(x, axis=1, keepdims=True, where=where)"
      }
    ]
  },
  {
    "title": "Edge Case: where is all False",
    "concepts": [
      "Explains the behavior when the 'where' array is all False, resulting in a sum of 0 along the specified axis."
    ],
    "code_examples": [
      {
        "description": "Illustrates the case where the 'where' array is all False, resulting in a sum of 0.",
        "code": "nan = jnp.nan\nx = jnp.array([[3, nan, 4, 5],\n               [nan, -2, nan, 7],\n               [2, 1, 6, nan]])\nwhere = jnp.array([[False],\n                   [False],\n                   [False]])\njnp.nansum(x, axis=0, keepdims=True, where=where)"
      }
    ]
  },
  {
    "title": "Alias of Array",
    "concepts": [
      "The document defines an alias for the Array type."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jnp.ndim",
    "concepts": [
      "jnp.ndim returns the number of dimensions of a JAX array.",
      "It raises a TypeError if the input is a list or tuple, unlike NumPy's np.ndim.",
      "It accepts an array-like object as input.",
      "It returns an integer representing the number of dimensions."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.ndim with Arrays",
    "concepts": [
      "Demonstrates how to use jnp.ndim with JAX arrays of different dimensions.",
      "Shows how to determine the number of dimensions for a one-dimensional and a two-dimensional array."
    ],
    "code_examples": [
      {
        "description": "Demonstrates finding the number of dimensions of a 1D JAX array.",
        "code": "x = jnp.arange(10)\njnp.ndim(x)"
      },
      {
        "description": "Demonstrates finding the number of dimensions of a 2D JAX array.",
        "code": "y = jnp.ones((2, 3))\njnp.ndim(y)"
      },
      {
        "description": "Demonstrates finding the number of dimensions of a 1D JAX array.",
        "code": "x = jnp.arange(10)\njnp.ndim(x)"
      },
      {
        "description": "Demonstrates finding the number of dimensions of a 2D JAX array.",
        "code": "y = jnp.ones((2, 3))\njnp.ndim(y)"
      }
    ]
  },
  {
    "title": "jnp.ndim with Scalars",
    "concepts": [
      "Illustrates how jnp.ndim works with scalar values.",
      "Scalars are treated as zero-dimensional arrays."
    ],
    "code_examples": [
      {
        "description": "Demonstrates that a scalar has 0 dimensions.",
        "code": "jnp.ndim(3.14)"
      },
      {
        "description": "Demonstrates that a scalar has 0 dimensions.",
        "code": "jnp.ndim(3.14)"
      }
    ]
  },
  {
    "title": "Accessing Dimensions using the ndim Property",
    "concepts": [
      "Shows how to access the number of dimensions using the .ndim property of a JAX array."
    ],
    "code_examples": [
      {
        "description": "Accessing the number of dimensions using the ndim property.",
        "code": "x.ndim"
      },
      {
        "description": "Accessing the number of dimensions using the ndim property.",
        "code": "x.ndim"
      }
    ]
  },
  {
    "title": "Introduction to jnp.negative",
    "concepts": [
      "The function returns the element-wise negative value of the input.",
      "It is a JAX implementation of numpy.negative.",
      "The output array has the same shape and dtype as the input.",
      "The function behaves differently for unsigned integers due to two's complement negation, potentially resulting in unexpected large positive values."
    ],
    "code_examples": []
  },
  {
    "title": "Real-valued input examples",
    "concepts": [
      "Demonstrates the usage of jnp.negative with real-valued JAX arrays.",
      "The output is the negation of each element in the input array."
    ],
    "code_examples": [
      {
        "description": "Example of using jnp.negative with a real-valued JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([0., -3., 7])\nprint(jnp.negative(x))"
      },
      {
        "description": "Another example of using jnp.negative with a real-valued JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([0., -3., 7])\nprint(jnp.negative(x))"
      }
    ]
  },
  {
    "title": "Complex-valued input examples",
    "concepts": [
      "Demonstrates the usage of jnp.negative with complex-valued JAX arrays.",
      "The output is the negation of each complex number in the input array."
    ],
    "code_examples": [
      {
        "description": "Example of using jnp.negative with a complex-valued JAX array.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([1-2j, -3+4j, 5-6j])\nprint(jnp.negative(x1))"
      },
      {
        "description": "Another example of using jnp.negative with a complex-valued JAX array.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([1-2j, -3+4j, 5-6j])\nprint(jnp.negative(x1))"
      }
    ]
  },
  {
    "title": "Unsigned Integer Input Example",
    "concepts": [
      "Illustrates the behavior of jnp.negative when applied to unsigned integer arrays.",
      "Due to two's complement, the negation of an unsigned integer may result in a large positive integer.",
      "Demonstrates how negative numbers are represented in uint32."
    ],
    "code_examples": [
      {
        "description": "Example showing the effect of jnp.negative on a uint32 array.",
        "code": "import jax.numpy as jnp\n\nx2 = jnp.array([5, 0, -7]).astype(jnp.uint32)\nprint(x2)\nprint(jnp.negative(x2))"
      },
      {
        "description": "Another example showing the effect of jnp.negative on a uint32 array.",
        "code": "import jax.numpy as jnp\n\nx2 = jnp.array([5, 0, -7]).astype(jnp.uint32)\nprint(x2)\nprint(jnp.negative(x2))"
      }
    ]
  },
  {
    "title": "JAX Implementation of nextafter",
    "concepts": [
      "Calculates the next floating-point value of x towards y element-wise.",
      "It is a JAX implementation of numpy.nextafter.",
      "x and y can be scalars or arrays.",
      "x and y should have the same shape or be broadcast compatible.",
      "Returns an array containing the next representable number of x in the direction of y."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of jnp.nextafter with scalar inputs.",
        "code": ">>> jnp.nextafter(2, 1)\nArray(1.9999999, dtype=float32, weak_type=True)"
      },
      {
        "description": "Demonstrates the use of jnp.nextafter with array inputs.",
        "code": ">>> x = jnp.array([3, -2, 1])\n>>> y = jnp.array([2, -1, 2])\n>>> jnp.nextafter(x, y)\nArray([ 2.9999998, -1.9999999,  1.0000001], dtype=float32)"
      },
      {
        "description": "Demonstrates the use of jnp.nextafter with scalar inputs.",
        "code": ">>> jnp.nextafter(2, 1)\nArray(1.9999999, dtype=float32, weak_type=True)"
      },
      {
        "description": "Demonstrates the use of jnp.nextafter with array inputs.",
        "code": ">>> x = jnp.array([3, -2, 1])\n>>> y = jnp.array([2, -1, 2])\n>>> jnp.nextafter(x, y)\nArray([ 2.9999998, -1.9999999,  1.0000001], dtype=float32)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.nonzero",
    "concepts": [
      "The function returns indices of nonzero elements of an array.",
      "It is a JAX implementation of numpy.nonzero().",
      "The size of the output is data-dependent, making it incompatible with JIT and other transformations without specifying the 'size' argument.",
      "The 'size' argument must be specified statically for use within JAX's transformations.",
      "The function takes an array 'a' and an optional 'size' argument.",
      "It also takes an optional 'fill_value' argument that specifies the value to use when padding the indices.",
      "The function returns a tuple of JAX Arrays of length a.ndim, containing the indices of each nonzero value."
    ],
    "code_examples": []
  },
  {
    "title": "One-Dimensional Array Example",
    "concepts": [
      "Demonstrates the usage of jnp.nonzero() with a one-dimensional array.",
      "The function returns a length-1 tuple of indices corresponding to the nonzero elements."
    ],
    "code_examples": [
      {
        "description": "Finds the indices of non-zero elements in a 1D JAX array.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\njnp.nonzero(x)"
      },
      {
        "description": "Finds the indices of non-zero elements in a 1D JAX array.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\njnp.nonzero(x)"
      }
    ]
  },
  {
    "title": "Two-Dimensional Array Example",
    "concepts": [
      "Demonstrates the usage of jnp.nonzero() with a two-dimensional array.",
      "The function returns a length-2 tuple of arrays, each containing the row and column indices of the nonzero elements."
    ],
    "code_examples": [
      {
        "description": "Finds the indices of non-zero elements in a 2D JAX array.",
        "code": "x = jnp.array([[0, 5, 0],\n              [6, 0, 7]])\njnp.nonzero(x)"
      },
      {
        "description": "Finds the indices of non-zero elements in a 2D JAX array.",
        "code": "x = jnp.array([[0, 5, 0],\n              [6, 0, 7]])\njnp.nonzero(x)"
      }
    ]
  },
  {
    "title": "Extracting Nonzero Values Using Indices",
    "concepts": [
      "Demonstrates how to use the returned indices to extract the nonzero values from the original array."
    ],
    "code_examples": [
      {
        "description": "Extracts the non-zero values from a JAX array using the indices returned by jnp.nonzero().",
        "code": "x = jnp.array([[0, 5, 0],\n              [6, 0, 7]])\nindices = jnp.nonzero(x)\nx[indices]"
      },
      {
        "description": "Extracts the non-zero values from a JAX array using the indices returned by jnp.nonzero().",
        "code": "x = jnp.array([[0, 5, 0],\n              [6, 0, 7]])\nindices = jnp.nonzero(x)\nx[indices]"
      }
    ]
  },
  {
    "title": "Incompatibility with JIT without static size",
    "concepts": [
      "Explains why jnp.nonzero() is incompatible with JIT compilation without specifying a static size.",
      "The output shape of nonzero is dynamic, depending on the content of the input array.",
      "Specifying the 'size' argument statically resolves this issue."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the error that occurs when using jnp.nonzero() with jax.jit without specifying the 'size' argument.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\njax.jit(jnp.nonzero)(x)"
      },
      {
        "description": "Demonstrates the error that occurs when using jnp.nonzero() with jax.jit without specifying the 'size' argument.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\njax.jit(jnp.nonzero)(x)"
      }
    ]
  },
  {
    "title": "Using static_argnames to specify size for JIT",
    "concepts": [
      "Shows how to use the static_argnames parameter in jax.jit to specify that the 'size' argument should be treated as static.",
      "This allows jnp.nonzero() to be used within JAX transformations."
    ],
    "code_examples": [
      {
        "description": "Compiles jnp.nonzero() with jax.jit, specifying that the 'size' argument is static.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\nnonzero_jit = jax.jit(jnp.nonzero, static_argnames='size')\nnonzero_jit(x, size=3)"
      },
      {
        "description": "Compiles jnp.nonzero() with jax.jit, specifying that the 'size' argument is static.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\nnonzero_jit = jax.jit(jnp.nonzero, static_argnames='size')\nnonzero_jit(x, size=3)"
      }
    ]
  },
  {
    "title": "Truncating and Padding with size",
    "concepts": [
      "Illustrates what happens when the specified 'size' argument does not match the true number of nonzero elements.",
      "If size is less than the number of nonzero elements, the result is truncated.",
      "If size is greater than the number of nonzero elements, the result is padded with zeros by default."
    ],
    "code_examples": [
      {
        "description": "Demonstrates truncation of indices when size is smaller than the actual number of non-zero elements.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\nnonzero_jit = jax.jit(jnp.nonzero, static_argnames='size')\nnonzero_jit(x, size=2)"
      },
      {
        "description": "Demonstrates padding of indices with zeros when size is larger than the actual number of non-zero elements.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\nnonzero_jit = jax.jit(jnp.nonzero, static_argnames='size')\nnonzero_jit(x, size=5)"
      },
      {
        "description": "Demonstrates truncation of indices when size is smaller than the actual number of non-zero elements.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\nnonzero_jit = jax.jit(jnp.nonzero, static_argnames='size')\nnonzero_jit(x, size=2)"
      },
      {
        "description": "Demonstrates padding of indices with zeros when size is larger than the actual number of non-zero elements.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\nnonzero_jit = jax.jit(jnp.nonzero, static_argnames='size')\nnonzero_jit(x, size=5)"
      }
    ]
  },
  {
    "title": "Custom Fill Value for Padding",
    "concepts": [
      "Explains how to specify a custom fill value for padding using the 'fill_value' argument.",
      "This allows you to pad the indices with values other than zero."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to specify a custom fill value when padding the indices.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\nnonzero_jit = jax.jit(jnp.nonzero, static_argnames='size')\nnonzero_jit(x, size=5, fill_value=len(x))"
      },
      {
        "description": "Demonstrates how to specify a custom fill value when padding the indices.",
        "code": "x = jnp.array([0, 5, 0, 6, 0, 7])\nnonzero_jit = jax.jit(jnp.nonzero, static_argnames='size')\nnonzero_jit(x, size=5, fill_value=len(x))"
      }
    ]
  },
  {
    "title": "JAX not_equal Function",
    "concepts": [
      "The function returns element-wise truth value of x != y.",
      "It is a JAX implementation of numpy.not_equal.",
      "It accepts two inputs, x and y, which can be arrays or scalars.",
      "x and y must have the same shape or be broadcast compatible.",
      "The function returns a boolean array indicating where elements are not equal.",
      "It implements the != operator for JAX arrays.",
      "The return array contains True where x != y and False otherwise."
    ],
    "code_examples": [
      {
        "description": "Demonstrates basic usage of jnp.not_equal with scalar inputs.",
        "code": "jnp.not_equal(0., -0.)\njnp.not_equal(-2, 2)\njnp.not_equal(1, 1.)\njnp.not_equal(5, jnp.array(5))"
      },
      {
        "description": "Demonstrates broadcasting with jnp.not_equal and array inputs.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\ny = jnp.array([1, 5, 9])\njnp.not_equal(x, y)\nx != y"
      },
      {
        "description": "Demonstrates basic usage of jnp.not_equal with scalar inputs.",
        "code": "jnp.not_equal(0., -0.)\njnp.not_equal(-2, 2)\njnp.not_equal(1, 1.)\njnp.not_equal(5, jnp.array(5))"
      },
      {
        "description": "Demonstrates broadcasting with jnp.not_equal and array inputs.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\ny = jnp.array([1, 5, 9])\njnp.not_equal(x, y)\nx != y"
      }
    ]
  },
  {
    "title": "Overview",
    "concepts": [
      "Describes an abstract base class for numeric scalar types.",
      "Lists available methods and attributes."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "The document lists various methods available for numeric scalar types.",
      "Most methods are scalar equivalents of array attributes."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "The document lists attributes available for numeric scalar types.",
      "Many attributes are scalar equivalents of array attributes.",
      "Attributes include data, dtype, flags, and shape."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of a Python Object",
    "concepts": [
      "The object is a Python object.",
      "The object has methods.",
      "The object has attributes."
    ],
    "code_examples": []
  },
  {
    "title": "Methods of the Object",
    "concepts": [
      "The object has several methods that are scalar methods identical to corresponding array attributes.",
      "The methods include operations such as all, any, argmax, argmin, argsort, astype, byteswap, choose, clip, compress, conj/conjugate, copy, cumprod, cumsum, diagonal, dump, dumps, fill, flatten, getfield, item, max, mean, min, nonzero, prod, put, ravel, repeat, reshape, resize, round, searchsorted, setfield, setflags, sort, squeeze, std, sum, swapaxes, take, to_device, tobytes, tofile, tolist, tostring, trace, transpose, var, and view."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes of the Object",
    "concepts": [
      "The object has attributes.",
      "Some attributes are scalar attributes identical to the corresponding array attribute, such as T and base.",
      "Other attributes include data (pointer to start of data), device, dtype (array data-descriptor), flags (integer value of flags), flat (1-D view of the scalar), imag (imaginary part), itemset, itemsize (length of one element in bytes), nbytes, ndim (number of array dimensions), newbyteorder, ptp, real (real part), shape (tuple of array dimensions), size (number of elements), and strides (tuple of bytes steps in each dimension)."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jnp.ogrid",
    "concepts": [
      "jnp.ogrid returns an open multi-dimensional meshgrid.",
      "It is a LAX-backend implementation of numpy.ogrid.",
      "It is a convenience wrapper for jax.numpy.meshgrid() with sparse=True.",
      "jnp.ogrid is a sparse version of jnp.mgrid."
    ],
    "code_examples": []
  },
  {
    "title": "Examples using jnp.ogrid with arange-like behavior",
    "concepts": [
      "Using [start:stop:step] generates values similar to jax.numpy.arange().",
      "Integer step values are used."
    ],
    "code_examples": [
      {
        "description": "Example showcasing arange-like behavior with integer step.",
        "code": "jnp.ogrid[0:4:1]"
      },
      {
        "description": "Redundant example showcasing arange-like behavior with integer step.",
        "code": "jnp.ogrid[0:4:1]"
      }
    ]
  },
  {
    "title": "Examples using jnp.ogrid with linspace-like behavior",
    "concepts": [
      "Using an imaginary step generates values similar to jax.numpy.linspace()."
    ],
    "code_examples": [
      {
        "description": "Example showcasing linspace-like behavior with imaginary step.",
        "code": "jnp.ogrid[0:1:4j]"
      },
      {
        "description": "Redundant example showcasing linspace-like behavior with imaginary step.",
        "code": "jnp.ogrid[0:1:4j]"
      }
    ]
  },
  {
    "title": "Creating Sparse Grids of Indices",
    "concepts": [
      "Multiple slices can be used to create sparse grids of indices."
    ],
    "code_examples": [
      {
        "description": "Example of creating sparse grids of indices with multiple slices.",
        "code": "jnp.ogrid[:2, :3]"
      },
      {
        "description": "Redundant example of creating sparse grids of indices with multiple slices.",
        "code": "jnp.ogrid[:2, :3]"
      }
    ]
  },
  {
    "title": "Creating Arrays of Ones with JAX",
    "concepts": [
      "The function creates an array filled with ones.",
      "It is a JAX implementation of numpy.ones().",
      "The shape parameter defines the array's dimensions.",
      "The dtype parameter specifies the data type of the array.",
      "The device parameter allows specifying a device or sharding strategy."
    ],
    "code_examples": [
      {
        "description": "Creates a 1D JAX array of four ones with the default float32 dtype.",
        "code": "jnp.ones(4)"
      },
      {
        "description": "Creates a 2D JAX array (2x3) of boolean ones.",
        "code": "jnp.ones((2, 3), dtype=bool)"
      },
      {
        "description": "Creates a 1D JAX array of four ones with the default float32 dtype.",
        "code": "jnp.ones(4)"
      },
      {
        "description": "Creates a 2D JAX array (2x3) of boolean ones.",
        "code": "jnp.ones((2, 3), dtype=bool)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.ones_like()",
    "concepts": [
      "Creates an array of ones with the same shape and dtype as a given array.",
      "It is a JAX implementation of numpy.ones_like().",
      "The shape and dtype of the created array can be overridden.",
      "The device to which the created array will be committed can be specified."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jax.numpy.ones_like()",
    "concepts": [
      "Creating an array of ones with the same shape and dtype as an existing JAX array.",
      "Overriding the dtype of the new array to boolean.",
      "Overriding the shape of the new array while retaining the original dtype."
    ],
    "code_examples": [
      {
        "description": "Creating an array of ones with the same shape and dtype as x.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(4)\nprint(jnp.ones_like(x))"
      },
      {
        "description": "Creating an array of ones with the same shape as x, but with a boolean dtype.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(4)\nprint(jnp.ones_like(x, dtype=bool))"
      },
      {
        "description": "Creating an array of ones with a specified shape, using the dtype of x.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(4)\nprint(jnp.ones_like(x, shape=(2, 3)))"
      }
    ]
  },
  {
    "title": "Redundant Examples",
    "concepts": [
      "Demonstrates the same functionality as the previous example."
    ],
    "code_examples": [
      {
        "description": "Creating an array of ones with the same shape and dtype as x.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(4)\nprint(jnp.ones_like(x))"
      },
      {
        "description": "Creating an array of ones with the same shape as x, but with a boolean dtype.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(4)\nprint(jnp.ones_like(x, dtype=bool))"
      },
      {
        "description": "Creating an array of ones with a specified shape, using the dtype of x.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(4)\nprint(jnp.ones_like(x, shape=(2, 3)))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.outer",
    "concepts": [
      "Computes the outer product of two arrays, similar to numpy.outer().",
      "The function flattens the input arrays if they are not 1D.",
      "The output array has shape (a.size, b.size)."
    ],
    "code_examples": []
  },
  {
    "title": "jax.numpy.outer Examples",
    "concepts": [
      "Demonstrates how to compute the outer product of two 1D JAX arrays.",
      "Shows the resulting array with the outer product values."
    ],
    "code_examples": [
      {
        "description": "Computes the outer product of two jax arrays a and b.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([1, 2, 3])\nb = jnp.array([4, 5, 6])\n\njnp.outer(a, b)"
      },
      {
        "description": "Computes the outer product of two jax arrays a and b. (duplicate example)",
        "code": "import jax.numpy as jnp\n\na = jnp.array([1, 2, 3])\nb = jnp.array([4, 5, 6])\n\njnp.outer(a, b)"
      }
    ]
  },
  {
    "title": "Overview of jnp.packbits()",
    "concepts": [
      "The function packs an array of bits into a uint8 array.",
      "It is a JAX implementation of numpy.packbits().",
      "The input is an N-dimensional array of bits.",
      "It can pack along a specified axis.",
      "The bit order can be specified as big-endian or little-endian.",
      "The output is a uint8 array of packed values.",
      "jax.numpy.unpackbits() is the inverse of packbits().",
      "If the number of bits is not a multiple of 8, it will be right-padded with zeros."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.packbits()",
    "concepts": [
      "Demonstrates packing a 1D array of bits into a uint8 array.",
      "Shows the default big-endian bit order.",
      "Illustrates the bitwise representation of the packed value."
    ],
    "code_examples": [
      {
        "description": "Packing bits in one dimension with default big-endian order.",
        "code": "bits = jnp.array([0, 0, 0, 0, 0, 1, 1, 1])\njnp.packbits(bits)"
      }
    ]
  },
  {
    "title": "Specifying Little-Endian Bit Order",
    "concepts": [
      "Demonstrates packing a 1D array of bits using the little-endian bit order.",
      "Illustrates the difference in the packed value compared to big-endian."
    ],
    "code_examples": [
      {
        "description": "Packing bits in one dimension with little-endian order.",
        "code": "jnp.packbits(bits, bitorder=\"little\")"
      }
    ]
  },
  {
    "title": "Padding with Zeros",
    "concepts": [
      "Shows how jnp.packbits() handles arrays where the number of bits is not a multiple of 8.",
      "Demonstrates that the array is right-padded with zeros to form a complete byte."
    ],
    "code_examples": [
      {
        "description": "Packing an array with a length that's not a multiple of 8, showing the padding with zeros.",
        "code": "jnp.packbits(jnp.array([1, 0, 1]))\njnp.packbits(jnp.array([1, 0, 1, 0, 0, 0, 0, 0]))"
      }
    ]
  },
  {
    "title": "Packing Along a Specified Axis",
    "concepts": [
      "Illustrates how to pack bits along a specific axis in a multi-dimensional array.",
      "Shows the resulting array after packing along axis=1."
    ],
    "code_examples": [
      {
        "description": "Packing bits along axis 1 of a 2D array.",
        "code": "a = jnp.array([[1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0],\n               [0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1]])\nvals = jnp.packbits(a, axis=1)\nvals"
      }
    ]
  },
  {
    "title": "Relation to jnp.unpackbits()",
    "concepts": [
      "Demonstrates the inverse relationship between jnp.packbits() and jnp.unpackbits().",
      "Shows how unpacking the packed array recovers the original bit array."
    ],
    "code_examples": [
      {
        "description": "Unpacking the previously packed array to recover the original bits.",
        "code": "jnp.unpackbits(vals, axis=1)"
      }
    ]
  },
  {
    "title": "Introduction to JAX Padding",
    "concepts": [
      "JAX implements array padding similar to NumPy's pad() function.",
      "It allows adding padding to arrays with various modes and configurations.",
      "The function name is jax.numpy.pad()."
    ],
    "code_examples": []
  },
  {
    "title": "Padding Width Specifications",
    "concepts": [
      "pad_width specifies the amount of padding for each dimension of the array.",
      "Padding can be symmetric or asymmetric before and after each dimension.",
      "Different formats for pad_width include an integer, a tuple of two integers (before, after), or a tuple of tuples for each dimension."
    ],
    "code_examples": []
  },
  {
    "title": "Padding Modes",
    "concepts": [
      "The mode parameter controls the padding method.",
      "Supported modes include 'constant', 'empty', 'edge', 'wrap', 'linear_ramp', 'maximum', 'mean', 'median', 'minimum', 'reflect', and 'symmetric'.",
      "A custom callable function can also be used as the padding mode."
    ],
    "code_examples": []
  },
  {
    "title": "Custom Padding Function Signature",
    "concepts": [
      "When using a callable as padding mode, the function must follow a specific signature.",
      "The function receives a 1D slice of the array, padding widths, the axis index, and keyword arguments.",
      "Unlike NumPy, the custom function must return the modified row instead of modifying it in-place.",
      "The function is mapped across the padded axis using jax.vmap()."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Padding Examples",
    "concepts": [
      "Padding a 1-dimensional array with zeros using different pad widths.",
      "Demonstrates padding with a single integer value for symmetric padding.",
      "Demonstrates padding with a tuple of two integers for asymmetric padding."
    ],
    "code_examples": [
      {
        "description": "Pad a 1-dimensional array with zeros symmetrically.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([10, 20, 30, 40])\njnp.pad(x, 2)"
      },
      {
        "description": "Pad a 1-dimensional array with zeros asymmetrically.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([10, 20, 30, 40])\njnp.pad(x, (2, 4))"
      }
    ]
  },
  {
    "title": "Padding with Constant Values",
    "concepts": [
      "Padding with a constant value using the constant_values parameter.",
      "This allows filling the padded region with a specific value instead of the default zero."
    ],
    "code_examples": [
      {
        "description": "Pad a 1-dimensional array with a constant value of 99.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([10, 20, 30, 40])\njnp.pad(x, 2, constant_values=99)"
      }
    ]
  },
  {
    "title": "Padding with Statistical Modes",
    "concepts": [
      "Padding with statistical modes such as 'mean'.",
      "The padded region is filled with the mean value of the original array."
    ],
    "code_examples": [
      {
        "description": "Pad a 1-dimensional array with the mean value.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([10, 20, 30, 40])\njnp.pad(x, 2, mode='mean')"
      }
    ]
  },
  {
    "title": "Padding with Reflective Modes",
    "concepts": [
      "Padding using the 'reflect' mode, which reflects the array values in the padded region."
    ],
    "code_examples": [
      {
        "description": "Pad a 1-dimensional array with reflected values.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([10, 20, 30, 40])\njnp.pad(x, 2, mode='reflect')"
      }
    ]
  },
  {
    "title": "Padding Multi-Dimensional Arrays",
    "concepts": [
      "Demonstrates padding a 2-dimensional array with different padding widths for each dimension.",
      "The pad_width parameter is a tuple of tuples, where each inner tuple specifies the padding before and after for a specific dimension."
    ],
    "code_examples": [
      {
        "description": "Pad a 2-dimensional array with different paddings in each dimension.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\njnp.pad(x, ((1, 2), (3, 0)))"
      }
    ]
  },
  {
    "title": "Custom Padding Function Example",
    "concepts": [
      "Shows how to use a custom padding function to define the padding logic.",
      "The example sets the values of the padded region to specific values passed as keyword arguments."
    ],
    "code_examples": [
      {
        "description": "Pad a 1-dimensional array with a custom padding function that sets before and after values.",
        "code": "import jax.numpy as jnp\n\ndef custom_pad(row, pad_width, iaxis, kwargs):\n    # row represents a 1D slice of the zero-padded array.\n    before, after = pad_width\n    before_value = kwargs.get('before_value', 0)\n    after_value = kwargs.get('after_value', 0)\n    row = row.at[:before].set(before_value)\n    return row.at[len(row)-after:].set(after_value)\n\nx = jnp.array([2, 3, 4])\njnp.pad(x, 2, custom_pad, before_value=-10, after_value=10)"
      }
    ]
  },
  {
    "title": "JAX Partition Function",
    "concepts": [
      "Returns a partially-sorted copy of an array.",
      "JAX implementation of numpy.partition().",
      "NaNs with the negative bit set are sorted to the beginning of the array.",
      "Requires the kth argument to be a static integer.",
      "Implemented via two calls to jax.lax.top_k().",
      "Entries before kth are values smaller than take(a, kth, axis), and entries after kth are indices of values larger than take(a, kth, axis)."
    ],
    "code_examples": []
  },
  {
    "title": "Partitioning an Array",
    "concepts": [
      "Demonstrates the usage of jnp.partition to partially sort an array around a kth element.",
      "Shows how to extract the smallest values, pivot value, and largest values after partitioning."
    ],
    "code_examples": [
      {
        "description": "Partition the array x around the 4th element.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([6, 8, 4, 3, 1, 9, 7, 5, 2, 3])\nkth = 4\nx_partitioned = jnp.partition(x, kth)\n\nprint(x_partitioned)"
      },
      {
        "description": "Extract the smallest values, pivot value, and largest values after partitioning.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([6, 8, 4, 3, 1, 9, 7, 5, 2, 3])\nkth = 4\nx_partitioned = jnp.partition(x, kth)\n\nsmallest_values = x_partitioned[:kth]\npivot_value = x_partitioned[kth]\nlargest_values = x_partitioned[kth + 1:]\nprint(smallest_values, pivot_value, largest_values)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.percentile",
    "concepts": [
      "Computes the percentile of data along a specified axis using JAX.",
      "It is a JAX implementation of numpy.percentile().",
      "Input array 'a' can be N-dimensional.",
      "Quantiles 'q' can be a scalar or 1D array between 0 and 100.",
      "Axis can be specified for computation.",
      "Interpolation methods include 'linear', 'lower', 'higher', 'midpoint', and 'nearest'.",
      "'out' and 'overwrite_input' parameters are not implemented and will error if used.",
      "keepdims parameter can maintain the input array's dimensions."
    ],
    "code_examples": []
  },
  {
    "title": "Computing Percentiles of a 1D Array",
    "concepts": [
      "Demonstrates computing the median and quartiles of a 1D JAX array using jnp.percentile().",
      "The example calculates the 25th, 50th, and 75th percentiles."
    ],
    "code_examples": [
      {
        "description": "Computes the 25th, 50th, and 75th percentiles of a 1D JAX array 'x'.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([0, 1, 2, 3, 4, 5, 6])\nq = jnp.array([25, 50, 75])\njnp.percentile(x, q)"
      },
      {
        "description": "Computes the 25th, 50th, and 75th percentiles of a 1D JAX array 'x'.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([0, 1, 2, 3, 4, 5, 6])\nq = jnp.array([25, 50, 75])\njnp.percentile(x, q)"
      }
    ]
  },
  {
    "title": "Using Nearest Interpolation",
    "concepts": [
      "Shows how to use the 'nearest' interpolation method when computing percentiles.",
      "The 'method' argument is set to 'nearest'."
    ],
    "code_examples": [
      {
        "description": "Computes percentiles using nearest neighbor interpolation.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([0, 1, 2, 3, 4, 5, 6])\nq = jnp.array([25, 50, 75])\njnp.percentile(x, q, method='nearest')"
      },
      {
        "description": "Computes percentiles using nearest neighbor interpolation.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([0, 1, 2, 3, 4, 5, 6])\nq = jnp.array([25, 50, 75])\njnp.percentile(x, q, method='nearest')"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.permute_dims",
    "concepts": [
      "The function `jax.numpy.permute_dims` permutes the axes/dimensions of an array.",
      "It is a JAX implementation of the array_api.permute_dims() function.",
      "The `axes` argument specifies the order in which the axes should be permuted.",
      "The function returns a copy of the input array with permuted axes.",
      "Related functions are `jax.numpy.transpose()` and `jax.numpy.matrix_transpose()`."
    ],
    "code_examples": [
      {
        "description": "Permute the axes of a 2D array using jnp.permute_dims.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\n\njnp.permute_dims(a, (1, 0))"
      },
      {
        "description": "Permute the axes of a 2D array using jnp.permute_dims.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\n\njnp.permute_dims(a, (1, 0))"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.piecewise",
    "concepts": [
      "Evaluates a function defined piecewise across the domain.",
      "JAX implementation of numpy.piecewise() using jax.lax.switch().",
      "Requires functions in funclist to be traceable by JAX.",
      "The length of each array in condlist must match the length of x.",
      "funclist can have a length of len(condlist) or len(condlist) + 1.",
      "Entries of funclist may be numerical values, indicating a constant function.",
      "Additional arguments can be passed to functions in funclist using args and kwargs."
    ],
    "code_examples": []
  },
  {
    "title": "Piecewise function with zero for negative values and linear for positive values",
    "concepts": [
      "Demonstrates a piecewise function that returns 0 for negative values and x for positive values.",
      "Uses lambda functions to define the piecewise functions."
    ],
    "code_examples": [
      {
        "description": "Define a piecewise function that is 0 for negative values and x for positive values using jnp.piecewise.",
        "code": "x = jnp.array([-4, -3, -2, -1, 0, 1, 2, 3, 4])\ncondlist = [x < 0, x >= 0]\nfunclist = [lambda x: 0 * x, lambda x: x]\njnp.piecewise(x, condlist, funclist)"
      },
      {
        "description": "Define a piecewise function that is 0 for negative values and x for positive values using jnp.piecewise.",
        "code": "x = jnp.array([-4, -3, -2, -1, 0, 1, 2, 3, 4])\ncondlist = [x < 0, x >= 0]\nfunclist = [lambda x: 0 * x, lambda x: x]\njnp.piecewise(x, condlist, funclist)"
      }
    ]
  },
  {
    "title": "Piecewise function with a scalar value",
    "concepts": [
      "funclist can contain scalar values for constant functions.",
      "Demonstrates using a scalar value (0) for the first function in funclist."
    ],
    "code_examples": [
      {
        "description": "Define a piecewise function that is 0 for negative values and x for positive values, using a scalar value 0 instead of a lambda function for the first condition.",
        "code": "condlist = [x < 0, x >= 0]\nfunclist = [0, lambda x: x]\njnp.piecewise(x, condlist, funclist)"
      },
      {
        "description": "Define a piecewise function that is 0 for negative values and x for positive values, using a scalar value 0 instead of a lambda function for the first condition.",
        "code": "condlist = [x < 0, x >= 0]\nfunclist = [0, lambda x: x]\njnp.piecewise(x, condlist, funclist)"
      }
    ]
  },
  {
    "title": "Piecewise function with a default value",
    "concepts": [
      "Specifying a default value by appending an extra condition to funclist.",
      "The last function in funclist is applied when none of the conditions are True."
    ],
    "code_examples": [
      {
        "description": "Define a piecewise function with a default value of 0 when x is between -1 and 1.",
        "code": "condlist = [x < -1, x > 1]\nfunclist = [lambda x: 1 + x, lambda x: x - 1, 0]\njnp.piecewise(x, condlist, funclist)"
      },
      {
        "description": "Define a piecewise function with a default value of 0 when x is between -1 and 1.",
        "code": "condlist = [x < -1, x > 1]\nfunclist = [lambda x: 1 + x, lambda x: x - 1, 0]\njnp.piecewise(x, condlist, funclist)"
      }
    ]
  },
  {
    "title": "Piecewise function with a simple array of scalar conditions",
    "concepts": [
      "condlist may be a simple array of scalar conditions.",
      "The associated function applies to the whole range."
    ],
    "code_examples": [
      {
        "description": "Demonstrates condlist as a simple boolean array, applying functions x*0, x*10, and x*100 based on the array.",
        "code": "condlist = jnp.array([False, True, False])\nfunclist = [lambda x: x * 0, lambda x: x * 10, lambda x: x * 100]\njnp.piecewise(x, condlist, funclist)"
      },
      {
        "description": "Demonstrates condlist as a simple boolean array, applying functions x*0, x*10, and x*100 based on the array.",
        "code": "condlist = jnp.array([False, True, False])\nfunclist = [lambda x: x * 0, lambda x: x * 10, lambda x: x * 100]\njnp.piecewise(x, condlist, funclist)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.place",
    "concepts": [
      "jax.numpy.place updates array elements based on a boolean mask.",
      "It is a JAX implementation of numpy.place().",
      "The JAX version returns a modified copy of the input array, as JAX arrays are immutable.",
      "The `inplace` parameter must be set to False to indicate that a modified copy is returned.",
      "The function takes an array `arr`, a boolean mask `mask`, and values `vals` as input.",
      "If not enough values are supplied in `vals`, they are repeated.",
      "If too many values are supplied in `vals`, they are truncated."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage with a Scalar Value",
    "concepts": [
      "Demonstrates placing a scalar value into an array based on a boolean mask.",
      "The mask is created by checking if the index modulo 3 is equal to 0."
    ],
    "code_examples": [
      {
        "description": "Creating a zero array and a boolean mask.",
        "code": "x = jnp.zeros((3, 5), dtype=int)\nmask = (jnp.arange(x.size) % 3 == 0).reshape(x.shape)\nmask"
      },
      {
        "description": "Placing the value 1 into the array x where the mask is True.",
        "code": "jnp.place(x, mask, 1, inplace=False)"
      }
    ]
  },
  {
    "title": "Comparison with masked array update syntax",
    "concepts": [
      "jax.numpy.place with a scalar value is similar to masked array updates using `.at[mask].set()`."
    ],
    "code_examples": [
      {
        "description": "Using `.at[mask].set()` to achieve the same result as jnp.place with a scalar value.",
        "code": "x.at[mask].set(1)"
      }
    ]
  },
  {
    "title": "Placing Values from an Array",
    "concepts": [
      "Demonstrates placing values from an array into another array based on a mask.",
      "The input array of values is repeated to fill the masked entries."
    ],
    "code_examples": [
      {
        "description": "Placing values from the array `vals` into the array `x` based on the mask.",
        "code": "vals = jnp.array([1, 3, 5])\njnp.place(x, mask, vals, inplace=False)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.poly()",
    "concepts": [
      "jax.numpy.poly() returns the coefficients of a polynomial given its roots.",
      "The input can be a scalar or an array of roots.",
      "The output is an array containing the polynomial coefficients.",
      "The output dtype is always promoted to inexact.",
      "jax.numpy.poly() handles scalar inputs differently than numpy.poly().",
      "jax.numpy.poly() always returns complex coefficients for complex or square inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Scalar and Array Inputs",
    "concepts": [
      "Demonstrates the usage of jax.numpy.poly() with scalar inputs.",
      "Demonstrates the usage of jax.numpy.poly() with array of integer values."
    ],
    "code_examples": [
      {
        "description": "Example with scalar input.",
        "code": "jnp.poly(1)"
      },
      {
        "description": "Example with an array of integer values.",
        "code": "x = jnp.array([1, 2, 3])\njnp.poly(x)"
      }
    ]
  },
  {
    "title": "Example with Complex Conjugates",
    "concepts": [
      "Demonstrates the usage of jax.numpy.poly() with an array of complex conjugates."
    ],
    "code_examples": [
      {
        "description": "Example with an array containing complex conjugates.",
        "code": "x = jnp.array([2, 1 + 2j, 1 - 2j])\njnp.poly(x)"
      }
    ]
  },
  {
    "title": "Example with Square Matrix Input",
    "concepts": [
      "Demonstrates the usage of jax.numpy.poly() with a square matrix as input.",
      "The output is a complex array."
    ],
    "code_examples": [
      {
        "description": "Example with a square matrix.  The jnp.round() function is used for display purposes.",
        "code": "x = jnp.array([[2, 1, 5],\n              [3, 4, 7],\n              [1, 3, 5]])\njnp.round(jnp.poly(x))"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.polydiv",
    "concepts": [
      "jax.numpy.polydiv() calculates the quotient and remainder of polynomial division.",
      "It is a JAX implementation of numpy.polydiv().",
      "The inputs u and v should be ArrayLike objects representing the dividend and divisor polynomial coefficients, respectively.",
      "The function returns a tuple containing the quotient and remainder arrays.",
      "The dtype of the output is promoted to inexact.",
      "jax.numpy.polydiv() only accepts arrays as input, unlike numpy.polydiv().",
      "trim_leading_zeros=True removes leading zeros in the remainder but can lead to inconsistent results in compiled code and with different JAX backends."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Example",
    "concepts": [
      "Demonstrates the basic usage of jnp.polydiv() with two JAX arrays.",
      "Compares the output of jnp.polydiv() with np.polydiv()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates polynomial division using JAX and compares it with NumPy.",
        "code": "x1 = jnp.array([5, 7, 9])\nx2 = jnp.array([4, 1])\n\n# NumPy version (for comparison)\nimport numpy as np\nprint(np.polydiv(x1, x2))\n\n# JAX version\nimport jax.numpy as jnp\nprint(jnp.polydiv(x1, x2))"
      }
    ]
  },
  {
    "title": "Using trim_leading_zeros",
    "concepts": [
      "Demonstrates the usage of the trim_leading_zeros parameter.",
      "Shows how to obtain results consistent with NumPy's polydiv by trimming leading zeros."
    ],
    "code_examples": [
      {
        "description": "Illustrates the effect of trim_leading_zeros=True on the output of jnp.polydiv.",
        "code": "x1 = jnp.array([5, 7, 9])\nx2 = jnp.array([4, 1])\nimport jax.numpy as jnp\nprint(jnp.polydiv(x1, x2, trim_leading_zeros=True))"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.polyfit",
    "concepts": [
      "The function performs a least squares polynomial fit to a given set of data points.",
      "It finds the polynomial coefficients that minimize the sum of the squares of the residuals.",
      "The function accepts x and y data points, and the degree of the polynomial as input.",
      "It returns the polynomial coefficients, and optionally residuals, rank, singular values, condition number, or the covariance matrix.",
      "This is a JAX implementation of numpy.polyfit()."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Example",
    "concepts": [
      "Demonstrates fitting a 2nd-degree polynomial to data points (x, y).",
      "The `jnp.polyfit` function is used to calculate the polynomial coefficients.",
      "The `jnp.printoptions` is used to format the output for better readability."
    ],
    "code_examples": [
      {
        "description": "Fits a 2nd-degree polynomial to the given data points and prints the resulting coefficients.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3., 6., 9., 4.])\ny = jnp.array([[0, 1, 2],\n               [2, 5, 7],\n               [8, 4, 9],\n               [1, 6, 3]])\n\np = jnp.polyfit(x, y, 2)\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(p)"
      },
      {
        "description": "Fits a 2nd-degree polynomial to the given data points and prints the resulting coefficients.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3., 6., 9., 4.])\ny = jnp.array([[0, 1, 2],\n               [2, 5, 7],\n               [8, 4, 9],\n               [1, 6, 3]])\n\np = jnp.polyfit(x, y, 2)\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(p)"
      }
    ]
  },
  {
    "title": "Example with full=True",
    "concepts": [
      "Shows how to obtain additional information about the fit when full=True.",
      "Returns the polynomial coefficients, residuals, rank, singular values, and condition number."
    ],
    "code_examples": [
      {
        "description": "Calculates the polynomial coefficients, residuals, rank, singular values, and condition number by setting `full=True`.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3., 6., 9., 4.])\ny = jnp.array([[0, 1, 2],\n               [2, 5, 7],\n               [8, 4, 9],\n               [1, 6, 3]])\n\np, resids, rank, s, rcond = jnp.polyfit(x, y, 2, full=True)\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(\"Polynomial Coefficients:\", \"\\n\", p, \"\\n\",\n        \"Residuals:\", resids, \"\\n\",\n        \"Rank:\", rank, \"\\n\",\n        \"s:\", s, \"\\n\",\n        \"rcond:\", rcond)"
      },
      {
        "description": "Calculates the polynomial coefficients, residuals, rank, singular values, and condition number by setting `full=True`.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3., 6., 9., 4.])\ny = jnp.array([[0, 1, 2],\n               [2, 5, 7],\n               [8, 4, 9],\n               [1, 6, 3]])\n\np, resids, rank, s, rcond = jnp.polyfit(x, y, 2, full=True)\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(\"Polynomial Coefficients:\", \"\\n\", p, \"\\n\",\n        \"Residuals:\", resids, \"\\n\",\n        \"Rank:\", rank, \"\\n\",\n        \"s:\", s, \"\\n\",\n        \"rcond:\", rcond)"
      }
    ]
  },
  {
    "title": "Example with cov=True",
    "concepts": [
      "Demonstrates how to obtain the covariance matrix of the polynomial coefficients.",
      "Setting cov=True returns the covariance matrix along with the polynomial coefficients."
    ],
    "code_examples": [
      {
        "description": "Calculates the polynomial coefficients and covariance matrix by setting `cov=True` and prints their shapes.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3., 6., 9., 4.])\ny = jnp.array([[0, 1, 2],\n               [2, 5, 7],\n               [8, 4, 9],\n               [1, 6, 3]])\n\np, C = jnp.polyfit(x, y, 2, cov=True)\n\nprint(p.shape, C.shape)"
      },
      {
        "description": "Calculates the polynomial coefficients and covariance matrix by setting `cov=True` and prints their shapes.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3., 6., 9., 4.])\ny = jnp.array([[0, 1, 2],\n               [2, 5, 7],\n               [8, 4, 9],\n               [1, 6, 3]])\n\np, C = jnp.polyfit(x, y, 2, cov=True)\n\nprint(p.shape, C.shape)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.polyint",
    "concepts": [
      "Calculates the coefficients of the indefinite integral of a polynomial.",
      "It is a JAX implementation of NumPy's polyint function.",
      "The polynomial is represented by its coefficients.",
      "The order of integration 'm' must be specified statically.",
      "Integration constants 'k' can be provided as a scalar or an array.",
      "The function returns the coefficients of the integrated polynomial."
    ],
    "code_examples": []
  },
  {
    "title": "First-Order Integration with Default Constant",
    "concepts": [
      "Demonstrates the first order integration of a polynomial.",
      "Shows the default behavior when the integration constant 'k' is not provided (k=0).",
      "Illustrates the usage of jnp.polyint() with a simple polynomial coefficient array."
    ],
    "code_examples": [
      {
        "description": "Calculates the first order integral of the polynomial represented by the coefficient array [12, 12, 6]. Since no integration constant 'k' is given, 0 is assumed.",
        "code": "import jax.numpy as jnp\n\np = jnp.array([12, 12, 6])\njnp.polyint(p)"
      },
      {
        "description": "Calculates the first order integral of the polynomial represented by the coefficient array [12, 12, 6]. Since no integration constant 'k' is given, 0 is assumed.",
        "code": "import jax.numpy as jnp\n\np = jnp.array([12, 12, 6])\njnp.polyint(p)"
      }
    ]
  },
  {
    "title": "First-Order Integration with a Specified Constant",
    "concepts": [
      "Illustrates the first order integration with a specified integration constant.",
      "Shows how to provide a scalar value for the 'k' parameter.",
      "Demonstrates the effect of the integration constant on the resulting polynomial coefficients."
    ],
    "code_examples": [
      {
        "description": "Calculates the first order integral of the polynomial represented by the coefficient array [12, 12, 6], specifying the constant of integration as 4.",
        "code": "import jax.numpy as jnp\n\np = jnp.array([12, 12, 6])\njnp.polyint(p, k=4)"
      },
      {
        "description": "Calculates the first order integral of the polynomial represented by the coefficient array [12, 12, 6], specifying the constant of integration as 4.",
        "code": "import jax.numpy as jnp\n\np = jnp.array([12, 12, 6])\njnp.polyint(p, k=4)"
      }
    ]
  },
  {
    "title": "Second-Order Integration with Default Constants",
    "concepts": [
      "Demonstrates the second order integration of a polynomial.",
      "Illustrates the usage of the 'm' parameter to specify the order of integration.",
      "Shows the default behavior when the integration constants 'k' are not provided (k=0 for each integration).",
      "When m>=2 , the constants k should be provided as an array having m elements."
    ],
    "code_examples": [
      {
        "description": "Calculates the second order integral of the polynomial represented by the coefficient array [12, 12, 6].  The integration constant defaults to 0.",
        "code": "import jax.numpy as jnp\n\np = jnp.array([12, 12, 6])\njnp.polyint(p, m=2)"
      },
      {
        "description": "Calculates the second order integral of the polynomial represented by the coefficient array [12, 12, 6].  The integration constant defaults to 0.",
        "code": "import jax.numpy as jnp\n\np = jnp.array([12, 12, 6])\njnp.polyint(p, m=2)"
      }
    ]
  },
  {
    "title": "Second-Order Integration with Specified Constants",
    "concepts": [
      "Demonstrates the second order integration with specified integration constants.",
      "Shows how to provide an array for the 'k' parameter when m >= 2.",
      "Illustrates the effect of the integration constants on the resulting polynomial coefficients."
    ],
    "code_examples": [
      {
        "description": "Calculates the second order integral of the polynomial represented by the coefficient array [12, 12, 6], specifying the constants of integration as 4 and 5.",
        "code": "import jax.numpy as jnp\n\np = jnp.array([12, 12, 6])\njnp.polyint(p, m=2, k=jnp.array([4, 5]))"
      },
      {
        "description": "Calculates the second order integral of the polynomial represented by the coefficient array [12, 12, 6], specifying the constants of integration as 4 and 5.",
        "code": "import jax.numpy as jnp\n\np = jnp.array([12, 12, 6])\njnp.polyint(p, m=2, k=jnp.array([4, 5]))"
      }
    ]
  },
  {
    "title": "Overview",
    "concepts": [
      "The function computes the product of two polynomials represented as 1D arrays of coefficients.",
      "It is a JAX implementation of numpy.polymul().",
      "The input arrays `a1` and `a2` represent the polynomial coefficients.",
      "The `trim_leading_zeros` parameter controls whether leading zeros are removed from the result.",
      "The output is an array containing the coefficients of the resulting polynomial.",
      "The dtype of the output is always promoted to inexact.",
      "jax.numpy.polymul() only accepts arrays as input, unlike numpy.polymul() which accepts scalar inputs as well."
    ],
    "code_examples": []
  },
  {
    "title": "Polynomial Multiplication Examples",
    "concepts": [
      "Demonstrates the usage of `jnp.polymul` with integer coefficients.",
      "Illustrates the effect of the `trim_leading_zeros` parameter.",
      "The default behaviour is to return an array with potential leading zeros.",
      "Setting `trim_leading_zeros=True` removes the leading zeros."
    ],
    "code_examples": [
      {
        "description": "Multiplies two polynomials with integer coefficients using both numpy and jax versions. Notice the difference in the output, where jax returns a float array and might contain leading zeros.",
        "code": "x1 = np.array([2, 1, 0])\nx2 = np.array([0, 5, 0, 3])\nnp.polymul(x1, x2)\n# array([10,  5,  6,  3,  0])\njnp.polymul(x1, x2)\n# Array([ 0., 10.,  5.,  6.,  3.,  0.], dtype=float32)"
      },
      {
        "description": "Demonstrates the usage of `trim_leading_zeros` parameter. Setting it to `True` removes leading zeros and matches numpy's result.",
        "code": "jnp.polymul(x1, x2, trim_leading_zeros=True)\n# Array([10.,  5.,  6.,  3.,  0.], dtype=float32)"
      }
    ]
  },
  {
    "title": "Complex Number Examples",
    "concepts": [
      "Demonstrates `jnp.polymul` with complex number coefficients.",
      "Shows how `trim_leading_zeros` affects the result with complex numbers."
    ],
    "code_examples": [
      {
        "description": "Multiplies two polynomials with complex coefficients using both numpy and jax versions, showing the difference in outputs and datatypes.",
        "code": "x3 = np.array([2., 1+2j, 1-2j])\nx4 = np.array([0, 5, 0, 3])\nnp.polymul(x3, x4)\n# array([10. +0.j,  5.+10.j, 11.-10.j,  3. +6.j,  3. -6.j])\njnp.polymul(x3, x4)\n# Array([ 0. +0.j, 10. +0.j,  5.+10.j, 11.-10.j,  3. +6.j,  3. -6.j], dtype=complex64)"
      },
      {
        "description": "Demonstrates using the `trim_leading_zeros` option with complex polynomial coefficients to remove any leading zeros.",
        "code": "jnp.polymul(x3, x4, trim_leading_zeros=True)\n# Array([10. +0.j,  5.+10.j, 11.-10.j,  3. +6.j,  3. -6.j], dtype=complex64)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.polysub",
    "concepts": [
      "jax.numpy.polysub() returns the difference of two polynomials.",
      "It is a JAX implementation of numpy.polysub().",
      "It takes two ArrayLike objects as input, representing the coefficients of the minuend and subtrahend polynomials.",
      "It returns an array containing the coefficients of the resulting polynomial difference.",
      "jax.numpy.polysub() only accepts arrays as input, unlike numpy.polysub()."
    ],
    "code_examples": []
  },
  {
    "title": "Polynomial Subtraction Examples",
    "concepts": [
      "Demonstrates subtraction of two polynomials represented as JAX arrays.",
      "Illustrates how the function handles different array shapes.",
      "Shows an example where broadcasting is not possible, resulting in a ValueError."
    ],
    "code_examples": [
      {
        "description": "Subtracting two polynomials with different degrees.",
        "code": "x1 = jnp.array([2, 3])\nx2 = jnp.array([5, 4, 1])\njnp.polysub(x1, x2)"
      },
      {
        "description": "Subtracting two polynomials with different degrees.",
        "code": "x1 = jnp.array([2, 3])\nx2 = jnp.array([5, 4, 1])\njnp.polysub(x1, x2)"
      },
      {
        "description": "Subtracting a 2D array representing multiple polynomials from a single polynomial.",
        "code": "x3 = jnp.array([[2, 3, 1]])\nx4 = jnp.array([[5, 7, 3],\n                [8, 2, 6]])\njnp.polysub(x3, x4)"
      },
      {
        "description": "Subtracting a 2D array representing multiple polynomials from a single polynomial.",
        "code": "x3 = jnp.array([[2, 3, 1]])\nx4 = jnp.array([[5, 7, 3],\n                [8, 2, 6]])\njnp.polysub(x3, x4)"
      },
      {
        "description": "Demonstrates a ValueError due to incompatible shapes when broadcasting is not possible.",
        "code": "x5 = jnp.array([1, 3, 5])\nx6 = jnp.array([[5, 7, 9],\n                [8, 6, 4]])\njnp.polysub(x5, x6)"
      },
      {
        "description": "Subtracting a scalar polynomial (represented as a 1-element array) from multiple polynomials in a 2D array.",
        "code": "x7 = jnp.array([2])\njnp.polysub(x6, x7)"
      },
      {
        "description": "Demonstrates a ValueError due to incompatible shapes when broadcasting is not possible.",
        "code": "x5 = jnp.array([1, 3, 5])\nx6 = jnp.array([[5, 7, 9],\n                [8, 6, 4]])\njnp.polysub(x5, x6)"
      },
      {
        "description": "Subtracting a scalar polynomial (represented as a 1-element array) from multiple polynomials in a 2D array.",
        "code": "x7 = jnp.array([2])\njnp.polysub(x6, x7)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.polyval",
    "concepts": [
      "Evaluates a polynomial at specified values.",
      "JAX implementation of numpy.polyval().",
      "The polynomial is defined by its coefficients p.",
      "The function computes p_0 * x^(M-1) + p_1 * x^(M-2) + ... + p_(M-1)."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters and Return Value",
    "concepts": [
      "p: ArrayLike. An array of polynomial coefficients of shape (M,).",
      "x: ArrayLike. A number or an array of numbers where the polynomial is evaluated.",
      "unroll: int. Controls the number of unrolled steps with lax.scan. This is JAX specific and affects performance.",
      "Returns an array of the same shape as x."
    ],
    "code_examples": []
  },
  {
    "title": "Performance Considerations",
    "concepts": [
      "The unroll parameter in jax.numpy.polyval is JAX-specific.",
      "It can significantly impact performance, especially for high-order polynomials.",
      "Increasing the unroll value (e.g., unroll=128) can improve runtime performance on accelerators.",
      "Higher unroll values may increase compilation time."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.polyfit(): Least squares polynomial fit.",
      "jax.numpy.poly(): Finds the coefficients of a polynomial with given roots.",
      "jax.numpy.roots(): Computes the roots of a polynomial for given coefficients."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with 1D Input",
    "concepts": [
      "Demonstrates how to use jax.numpy.polyval with a 1D array of coefficients and a single value for evaluation.",
      "Shows the result of evaluating a polynomial at a specific point."
    ],
    "code_examples": [
      {
        "description": "Evaluates the polynomial defined by coefficients [2, 5, 1] at x = 3.",
        "code": "p = jnp.array([2, 5, 1])\njnp.polyval(p, 3)"
      },
      {
        "description": "Evaluates the polynomial defined by coefficients [2, 5, 1] at x = 3. (Duplicate example)",
        "code": "p = jnp.array([2, 5, 1])\njnp.polyval(p, 3)"
      }
    ]
  },
  {
    "title": "Examples with 2D Input",
    "concepts": [
      "Demonstrates how to use jax.numpy.polyval with a 1D array of coefficients and a 2D array for evaluation.",
      "Shows that the output array has the same shape as the input x array."
    ],
    "code_examples": [
      {
        "description": "Evaluates the polynomial defined by coefficients [2, 5, 1] at each element of the 2D array x.",
        "code": "x = jnp.array([[2, 1, 5],\n                [3, 4, 7],\n                [1, 3, 5]])\njnp.polyval(p, x)"
      },
      {
        "description": "Evaluates the polynomial defined by coefficients [2, 5, 1] at each element of the 2D array x. (Duplicate example)",
        "code": "x = jnp.array([[2, 1, 5],\n                [3, 4, 7],\n                [1, 3, 5]])\njnp.polyval(p, x)"
      }
    ]
  },
  {
    "title": "Description of jnp.positive",
    "concepts": [
      "Returns element-wise positive values of the input array.",
      "JAX implementation of numpy.positive.",
      "Equivalent to x.copy() for types supporting arithmetic operations."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Real-Valued Inputs",
    "concepts": [
      "Demonstrates jnp.positive with a real-valued JAX array.",
      "Shows that jnp.positive returns the same array as x.copy()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates jnp.positive on a real-valued array. The output is identical to the input array.",
        "code": "x = jnp.array([-5, 4, 7., -9.5])\njnp.positive(x)\n"
      },
      {
        "description": "Shows that x.copy() produces the same result as jnp.positive(x).",
        "code": "x.copy()"
      },
      {
        "description": "Demonstrates jnp.positive on a real-valued array. The output is identical to the input array.",
        "code": "x = jnp.array([-5, 4, 7., -9.5])\njnp.positive(x)"
      },
      {
        "description": "Shows that x.copy() produces the same result as jnp.positive(x).",
        "code": "x.copy()"
      }
    ]
  },
  {
    "title": "Examples with Complex Inputs",
    "concepts": [
      "Demonstrates jnp.positive with a complex-valued JAX array.",
      "Shows that jnp.positive returns the same array as x.copy() for complex numbers."
    ],
    "code_examples": [
      {
        "description": "Demonstrates jnp.positive on a complex-valued array. The output is identical to the input array.",
        "code": "x1 = jnp.array([1-2j, -3+4j, 5-6j])\njnp.positive(x1)"
      },
      {
        "description": "Shows that x1.copy() produces the same result as jnp.positive(x1).",
        "code": "x1.copy()"
      },
      {
        "description": "Demonstrates jnp.positive on a complex-valued array. The output is identical to the input array.",
        "code": "x1 = jnp.array([1-2j, -3+4j, 5-6j])\njnp.positive(x1)"
      },
      {
        "description": "Shows that x1.copy() produces the same result as jnp.positive(x1).",
        "code": "x1.copy()"
      }
    ]
  },
  {
    "title": "Examples with uint32 Inputs",
    "concepts": [
      "Demonstrates jnp.positive with a uint32 JAX array.",
      "Shows how negative values are interpreted as large positive values in uint32.",
      "Demonstrates that jnp.positive returns the same array."
    ],
    "code_examples": [
      {
        "description": "Illustrates how negative numbers are converted when casting to uint32.",
        "code": "x2 = jnp.array([6, 0, -4]).astype(jnp.uint32)\nx2"
      },
      {
        "description": "Demonstrates jnp.positive on a uint32 array.",
        "code": "jnp.positive(x2)"
      },
      {
        "description": "Illustrates how negative numbers are converted when casting to uint32.",
        "code": "x2 = jnp.array([6, 0, -4]).astype(jnp.uint32)\nx2"
      },
      {
        "description": "Demonstrates jnp.positive on a uint32 array.",
        "code": "jnp.positive(x2)"
      }
    ]
  },
  {
    "title": "jax.numpy.power() Alias",
    "concepts": [
      "jax.numpy.power() is an alias for another function.",
      "The function takes two ArrayLike arguments, x1 and x2.",
      "The function returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jnp.power",
    "concepts": [
      "Calculates element-wise base x1 exponential of x2.",
      "It is a JAX implementation of numpy.power.",
      "x1 specifies the bases and x2 specifies the exponent.",
      "x1 and x2 should either have the same shape or be broadcast compatible.",
      "Returns an array containing the base x1 exponentials of x2 with the same dtype as the input.",
      "When x2 is a concrete integer scalar, jnp.power lowers to jax.lax.integer_pow().",
      "When x2 is a traced scalar or an array, jnp.power lowers to jax.lax.pow().",
      "jnp.power raises a TypeError for integer type raised to negative integer power.",
      "jnp.power returns nan for a negative value raised to the power of non-integer values."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Scalar Integers",
    "concepts": [
      "Demonstrates jnp.power with scalar integer inputs."
    ],
    "code_examples": [
      {
        "description": "Calculating 4 to the power of 3.",
        "code": "jnp.power(4, 3)"
      },
      {
        "description": "Calculating 4 to the power of 3 again.",
        "code": "jnp.power(4, 3)"
      }
    ]
  },
  {
    "title": "Examples with Same Shape Arrays",
    "concepts": [
      "Demonstrates jnp.power with arrays of the same shape.",
      "Calculates element-wise power with array inputs."
    ],
    "code_examples": [
      {
        "description": "Calculating element-wise power of two arrays with the same shape.",
        "code": "x1 = jnp.array([2, 4, 5])\nx2 = jnp.array([3, 0.5, 2])\njnp.power(x1, x2)"
      },
      {
        "description": "Calculating element-wise power of two arrays with the same shape again.",
        "code": "x1 = jnp.array([2, 4, 5])\nx2 = jnp.array([3, 0.5, 2])\njnp.power(x1, x2)"
      }
    ]
  },
  {
    "title": "Examples with Broadcast Compatible Arrays",
    "concepts": [
      "Demonstrates jnp.power with broadcast compatible arrays.",
      "Shows how jnp.power handles broadcasting."
    ],
    "code_examples": [
      {
        "description": "Calculating element-wise power of broadcast compatible arrays. Note the NaN result due to negative base and non-integer exponent.",
        "code": "x3 = jnp.array([-2, 3, 1])\nx4 = jnp.array([[4, 1, 6],\n                [1.3, 3, 5]])\njnp.power(x3, x4)"
      },
      {
        "description": "Calculating element-wise power of broadcast compatible arrays again. Note the NaN result due to negative base and non-integer exponent.",
        "code": "x3 = jnp.array([-2, 3, 1])\nx4 = jnp.array([[4, 1, 6],\n                [1.3, 3, 5]])\njnp.power(x3, x4)"
      }
    ]
  },
  {
    "title": "Overview of JAX Array Printing",
    "concepts": [
      "JAX arrays are printed using NumPy's printing functionality.",
      "NumPy's print options configurations apply to JAX array printing.",
      "Refer to numpy.set_printoptions() documentation for details on available options."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.numpy.prod",
    "concepts": [
      "Calculates the product of array elements over a specified axis.",
      "JAX implementation of numpy.prod().",
      "Supports specifying the axis for product calculation.",
      "Allows specifying the output data type (dtype).",
      "Offers the option to keep reduced dimensions in the result (keepdims).",
      "Accepts an initial value for the product.",
      "Allows including only specific elements in the product using a 'where' condition.",
      "Includes a promote_integers option to control promotion of integer input types."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage and Axis Specification",
    "concepts": [
      "By default, jnp.prod computes the product along all axes.",
      "The axis parameter specifies the axis along which the product is computed."
    ],
    "code_examples": [
      {
        "description": "Compute the product of all elements in the array.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 1, 3],\n               [2, 1, 3, 1]])\njnp.prod(x)"
      },
      {
        "description": "Compute the product along axis 1.",
        "code": "jnp.prod(x, axis=1)"
      }
    ]
  },
  {
    "title": "Keepdims Parameter",
    "concepts": [
      "The keepdims parameter preserves the number of dimensions of the input array in the output.",
      "When keepdims is set to True, the reduced axes are left in the result with size 1."
    ],
    "code_examples": [
      {
        "description": "Compute the product along axis 1, keeping the dimensions.",
        "code": "jnp.prod(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Where Parameter for Conditional Product",
    "concepts": [
      "The where parameter allows including only specific elements in the product.",
      "The 'where' array should be broadcast compatible with the input array.",
      "If all elements specified by 'where' are false, the product defaults to 1."
    ],
    "code_examples": [
      {
        "description": "Compute the product along axis 1, including only specific elements based on the 'where' condition.",
        "code": "where = jnp.array([[1, 0, 1, 0],\n               [0, 0, 1, 1],\n               [1, 1, 1, 0]], dtype=bool)\njnp.prod(x, axis=1, keepdims=True, where=where)"
      },
      {
        "description": "Compute the product along axis 1, where the 'where' condition excludes all elements, resulting in a product of 1 for each row.",
        "code": "where = jnp.array([[False],\n               [False],\n               [False]])\njnp.prod(x, axis=1, keepdims=True, where=where)"
      }
    ]
  },
  {
    "title": "Type Promotion with jnp.promote_types",
    "concepts": [
      "The jnp.promote_types function determines the resulting data type after a binary operation on two arguments.",
      "It mimics numpy.promote_types but adheres to JAX's type promotion semantics.",
      "Type specifiers can be strings, jnp.dtypes, or scalar types.",
      "The function always returns a jnp.dtype object.",
      "Built-in scalar types (int, float, complex) are treated as weakly-typed.",
      "Weakly typed scalars won't change the bit width of a strongly-typed counterpart."
    ],
    "code_examples": [
      {
        "description": "Demonstrates type promotion using strings, jnp.dtypes, and scalar types with jnp.promote_types.",
        "code": "import jax.numpy as jnp\n\nprint(jnp.promote_types('int32', 'float32'))  # strings\nprint(jnp.promote_types(jnp.dtype('int32'), jnp.dtype('float32')))  # dtypes\nprint(jnp.promote_types(jnp.int32, jnp.float32))  # scalar types\nprint(jnp.promote_types('int32', 'float32'))  # strings\nprint(jnp.promote_types(jnp.dtype('int32'), jnp.dtype('float32')))  # dtypes\nprint(jnp.promote_types(jnp.int32, jnp.float32))  # scalar types"
      },
      {
        "description": "Illustrates how built-in scalar types (int, float) interact with strongly-typed counterparts in jnp.promote_types, preserving the bit width of the strongly-typed side.",
        "code": "import jax.numpy as jnp\n\nprint(jnp.promote_types('uint8', int))\nprint(jnp.promote_types('float16', float))\nprint(jnp.promote_types('uint8', int))\nprint(jnp.promote_types('float16', float))"
      }
    ]
  },
  {
    "title": "Comparison with NumPy's promote_types",
    "concepts": [
      "jnp.promote_types handles built-in scalar types differently than numpy.promote_types.",
      "NumPy treats int, float, and complex as equivalent to 64-bit types.",
      "JAX treats them as weakly typed and preserves the bit width of the strongly typed counterpart."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the difference in behavior between jnp.promote_types and numpy.promote_types when using built-in scalar types.",
        "code": "import numpy\n\nprint(numpy.promote_types('uint8', int))\nprint(numpy.promote_types('float16', float))\nprint(numpy.promote_types('uint8', int))\nprint(numpy.promote_types('float16', float)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.ptp",
    "concepts": [
      "Calculates the peak-to-peak range along a given axis of an array.",
      "JAX implementation of numpy.ptp().",
      "If axis is None, the range is computed on the flattened array.",
      "The 'keepdims' argument can be used to preserve the dimensions of the input array.",
      "The 'out' argument is unused by JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.ptp",
    "concepts": [
      "Demonstrates how jnp.ptp computes the range along all axes by default.",
      "Shows the output when no axis is specified."
    ],
    "code_examples": [
      {
        "description": "Computes the range of the entire array when no axis is specified.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 5, 2],\n               [4, 6, 8, 1],\n               [7, 9, 3, 4]])\n\njnp.ptp(x)"
      }
    ]
  },
  {
    "title": "Specifying the Axis",
    "concepts": [
      "Illustrates how to compute the range along a specific axis.",
      "Shows the output when axis=1 is specified."
    ],
    "code_examples": [
      {
        "description": "Computes the range along axis 1.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 5, 2],\n               [4, 6, 8, 1],\n               [7, 9, 3, 4]])\n\njnp.ptp(x, axis=1)"
      }
    ]
  },
  {
    "title": "Keeping Dimensions",
    "concepts": [
      "Demonstrates how to preserve the dimensions of the input array using the 'keepdims' argument.",
      "Shows the output when axis=1 and keepdims=True are specified."
    ],
    "code_examples": [
      {
        "description": "Computes the range along axis 1 and preserves the dimensions of the input array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 5, 2],\n               [4, 6, 8, 1],\n               [7, 9, 3, 4]])\n\njnp.ptp(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "JAX Implementation of `numpy.put_along_axis()`",
    "concepts": [
      "JAX's `put_along_axis()` function mimics NumPy's but returns a modified copy due to immutability.",
      "The `inplace` parameter must be set to `False` to reflect that a copy is returned, not an in-place modification.",
      "The function takes an array, indices, values, axis, inplace and mode as input.",
      "The function modifies the input array along the specified axis based on the given indices and values.",
      "The function returns a copy of the array with specified entries updated.",
      "See also `jax.numpy.put()`, `jax.numpy.place()`, `jax.numpy.ndarray.at()`, `jax.numpy.take()` and `jax.numpy.take_along_axis()`."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use `jnp.put_along_axis()` to replace values in an array based on the indices obtained from `jnp.argmax()` along a specific axis. The `inplace` parameter is set to `False`.",
        "code": "from jax import numpy as jnp\n\na = jnp.array([[10, 30, 20],\n               [60, 40, 50]])\n\ni = jnp.argmax(a, axis=1, keepdims=True)\n\nprint(i)\n\nb = jnp.put_along_axis(a, i, 99, axis=1, inplace=False)\n\nprint(b)"
      },
      {
        "description": "Demonstrates how to use `jnp.put_along_axis()` to replace values in an array based on the indices obtained from `jnp.argmax()` along a specific axis. The `inplace` parameter is set to `False`.",
        "code": "from jax import numpy as jnp\n\na = jnp.array([[10, 30, 20],\n               [60, 40, 50]])\n\ni = jnp.argmax(a, axis=1, keepdims=True)\n\nprint(i)\n\nb = jnp.put_along_axis(a, i, 99, axis=1, inplace=False)\n\nprint(b)"
      }
    ]
  },
  {
    "title": "Concatenating Slices and Arrays",
    "concepts": [
      "jnp.r_ concatenates slices, scalars, and array-like objects along the first axis.",
      "Slices in the form [start:stop:step] generate jnp.arange objects.",
      "An imaginary value for step creates a jnp.linspace object, including the right endpoint."
    ],
    "code_examples": [
      {
        "description": "Demonstrates concatenating a slice, scalars and an array.",
        "code": "import jax.numpy as jnp\n\njnp.r_[-1:5:1, 0, 0, jnp.array([1, 2, 3])]"
      },
      {
        "description": "Demonstrates concatenating a slice with an imaginary step, scalars and an array.",
        "code": "import jax.numpy as jnp\n\njnp.r_[-1:1:6j, 0, jnp.array([1, 2, 3])]"
      }
    ]
  },
  {
    "title": "String Directives for Concatenation",
    "concepts": [
      "A string directive can be used to specify concatenation axis, minimum number of dimensions, and the position of the upgraded array\u2019s original dimensions.",
      "The format of the string directive is 'axis,dims,trans1d'.",
      "Negative values for trans1d offset the last axis towards the start of the shape tuple."
    ],
    "code_examples": [
      {
        "description": "Concatenating along the first axis, 2D output.",
        "code": "import jax.numpy as jnp\n\njnp.r_['0,2', [1, 2, 3], [4, 5, 6]]"
      },
      {
        "description": "Pushing the last input axis to the front.",
        "code": "import jax.numpy as jnp\n\njnp.r_['0,2,0', [1, 2, 3], [4, 5, 6]]"
      },
      {
        "description": "Offsetting the last axis towards the start of the shape tuple using a negative value for trans1d.",
        "code": "import jax.numpy as jnp\n\njnp.r_['0,2,-2', [1, 2, 3], [4, 5, 6]]"
      }
    ]
  },
  {
    "title": "Special Directives 'r' and 'c'",
    "concepts": [
      "The directives 'r' or 'c' can be used as the first argument on flat inputs.",
      "'r' creates an array with an extra row axis.",
      "'c' creates an array with an extra column axis.",
      "For higher-dimensional inputs (dim >= 2), both 'r' and 'c' give the same result."
    ],
    "code_examples": [
      {
        "description": "Using 'r' to create an array with an extra row axis.",
        "code": "import jax.numpy as jnp\n\njnp.r_['r',[1, 2, 3], [4, 5, 6]]"
      },
      {
        "description": "Using 'c' to create an array with an extra column axis.",
        "code": "import jax.numpy as jnp\n\njnp.r_['c',[1, 2, 3], [4, 5, 6]]"
      }
    ]
  },
  {
    "title": "Radians to Degrees Conversion",
    "concepts": [
      "Converts angles from radians to degrees.",
      "Implements the numpy.rad2deg function using JAX.",
      "The conversion formula is: rad2deg(x) = x * 180/pi.",
      "It accepts a scalar or array as input.",
      "The output is an array containing the angles in degrees."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of jnp.rad2deg to convert an array of angles from radians to degrees.",
        "code": "pi = jnp.pi\nx = jnp.array([pi / 4, pi / 2, 2 * pi / 3])\njnp.rad2deg(x)"
      },
      {
        "description": "Verifies the jnp.rad2deg implementation by manually calculating the conversion.",
        "code": "pi = jnp.pi\nx = jnp.array([pi / 4, pi / 2, 2 * pi / 3])\nx * 180 / pi"
      },
      {
        "description": "Demonstrates the use of jnp.rad2deg to convert an array of angles from radians to degrees.",
        "code": "pi = jnp.pi\nx = jnp.array([pi / 4, pi / 2, 2 * pi / 3])\njnp.rad2deg(x)"
      },
      {
        "description": "Verifies the jnp.rad2deg implementation by manually calculating the conversion.",
        "code": "pi = jnp.pi\nx = jnp.array([pi / 4, pi / 2, 2 * pi / 3])\nx * 180 / pi"
      }
    ]
  },
  {
    "title": "Alias of jax.numpy.deg2rad()",
    "concepts": [
      "The function deg2rad() is an alias for jax.numpy.deg2rad().",
      "The function takes an ArrayLike as input, denoted by x."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.numpy.ravel",
    "concepts": [
      "The function flattens an array into a 1-dimensional array.",
      "It's a JAX implementation of numpy.ravel.",
      "It is implemented using jax.lax.reshape.",
      "ravel(arr, order=order) is equivalent to reshape(arr, -1, order=order).",
      "The 'order' parameter specifies row-major ('C') or column-major ('F') order.",
      "JAX does not support order='A' or order='K'.",
      "jax.numpy.ravel() returns a copy of the input array, not a view."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jax.numpy.ravel",
    "concepts": [
      "Demonstrates flattening a 2D array into a 1D array using jnp.ravel with default C-style order.",
      "The default order is row-major."
    ],
    "code_examples": [
      {
        "description": "Flattening a 2D array in C-style (row-major) order.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\njnp.ravel(x)"
      },
      {
        "description": "Flattening a 2D array in C-style (row-major) order.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\njnp.ravel(x)"
      }
    ]
  },
  {
    "title": "Fortran-style Flattening",
    "concepts": [
      "Demonstrates flattening a 2D array using jnp.ravel with Fortran-style (column-major) order.",
      "The order='F' argument specifies column-major ordering."
    ],
    "code_examples": [
      {
        "description": "Flattening a 2D array in Fortran-style (column-major) order.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\njnp.ravel(x, order='F')"
      },
      {
        "description": "Flattening a 2D array in Fortran-style (column-major) order.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\njnp.ravel(x, order='F')"
      }
    ]
  },
  {
    "title": "Using the jax.Array.ravel() Method",
    "concepts": [
      "Demonstrates flattening a JAX array using the array's ravel() method.",
      "This provides equivalent functionality to jnp.ravel(x)."
    ],
    "code_examples": [
      {
        "description": "Flattening a 2D array using the array's ravel() method.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\nx.ravel()"
      },
      {
        "description": "Flattening a 2D array using the array's ravel() method.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\nx.ravel()"
      }
    ]
  },
  {
    "title": "Introduction to ravel_multi_index",
    "concepts": [
      "Converts multi-dimensional indices into flat indices.",
      "JAX implementation of numpy.ravel_multi_index().",
      "The multi_index argument is a sequence of integer arrays containing indices in each dimension.",
      "The dims argument is a sequence of integer sizes; must have len(dims) == len(multi_index).",
      "The mode argument specifies how to handle out-of-bound indices ('raise', 'clip', 'wrap').",
      "The order argument specifies the array order ('C' or 'F').",
      "jax.numpy.unravel_index() is the inverse of this function."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of ravel_multi_index",
    "concepts": [
      "Demonstrates how to use ravel_multi_index to flatten indices in a 2D array.",
      "Shows how to extract values from both the original and flattened arrays using the computed indices.",
      "Illustrates the inverse relationship between ravel_multi_index and unravel_index."
    ],
    "code_examples": [
      {
        "description": "Define a 2-dimensional array and find indices of even values.",
        "code": "x = jnp.array([[2., 3., 4.],\n               [5., 6., 7.]])\nindices = jnp.where(x % 2 == 0)\nindices"
      },
      {
        "description": "Extract values from the array using the indices.",
        "code": "x[indices]"
      },
      {
        "description": "Compute the flattened indices using ravel_multi_index.",
        "code": "indices_flat = jnp.ravel_multi_index(indices, x.shape)\nindices_flat"
      },
      {
        "description": "Flatten the original array.",
        "code": "x_flat = x.ravel()\nx_flat"
      },
      {
        "description": "Extract values from the flattened array using the flattened indices.",
        "code": "x_flat[indices_flat]"
      },
      {
        "description": "Recover the original indices using unravel_index.",
        "code": "jnp.unravel_index(indices_flat, x.shape)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.real",
    "concepts": [
      "Returns the real part of a complex number or array.",
      "It is a JAX implementation of numpy.real.",
      "The input can be an array or scalar.",
      "Related functions include jax.numpy.conjugate(), jax.numpy.conj(), and jax.numpy.imag()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates jnp.real with a real number input.",
        "code": "jnp.real(5)"
      },
      {
        "description": "Demonstrates jnp.real with a purely imaginary number input.",
        "code": "jnp.real(2j)"
      },
      {
        "description": "Demonstrates jnp.real with an array of complex numbers as input.",
        "code": "x = jnp.array([3 - 2j, 4 + 7j, -2j])\njnp.real(x)"
      },
      {
        "description": "Demonstrates jnp.real with a real number input.",
        "code": "jnp.real(5)"
      },
      {
        "description": "Demonstrates jnp.real with a purely imaginary number input.",
        "code": "jnp.real(2j)"
      },
      {
        "description": "Demonstrates jnp.real with an array of complex numbers as input.",
        "code": "x = jnp.array([3 - 2j, 4 + 7j, -2j])\njnp.real(x)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.reciprocal",
    "concepts": [
      "Calculates the element-wise reciprocal of the input.",
      "JAX implementation of numpy.reciprocal.",
      "The reciprocal is calculated as 1/x.",
      "The input can be an array or a scalar.",
      "Returns an array of the same shape as x containing the reciprocal of each element.",
      "Integer inputs are promoted to floating point in jnp.reciprocal."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.reciprocal usage",
    "concepts": [
      "Demonstrates the usage of jnp.reciprocal with scalar and array inputs.",
      "Shows how integers are promoted to floating point numbers.",
      "Illustrates the output for floating-point numbers, including infinity."
    ],
    "code_examples": [
      {
        "description": "Calculates the reciprocal of the integer 2.",
        "code": "jnp.reciprocal(2)"
      },
      {
        "description": "Calculates the reciprocal of 0.0, which results in infinity.",
        "code": "jnp.reciprocal(0.)"
      },
      {
        "description": "Calculates the reciprocal of a JAX array.",
        "code": "x = jnp.array([1, 5., 4.])\njnp.reciprocal(x)"
      },
      {
        "description": "Calculates the reciprocal of the integer 2.",
        "code": "jnp.reciprocal(2)"
      },
      {
        "description": "Calculates the reciprocal of 0.0, which results in infinity.",
        "code": "jnp.reciprocal(0.)"
      },
      {
        "description": "Calculates the reciprocal of a JAX array.",
        "code": "x = jnp.array([1, 5., 4.])\njnp.reciprocal(x)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.remainder",
    "concepts": [
      "jnp.remainder returns the element-wise remainder of the division.",
      "It's a JAX implementation of numpy.remainder.",
      "x1 is the dividend and x2 is the divisor.",
      "x1 and x2 should have the same shape or be broadcast compatible.",
      "The result has the same sign as the elements of x2.",
      "jnp.remainder is equivalent to x1 - x2 * jnp.floor(x1 / x2)."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.remainder",
    "concepts": [
      "Demonstrates the usage of jnp.remainder with jnp arrays.",
      "Shows the equivalence between jnp.remainder and x1 - x2 * jnp.floor(x1 / x2)."
    ],
    "code_examples": [
      {
        "description": "Calculating element-wise remainder using jnp.remainder and verifying with x1 - x2 * jnp.floor(x1 / x2).",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[3, -1, 4],\n              [8, 5, -2]])\nx2 = jnp.array([2, 3, -5])\n\nprint(jnp.remainder(x1, x2))\nprint(x1 - x2 * jnp.floor(x1 / x2))"
      },
      {
        "description": "Calculating element-wise remainder using jnp.remainder and verifying with x1 - x2 * jnp.floor(x1 / x2).",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[3, -1, 4],\n              [8, 5, -2]])\nx2 = jnp.array([2, 3, -5])\n\nprint(jnp.remainder(x1, x2))\nprint(x1 - x2 * jnp.floor(x1 / x2))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.repeat",
    "concepts": [
      "jnp.repeat() is the JAX implementation of numpy.repeat().",
      "It constructs an array by repeating elements from an input array.",
      "The input array 'a' can be N-dimensional.",
      "The 'repeats' argument is a 1D integer array specifying the number of repeats for each element along the given axis.",
      "The 'axis' argument specifies the axis along which to repeat. If None, the input array is flattened first.",
      "The 'total_repeat_length' argument is required for compatibility with JAX transformations like jit().",
      "jax.numpy.tile() repeats the entire array, unlike jnp.repeat() which repeats individual values."
    ],
    "code_examples": []
  },
  {
    "title": "Repeating Elements Along an Axis",
    "concepts": [
      "Elements of an array can be repeated along a specific axis.",
      "The 'axis' argument specifies which axis to repeat along."
    ],
    "code_examples": [
      {
        "description": "Repeat each value twice along the last axis.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\njnp.repeat(a, 2, axis=-1)"
      },
      {
        "description": "Repeat each value twice along the last axis.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\njnp.repeat(a, 2, axis=-1)"
      }
    ]
  },
  {
    "title": "Flattening the Input Array",
    "concepts": [
      "If the 'axis' argument is not specified, the input array is flattened before repeating elements."
    ],
    "code_examples": [
      {
        "description": "Repeat each value twice after flattening the array.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\njnp.repeat(a, 2)"
      },
      {
        "description": "Repeat each value twice after flattening the array.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\njnp.repeat(a, 2)"
      }
    ]
  },
  {
    "title": "Variable Repeats",
    "concepts": [
      "The 'repeats' argument can be an array to specify a different number of repetitions for each element along an axis."
    ],
    "code_examples": [
      {
        "description": "Repeat each value along axis 1 a different number of times, as specified by the 'repeats' array.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\nrepeats = jnp.array([2, 3])\njnp.repeat(a, repeats, axis=1)"
      },
      {
        "description": "Repeat each value along axis 1 a different number of times, as specified by the 'repeats' array.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\nrepeats = jnp.array([2, 3])\njnp.repeat(a, repeats, axis=1)"
      }
    ]
  },
  {
    "title": "Using jnp.repeat with JIT",
    "concepts": [
      "To use jnp.repeat within JIT-compiled functions, the size of the output must be known statically.",
      "The 'total_repeat_length' argument must be specified as a static argument to jit().",
      "If the sum of repeats exceeds 'total_repeat_length', the result is truncated.",
      "If the sum of repeats is less than 'total_repeat_length', the final value is repeated to fill the output."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating how to use jnp.repeat within a jitted function, specifying total_repeat_length.",
        "code": "jit_repeat = jax.jit(jnp.repeat, static_argnames=['axis', 'total_repeat_length'])\na = jnp.array([[1, 2],\n               [3, 4]])\nrepeats = jnp.array([2, 3])\njit_repeat(a, repeats, axis=1, total_repeat_length=5)"
      },
      {
        "description": "Example demonstrating how to use jnp.repeat within a jitted function, specifying total_repeat_length.",
        "code": "jit_repeat = jax.jit(jnp.repeat, static_argnames=['axis', 'total_repeat_length'])\na = jnp.array([[1, 2],\n               [3, 4]])\nrepeats = jnp.array([2, 3])\njit_repeat(a, repeats, axis=1, total_repeat_length=5)"
      },
      {
        "description": "Demonstrates truncation when total_repeat_length is smaller than the sum of repeats.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\nrepeats = jnp.array([2, 3])\njit_repeat = jax.jit(jnp.repeat, static_argnames=['axis', 'total_repeat_length'])\njit_repeat(a, repeats, axis=1, total_repeat_length=4)"
      },
      {
        "description": "Demonstrates truncation when total_repeat_length is smaller than the sum of repeats.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\nrepeats = jnp.array([2, 3])\njit_repeat = jax.jit(jnp.repeat, static_argnames=['axis', 'total_repeat_length'])\njit_repeat(a, repeats, axis=1, total_repeat_length=4)"
      },
      {
        "description": "Demonstrates padding with the final value when total_repeat_length is larger than the sum of repeats.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\nrepeats = jnp.array([2, 3])\njit_repeat = jax.jit(jnp.repeat, static_argnames=['axis', 'total_repeat_length'])\njit_repeat(a, repeats, axis=1, total_repeat_length=7)"
      },
      {
        "description": "Demonstrates padding with the final value when total_repeat_length is larger than the sum of repeats.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\nrepeats = jnp.array([2, 3])\njit_repeat = jax.jit(jnp.repeat, static_argnames=['axis', 'total_repeat_length'])\njit_repeat(a, repeats, axis=1, total_repeat_length=7)"
      }
    ]
  },
  {
    "title": "JAX numpy.resize() Overview",
    "concepts": [
      "Returns a new array with a specified shape.",
      "JAX implementation of numpy.resize().",
      "Elements of the input array are repeated if the resized array is larger than the original.",
      "The function takes an array-like input and a new shape as arguments.",
      "Related functions include jax.numpy.reshape() and jax.numpy.repeat()."
    ],
    "code_examples": []
  },
  {
    "title": "JAX numpy.resize() Examples",
    "concepts": [
      "Demonstrates resizing a 1D JAX array to a 3x3 array.",
      "Demonstrates resizing a 1D JAX array to a 3x4 array, showing element repetition.",
      "Demonstrates resizing a scalar value to a 3x2 array, filling it with the scalar value.",
      "Resizing an array repeats the elements of the original array to fill the new shape."
    ],
    "code_examples": [
      {
        "description": "Resizes a 1D array to a 3x3 array.",
        "code": "x = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\njnp.resize(x, (3, 3))"
      },
      {
        "description": "Resizes a 1D array to a 3x4 array, showcasing element repetition.",
        "code": "x = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\njnp.resize(x, (3, 4))"
      },
      {
        "description": "Resizes a scalar value to a 3x2 array, filling it with the scalar value.",
        "code": "jnp.resize(4, (3, 2))"
      },
      {
        "description": "Resizes a 1D array to a 3x3 array.",
        "code": "x = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\njnp.resize(x, (3, 3))"
      },
      {
        "description": "Resizes a 1D array to a 3x4 array, showcasing element repetition.",
        "code": "x = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\njnp.resize(x, (3, 4))"
      },
      {
        "description": "Resizes a scalar value to a 3x2 array, filling it with the scalar value.",
        "code": "jnp.resize(4, (3, 2))"
      }
    ]
  },
  {
    "title": "JAX Type Promotion with result_type",
    "concepts": [
      "JAX's `result_type` function mimics NumPy's `result_type`.",
      "It applies JAX promotion rules to determine the resulting dtype.",
      "Inputs can be dtype specifiers, scalars, or arrays.",
      "The result type can be affected by the `jax_enable_x64` configuration, potentially downcasting 64-bit types to 32-bit.",
      "JAX's dtype promotion behavior is described in Type promotion semantics."
    ],
    "code_examples": [
      {
        "description": "Demonstrates `jnp.result_type` with dtype specifiers.",
        "code": ">>> jnp.result_type('int32', 'float32')\ndtype('float32')\n>>> jnp.result_type(np.uint16, np.dtype('int32'))\ndtype('int32')\n>>> jnp.result_type('int32', 'float32')\ndtype('float32')\n>>> jnp.result_type(np.uint16, np.dtype('int32'))\ndtype('int32')"
      },
      {
        "description": "Demonstrates `jnp.result_type` with scalars and arrays.",
        "code": ">>> jnp.result_type(1.0, jnp.bfloat16(2))\ndtype(bfloat16)\n>>> jnp.result_type(jnp.arange(4), jnp.zeros(4))\ndtype('float32')\n>>> jnp.result_type(1.0, jnp.bfloat16(2))\ndtype(bfloat16)\n>>> jnp.result_type(jnp.arange(4), jnp.zeros(4))\ndtype('float32')"
      },
      {
        "description": "Demonstrates how `jax_enable_x64` affects the result type.",
        "code": ">>> jnp.result_type('float64')\ndtype('float32')\n>>> jnp.result_type('float64')\ndtype('float32')"
      }
    ]
  },
  {
    "title": "Description of jnp.right_shift",
    "concepts": [
      "The function `jnp.right_shift` shifts the bits of an input array `x1` to the right by the amount specified in another array `x2`.",
      "`x1` must be an array-like object with unsigned integer subtypes.",
      "`x2` must be an array-like object with integer subtypes.",
      "If the shapes of `x1` and `x2` are different, they must be compatible for broadcasting.",
      "Right shifting a scalar x1 by scalar x2 is equivalent to x1 // 2**x2."
    ],
    "code_examples": []
  },
  {
    "title": "Example 1: Right shifting an array by a scalar",
    "concepts": [
      "Demonstrates right shifting an array of integers by a scalar value using `jnp.right_shift`.",
      "Uses the `print_binary` utility function to display the binary representation of the input and output arrays.",
      "Illustrates the basic usage of `jnp.right_shift` with an array and a scalar."
    ],
    "code_examples": [
      {
        "description": "Defines a helper function to print the binary representation of an array.",
        "code": "def print_binary(x):\n    return [bin(int(val)) for val in x]"
      },
      {
        "description": "Demonstrates right shifting the elements of the array x1 by 1 bit.",
        "code": "x1 = jnp.array([1, 2, 4, 8])\nprint_binary(x1)\n\nx2 = 1\nresult = jnp.right_shift(x1, x2)\nprint(result)\nprint_binary(result)"
      }
    ]
  },
  {
    "title": "Example 2: Right shifting a scalar by an array",
    "concepts": [
      "Demonstrates right shifting a scalar value by an array of integers using `jnp.right_shift`.",
      "Shows how broadcasting works when one of the inputs is a scalar.",
      "Uses the `print_binary` utility function to display the binary representation of the input and output arrays."
    ],
    "code_examples": [
      {
        "description": "Defines a helper function to print the binary representation of an array.",
        "code": "def print_binary(x):\n    return [bin(int(val)) for val in x]"
      },
      {
        "description": "Demonstrates right shifting the scalar x1 by the elements of the array x2.",
        "code": "x1 = 16\nprint_binary([x1])\n\nx2 = jnp.array([1, 2, 3, 4])\nresult = jnp.right_shift(x1, x2)\nprint(result)\nprint_binary(result)"
      }
    ]
  },
  {
    "title": "Rounding to Nearest Integer with jnp.rint",
    "concepts": [
      "The function `jnp.rint` rounds elements of an array to the nearest integer.",
      "It is a JAX implementation of NumPy's `rint` function.",
      "The input is an array-like object.",
      "The output is an array containing the rounded elements of the input array.",
      "The output array is always promoted to an inexact data type (e.g., float32).",
      "If an element is exactly halfway between two integers, it rounds to the nearest even integer."
    ],
    "code_examples": [
      {
        "description": "Demonstrates rounding of integer array using `jnp.rint`.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([5, 4, 7])\nprint(jnp.rint(x1))"
      },
      {
        "description": "Demonstrates rounding of floating-point array using `jnp.rint`, including numbers exactly halfway between integers.",
        "code": "import jax.numpy as jnp\n\nx2 = jnp.array([-2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5])\nprint(jnp.rint(x2))"
      },
      {
        "description": "Demonstrates rounding of complex number array using `jnp.rint`.",
        "code": "import jax.numpy as jnp\n\nx3 = jnp.array([-2.5 + 3.5j, 4.5 - 0.5j])\nprint(jnp.rint(x3))"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.roll",
    "concepts": [
      "The function `jax.numpy.roll` rolls array elements along a specified axis.",
      "The input array `a` is rolled.",
      "The `shift` parameter determines the number of positions to roll.",
      "The `axis` parameter specifies the axis or axes to roll.",
      "If `axis` is None, the array is flattened, shifted, and reshaped.",
      "The function returns a copy of the array with elements rolled."
    ],
    "code_examples": []
  },
  {
    "title": "One-Dimensional Array Rolling",
    "concepts": [
      "Demonstrates rolling a one-dimensional array by a specified number of positions."
    ],
    "code_examples": [
      {
        "description": "Rolls the elements of a 1D array by 2 positions.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([0, 1, 2, 3, 4, 5])\n\njnp.roll(a, 2)"
      },
      {
        "description": "Rolls the elements of a 1D array by 2 positions (duplicate example).",
        "code": "import jax.numpy as jnp\n\na = jnp.array([0, 1, 2, 3, 4, 5])\n\njnp.roll(a, 2)"
      }
    ]
  },
  {
    "title": "Multi-Dimensional Array Rolling",
    "concepts": [
      "Demonstrates rolling elements along specific axes in a multi-dimensional array.",
      "Rolling can be performed along a single axis.",
      "Rolling can be performed along multiple axes with different shift values for each axis."
    ],
    "code_examples": [
      {
        "description": "Rolls the elements of a 2D array along axis 0 by 1 position.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[0, 1, 2, 3],\n               [4, 5, 6, 7],\n               [8, 9, 10, 11]])\n\njnp.roll(a, 1, axis=0)"
      },
      {
        "description": "Rolls the elements of a 2D array along axis 0 by 2 positions and along axis 1 by 3 positions.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[0, 1, 2, 3],\n               [4, 5, 6, 7],\n               [8, 9, 10, 11]])\n\njnp.roll(a, [2, 3], axis=[0, 1])"
      },
      {
        "description": "Rolls the elements of a 2D array along axis 0 by 1 position (duplicate example).",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[0, 1, 2, 3],\n               [4, 5, 6, 7],\n               [8, 9, 10, 11]])\n\njnp.roll(a, 1, axis=0)"
      },
      {
        "description": "Rolls the elements of a 2D array along axis 0 by 2 positions and along axis 1 by 3 positions (duplicate example).",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[0, 1, 2, 3],\n               [4, 5, 6, 7],\n               [8, 9, 10, 11]])\n\njnp.roll(a, [2, 3], axis=[0, 1])"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.rollaxis",
    "concepts": [
      "jax.numpy.rollaxis rolls the specified axis to a given position.",
      "It is a JAX implementation of numpy.rollaxis.",
      "jax.numpy.moveaxis() is a newer and more intuitive alternative.",
      "The function returns a copy of the array with the rolled axis.",
      "Copies may be optimized away under JIT."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.rollaxis usage",
    "concepts": [
      "Roll axis 2 to the start of the array.",
      "Roll axis 1 to the end of the array.",
      "Demonstration of equivalent operations using jax.numpy.moveaxis."
    ],
    "code_examples": [
      {
        "description": "Define a 4-dimensional array of ones using jax.numpy.",
        "code": "a = jnp.ones((2, 3, 4, 5))"
      },
      {
        "description": "Roll axis 2 to the start of the array using jax.numpy.rollaxis and check its shape.",
        "code": "jnp.rollaxis(a, 2).shape\n(4, 2, 3, 5)"
      },
      {
        "description": "Roll axis 1 to the end of the array using jax.numpy.rollaxis and check its shape.",
        "code": "jnp.rollaxis(a, 1, a.ndim).shape\n(2, 4, 5, 3)"
      },
      {
        "description": "Equivalent operations using jax.numpy.moveaxis. Roll axis 2 to the start using moveaxis.",
        "code": "jnp.moveaxis(a, 2, 0).shape\n(4, 2, 3, 5)"
      },
      {
        "description": "Equivalent operations using jax.numpy.moveaxis. Roll axis 1 to the end using moveaxis.",
        "code": "jnp.moveaxis(a, 1, -1).shape\n(2, 4, 5, 3)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.roots()",
    "concepts": [
      "jax.numpy.roots() finds the roots of a polynomial.",
      "The function takes an array of polynomial coefficients as input.",
      "The function returns an array containing the roots of the polynomial.",
      "The output is always a complex array, even for real roots.",
      "strip_zeros parameter controls the handling of leading zeros in coefficients."
    ],
    "code_examples": []
  },
  {
    "title": "Usage with strip_zeros = True (default)",
    "concepts": [
      "Leading zeros in the coefficient array are stripped by default.",
      "This behavior matches numpy.roots()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the default behavior of jnp.roots() with leading zeros being stripped.",
        "code": "coeffs = jnp.array([0, 1, 2])\njnp.roots(coeffs)"
      },
      {
        "description": "Demonstrates the default behavior of jnp.roots() with leading zeros being stripped.",
        "code": "coeffs = jnp.array([0, 1, 2])\njnp.roots(coeffs)"
      }
    ]
  },
  {
    "title": "Usage with strip_zeros = False",
    "concepts": [
      "Setting strip_zeros=False prevents leading zeros from being stripped.",
      "Extra roots corresponding to the leading zeros are represented as NaN values.",
      "strip_zeros=False is required for compatibility with jax.jit() and other JAX transformations."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the behavior of jnp.roots() with strip_zeros=False, showing NaN values for roots corresponding to leading zeros.",
        "code": "coeffs = jnp.array([0, 1, 2])\njnp.roots(coeffs, strip_zeros=False)"
      },
      {
        "description": "Demonstrates the behavior of jnp.roots() with strip_zeros=False, showing NaN values for roots corresponding to leading zeros.",
        "code": "coeffs = jnp.array([0, 1, 2])\njnp.roots(coeffs, strip_zeros=False)"
      }
    ]
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.poly() finds polynomial coefficients from roots.",
      "jax.numpy.polyfit() performs least squares polynomial fitting.",
      "jax.numpy.polyval() evaluates a polynomial at given values."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Array Rotation",
    "concepts": [
      "Array rotation by 90 degrees counterclockwise.",
      "JAX implementation of numpy.rot90().",
      "The 'm' parameter is the input array with ndim >= 2.",
      "The 'k' parameter specifies the number of rotations (default is 1). Negative 'k' rotates clockwise.",
      "The 'axes' parameter is a tuple of 2 integers defining the rotation plane (default is (0, 1)). Axes must be different."
    ],
    "code_examples": []
  },
  {
    "title": "Basic 2D Array Rotation Examples",
    "concepts": [
      "Rotating a 2D array by 90 degrees counterclockwise (default).",
      "Rotating a 2D array by 180 degrees counterclockwise (k=2)."
    ],
    "code_examples": [
      {
        "description": "Rotate a 2D array 'm' by 90 degrees counterclockwise (default k=1).",
        "code": "import jax.numpy as jnp\n\nm = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\nrotated_m = jnp.rot90(m)\nprint(rotated_m)"
      },
      {
        "description": "Rotate a 2D array 'm' by 180 degrees counterclockwise (k=2).",
        "code": "import jax.numpy as jnp\n\nm = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\nrotated_m = jnp.rot90(m, k=2)\nprint(rotated_m)"
      }
    ]
  },
  {
    "title": "Rotation with Specified Axes",
    "concepts": [
      "Rotating a 2D array with specified axes (1, 0).",
      "Equivalent rotations using positive and negative 'k' values with adjusted axes."
    ],
    "code_examples": [
      {
        "description": "Rotate a 2D array 'm' using specified axes (1, 0).",
        "code": "import jax.numpy as jnp\n\nm = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\nrotated_m = jnp.rot90(m, axes=(1, 0))\nprint(rotated_m)"
      },
      {
        "description": "Rotate a 2D array 'm' by -1 times with axes (0, 1).",
        "code": "import jax.numpy as jnp\n\nm = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\nrotated_m = jnp.rot90(m, k=-1, axes=(0, 1))\nprint(rotated_m)"
      }
    ]
  },
  {
    "title": "Rotation of Multi-Dimensional Arrays",
    "concepts": [
      "Rotating arrays with ndim > 2.",
      "Specifying the axes for rotation in higher-dimensional arrays."
    ],
    "code_examples": [
      {
        "description": "Rotate a 3D array 'm1' with k=1 and axes=(2, 1).",
        "code": "import jax.numpy as jnp\n\nm1 = jnp.array([[[1, 2, 3],\n                [4, 5, 6]],\n               [[7, 8, 9],\n                [10, 11, 12]]])\n\nrotated_m1 = jnp.rot90(m1, k=1, axes=(2, 1))\nprint(rotated_m1)"
      }
    ]
  },
  {
    "title": "Rounding with jnp.round",
    "concepts": [
      "jnp.round rounds input arrays evenly to a specified number of decimals.",
      "It is a JAX implementation of numpy.round().",
      "The 'decimals' argument specifies the number of decimal points for rounding.",
      "jnp.round rounds to the nearest even integer for values exactly halfway between rounded decimal values."
    ],
    "code_examples": [
      {
        "description": "Demonstrates rounding an array to the nearest integer.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1.532, 3.267, 6.149])\n\njnp.round(x)"
      },
      {
        "description": "Demonstrates rounding an array to two decimal places.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1.532, 3.267, 6.149])\n\njnp.round(x, decimals=2)"
      },
      {
        "description": "Demonstrates rounding an array to the nearest integer, showing the even rounding behavior.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([10.5, 21.5, 12.5, 31.5])\n\njnp.round(x1)"
      }
    ]
  },
  {
    "title": "Introduction to IndexExpression",
    "concepts": [
      "IndexExpression provides a way to build index tuples for arrays.",
      "It simplifies array indexing, slicing, and axis insertion.",
      "Use predefined instances `index_exp` or `s_` instead of directly using `IndexExpression`.",
      "a[indices] is the same as a[np.index_exp[indices]] for any array a.",
      "np.index_exp[indices] returns a tuple of slice objects.",
      "The `maketuple` parameter controls whether a tuple is always returned."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of np.s_ to create a slice object.",
        "code": "import numpy as np\nnp.s_[2::2]"
      },
      {
        "description": "Demonstrates the use of np.index_exp to create a tuple containing a slice object.",
        "code": "import numpy as np\nnp.index_exp[2::2]"
      }
    ]
  },
  {
    "title": "Examples of Indexing with s_",
    "concepts": [
      "np.s_ can be used for slicing arrays.",
      "np.s_ creates slice objects.",
      "The slice objects can be used to extract array elements.",
      "np.s_ simplifies array indexing."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of np.s_ for array slicing.",
        "code": "import numpy as np\nnp.array([0, 1, 2, 3, 4])[np.s_[2::2]]"
      }
    ]
  },
  {
    "title": "Overview of savez Function",
    "concepts": [
      "The savez function saves multiple arrays into a single file in uncompressed .npz format.",
      "Arrays can be saved using positional arguments (named arr_0, arr_1, etc.) or keyword arguments (using the provided name).",
      "The file argument can be a filename (string) or an open file-like object.",
      "The .npz format is a ZIP archive of files, each containing one variable in .npy format.",
      "The allow_pickle argument determines whether object arrays can be saved using Python pickles.",
      "Loading a .npz file returns a dictionary-like NpzFile object.",
      "Keys passed as keyword arguments are used as filenames inside the ZIP archive.",
      "It is not possible to name a variable 'file' when using keyword arguments."
    ],
    "code_examples": []
  },
  {
    "title": "Saving Arrays with Positional Arguments",
    "concepts": [
      "Arrays passed as positional arguments are saved with default names: arr_0, arr_1, etc.",
      "The np.load function is used to load the .npz file.",
      "The .files attribute of the loaded object lists the names of the saved arrays.",
      "Arrays are accessed from the loaded object using their names as keys."
    ],
    "code_examples": [
      {
        "description": "Saving arrays x and y using positional arguments and loading them.",
        "code": "import numpy as np\nfrom tempfile import TemporaryFile\n\noutfile = TemporaryFile()\nx = np.arange(10)\ny = np.sin(x)\n\nnp.savez(outfile, x, y)\n\n_ = outfile.seek(0)  # Only needed to simulate closing & reopening file\nnpzfile = np.load(outfile)\n\nprint(npzfile.files)\nprint(npzfile['arr_0'])"
      },
      {
        "description": "Saving arrays x and y using positional arguments and loading them - duplicated example",
        "code": "import numpy as np\nfrom tempfile import TemporaryFile\n\noutfile = TemporaryFile()\nx = np.arange(10)\ny = np.sin(x)\n\nnp.savez(outfile, x, y)\n\n_ = outfile.seek(0)  # Only needed to simulate closing & reopening file\nnpzfile = np.load(outfile)\n\nprint(npzfile.files)\nprint(npzfile['arr_0'])"
      }
    ]
  },
  {
    "title": "Saving Arrays with Keyword Arguments",
    "concepts": [
      "Arrays passed as keyword arguments are saved with the names provided.",
      "The .files attribute of the loaded object lists the names of the saved arrays.",
      "Arrays are accessed from the loaded object using their names as keys."
    ],
    "code_examples": [
      {
        "description": "Saving arrays x and y using keyword arguments and loading them.",
        "code": "import numpy as np\nfrom tempfile import TemporaryFile\n\noutfile = TemporaryFile()\nx = np.arange(10)\ny = np.sin(x)\n\nnp.savez(outfile, x=x, y=y)\n\n_ = outfile.seek(0)\nnpzfile = np.load(outfile)\n\nprint(sorted(npzfile.files))\nprint(npzfile['x'])"
      },
      {
        "description": "Saving arrays x and y using keyword arguments and loading them - duplicated example.",
        "code": "import numpy as np\nfrom tempfile import TemporaryFile\n\noutfile = TemporaryFile()\nx = np.arange(10)\ny = np.sin(x)\n\nnp.savez(outfile, x=x, y=y)\n\n_ = outfile.seek(0)\nnpzfile = np.load(outfile)\n\nprint(sorted(npzfile.files))\nprint(npzfile['x'])"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.searchsorted",
    "concepts": [
      "The function performs a binary search in a sorted array.",
      "It is a JAX implementation of numpy.searchsorted().",
      "It returns indices where values can be inserted to maintain sort order.",
      "The input array 'a' is assumed to be sorted.",
      "The input array 'v' contains the values to be inserted.",
      "The 'side' argument specifies insertion to the left or right in case of ties.",
      "The 'sorter' argument specifies the sort order of a.",
      "The 'method' argument controls the algorithm used to compute the insertion indices.",
      "The return value is an array of insertion indices of shape v.shape."
    ],
    "code_examples": []
  },
  {
    "title": "Searching for a Single Value",
    "concepts": [
      "Demonstrates using jax.numpy.searchsorted to find the index for inserting a single value into a sorted array.",
      "Uses the 'side' argument to specify the insertion position in case of ties."
    ],
    "code_examples": [
      {
        "description": "Find the index to insert the value '2' into the sorted array 'a' on the left side.",
        "code": "a = jnp.array([1, 2, 2, 3, 4, 5, 5])\njnp.searchsorted(a, 2)"
      },
      {
        "description": "Find the index to insert the value '2' into the sorted array 'a' on the right side.",
        "code": "a = jnp.array([1, 2, 2, 3, 4, 5, 5])\njnp.searchsorted(a, 2, side='right')"
      }
    ]
  },
  {
    "title": "Searching for a Batch of Values",
    "concepts": [
      "Demonstrates using jax.numpy.searchsorted to find the insertion indices for a batch of values."
    ],
    "code_examples": [
      {
        "description": "Find the indices to insert the batch of values 'vals' into the sorted array 'a'.",
        "code": "a = jnp.array([1, 2, 2, 3, 4, 5, 5])\nvals = jnp.array([0, 3, 8, 1.5, 2])\njnp.searchsorted(a, vals)"
      }
    ]
  },
  {
    "title": "Using the 'sorter' Argument",
    "concepts": [
      "Demonstrates using the 'sorter' argument to find insertion indices into an array sorted via jax.numpy.argsort().",
      "The result is equivalent to passing the sorted array directly to jax.numpy.searchsorted()."
    ],
    "code_examples": [
      {
        "description": "Find the indices to insert the batch of values 'vals' into the array 'a' using a precomputed sorter.",
        "code": "a = jnp.array([4, 3, 5, 1, 2])\nvals = jnp.array([0, 3, 8, 1.5, 2])\nsorter = jnp.argsort(a)\njnp.searchsorted(a, vals, sorter=sorter)"
      },
      {
        "description": "Demonstrates that using the sorter argument is equivalent to sorting the array first.",
        "code": "a = jnp.array([4, 3, 5, 1, 2])\nvals = jnp.array([0, 3, 8, 1.5, 2])\njnp.searchsorted(jnp.sort(a), vals)"
      }
    ]
  },
  {
    "title": "JAX Implementation of numpy.select()",
    "concepts": [
      "Selects values based on a series of conditions.",
      "Implemented using jax.lax.select_n().",
      "Requires a sequence of boolean conditions (condlist).",
      "Requires a sequence of values to choose from (choicelist).",
      "A default value is returned if all conditions are false.",
      "Returns an array of selected values."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of jnp.select()",
    "concepts": [
      "Demonstrates how to use jnp.select() with multiple conditions and choices.",
      "Shows how the default value is used when no condition is met."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of jnp.select() function. It selects values from `choicelist` based on the corresponding `condlist` being True. If no condition is True, the `default` value is selected.",
        "code": "condlist = [\n    jnp.array([False, True, False, False]),\n    jnp.array([True, False, False, False]),\n    jnp.array([False, True, True, False]),\n]\nchoicelist = [\n    jnp.array([1, 2, 3, 4]),\n    jnp.array([10, 20, 30, 40]),\n    jnp.array([100, 200, 300, 400]),\n]\njnp.select(condlist, choicelist, default=0)"
      },
      {
        "description": "Demonstrates the usage of jnp.select() function. It selects values from `choicelist` based on the corresponding `condlist` being True. If no condition is True, the `default` value is selected.",
        "code": "condlist = [\n    jnp.array([False, True, False, False]),\n    jnp.array([True, False, False, False]),\n    jnp.array([False, True, True, False]),\n]\nchoicelist = [\n    jnp.array([1, 2, 3, 4]),\n    jnp.array([10, 20, 30, 40]),\n    jnp.array([100, 200, 300, 400]),\n]\njnp.select(condlist, choicelist, default=0)"
      }
    ]
  },
  {
    "title": "Equivalence to Nested jnp.where()",
    "concepts": [
      "jnp.select() is logically equivalent to nested jnp.where() calls.",
      "Demonstrates how to achieve the same result using nested jnp.where()."
    ],
    "code_examples": [
      {
        "description": "Shows that the jnp.select() is equivalent to a nested jnp.where() statement.",
        "code": "default = 0\njnp.where(\n    condlist[0],\n    choicelist[0],\n    jnp.where(\n        condlist[1],\n        choicelist[1],\n        jnp.where(\n            condlist[2],\n            choicelist[2],\n            default\n        )\n    )\n)"
      },
      {
        "description": "Shows that the jnp.select() is equivalent to a nested jnp.where() statement.",
        "code": "default = 0\njnp.where(\n    condlist[0],\n    choicelist[0],\n    jnp.where(\n        condlist[1],\n        choicelist[1],\n        jnp.where(\n            condlist[2],\n            choicelist[2],\n            default\n        )\n    )\n)"
      }
    ]
  },
  {
    "title": "Alias of numpy.set_printoptions()",
    "concepts": [
      "This is an alias for numpy.set_printoptions().",
      "JAX arrays are printed using NumPy.",
      "NumPy's print options apply to JAX arrays.",
      "See numpy.set_printoptions() documentation for details on available options and meanings."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jnp.setdiff1d",
    "concepts": [
      "jnp.setdiff1d computes the set difference of two 1D arrays, similar to numpy.setdiff1d.",
      "The output size is data-dependent, making it incompatible with JAX transformations like jit() by default.",
      "The JAX version requires a static 'size' argument for use with jit()."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.setdiff1d",
    "concepts": [
      "Demonstrates how to compute the set difference of two arrays using jnp.setdiff1d."
    ],
    "code_examples": [
      {
        "description": "Computes the set difference between two arrays.",
        "code": "ar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\njnp.setdiff1d(ar1, ar2)"
      },
      {
        "description": "Computes the set difference between two arrays.",
        "code": "ar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\njnp.setdiff1d(ar1, ar2)"
      }
    ]
  },
  {
    "title": "Limitations with JIT Compilation",
    "concepts": [
      "jnp.setdiff1d is incompatible with jit() without specifying the size argument.",
      "The dynamic output shape causes a ConcretizationTypeError during JIT compilation."
    ],
    "code_examples": [
      {
        "description": "Shows that jnp.setdiff1d fails under jax.jit() because the output shape is dynamic.",
        "code": "jax.jit(jnp.setdiff1d)(ar1, ar2)"
      },
      {
        "description": "Shows that jnp.setdiff1d fails under jax.jit() because the output shape is dynamic.",
        "code": "jax.jit(jnp.setdiff1d)(ar1, ar2)"
      }
    ]
  },
  {
    "title": "Using jnp.setdiff1d with JIT and Static Size",
    "concepts": [
      "The 'size' argument allows jnp.setdiff1d to be used with jit() by providing a static output shape.",
      "If size is smaller than the actual difference, the output is truncated.",
      "If size is larger, the output is padded with fill_value."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of jnp.setdiff1d with jax.jit() and a static size argument.",
        "code": "jit_setdiff1d = jax.jit(jnp.setdiff1d, static_argnames=['size'])\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njit_setdiff1d(ar1, ar2, size=2)"
      },
      {
        "description": "Demonstrates the usage of jnp.setdiff1d with jax.jit() and a static size argument.",
        "code": "jit_setdiff1d = jax.jit(jnp.setdiff1d, static_argnames=['size'])\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njit_setdiff1d(ar1, ar2, size=2)"
      },
      {
        "description": "Shows how the output is truncated if the specified size is too small.",
        "code": "jit_setdiff1d(ar1, ar2, size=1)"
      },
      {
        "description": "Shows how the output is truncated if the specified size is too small.",
        "code": "jit_setdiff1d(ar1, ar2, size=1)"
      },
      {
        "description": "Shows how the output is padded with fill_value when the specified size is larger than needed.",
        "code": "jit_setdiff1d(ar1, ar2, size=4, fill_value=0)"
      },
      {
        "description": "Shows how the output is padded with fill_value when the specified size is larger than needed.",
        "code": "jit_setdiff1d(ar1, ar2, size=4, fill_value=0)"
      }
    ]
  },
  {
    "title": "Introduction to setxor1d",
    "concepts": [
      "Computes the set-wise XOR of elements in two arrays.",
      "JAX implementation of numpy.setxor1d().",
      "Not compatible with JIT or other JAX transformations because the output size is data-dependent."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of setxor1d",
    "concepts": [
      "ar1: First array of values.",
      "ar2: Second array of values.",
      "assume_unique: If True, assumes input arrays contain unique values for more efficient implementation.",
      "size: If specified, return only the first 'size' sorted elements; pad with fill_value if fewer elements exist.",
      "fill_value: Value to pad with when size is specified and there are fewer elements than indicated."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value and Related Functions",
    "concepts": [
      "Returns an array of values found in exactly one of the input arrays.",
      "See also: jax.numpy.intersect1d(), jax.numpy.union1d(), jax.numpy.setdiff1d()"
    ],
    "code_examples": []
  },
  {
    "title": "Examples of setxor1d usage",
    "concepts": [
      "Demonstrates basic usage of jnp.setxor1d with two sample arrays."
    ],
    "code_examples": [
      {
        "description": "Compute the set-wise XOR of two arrays.",
        "code": "import jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njnp.setxor1d(ar1, ar2)"
      },
      {
        "description": "Another example demonstrating set-wise XOR of two arrays.",
        "code": "import jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njnp.setxor1d(ar1, ar2)"
      }
    ]
  },
  {
    "title": "Array Shape in JAX",
    "concepts": [
      "JAX implementation of numpy.shape().",
      "Raises TypeError for lists or tuples.",
      "Works for arrays and scalars.",
      "Shape can be accessed via jax.Array.shape property."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to get the shape of a 1D JAX array.",
        "code": "x = jnp.arange(10)\njnp.shape(x)"
      },
      {
        "description": "Demonstrates how to get the shape of a 2D JAX array.",
        "code": "y = jnp.ones((2, 3))\njnp.shape(y)"
      },
      {
        "description": "Demonstrates how to get the shape of a scalar in JAX.",
        "code": "jnp.shape(3.14)"
      },
      {
        "description": "Demonstrates how to access shape using the .shape property.",
        "code": "x = jnp.arange(10)\nx.shape"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.sign",
    "concepts": [
      "The function returns an element-wise indication of the sign of the input.",
      "For real-valued inputs, sign(x) is 1 if x > 0, 0 if x = 0, and -1 if x < 0.",
      "For complex-valued inputs, sign(x) returns a unit vector representing the phase, which is x/abs(x) if x != 0, and 0 if x = 0."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Real-valued inputs",
    "concepts": [
      "Demonstrates the usage of jnp.sign with a real-valued JAX array.",
      "The output array contains the sign of each element in the input array."
    ],
    "code_examples": [
      {
        "description": "Computes the sign of a real-valued JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([0., -3., 7.])\n\njnp.sign(x)"
      },
      {
        "description": "Computes the sign of a real-valued JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([0., -3., 7.])\n\njnp.sign(x)"
      }
    ]
  },
  {
    "title": "Examples with Complex-valued inputs",
    "concepts": [
      "Demonstrates the usage of jnp.sign with a complex-valued JAX array.",
      "The output array contains the complex sign of each element in the input array."
    ],
    "code_examples": [
      {
        "description": "Computes the sign of a complex-valued JAX array.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([1, 3 + 4j, 5j])\n\njnp.sign(x1)"
      },
      {
        "description": "Computes the sign of a complex-valued JAX array.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([1, 3 + 4j, 5j])\n\njnp.sign(x1)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.signbit",
    "concepts": [
      "Returns the sign bit of array elements.",
      "JAX implementation of numpy.signbit.",
      "Input array cannot contain complex values.",
      "Returns a boolean array indicating where the sign of x is negative."
    ],
    "code_examples": []
  },
  {
    "title": "Signbit with Boolean Values",
    "concepts": [
      "signbit() on boolean values is always False."
    ],
    "code_examples": [
      {
        "description": "Demonstrates signbit() with boolean array input.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([True, False])\nprint(jnp.signbit(x))"
      }
    ]
  },
  {
    "title": "Signbit with Integer Values",
    "concepts": [
      "signbit() on integer values is equivalent to x < 0."
    ],
    "code_examples": [
      {
        "description": "Demonstrates signbit() with integer array input.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([-2, -1, 0, 1, 2])\nprint(jnp.signbit(x))"
      }
    ]
  },
  {
    "title": "Signbit with Floating-Point Values",
    "concepts": [
      "signbit() on floating point values returns the actual sign bit.",
      "Includes signed zero."
    ],
    "code_examples": [
      {
        "description": "Demonstrates signbit() with floating-point array input, including negative zero.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([-1.5, -0.0, 0.0, 1.5])\nprint(jnp.signbit(x))"
      }
    ]
  },
  {
    "title": "Signbit with Special Values (NaN and Infinity)",
    "concepts": [
      "Returns the sign bit for special values such as signed NaN and signed infinity."
    ],
    "code_examples": [
      {
        "description": "Demonstrates signbit() with NaN and Infinity values.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([jnp.nan, -jnp.nan, jnp.inf, -jnp.inf])\nprint(jnp.signbit(x))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.sin",
    "concepts": [
      "Computes the trigonometric sine of each element of the input array.",
      "It is a JAX implementation of numpy.sin.",
      "The input 'x' is an array or scalar representing the angle in radians.",
      "The output is an array containing the sine of each element in 'x', promoting to inexact dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of jax.numpy.sin",
    "concepts": [
      "Demonstrates how to use jax.numpy.sin with a JAX array.",
      "Uses jnp.printoptions to control the output format (precision and suppression of scientific notation)."
    ],
    "code_examples": [
      {
        "description": "Computes the sine of an array of angles and prints the result with specified precision.",
        "code": "pi = jnp.pi\nx = jnp.array([pi / 4, pi / 2, 3 * pi / 4, pi])\nwith jnp.printoptions(precision=3, suppress=True):\n  print(jnp.sin(x))"
      },
      {
        "description": "Computes the sine of an array of angles and prints the result with specified precision.",
        "code": "pi = jnp.pi\nx = jnp.array([pi / 4, pi / 2, 3 * pi / 4, pi])\nwith jnp.printoptions(precision=3, suppress=True):\n  print(jnp.sin(x))"
      }
    ]
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.cos(): Computes a trigonometric cosine of each element of input.",
      "jax.numpy.tan(): Computes a trigonometric tangent of each element of input.",
      "jax.numpy.arcsin() and jax.numpy.asin(): Computes the inverse of trigonometric sine of each element of input."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to the Normalized Sinc Function",
    "concepts": [
      "The normalized sinc function is defined as sin(\u03c0x) / (\u03c0x).",
      "sinc(0) is defined as 1, the limit value.",
      "The sinc function is smooth and infinitely differentiable.",
      "The input array will be promoted to an inexact type."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Implementation and Examples",
    "concepts": [
      "JAX provides an implementation of the sinc function.",
      "The jnp.sinc() function handles the case where x=0 correctly.",
      "The example shows how to use jnp.sinc() with a JAX array.",
      "Comparison with a naive approach (sin(pi*x) / (pi*x)) demonstrates how jnp.sinc() avoids NaN at x=0."
    ],
    "code_examples": [
      {
        "description": "Example usage of jnp.sinc with a JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([-1, -0.5, 0, 0.5, 1])\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.sinc(x))"
      },
      {
        "description": "Naive sinc implementation showing NaN at x=0.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([-1, -0.5, 0, 0.5, 1])\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.sin(jnp.pi * x) / (jnp.pi * x))"
      }
    ]
  },
  {
    "title": "Gradient of the Sinc Function",
    "concepts": [
      "JAX defines a custom gradient rule for the sinc function.",
      "The custom gradient allows accurate evaluation of derivatives at zero.",
      "The example demonstrates calculating higher-order derivatives of sinc at 0 using jax.grad."
    ],
    "code_examples": [
      {
        "description": "Calculating higher-order derivatives of the sinc function at x=0 using jax.grad.",
        "code": "import jax\nimport jax.numpy as jnp\n\nf = jnp.sinc\nfor i in range(1, 6):\n    f = jax.grad(f)\n    print(f\"(d/dx)^{{i}} f(0.0) = {f(0.0):.2f}\")"
      }
    ]
  },
  {
    "title": "Alias of float32",
    "concepts": [
      "float32 is being aliased to another name."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.sort()",
    "concepts": [
      "The function returns a sorted copy of an array.",
      "It is a JAX implementation of numpy.sort().",
      "The axis parameter specifies the axis along which to sort.",
      "If axis is None, the array is flattened before sorting.",
      "The stable parameter specifies whether a stable sort should be used.",
      "The descending parameter specifies whether to sort in descending order.",
      "The kind and order parameters are deprecated or unsupported.",
      "The function returns a sorted array."
    ],
    "code_examples": []
  },
  {
    "title": "Simple 1-dimensional sort",
    "concepts": [
      "Demonstrates sorting a 1-dimensional JAX array.",
      "The jnp.sort() function is used to sort the array."
    ],
    "code_examples": [
      {
        "description": "Sorts a 1D JAX array in ascending order.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 3, 5, 4, 2, 1])\njnp.sort(x)"
      },
      {
        "description": "Sorts a 1D JAX array in ascending order (repeated example).",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 3, 5, 4, 2, 1])\njnp.sort(x)"
      }
    ]
  },
  {
    "title": "Sort along the last axis of an array",
    "concepts": [
      "Demonstrates sorting along a specific axis of a multi-dimensional JAX array.",
      "The axis parameter is used to specify the axis for sorting."
    ],
    "code_examples": [
      {
        "description": "Sorts a 2D JAX array along the last axis (axis=1).",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[2, 1, 3],\n               [4, 3, 6]])\njnp.sort(x, axis=1)"
      },
      {
        "description": "Sorts a 2D JAX array along the last axis (axis=1) - repeated example.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[2, 1, 3],\n               [4, 3, 6]])\njnp.sort(x, axis=1)"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.numpy.argsort(): return indices of sorted values.",
      "jax.numpy.lexsort(): lexicographical sort of multiple arrays.",
      "jax.lax.sort(): lower-level function wrapping XLA\u2019s Sort operator."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.sort_complex",
    "concepts": [
      "Returns a sorted copy of a complex array.",
      "JAX implementation of numpy.sort_complex().",
      "Complex numbers are sorted lexicographically (real part first, then imaginary part).",
      "Input array can be of type Array, ndarray, bool, number, int, float, or complex.",
      "If the input dtype is not complex, it is upcast to complex.",
      "The output is a sorted array of the same shape and complex dtype as the input.",
      "For multi-dimensional arrays, sorting is done along the last axis."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.sort_complex with 1D arrays",
    "concepts": [
      "Demonstrates sorting a 1D complex array using jnp.sort_complex().",
      "Shows the lexicographical sorting of complex numbers."
    ],
    "code_examples": [
      {
        "description": "Sorts a 1D complex array.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([1+2j, 2+4j, 3-1j, 2+3j])\njnp.sort_complex(a)"
      },
      {
        "description": "Sorts a 1D complex array.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([1+2j, 2+4j, 3-1j, 2+3j])\njnp.sort_complex(a)"
      }
    ]
  },
  {
    "title": "Examples of jax.numpy.sort_complex with multi-dimensional arrays",
    "concepts": [
      "Demonstrates sorting a multi-dimensional array using jnp.sort_complex().",
      "Shows that sorting is performed along the last axis."
    ],
    "code_examples": [
      {
        "description": "Sorts a 2D array, casting it to complex and sorting along the last axis.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[5, 3, 4],\n              [6, 9, 2]])\njnp.sort_complex(a)"
      },
      {
        "description": "Sorts a 2D array, casting it to complex and sorting along the last axis.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[5, 3, 4],\n              [6, 9, 2]])\njnp.sort_complex(a)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.spacing",
    "concepts": [
      "Calculates the spacing between a number and the next adjacent representable number.",
      "The input array can be real-valued, and integer/boolean types will be cast to float.",
      "The function returns an array of the same shape as the input, containing the spacing for each element.",
      "jax.numpy.spacing() is a JAX implementation of numpy.spacing()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of jnp.spacing() with a float32 array.",
        "code": "x = jnp.array([0.0, 0.25, 0.5, 0.75, 1.0], dtype='float32')\njnp.spacing(x)"
      },
      {
        "description": "Demonstrates the use of jnp.spacing() with a float32 array.",
        "code": "x = jnp.array([0.0, 0.25, 0.5, 0.75, 1.0], dtype='float32')\njnp.spacing(x)"
      },
      {
        "description": "Shows that the spacing of 1.0 is equal to the epsilon value provided by jax.numpy.finfo for float32.",
        "code": "x = jnp.float32(1)\njnp.spacing(x) == jnp.finfo(x.dtype).eps"
      },
      {
        "description": "Shows that the spacing of 1.0 is equal to the epsilon value provided by jax.numpy.finfo for float32.",
        "code": "x = jnp.float32(1)\njnp.spacing(x) == jnp.finfo(x.dtype).eps"
      }
    ]
  },
  {
    "title": "Overview of jnp.sqrt",
    "concepts": [
      "Calculates the element-wise non-negative square root of an input array.",
      "JAX implementation of numpy.sqrt.",
      "For real-valued negative inputs, it produces a NaN output.",
      "For complex-valued negative inputs, it produces a complex output."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.sqrt usage",
    "concepts": [
      "Demonstrates calculating square roots of complex and real numbers.",
      "Shows the output for negative real numbers (NaN).",
      "Uses jnp.printoptions for formatted output."
    ],
    "code_examples": [
      {
        "description": "Calculates the square root of complex and real numbers using jnp.sqrt, then prints the result with specified precision and suppression of scientific notation.",
        "code": "x = jnp.array([-8 - 6j, 1j, 4])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.sqrt(x)"
      },
      {
        "description": "Calculates the square root of -1, which results in NaN due to the input being a real-valued negative number.",
        "code": "jnp.sqrt(-1)"
      },
      {
        "description": "Calculates the square root of complex and real numbers using jnp.sqrt, then prints the result with specified precision and suppression of scientific notation.",
        "code": "x = jnp.array([-8 - 6j, 1j, 4])\nwith jnp.printoptions(precision=3, suppress=True):\n    jnp.sqrt(x)"
      },
      {
        "description": "Calculates the square root of -1, which results in NaN due to the input being a real-valued negative number.",
        "code": "jnp.sqrt(-1)"
      }
    ]
  },
  {
    "title": "Description of jnp.square",
    "concepts": [
      "Calculates the element-wise square of the input array.",
      "JAX implementation of numpy.square.",
      "Equivalent to computing jnp.power(x, 2)."
    ],
    "code_examples": []
  },
  {
    "title": "Real-valued array example",
    "concepts": [
      "Demonstrates the use of jnp.square with a real-valued JAX array.",
      "Shows that jnp.square and jnp.power with exponent 2 produce the same result."
    ],
    "code_examples": [
      {
        "description": "Calculates the square of elements in a real-valued JAX array using jnp.square and jnp.power.",
        "code": "x = jnp.array([3, -2, 5.3, 1])\nprint(jnp.square(x))\nprint(jnp.power(x, 2))"
      },
      {
        "description": "Calculates the square of elements in a real-valued JAX array using jnp.square and jnp.power.",
        "code": "x = jnp.array([3, -2, 5.3, 1])\nprint(jnp.square(x))\nprint(jnp.power(x, 2))"
      }
    ]
  },
  {
    "title": "Integer array example",
    "concepts": [
      "Demonstrates the use of jnp.square with an integer-valued JAX array."
    ],
    "code_examples": [
      {
        "description": "Calculates the square of elements in an integer-valued JAX array using jnp.square.",
        "code": "x1 = jnp.array([2, 4, 5, 6])\nprint(jnp.square(x1))"
      },
      {
        "description": "Calculates the square of elements in an integer-valued JAX array using jnp.square.",
        "code": "x1 = jnp.array([2, 4, 5, 6])\nprint(jnp.square(x1))"
      }
    ]
  },
  {
    "title": "Complex-valued array example",
    "concepts": [
      "Demonstrates the use of jnp.square with a complex-valued JAX array."
    ],
    "code_examples": [
      {
        "description": "Calculates the square of elements in a complex-valued JAX array using jnp.square.",
        "code": "x2 = jnp.array([1-3j, -1j, 2])\nprint(jnp.square(x2))"
      },
      {
        "description": "Calculates the square of elements in a complex-valued JAX array using jnp.square.",
        "code": "x2 = jnp.array([1-3j, -1j, 2])\nprint(jnp.square(x2))"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.squeeze",
    "concepts": [
      "Removes axes of length one from an array.",
      "It is a JAX implementation of numpy.squeeze.",
      "Implemented via jax.lax.squeeze().",
      "Returns a copy of the array, not a view.",
      "The compiler can optimize away copies under JIT."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters and Return Value",
    "concepts": [
      "The 'a' parameter is the input array (ArrayLike).",
      "The 'axis' parameter specifies the axes to remove; if None, all length-1 axes are squeezed.",
      "An error is raised if a specified axis does not have a length of 1.",
      "Returns a copy of the array with length-1 axes removed."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.expand_dims() is the inverse of squeeze, adding dimensions of length 1.",
      "jax.Array.squeeze() provides equivalent functionality as an array method.",
      "jax.lax.squeeze() is the equivalent XLA API.",
      "jax.numpy.ravel() flattens an array into a 1D shape.",
      "jax.numpy.reshape() provides general array reshaping capabilities."
    ],
    "code_examples": []
  },
  {
    "title": "Squeezing Length-1 Dimensions",
    "concepts": [
      "Demonstrates squeezing all length-1 dimensions from an array.",
      "Shows how to explicitly specify the axes to squeeze."
    ],
    "code_examples": [
      {
        "description": "Defines a 3x1x1 JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[0]], [[1]], [[2]]])\nprint(x.shape)"
      },
      {
        "description": "Squeezes all length-1 dimensions from the array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[0]], [[1]], [[2]]])\nprint(jnp.squeeze(x))\nprint(jnp.squeeze(x).shape)"
      },
      {
        "description": "Squeezes specified axes (1 and 2) from the array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[0]], [[1]], [[2]]])\nprint(jnp.squeeze(x, axis=(1, 2)))"
      }
    ]
  },
  {
    "title": "Error Handling",
    "concepts": [
      "Illustrates the ValueError that is raised when attempting to squeeze a non-unit axis."
    ],
    "code_examples": [
      {
        "description": "Attempts to squeeze axis 0, which has a size of 3, resulting in a ValueError.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[0]], [[1]], [[2]]])\n\n# This will raise a ValueError\n# jnp.squeeze(x, axis=0)"
      }
    ]
  },
  {
    "title": "Array Method Squeeze",
    "concepts": [
      "Demonstrates the usage of the squeeze() method available directly on jax.Array objects."
    ],
    "code_examples": [
      {
        "description": "Uses the squeeze() method on a jax.Array to remove length-1 dimensions.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[0]], [[1]], [[2]]])\nprint(x.squeeze())"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.stack",
    "concepts": [
      "jax.numpy.stack() joins arrays along a new axis.",
      "It is a JAX implementation of numpy.stack().",
      "The input is a sequence of arrays with the same shape.",
      "The axis parameter specifies the axis along which to stack.",
      "The dtype of the resulting array can be specified.",
      "jax.numpy.unstack() is the inverse of stack().",
      "It relates to jax.numpy.concatenate() as well as vertical, horizontal and depth-wise stacking methods."
    ],
    "code_examples": []
  },
  {
    "title": "Stacking Arrays Example",
    "concepts": [
      "Demonstrates stacking two arrays along the default axis (0).",
      "Demonstrates stacking two arrays along axis 1."
    ],
    "code_examples": [
      {
        "description": "Stacking two arrays along the default axis (axis=0).",
        "code": "x = jnp.array([1, 2, 3])\ny = jnp.array([4, 5, 6])\njnp.stack([x, y])"
      },
      {
        "description": "Stacking two arrays along axis 1.",
        "code": "x = jnp.array([1, 2, 3])\ny = jnp.array([4, 5, 6])\njnp.stack([x, y], axis=1)"
      }
    ]
  },
  {
    "title": "Unstacking Arrays Example",
    "concepts": [
      "Demonstrates using jax.numpy.unstack() as the inverse operation of jax.numpy.stack().",
      "Shows how to unstack an array along a specific axis."
    ],
    "code_examples": [
      {
        "description": "Shows how to unstack an array along axis 1 after it has been stacked along the same axis.",
        "code": "x = jnp.array([1, 2, 3])\ny = jnp.array([4, 5, 6])\narr = jnp.stack([x, y], axis=1)\nx, y = jnp.unstack(arr, axis=1)\nx\ny"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.std",
    "concepts": [
      "Computes the standard deviation along a specified axis.",
      "Implements the functionality of numpy.std() using JAX.",
      "The divisor used in calculations is N - ddof, where N is the number of elements and ddof is the degrees of freedom."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage: Standard Deviation Along All Axes",
    "concepts": [
      "Demonstrates calculating the standard deviation across all elements of an array.",
      "Uses jnp.printoptions to format the output for better readability."
    ],
    "code_examples": [
      {
        "description": "Calculates the standard deviation of all elements in the array x.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [4, 2, 5, 3],\n               [5, 4, 2, 3]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    jnp.std(x)"
      }
    ]
  },
  {
    "title": "Specifying the Axis for Standard Deviation",
    "concepts": [
      "Shows how to compute the standard deviation along a specific axis (axis=0).",
      "The output represents the standard deviation for each column."
    ],
    "code_examples": [
      {
        "description": "Calculates the standard deviation along axis 0 (columns).",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n    print(jnp.std(x, axis=0))"
      }
    ]
  },
  {
    "title": "Preserving Dimensions with keepdims",
    "concepts": [
      "Illustrates using the keepdims parameter to retain the original number of dimensions in the output.",
      "The reduced axis will have a size of 1."
    ],
    "code_examples": [
      {
        "description": "Calculates standard deviation along axis 0 and keeps the dimensions.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n    print(jnp.std(x, axis=0, keepdims=True))"
      }
    ]
  },
  {
    "title": "Degrees of Freedom (ddof)",
    "concepts": [
      "Explains the use of the ddof parameter for adjusting degrees of freedom.",
      "Demonstrates how ddof affects the calculated standard deviation."
    ],
    "code_examples": [
      {
        "description": "Calculates standard deviation along axis 0, keeps dimensions, and sets ddof to 1.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n    print(jnp.std(x, axis=0, keepdims=True, ddof=1))"
      }
    ]
  },
  {
    "title": "Conditional Standard Deviation with 'where'",
    "concepts": [
      "Shows how to compute the standard deviation based on a boolean mask (where parameter).",
      "Allows including only specific elements of the array in the computation."
    ],
    "code_examples": [
      {
        "description": "Calculates the standard deviation along axis 0, keeping dimensions, and using a 'where' mask to include specific elements.",
        "code": "where = jnp.array([[1, 0, 1, 0],\n                   [0, 1, 0, 1],\n                   [1, 1, 1, 0]], dtype=bool)\n\njnp.std(x, axis=0, keepdims=True, where=where)"
      }
    ]
  },
  {
    "title": "Subtracting Arrays Element-wise",
    "concepts": [
      "JAX implementation of numpy.subtract.",
      "It's a universal function (ufunc).",
      "It supports APIs described at jax.numpy.ufunc.",
      "Implements the - operator for JAX arrays.",
      "Arrays must be broadcastable to a common shape."
    ],
    "code_examples": [
      {
        "description": "Subtracting a scalar from a JAX array using jnp.subtract.",
        "code": "x = jnp.arange(4)\njnp.subtract(x, 10)"
      },
      {
        "description": "Subtracting a scalar from a JAX array using jnp.subtract. (Repeated)",
        "code": "x = jnp.arange(4)\njnp.subtract(x, 10)"
      },
      {
        "description": "Subtracting a scalar from a JAX array using the '-' operator.",
        "code": "x - 10"
      },
      {
        "description": "Subtracting a scalar from a JAX array using the '-' operator. (Repeated)",
        "code": "x - 10"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.swapaxes",
    "concepts": [
      "jax.numpy.swapaxes() swaps two axes of an array.",
      "The function is implemented using jax.lax.transpose().",
      "It returns a copy of the array with the specified axes swapped.",
      "JIT compilation can optimize away copies for performance.",
      "Unlike numpy.swapaxes(), it returns a copy, not a view."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Examples",
    "concepts": [
      "Demonstrates how to use jax.numpy.swapaxes() with a sample array.",
      "Shows how to verify the output shape after swapping axes.",
      "Demonstrates the array method version of swapaxes().",
      "Shows how to achieve the same result using transpose()."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating jax.numpy.swapaxes() with a 4D array.",
        "code": "a = jnp.ones((2, 3, 4, 5))\njnp.swapaxes(a, 1, 3).shape\n(2, 5, 4, 3)"
      },
      {
        "description": "Demonstrates the equivalent array method swapaxes().",
        "code": "a.swapaxes(1, 3).shape\n(2, 5, 4, 3)"
      },
      {
        "description": "Demonstrates the equivalent transpose().",
        "code": "a.transpose(0, 3, 2, 1).shape\n(2, 5, 4, 3)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.take()",
    "concepts": [
      "JAX implementation of numpy.take() using jax.lax.gather().",
      "The function takes elements from an array based on provided indices.",
      "It supports specifying the axis along which to take values.",
      "Out-of-bounds indexing behavior can be controlled with the 'mode' parameter ('fill' or 'clip').",
      "The 'fill_value' argument determines the value returned for out-of-bounds indices when mode is 'fill'.",
      "The function assumes indices are unique and/or sorted for more efficient execution if unique_indices and indices_are_sorted are true, respectively.",
      "The function returns an array of values extracted from the input array."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.take()",
    "concepts": [
      "Demonstrates taking elements from a flattened array using jnp.take().",
      "Demonstrates equivalent indexing syntax using array.ravel()[indices]."
    ],
    "code_examples": [
      {
        "description": "Shows basic usage of jnp.take() without specifying an axis, which flattens the array before indexing.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., 3.],\n               [4., 5., 6.]])\nindices = jnp.array([2, 0])\n\nprint(jnp.take(x, indices))\nprint(x.ravel()[indices]) # equivalent indexing syntax"
      }
    ]
  },
  {
    "title": "Using the axis parameter",
    "concepts": [
      "Demonstrates taking elements along a specific axis using jnp.take().",
      "Demonstrates equivalent indexing syntax using array[:, indices]."
    ],
    "code_examples": [
      {
        "description": "Illustrates how to use the axis parameter to select elements along a specific axis.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., 3.],\n               [4., 5., 6.]])\nindices = jnp.array([2, 0])\n\nprint(jnp.take(x, indices, axis=1))\nprint(x[:, indices]) # equivalent indexing syntax"
      }
    ]
  },
  {
    "title": "Handling out-of-bound indices",
    "concepts": [
      "Demonstrates the default behavior of jnp.take() with out-of-bound indices, which fills with invalid values (NaN for float inputs).",
      "Demonstrates equivalent indexing syntax using array.at[indices].get(mode='fill', fill_value=jnp.nan)."
    ],
    "code_examples": [
      {
        "description": "Shows how jnp.take() handles out-of-bounds indices by filling with NaN by default.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., 3.],\n               [4., 5., 6.]])\nindices = jnp.array([2, 0])\n\nprint(jnp.take(x, indices, axis=0))\nprint(x.at[indices].get(mode='fill', fill_value=jnp.nan) )# equivalent indexing syntax"
      }
    ]
  },
  {
    "title": "Using the mode parameter for clipping",
    "concepts": [
      "Demonstrates using the 'mode' parameter to clip out-of-bound indices to the last valid value.",
      "Demonstrates equivalent indexing syntax using array.at[indices].get(mode='clip')."
    ],
    "code_examples": [
      {
        "description": "Illustrates how to use the 'mode' parameter to clip out-of-bounds indices.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., 3.],\n               [4., 5., 6.]])\nindices = jnp.array([2, 0])\n\nprint(jnp.take(x, indices, axis=0, mode='clip'))\nprint(x.at[indices].get(mode='clip')) # equivalent indexing syntax"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.take_along_axis",
    "concepts": [
      "This is a JAX implementation of numpy.take_along_axis.",
      "It is implemented using jax.lax.gather().",
      "JAX's behavior differs from NumPy in handling out-of-bound indices; controlled by the 'mode' parameter.",
      "The function takes an array and indices to extract values along a specified axis.",
      "The 'mode' parameter specifies how to handle out-of-bounds indices ('fill' or 'clip'). The default returns invalid values (e.g. NaN)."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage with a 2D Array",
    "concepts": [
      "Demonstrates how to use jnp.take_along_axis to extract elements from a 2D array based on provided indices along a specific axis.",
      "Illustrates the equivalence of using jnp.take_along_axis with explicit indexing.",
      "Uses jnp.arange() to create index arrays for advanced indexing."
    ],
    "code_examples": [
      {
        "description": "Demonstrates taking elements along axis 1 using specified indices.",
        "code": "x = jnp.array([[1., 2., 3.],\n               [4., 5., 6.]])\nindices = jnp.array([[0, 2],\n               [1, 0]])\njnp.take_along_axis(x, indices, axis=1)"
      },
      {
        "description": "Shows the equivalent operation using indexing syntax.",
        "code": "x[jnp.arange(2)[:, None], indices]  # equivalent via indexing syntax"
      }
    ]
  },
  {
    "title": "Handling Out-of-Bound Indices",
    "concepts": [
      "Illustrates how jnp.take_along_axis handles out-of-bounds indices by filling with invalid values (NaN for float inputs).",
      "Shows how to achieve the same behavior using jax.numpy.ndarray.at with mode='fill' and fill_value=jnp.nan."
    ],
    "code_examples": [
      {
        "description": "Demonstrates out-of-bound indexing, which results in NaN values.",
        "code": "indices = jnp.array([[1, 0, 2]])\njnp.take_along_axis(x, indices, axis=0)"
      },
      {
        "description": "Shows the equivalent operation using .at indexing syntax and specifying the 'fill' mode.",
        "code": "x.at[indices, jnp.arange(3)].get(\n    mode='fill',\n    fill_value=jnp.nan\n)  # equivalent via indexing syntax"
      }
    ]
  },
  {
    "title": "Applications with argsort and argmin",
    "concepts": [
      "Demonstrates the usage of take_along_axis in conjunction with jnp.argsort() to sort an array.",
      "Demonstrates the usage of take_along_axis in conjunction with jnp.argmin() to get the minimum value along an axis.",
      "Shows how to use keepdims=True with argmin to maintain the original dimensions of the array."
    ],
    "code_examples": [
      {
        "description": "Shows how to sort an array using argsort and take_along_axis.",
        "code": "x = jnp.array([[5, 3, 4],\n               [2, 7, 6]])\nindices = jnp.argsort(x, axis=1)\njnp.take_along_axis(x, indices, axis=1)"
      },
      {
        "description": "Demonstrates how to extract minimum values using argmin and take_along_axis.",
        "code": "idx = jnp.argmin(x, axis=1, keepdims=True)\njnp.take_along_axis(x, idx, axis=1)"
      }
    ]
  },
  {
    "title": "Overview",
    "concepts": [
      "Computes the trigonometric tangent of each element of an input array.",
      "JAX implementation of numpy.tan.",
      "Input 'x' can be a scalar or array representing angles in radians.",
      "Returns an array containing the tangent of each element in 'x', promoting to inexact dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.sin(): Computes the trigonometric sine of each element of input.",
      "jax.numpy.cos(): Computes the trigonometric cosine of each element of input.",
      "jax.numpy.arctan() and jax.numpy.atan(): Computes the inverse of the trigonometric tangent of each element of input."
    ],
    "code_examples": []
  },
  {
    "title": "Examples",
    "concepts": [
      "Demonstrates the use of jnp.tan() to calculate the tangent of an array of angles.",
      "Uses jnp.printoptions to format the output for better readability."
    ],
    "code_examples": [
      {
        "description": "Calculates the tangent of an array of angles and prints the result with specified precision.",
        "code": "pi = jnp.pi\nx = jnp.array([0, pi/6, pi/4, 3*pi/4, 5*pi/6])\nwith jnp.printoptions(precision=3, suppress=True):\n  print(jnp.tan(x))"
      },
      {
        "description": "Calculates the tangent of an array of angles and prints the result with specified precision.",
        "code": "pi = jnp.pi\nx = jnp.array([0, pi/6, pi/4, 3*pi/4, 5*pi/6])\nwith jnp.printoptions(precision=3, suppress=True):\n  print(jnp.tan(x))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.tanh",
    "concepts": [
      "The function calculates the element-wise hyperbolic tangent of an input array.",
      "It is a JAX implementation of numpy.tanh.",
      "The hyperbolic tangent is defined as sinh(x)/cosh(x) or (e^x - e^{-x})/(e^x + e^{-x}).",
      "The input can be an array or a scalar.",
      "The output is an array containing the hyperbolic tangent of each element, promoting to inexact dtype.",
      "jnp.tanh is equivalent to computing -1j * jnp.tan(1j * x)."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with Real-Valued Input",
    "concepts": [
      "Demonstrates the usage of jnp.tanh with a real-valued JAX array.",
      "Illustrates the equivalence of jnp.tanh(x) and -1j * jnp.tan(1j * x) for real-valued inputs."
    ],
    "code_examples": [
      {
        "description": "Calculates the hyperbolic tangent of a real-valued array using jnp.tanh and its equivalent trigonometric representation.",
        "code": "x = jnp.array([[-1, 0, 1],\n               [3, -2, 5]])\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.tanh(x))\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(-1j * jnp.tan(1j * x))"
      },
      {
        "description": "Calculates the hyperbolic tangent of a real-valued array using jnp.tanh and its equivalent trigonometric representation (repeated example).",
        "code": "x = jnp.array([[-1, 0, 1],\n               [3, -2, 5]])\nwith jnp.printoptions(precision=3, suppress=True):\n    print(jnp.tanh(x))\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(-1j * jnp.tan(1j * x))"
      }
    ]
  },
  {
    "title": "Examples with Complex-Valued Input",
    "concepts": [
      "Demonstrates the usage of jnp.tanh with a complex-valued JAX number.",
      "Illustrates the equivalence of jnp.tanh(x) and -1j * jnp.tan(1j * x) for complex-valued inputs."
    ],
    "code_examples": [
      {
        "description": "Calculates the hyperbolic tangent of a complex number using jnp.tanh and its equivalent trigonometric representation.",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    print(jnp.tanh(2 - 5j))\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(-1j * jnp.tan(1j * (2 - 5j)))"
      },
      {
        "description": "Calculates the hyperbolic tangent of a complex number using jnp.tanh and its equivalent trigonometric representation (repeated example).",
        "code": "with jnp.printoptions(precision=3, suppress=True):\n    print(jnp.tanh(2 - 5j))\n\nwith jnp.printoptions(precision=3, suppress=True):\n    print(-1j * jnp.tan(1j * (2 - 5j)))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.tensordot",
    "concepts": [
      "The function computes the tensor dot product of two N-dimensional arrays.",
      "It is a JAX implementation of NumPy's linalg.tensordot.",
      "The function takes two arrays, `a` and `b`, and an `axes` argument to specify the axes to sum over.",
      "The `precision` argument controls the numerical precision used in the computation.",
      "The `preferred_element_type` argument controls the accumulation type used in the computation.",
      "It returns an array containing the tensor dot product of the inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Example",
    "concepts": [
      "Demonstrates calculating the tensor dot product of two 3D arrays.",
      "The default behavior sums over the last k axes of the first array and the first k axes of the second array."
    ],
    "code_examples": [
      {
        "description": "Calculates the tensor dot product of two 3D arrays, x1 and x2, using the default axes.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.arange(24.).reshape(2, 3, 4)\nx2 = jnp.ones((3, 4, 5))\n\njnp.tensordot(x1, x2)"
      }
    ]
  },
  {
    "title": "Specifying Axes Explicitly",
    "concepts": [
      "Shows how to explicitly define which axes to sum over using a tuple of sequences.",
      "The first sequence in the tuple specifies the axes of the first array.",
      "The second sequence specifies the axes of the second array."
    ],
    "code_examples": [
      {
        "description": "Calculates the tensor dot product, explicitly specifying the axes to contract over.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.arange(24.).reshape(2, 3, 4)\nx2 = jnp.ones((3, 4, 5))\n\njnp.tensordot(x1, x2, axes=([1, 2], [0, 1]))"
      }
    ]
  },
  {
    "title": "Equivalence to einsum",
    "concepts": [
      "Illustrates the equivalence between `jnp.tensordot` and `jnp.einsum` for tensor contractions.",
      "`jnp.einsum` provides a more general interface for specifying tensor contractions using Einstein notation."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using jnp.einsum to achieve the same result as jnp.tensordot in the previous examples.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.arange(24.).reshape(2, 3, 4)\nx2 = jnp.ones((3, 4, 5))\n\njnp.einsum('ijk,jkm->im', x1, x2)"
      }
    ]
  },
  {
    "title": "Matrix Multiplication with tensordot",
    "concepts": [
      "Shows how `jnp.tensordot` with `axes=1` is equivalent to matrix multiplication for 2D arrays.",
      "Demonstrates the equivalence with the `@` operator."
    ],
    "code_examples": [
      {
        "description": "Demonstrates matrix multiplication using jnp.linalg.tensordot with axes=1.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[1, 2],\n                [3, 4]])\nx2 = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\n\njnp.linalg.tensordot(x1, x2, axes=1)"
      },
      {
        "description": "Demonstrates matrix multiplication using the @ operator.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([[1, 2],\n                [3, 4]])\nx2 = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\n\nx1 @ x2"
      }
    ]
  },
  {
    "title": "Outer Product with tensordot",
    "concepts": [
      "Shows how `jnp.tensordot` with `axes=0` is equivalent to the outer product for 1D arrays.",
      "Demonstrates the equivalence with `jnp.outer`."
    ],
    "code_examples": [
      {
        "description": "Demonstrates calculating the outer product using jnp.linalg.tensordot with axes=0.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([1, 2])\nx2 = jnp.array([1, 2, 3])\n\njnp.linalg.tensordot(x1, x2, axes=0)"
      },
      {
        "description": "Demonstrates calculating the outer product using jnp.outer.",
        "code": "import jax.numpy as jnp\n\nx1 = jnp.array([1, 2])\nx2 = jnp.array([1, 2, 3])\n\njnp.outer(x1, x2)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.tile()",
    "concepts": [
      "The function `jax.numpy.tile()` repeats an array along specified dimensions.",
      "The input array `A` can be of any shape or dimension.",
      "The `reps` argument specifies the number of repetitions along each axis.",
      "The function returns a new array where the input array has been repeated according to `reps`."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.tile() with 1D Arrays",
    "concepts": [
      "Demonstrates tiling a 1D JAX array using `jnp.tile()`.",
      "The `reps` argument specifies how many times to repeat the array."
    ],
    "code_examples": [
      {
        "description": "Tiles a 1D array [1, 2] by a factor of 2, resulting in [1, 2, 1, 2].",
        "code": "import jax.numpy as jnp\n\narr = jnp.array([1, 2])\njnp.tile(arr, 2)"
      }
    ]
  },
  {
    "title": "Examples of jax.numpy.tile() with 2D Arrays",
    "concepts": [
      "Demonstrates tiling a 2D JAX array using `jnp.tile()`.",
      "The `reps` argument is a tuple specifying repetitions along each axis."
    ],
    "code_examples": [
      {
        "description": "Tiles a 2D array by (2, 1), repeating rows twice and columns once.",
        "code": "import jax.numpy as jnp\n\narr = jnp.array([[1, 2],\n                [3, 4]])\njnp.tile(arr, (2, 1))"
      }
    ]
  },
  {
    "title": "Redundant Examples",
    "concepts": [
      "This section includes redundant examples that are identical to the previous ones."
    ],
    "code_examples": [
      {
        "description": "Tiles a 1D array [1, 2] by a factor of 2, resulting in [1, 2, 1, 2].",
        "code": "import jax.numpy as jnp\n\narr = jnp.array([1, 2])\njnp.tile(arr, 2)"
      },
      {
        "description": "Tiles a 2D array by (2, 1), repeating rows twice and columns once.",
        "code": "import jax.numpy as jnp\n\narr = jnp.array([[1, 2],\n                [3, 4]])\njnp.tile(arr, (2, 1))"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.trapezoid",
    "concepts": [
      "The trapezoidal rule is used for numerical integration.",
      "It approximates the integral by summing trapezoid areas.",
      "This is a JAX implementation of numpy.trapezoid().",
      "The function integrates along a specified axis.",
      "The function can handle regular and irregular grids.",
      "The function returns the definite integral approximated by the trapezoidal rule."
    ],
    "code_examples": []
  },
  {
    "title": "Regular Grid Integration Example",
    "concepts": [
      "Integration over a grid with uniform spacing.",
      "The spacing between sample points is specified using the 'dx' parameter."
    ],
    "code_examples": [
      {
        "description": "Integrate over a regular grid with spacing 1.0.",
        "code": "y = jnp.array([1, 2, 3, 2, 3, 2, 1])\njnp.trapezoid(y, dx=1.0)"
      },
      {
        "description": "Integrate over a regular grid with spacing 1.0.",
        "code": "y = jnp.array([1, 2, 3, 2, 3, 2, 1])\njnp.trapezoid(y, dx=1.0)"
      }
    ]
  },
  {
    "title": "Irregular Grid Integration Example",
    "concepts": [
      "Integration over a grid with non-uniform spacing.",
      "The x-coordinates of the sample points are explicitly provided."
    ],
    "code_examples": [
      {
        "description": "Integrate over an irregular grid.",
        "code": "x = jnp.array([0, 2, 5, 7, 10, 15, 20])\ny = jnp.array([1, 2, 3, 2, 3, 2, 1])\njnp.trapezoid(y, x)"
      },
      {
        "description": "Integrate over an irregular grid.",
        "code": "x = jnp.array([0, 2, 5, 7, 10, 15, 20])\ny = jnp.array([1, 2, 3, 2, 3, 2, 1])\njnp.trapezoid(y, x)"
      }
    ]
  },
  {
    "title": "Approximating the Integral of sin^2(x)",
    "concepts": [
      "Approximating a definite integral using the trapezoidal rule.",
      "Using jnp.linspace to create a set of evenly spaced points.",
      "Verifying the result using jnp.allclose."
    ],
    "code_examples": [
      {
        "description": "Approximate the integral of sin^2(x) from 0 to 2*pi, which equals pi.",
        "code": "x = jnp.linspace(0, 2 * jnp.pi, 1000)\ny = jnp.sin(x)**2\nresult = jnp.trapezoid(y, x)\njnp.allclose(result, jnp.pi)"
      },
      {
        "description": "Approximate the integral of sin^2(x) from 0 to 2*pi, which equals pi.",
        "code": "x = jnp.linspace(0, 2 * jnp.pi, 1000)\ny = jnp.sin(x)**2\nresult = jnp.trapezoid(y, x)\njnp.allclose(result, jnp.pi)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.transpose",
    "concepts": [
      "jax.numpy.transpose() returns a transposed version of an N-dimensional array.",
      "It is a JAX implementation of numpy.transpose().",
      "The function is implemented in terms of jax.lax.transpose().",
      "The default behavior reverses the order of the axes.",
      "Unlike numpy.transpose(), jax.numpy.transpose() returns a copy, but JIT optimizes copies away when possible.",
      "jax.Array.transpose() method and jax.Array.T property offer equivalent functionality.",
      "jax.numpy.matrix_transpose() transposes the last two axes.",
      "jax.numpy.swapaxes() swaps any two axes.",
      "jax.numpy.moveaxis() moves an axis to a different position."
    ],
    "code_examples": []
  },
  {
    "title": "Transpose of a 1D Array",
    "concepts": [
      "The transpose of a 1D array is the identity operation."
    ],
    "code_examples": [
      {
        "description": "Demonstrates transposing a 1D array using jnp.transpose(). The output is the same as the input.",
        "code": "x = jnp.array([1, 2, 3, 4])\njnp.transpose(x)"
      },
      {
        "description": "Demonstrates transposing a 1D array using jnp.transpose(). The output is the same as the input.",
        "code": "x = jnp.array([1, 2, 3, 4])\njnp.transpose(x)"
      }
    ]
  },
  {
    "title": "Transpose of a 2D Array",
    "concepts": [
      "The transpose of a 2D array is a matrix transpose, swapping rows and columns."
    ],
    "code_examples": [
      {
        "description": "Demonstrates transposing a 2D array using jnp.transpose(). The rows and columns are swapped.",
        "code": "x = jnp.array([[1, 2],\n              [3, 4]])\njnp.transpose(x)"
      },
      {
        "description": "Demonstrates transposing a 2D array using jnp.transpose(). The rows and columns are swapped.",
        "code": "x = jnp.array([[1, 2],\n              [3, 4]])\njnp.transpose(x)"
      }
    ]
  },
  {
    "title": "Transpose of an N-Dimensional Array",
    "concepts": [
      "For an N-dimensional array, the transpose reverses the order of the axes by default.",
      "The axes argument allows specifying a custom permutation of the axes."
    ],
    "code_examples": [
      {
        "description": "Demonstrates transposing a 3D array using jnp.transpose() without specifying axes, resulting in reversed axis order.",
        "code": "x = jnp.zeros(shape=(3, 4, 5))\njnp.transpose(x).shape"
      },
      {
        "description": "Demonstrates transposing a 3D array using jnp.transpose() without specifying axes, resulting in reversed axis order.",
        "code": "x = jnp.zeros(shape=(3, 4, 5))\njnp.transpose(x).shape"
      },
      {
        "description": "Demonstrates transposing a 3D array using jnp.transpose() with a specified axes permutation.",
        "code": "jnp.transpose(x, (0, 2, 1)).shape"
      },
      {
        "description": "Demonstrates transposing a 3D array using jnp.transpose() with a specified axes permutation.",
        "code": "jnp.transpose(x, (0, 2, 1)).shape"
      }
    ]
  },
  {
    "title": "Matrix Transpose",
    "concepts": [
      "jax.numpy.matrix_transpose() transposes the last two axes of an array.",
      "It is useful for working with batched 2D matrices."
    ],
    "code_examples": [
      {
        "description": "Demonstrates transposing the last two axes of a 3D array using jnp.matrix_transpose().",
        "code": "jnp.matrix_transpose(x).shape"
      },
      {
        "description": "Demonstrates transposing the last two axes of a 3D array using jnp.matrix_transpose().",
        "code": "jnp.matrix_transpose(x).shape"
      }
    ]
  },
  {
    "title": "Using the jax.Array Methods and Properties",
    "concepts": [
      "The jax.Array.transpose() method and jax.Array.T property provide convenient ways to perform transposes."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using the jax.Array.transpose() method to transpose a 2D array.",
        "code": "x = jnp.array([[1, 2],\n              [3, 4]])\nx.transpose()"
      },
      {
        "description": "Demonstrates using the jax.Array.T property to transpose a 2D array.",
        "code": "x.T"
      },
      {
        "description": "Demonstrates using the jax.Array.transpose() method to transpose a 2D array.",
        "code": "x = jnp.array([[1, 2],\n              [3, 4]])\nx.transpose()"
      },
      {
        "description": "Demonstrates using the jax.Array.T property to transpose a 2D array.",
        "code": "x.T"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.tri()",
    "concepts": [
      "The function returns an array with ones on and below the diagonal and zeros elsewhere.",
      "It is a JAX implementation of numpy.tri().",
      "The parameter 'N' specifies the number of rows.",
      "The parameter 'M' specifies the number of columns, defaulting to N if not specified.",
      "The parameter 'k' specifies the sub-diagonal to fill with ones, defaulting to 0.",
      "The parameter 'dtype' specifies the data type of the returned array, defaulting to float.",
      "The function is similar to jax.numpy.tril() and jax.numpy.triu()."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with N=M",
    "concepts": [
      "Example demonstrating jnp.tri() with the default parameters."
    ],
    "code_examples": [
      {
        "description": "Example where N = M = 3 and k defaults to 0.",
        "code": "jnp.tri(3)\n#Array([[1., 0., 0.],\n#      [1., 1., 0.],\n#      [1., 1., 1.]], dtype=float32)"
      },
      {
        "description": "Example where N = M = 3 and k defaults to 0.",
        "code": "jnp.tri(3)\n#Array([[1., 0., 0.],\n#      [1., 1., 0.],\n#      [1., 1., 1.]], dtype=float32)"
      }
    ]
  },
  {
    "title": "Examples with N!=M",
    "concepts": [
      "Example demonstrating jnp.tri() when M is not equal to N."
    ],
    "code_examples": [
      {
        "description": "Example where N = 3, M = 4 and k defaults to 0.",
        "code": "jnp.tri(3, 4)\n#Array([[1., 0., 0., 0.],\n#      [1., 1., 0., 0.],\n#      [1., 1., 1., 0.]], dtype=float32)"
      },
      {
        "description": "Example where N = 3, M = 4 and k defaults to 0.",
        "code": "jnp.tri(3, 4)\n#Array([[1., 0., 0., 0.],\n#      [1., 1., 0., 0.],\n#      [1., 1., 1., 0.]], dtype=float32)"
      }
    ]
  },
  {
    "title": "Examples with k > 0",
    "concepts": [
      "Example demonstrating jnp.tri() when k is greater than 0."
    ],
    "code_examples": [
      {
        "description": "Example where N = 3, M defaults to 3, and k = 1.",
        "code": "jnp.tri(3, k=1)\n#Array([[1., 1., 0.],\n#      [1., 1., 1.],\n#      [1., 1., 1.]], dtype=float32)"
      },
      {
        "description": "Example where N = 3, M defaults to 3, and k = 1.",
        "code": "jnp.tri(3, k=1)\n#Array([[1., 1., 0.],\n#      [1., 1., 1.],\n#      [1., 1., 1.]], dtype=float32)"
      }
    ]
  },
  {
    "title": "Examples with k < 0",
    "concepts": [
      "Example demonstrating jnp.tri() when k is less than 0."
    ],
    "code_examples": [
      {
        "description": "Example where N = 3, M = 4, and k = -1.",
        "code": "jnp.tri(3, 4, k=-1)\n#Array([[0., 0., 0., 0.],\n#      [1., 0., 0., 0.],\n#      [1., 1., 0., 0.]], dtype=float32)"
      },
      {
        "description": "Example where N = 3, M = 4, and k = -1.",
        "code": "jnp.tri(3, 4, k=-1)\n#Array([[0., 0., 0., 0.],\n#      [1., 0., 0., 0.],\n#      [1., 1., 0., 0.]], dtype=float32)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.tril_indices()",
    "concepts": [
      "The function returns the indices of the lower triangle of an array.",
      "It is a JAX implementation of numpy.tril_indices().",
      "The function accepts the number of rows (n) and an optional number of columns (m).",
      "The function accepts an optional k parameter which specifies the sub-diagonal on and below which the indices of the lower triangle are returned. k=0 refers to the main diagonal, k<0 refers to sub-diagonal below the main diagonal and k>0 refers to sub-diagonal above the main diagonal.",
      "If m is not specified, it defaults to n."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with n only",
    "concepts": [
      "When only n is provided, the function returns indices for an (n, n) array."
    ],
    "code_examples": [
      {
        "description": "Example usage with n=3.",
        "code": "jnp.tril_indices(3)\n(Array([0, 1, 1, 2, 2, 2], dtype=int32), Array([0, 0, 1, 0, 1, 2], dtype=int32))"
      },
      {
        "description": "Example usage with n=3 (repeated).",
        "code": "jnp.tril_indices(3)\n(Array([0, 1, 1, 2, 2, 2], dtype=int32), Array([0, 0, 1, 0, 1, 2], dtype=int32))"
      }
    ]
  },
  {
    "title": "Examples with n and m",
    "concepts": [
      "When both n and m are provided, the function returns indices for an (n, m) array."
    ],
    "code_examples": [
      {
        "description": "Example usage with n=3 and m=2.",
        "code": "jnp.tril_indices(3, m=2)\n(Array([0, 1, 1, 2, 2], dtype=int32), Array([0, 0, 1, 0, 1], dtype=int32))"
      },
      {
        "description": "Example usage with n=3 and m=2 (repeated).",
        "code": "jnp.tril_indices(3, m=2)\n(Array([0, 1, 1, 2, 2], dtype=int32), Array([0, 0, 1, 0, 1], dtype=int32))"
      }
    ]
  },
  {
    "title": "Examples with k > 0",
    "concepts": [
      "The k parameter can shift the diagonal used for determining the lower triangle.",
      "k=1 includes the first sub-diagonal above the main diagonal."
    ],
    "code_examples": [
      {
        "description": "Example usage with n=3 and k=1.",
        "code": "jnp.tril_indices(3, k=1)\n(Array([0, 0, 1, 1, 1, 2, 2, 2], dtype=int32), Array([0, 1, 0, 1, 2, 0, 1, 2], dtype=int32))"
      },
      {
        "description": "Example usage with n=3 and k=1 (repeated).",
        "code": "jnp.tril_indices(3, k=1)\n(Array([0, 0, 1, 1, 1, 2, 2, 2], dtype=int32), Array([0, 1, 0, 1, 2, 0, 1, 2], dtype=int32))"
      }
    ]
  },
  {
    "title": "Examples with k < 0",
    "concepts": [
      "The k parameter can shift the diagonal used for determining the lower triangle.",
      "k=-1 includes the first sub-diagonal below the main diagonal."
    ],
    "code_examples": [
      {
        "description": "Example usage with n=3 and k=-1.",
        "code": "jnp.tril_indices(3, k=-1)\n(Array([1, 2, 2], dtype=int32), Array([0, 0, 1], dtype=int32))"
      },
      {
        "description": "Example usage with n=3 and k=-1 (repeated).",
        "code": "jnp.tril_indices(3, k=-1)\n(Array([1, 2, 2], dtype=int32), Array([0, 0, 1], dtype=int32))"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.tril_indices_from",
    "concepts": [
      "The function returns the indices of the lower triangle of a given array.",
      "The input array must be two-dimensional.",
      "The parameter `k` specifies the sub-diagonal to consider.",
      "`k=0` refers to the main diagonal.",
      "`k<0` refers to sub-diagonals below the main diagonal.",
      "`k>0` refers to sub-diagonals above the main diagonal.",
      "The function returns a tuple of two arrays representing the row and column indices of the lower triangle."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Example",
    "concepts": [
      "Demonstrates the basic usage of `jnp.tril_indices_from` with a sample 2D array.",
      "Shows how to obtain the indices of the lower triangle (including the main diagonal) by default.",
      "The indices can be used to access the elements of the lower triangle in the original array."
    ],
    "code_examples": [
      {
        "description": "Shows how to use `jnp.tril_indices_from` to get the indices of the lower triangle of a 2D array and how these indices can be used to extract the elements of the lower triangle.",
        "code": "import jax.numpy as jnp\n\narr = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\nprint(jnp.tril_indices_from(arr))\n# (Array([0, 1, 1, 2, 2, 2], dtype=int32), Array([0, 0, 1, 0, 1, 2], dtype=int32))\n\nind = jnp.tril_indices_from(arr)\nprint(arr[ind])\n# Array([1, 4, 5, 7, 8, 9], dtype=int32)\n\nprint(jnp.tril(arr))\n#Array([[1, 0, 0],\n#      [4, 5, 0],\n#      [7, 8, 9]], dtype=int32)"
      }
    ]
  },
  {
    "title": "Usage with k > 0",
    "concepts": [
      "Demonstrates the usage of `jnp.tril_indices_from` with `k=1`.",
      "This returns the indices of the lower triangle, including the diagonal above the main diagonal."
    ],
    "code_examples": [
      {
        "description": "Shows how to use `jnp.tril_indices_from` with `k=1` to get the indices of the lower triangle including the first upper diagonal.",
        "code": "import jax.numpy as jnp\n\narr = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\nprint(jnp.tril_indices_from(arr, k=1))\n# (Array([0, 0, 1, 1, 1, 2, 2, 2], dtype=int32), Array([0, 1, 0, 1, 2, 0, 1, 2], dtype=int32))"
      }
    ]
  },
  {
    "title": "Usage with k < 0",
    "concepts": [
      "Demonstrates the usage of `jnp.tril_indices_from` with `k=-1`.",
      "This returns the indices of the lower triangle, excluding the main diagonal."
    ],
    "code_examples": [
      {
        "description": "Shows how to use `jnp.tril_indices_from` with `k=-1` to get the indices of the lower triangle excluding the main diagonal.",
        "code": "import jax.numpy as jnp\n\narr = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\nprint(jnp.tril_indices_from(arr, k=-1))\n# (Array([1, 2, 2], dtype=int32), Array([0, 0, 1], dtype=int32))"
      }
    ]
  },
  {
    "title": "Description of jnp.trim_zeros()",
    "concepts": [
      "The function `jnp.trim_zeros()` removes leading and/or trailing zeros from a JAX array.",
      "The input array must be one-dimensional.",
      "The `trim` parameter specifies which end of the array to trim.",
      "The function returns a new array with the zeros removed.",
      "`trim` can be 'f' for leading zeros, 'b' for trailing zeros, or 'fb' for both."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.trim_zeros()",
    "concepts": [
      "Demonstrates the usage of `jnp.trim_zeros()` to remove leading and trailing zeros from a JAX array.",
      "The examples showcase trimming both leading and trailing zeros using the default 'fb' option."
    ],
    "code_examples": [
      {
        "description": "Example of trimming leading and trailing zeros from a JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([0, 0, 2, 0, 1, 4, 3, 0, 0, 0])\n\njnp.trim_zeros(x)"
      },
      {
        "description": "Another example of trimming leading and trailing zeros from a JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([0, 0, 2, 0, 1, 4, 3, 0, 0, 0])\n\njnp.trim_zeros(x)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.triu",
    "concepts": [
      "Returns the upper triangle of an array.",
      "It is a JAX implementation of numpy.triu().",
      "The input array must have at least two dimensions.",
      "The 'k' parameter controls the sub-diagonal, with k=0 being the main diagonal.",
      "The function operates batch-wise on the trailing axes if the input array has more than two dimensions."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Examples",
    "concepts": [
      "Demonstrates the basic usage of jnp.triu with different values of 'k'.",
      "Shows how 'k' affects which elements are zeroed out."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating jnp.triu with a 2D array and different values for k (0, 1, -1).",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9],\n               [10, 11, 12]])\n\nprint(jnp.triu(x))\nprint(jnp.triu(x, k=1))\nprint(jnp.triu(x, k=-1))"
      },
      {
        "description": "Example demonstrating jnp.triu with a 2D array and different values for k (0, 1, -1).",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9],\n               [10, 11, 12]])\n\nprint(jnp.triu(x))\nprint(jnp.triu(x, k=1))\nprint(jnp.triu(x, k=-1))"
      }
    ]
  },
  {
    "title": "Batch-wise Operation Example",
    "concepts": [
      "Illustrates how jnp.triu operates on multi-dimensional arrays (ndim > 2).",
      "The function processes the trailing axes in a batch-wise manner."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating batch-wise operation of jnp.triu on a 3D array.",
        "code": "x1 = jnp.array([[[1, 2],\n                [3, 4]],\n               [[5, 6],\n                [7, 8]]])\n\nprint(jnp.triu(x1))"
      },
      {
        "description": "Example demonstrating batch-wise operation of jnp.triu on a 3D array.",
        "code": "x1 = jnp.array([[[1, 2],\n                [3, 4]],\n               [[5, 6],\n                [7, 8]]])\n\nprint(jnp.triu(x1))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.triu_indices_from",
    "concepts": [
      "The function returns indices of the upper triangle of a given array.",
      "It is a JAX implementation of numpy.triu_indices_from().",
      "The input array must have ndim == 2.",
      "The parameter k specifies the sub-diagonal above which the indices are returned.",
      "Returns a tuple of two arrays containing the indices along each axis."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Example",
    "concepts": [
      "Demonstrates how to use jnp.triu_indices_from with a default k value (k=0).",
      "Shows the indices of the upper triangle including the main diagonal."
    ],
    "code_examples": [
      {
        "description": "Finds the indices of the upper triangle of a 2D array using jnp.triu_indices_from().",
        "code": "arr = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\njnp.triu_indices_from(arr)"
      }
    ]
  },
  {
    "title": "Relationship with jnp.triu",
    "concepts": [
      "Illustrates how the elements indexed by jnp.triu_indices_from correspond to the output of jnp.triu.",
      "Shows how to use the indices returned by jnp.triu_indices_from to access the elements of the upper triangle."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the relationship between jnp.triu_indices_from and jnp.triu. Extracts the upper triangle elements using indices and compares it to the result of jnp.triu.",
        "code": "arr = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\nind = jnp.triu_indices_from(arr)\narr[ind]\n\njnp.triu(arr)"
      }
    ]
  },
  {
    "title": "Usage with k > 0",
    "concepts": [
      "Demonstrates how to use jnp.triu_indices_from with a positive k value.",
      "Specifies the sub-diagonal above the main diagonal to consider."
    ],
    "code_examples": [
      {
        "description": "Finds the indices of the upper triangle with k=1, considering only elements above the main diagonal.",
        "code": "arr = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\njnp.triu_indices_from(arr, k=1)"
      }
    ]
  },
  {
    "title": "Usage with k < 0",
    "concepts": [
      "Demonstrates how to use jnp.triu_indices_from with a negative k value.",
      "Includes elements from the sub-diagonals below the main diagonal."
    ],
    "code_examples": [
      {
        "description": "Finds the indices of the upper triangle with k=-1, including elements from the sub-diagonal below the main diagonal.",
        "code": "arr = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\njnp.triu_indices_from(arr, k=-1)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.trunc()",
    "concepts": [
      "The function rounds input to the nearest integer towards zero.",
      "It is a JAX implementation of numpy.trunc().",
      "The input can be an array or a scalar.",
      "The function returns an array with the same shape and dtype as the input, containing the rounded values."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.trunc() usage",
    "concepts": [
      "The example demonstrates how to use jax.numpy.trunc() with a JAX array.",
      "It uses jax.random.uniform() to generate a random array.",
      "The example shows how the function rounds the elements towards zero."
    ],
    "code_examples": [
      {
        "description": "This example demonstrates the usage of jax.numpy.trunc() to round the elements of a JAX array towards zero.",
        "code": "key = jax.random.key(42)\nx = jax.random.uniform(key, (3, 3), minval=-10, maxval=10)\nwith jnp.printoptions(precision=2, suppress=True):\n    print(x)\nprint(jnp.trunc(x))"
      },
      {
        "description": "This example demonstrates the usage of jax.numpy.trunc() to round the elements of a JAX array towards zero. It is repeated in the documentation.",
        "code": "key = jax.random.key(42)\nx = jax.random.uniform(key, (3, 3), minval=-10, maxval=10)\nwith jnp.printoptions(precision=2, suppress=True):\n    print(x)\nprint(jnp.trunc(x))"
      }
    ]
  },
  {
    "title": "Introduction to Universal Functions (ufuncs)",
    "concepts": [
      "Universal functions (ufuncs) operate element-wise on arrays.",
      "JAX implements NumPy's ufunc API.",
      "Most users will use pre-defined ufuncs in jax.numpy.",
      "jax.numpy.frompyfunc() is used to construct custom ufuncs."
    ],
    "code_examples": []
  },
  {
    "title": "Ufunc Example: jax.numpy.add",
    "concepts": [
      "Ufuncs apply element-wise to broadcasted arrays.",
      "Ufuncs have attributes describing their behavior (nin, nout, identity)."
    ],
    "code_examples": [
      {
        "description": "Demonstrates element-wise addition using jnp.add.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5])\nprint(jnp.add(x, 1))"
      },
      {
        "description": "Shows how to access ufunc attributes like nin, nout, and identity.",
        "code": "import jax.numpy as jnp\n\nprint(jnp.add.nin)  # number of inputs\nprint(jnp.add.nout) # number of outputs\nprint(jnp.add.identity) # identity value, or None if no identity exists"
      }
    ]
  },
  {
    "title": "Ufunc Methods: outer, reduce, accumulate, at, reduceat",
    "concepts": [
      "Binary ufuncs have methods like outer(), reduce(), accumulate(), at(), and reduceat().",
      "outer() applies the function to the pair-wise outer-product of the input array values.",
      "reduce() performs a reduction over the array (e.g., jnp.add.reduce() is equivalent to jnp.sum).",
      "accumulate() performs a cumulative reduction over the array (e.g., jnp.add.accumulate() is equivalent to jax.numpy.cumulative_sum()).",
      "at() applies the function at particular indices in the array (similar to jax.lax.scatter_add()).",
      "reduceat() performs reduce operations between specified indices of an array (similar to jax.ops.segment_sum())."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the outer() method of jnp.add.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5])\nprint(jnp.add.outer(x, x))"
      },
      {
        "description": "Demonstrates the reduce() method of jnp.add.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5])\nprint(jnp.add.reduce(x))"
      },
      {
        "description": "Demonstrates the accumulate() method of jnp.add.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5])\nprint(jnp.add.accumulate(x))"
      },
      {
        "description": "Demonstrates the at() method of jnp.add.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5])\nprint(jnp.add.at(x, 0, 100, inplace=False))"
      },
      {
        "description": "Demonstrates the reduceat() method of jnp.add.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5])\nprint(jnp.add.reduceat(x, jnp.array([0, 2])))"
      }
    ]
  },
  {
    "title": "Type Alias: uint64",
    "concepts": [
      "The type `uint64` is an alias.",
      "The `uint64` alias represents an unsigned 64-bit integer."
    ],
    "code_examples": []
  },
  {
    "title": "JAX uint16 Scalar Constructor",
    "concepts": [
      "JAX scalar constructors are used to create scalar values of specific data types.",
      "uint16 is a specific scalar constructor in JAX.",
      "JAX represents scalars as zero-dimensional arrays, unlike NumPy.",
      "The constructor takes an argument 'x' of any type, presumably to initialize the scalar.",
      "The constructor returns an Array.",
      "The class has an '__init__' method.",
      "The class has a 'dtype' attribute."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Scalar Constructor for uint32",
    "concepts": [
      "JAX provides a scalar constructor for uint32.",
      "JAX represents scalars as zero-dimensional arrays, unlike NumPy.",
      "The input 'x' can be of any type (Any) and will be converted to an array.",
      "The constructor initializes an array with the specified dtype."
    ],
    "code_examples": []
  },
  {
    "title": "JAX uint8 Scalar Constructor",
    "concepts": [
      "JAX provides scalar constructors for each data type.",
      "JAX represents scalars as zero-dimensional arrays, unlike NumPy.",
      "The input `x` can be of any type.",
      "The scalar constructor returns an Array object."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.numpy.unique()",
    "concepts": [
      "jax.numpy.unique() returns unique values from an array, similar to numpy.unique().",
      "The output size of unique is data-dependent, making it incompatible with jit() without specifying the size argument.",
      "The size argument must be specified statically for use with JAX transformations.",
      "The function can return unique values, indices, inverse indices, and counts of unique values.",
      "It supports specifying an axis to compute unique values along a specific dimension."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage",
    "concepts": [
      "Demonstrates finding unique values in a 1D JAX array.",
      "The jnp.unique() function returns a sorted array of unique elements."
    ],
    "code_examples": [
      {
        "description": "Find unique values in a 1D array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nprint(jnp.unique(x))"
      }
    ]
  },
  {
    "title": "JIT Compilation and the 'size' Argument",
    "concepts": [
      "Using jnp.unique() with JIT compilation requires a static size argument.",
      "The 'size' argument determines the output size, making the function compatible with JIT.",
      "If the static size is smaller than the number of unique values, the output will be truncated.",
      "If the static size is larger, the output will be padded with a fill value.",
      "The default fill value is the minimum unique value."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using jnp.unique() with jax.jit() and the 'size' argument.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\n\njit_unique = jax.jit(jnp.unique, static_argnames=['size'])\nprint(jit_unique(x, size=3))"
      },
      {
        "description": "Truncating unique values with a smaller size.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\n\njit_unique = jax.jit(jnp.unique, static_argnames=['size'])\nprint(jit_unique(x, size=2))"
      },
      {
        "description": "Padding unique values with a fill value when size is larger than the number of unique elements.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\n\njit_unique = jax.jit(jnp.unique, static_argnames=['size'])\nprint(jit_unique(x, size=5))\nprint(jit_unique(x, size=5, fill_value=0))"
      }
    ]
  },
  {
    "title": "Multi-dimensional Arrays",
    "concepts": [
      "By default, jnp.unique() flattens multi-dimensional arrays.",
      "The 'axis' argument can be used to find unique slices along a specified axis."
    ],
    "code_examples": [
      {
        "description": "Find unique values in a flattened multi-dimensional array.",
        "code": "import jax.numpy as jnp\n\nM = jnp.array([[1, 2],\n               [2, 3],\n               [1, 2]])\nprint(jnp.unique(M))"
      },
      {
        "description": "Find unique slices along a specified axis.",
        "code": "import jax.numpy as jnp\n\nM = jnp.array([[1, 2],\n               [2, 3],\n               [1, 2]])\nprint(jnp.unique(M, axis=0))"
      }
    ]
  },
  {
    "title": "Returning Indices",
    "concepts": [
      "The 'return_index' argument returns the indices of the first occurrence of each unique value.",
      "For 1D inputs, ar[unique_index] is equivalent to unique_values.",
      "For multi-dimensional inputs, jax.numpy.take() can be used to extract unique values along a specified axis."
    ],
    "code_examples": [
      {
        "description": "Returning indices of unique values in a 1D array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nvalues, indices = jnp.unique(x, return_index=True)\nprint(values)\nprint(indices)\nprint(jnp.all(values == x[indices]))"
      },
      {
        "description": "Returning indices of unique values in a multi-dimensional array using jax.numpy.take().",
        "code": "import jax.numpy as jnp\n\nM = jnp.array([[1, 2],\n               [2, 3],\n               [1, 2]])\nvalues, indices = jnp.unique(M, axis=0, return_index=True)\nprint(jnp.all(values == jnp.take(M, indices, axis=0)))"
      }
    ]
  },
  {
    "title": "Returning Inverse Indices",
    "concepts": [
      "The 'return_inverse' argument returns the indices within the unique values for every entry in the input array.",
      "For 1D inputs, unique_values[unique_inverse] is equivalent to ar.",
      "For multi-dimensional inputs, jax.numpy.take() can be used to reconstruct the original array."
    ],
    "code_examples": [
      {
        "description": "Returning inverse indices for a 1D array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nvalues, inverse = jnp.unique(x, return_inverse=True)\nprint(values)\nprint(inverse)\nprint(jnp.all(values[inverse] == x))"
      },
      {
        "description": "Returning inverse indices for a multi-dimensional array and reconstructing the original array using jax.numpy.take().",
        "code": "import jax.numpy as jnp\n\nM = jnp.array([[1, 2],\n               [2, 3],\n               [1, 2]])\nvalues, inverse = jnp.unique(M, axis=0, return_inverse=True)\nprint(jnp.all(jnp.take(values, inverse, axis=0) == M))"
      }
    ]
  },
  {
    "title": "Returning Counts",
    "concepts": [
      "The 'return_counts' argument returns the number of occurrences of each unique value.",
      "For multi-dimensional arrays, it returns a 1D array of counts indicating the number of occurrences along the specified axis."
    ],
    "code_examples": [
      {
        "description": "Returning counts of unique values in a 1D array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nvalues, counts = jnp.unique(x, return_counts=True)\nprint(values)\nprint(counts)"
      },
      {
        "description": "Returning counts of unique values in a multi-dimensional array.",
        "code": "import jax.numpy as jnp\n\nM = jnp.array([[1, 2],\n               [2, 3],\n               [1, 2]])\nvalues, counts = jnp.unique(M, axis=0, return_counts=True)\nprint(values)\nprint(counts)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.unique_all",
    "concepts": [
      "The function returns unique values, indices, inverse indices, and counts from an array.",
      "It's a JAX implementation equivalent to numpy.unique() with specific flags set.",
      "It may not be compatible with `jit()` transformations without a specified size argument.",
      "The function takes an array as input and an optional size and fill_value.",
      "The return value is a tuple containing values, indices, inverse indices, and counts."
    ],
    "code_examples": []
  },
  {
    "title": "Using jax.numpy.unique_all with a 1D array",
    "concepts": [
      "Demonstrates the usage of `jax.numpy.unique_all` to find unique elements and associated information in a 1D JAX array.",
      "The `values` attribute of the result contains the unique values.",
      "The `indices` attribute provides the indices of the first occurrence of each unique value.",
      "The `inverse_indices` attribute maps each element of the input array to its index in the `values` array.",
      "The `counts` attribute indicates how many times each unique value appears in the input array."
    ],
    "code_examples": [
      {
        "description": "Compute unique values, indices, inverse indices and counts in a 1D JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nresult = jnp.unique_all(x)\n\nprint(result.values)\nprint(result.indices)\nprint(jnp.all(result.values == x[result.indices]))\nprint(result.inverse_indices)\nprint(jnp.all(x == result.values[result.inverse_indices]))\nprint(result.counts)"
      }
    ]
  },
  {
    "title": "Overview of jnp.unique_counts",
    "concepts": [
      "Returns unique values and their counts from an array.",
      "JAX implementation of numpy.unique_counts().",
      "Equivalent to jax.numpy.unique() with return_counts=True and equal_nan=True.",
      "Not typically compatible with jit() due to data-dependent output size.",
      "The `size` argument must be specified statically for use with `jit()`.",
      "Optional `fill_value` argument to pad the output when `size` is specified.",
      "Returns a tuple containing the unique values and counts."
    ],
    "code_examples": []
  },
  {
    "title": "Arguments of jnp.unique_counts",
    "concepts": [
      "x: N-dimensional array from which unique values are extracted.",
      "size: If specified, returns only the first size sorted unique elements; otherwise, returns all.",
      "fill_value: When size is specified, fill the remaining entries fill_value if there are fewer unique elements than size indicates. Defaults to the minimum unique value."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value of jnp.unique_counts",
    "concepts": [
      "values: An array of shape (n_unique,) containing the unique values from x.",
      "counts: An array of shape (n_unique,) containing the number of occurrences of each unique value in x.",
      "Returns a tuple (values, counts)."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.unique(): General function for computing unique values.",
      "jax.numpy.unique_values(): Compute only values.",
      "jax.numpy.unique_inverse(): Compute only values and inverse.",
      "jax.numpy.unique_all(): Compute values, indices, inverse_indices, and counts."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage",
    "concepts": [
      "Demonstrates how to use jnp.unique_counts to find unique values and their counts in a 1D array.",
      "Shows how to access the unique values and their counts from the result."
    ],
    "code_examples": [
      {
        "description": "Computes the unique values and their counts in a 1D array.",
        "code": "x = jnp.array([3, 4, 1, 3, 1])\nresult = jnp.unique_counts(x)"
      },
      {
        "description": "Accesses the unique values from the result.",
        "code": "result.values\n# Array([1, 3, 4], dtype=int32)"
      },
      {
        "description": "Accesses the counts of each unique value from the result.",
        "code": "result.counts\n# Array([2, 2, 1], dtype=int32)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.unique_inverse",
    "concepts": [
      "jax.numpy.unique_inverse() is a JAX implementation of numpy.unique_inverse().",
      "It returns unique values, indices, inverse indices, and counts from an array.",
      "It is equivalent to calling jax.numpy.unique() with return_inverse and equal_nan set to True.",
      "The function is typically not compatible with jit() and other JAX transformations because the output size is data-dependent.",
      "The JAX version adds the optional size argument which must be specified statically for jnp.unique to be used in such contexts.",
      "The function returns a tuple containing values, indices, inverse_indices, and counts."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of jax.numpy.unique_inverse",
    "concepts": [
      "x is the N-dimensional array from which unique values will be extracted.",
      "size (optional) specifies the number of sorted unique elements to return. If there are fewer unique elements than size indicates, the return value will be padded with fill_value.",
      "fill_value (optional) is used to pad the return value when size is specified and there are fewer unique elements than the indicated number of elements. It defaults to the minimum unique value."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value of jax.numpy.unique_inverse",
    "concepts": [
      "values: An array of shape (n_unique,) containing the unique values from x.",
      "inverse_indices: An array of shape x.shape containing the indices within values of each value in x. For 1D inputs, values[inverse_indices] is equivalent to x."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of jax.numpy.unique_inverse",
    "concepts": [
      "Demonstrates how to compute the unique values and inverse indices in a 1D array using jax.numpy.unique_inverse().",
      "Shows how to access the values and inverse_indices attributes of the result.",
      "Demonstrates how to verify that x is equivalent to result.values[result.inverse_indices]."
    ],
    "code_examples": [
      {
        "description": "Compute the unique values and inverse indices in a 1D array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nresult = jnp.unique_inverse(x)"
      },
      {
        "description": "Access the unique values from the array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nresult = jnp.unique_inverse(x)\n\nresult.values"
      },
      {
        "description": "Access the inverse indices.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nresult = jnp.unique_inverse(x)\n\nresult.inverse_indices"
      },
      {
        "description": "Verify that x is equivalent to result.values[result.inverse_indices].",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nresult = jnp.unique_inverse(x)\n\njnp.all(x == result.values[result.inverse_indices])"
      }
    ]
  },
  {
    "title": "Introduction to `jax.numpy.unique_values()`",
    "concepts": [
      "The function `jax.numpy.unique_values()` returns the unique values from an array.",
      "It's the JAX equivalent of `numpy.unique()` with `equal_nan` set to `True`.",
      "The output size is data-dependent, which can be problematic for JIT compilation.",
      "An optional `size` argument can be provided for JIT compatibility.",
      "A `fill_value` argument is available to pad the results if the number of unique elements is less than the specified size."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Example",
    "concepts": [
      "Demonstrates finding unique values in a 1D JAX array."
    ],
    "code_examples": [
      {
        "description": "Computes the unique values in a 1D JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nresult = jnp.unique_values(x)\nprint(result)"
      },
      {
        "description": "Computes the unique values in a 1D JAX array (repeated example).",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([3, 4, 1, 3, 1])\nresult = jnp.unique_values(x)\nprint(result)"
      }
    ]
  },
  {
    "title": "Size and Fill Value",
    "concepts": [
      "Refer to jax.numpy.unique() for examples on size and fill_value arguments."
    ],
    "code_examples": []
  },
  {
    "title": "Unpacking Bits with jnp.unpackbits()",
    "concepts": [
      "The function unpacks bits from a uint8 array.",
      "The input array can be N-dimensional.",
      "The 'axis' parameter specifies the axis along which to unpack. If not specified, the array is flattened.",
      "The 'count' parameter specifies the number of bits to unpack (positive) or trim (negative).",
      "The 'bitorder' parameter specifies the bit order: 'big' (default) or 'little'.",
      "The function returns a uint8 array of unpacked bits.",
      "jax.numpy.packbits() is the inverse of unpackbits()."
    ],
    "code_examples": [
      {
        "description": "Unpacking bits from a scalar with big-endian and little-endian order.",
        "code": ">>> jnp.unpackbits(jnp.uint8(27))\n# big-endian by default\nArray([0, 0, 0, 1, 1, 0, 1, 1], dtype=uint8)\n>>> jnp.unpackbits(jnp.uint8(27), bitorder=\"little\")\nArray([1, 1, 0, 1, 1, 0, 0, 0], dtype=uint8)\n>>> jnp.unpackbits(jnp.uint8(27))\n# big-endian by default\nArray([0, 0, 0, 1, 1, 0, 1, 1], dtype=uint8)\n>>> jnp.unpackbits(jnp.uint8(27), bitorder=\"little\")\nArray([1, 1, 0, 1, 1, 0, 0, 0], dtype=uint8)\n>>> 0b00011011\n27\n>>> 0b00011011\n27"
      },
      {
        "description": "Unpacking bits along an axis.",
        "code": ">>> vals = jnp.array([[154],\n...                [ 49]], dtype='uint8')\n>>> bits = jnp.unpackbits(vals, axis=1)\n>>> bits\nArray([[1, 0, 0, 1, 1, 0, 1, 0],\n       [0, 0, 1, 1, 0, 0, 0, 1]], dtype=uint8)\n>>> vals = jnp.array([[154],\n...                [ 49]], dtype='uint8')\n>>> bits = jnp.unpackbits(vals, axis=1)\n>>> bits\nArray([[1, 0, 0, 1, 1, 0, 1, 0],\n       [0, 0, 1, 1, 0, 0, 0, 1]], dtype=uint8)"
      },
      {
        "description": "Using packbits() to invert unpackbits().",
        "code": ">>> jnp.packbits(bits, axis=1)\nArray([[154],\n       [ 49]], dtype=uint8)\n>>> jnp.packbits(bits, axis=1)\nArray([[154],\n       [ 49]], dtype=uint8)"
      },
      {
        "description": "Using the count keyword to specify the number of bits to unpack or trim.",
        "code": ">>> bits = jnp.array([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1])  # 11 bits\n>>> vals = jnp.packbits(bits)\n>>> vals\nArray([219,  96], dtype=uint8)\n>>> jnp.unpackbits(vals)  # 16 zero-padded bits\nArray([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0], dtype=uint8)\n>>> jnp.unpackbits(vals, count=11)  # specify 11 output bits\nArray([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], dtype=uint8)\n>>> jnp.unpackbits(vals, count=-5)  # specify 5 bits to be trimmed\nArray([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], dtype=uint8)\n>>> bits = jnp.array([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1])  # 11 bits\n>>> vals = jnp.packbits(bits)\n>>> vals\nArray([219,  96], dtype=uint8)\n>>> jnp.unpackbits(vals)  # 16 zero-padded bits\nArray([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0], dtype=uint8)\n>>> jnp.unpackbits(vals, count=11)  # specify 11 output bits\nArray([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], dtype=uint8)\n>>> jnp.unpackbits(vals, count=-5)  # specify 5 bits to be trimmed\nArray([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1], dtype=uint8)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.unravel_index",
    "concepts": [
      "Converts flat indices to multi-dimensional indices.",
      "JAX implementation of numpy.unravel_index().",
      "Supports negative indices and clips out-of-bound indices.",
      "The function takes flat indices and the shape of the array as input.",
      "Returns a tuple of unraveled indices."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.unravel_index",
    "concepts": [
      "Demonstrates accessing elements of a 1D JAX array using indices.",
      "Reshapes a 1D array into a 2D array.",
      "Uses jnp.unravel_index to convert flat indices to 2D indices.",
      "Accesses elements of the reshaped array using the unraveled indices.",
      "Shows that jnp.unravel_index and array reshaping can be used to access the same elements."
    ],
    "code_examples": [
      {
        "description": "Accessing elements of a 1D array using indices.",
        "code": "x = jnp.array([2., 3., 4., 5., 6., 7.])\nindices = jnp.array([1, 3, 5])\nprint(x[indices])"
      },
      {
        "description": "Reshaping a 1D array and using unravel_index to access elements.",
        "code": "x = jnp.array([2., 3., 4., 5., 6., 7.])\nindices = jnp.array([1, 3, 5])\nshape = (2, 3)\nx_2D = x.reshape(shape)\nindices_2D = jnp.unravel_index(indices, shape)\nprint(x_2D[indices_2D])"
      }
    ]
  },
  {
    "title": "Inverse Function: jnp.ravel_multi_index",
    "concepts": [
      "Demonstrates the use of jnp.ravel_multi_index as the inverse of jnp.unravel_index.",
      "jnp.ravel_multi_index converts multi-dimensional indices back to flat indices.",
      "It takes the tuple of multi-dimensional indices and the shape of the array as input.",
      "The output of jnp.ravel_multi_index matches the original flat indices."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the inverse function, ravel_multi_index, to obtain the original indices.",
        "code": "x = jnp.array([2., 3., 4., 5., 6., 7.])\nindices = jnp.array([1, 3, 5])\nshape = (2, 3)\nx_2D = x.reshape(shape)\nindices_2D = jnp.unravel_index(indices, shape)\nprint(jnp.ravel_multi_index(indices_2D, shape))"
      }
    ]
  },
  {
    "title": "Unstacking Arrays with JAX",
    "concepts": [
      "The function `jnp.unstack()` unstacks an array along a specified axis.",
      "The input array must have at least one dimension.",
      "The axis along which to unstack must be a valid axis for the array.",
      "`jnp.unstack()` returns a tuple of arrays.",
      "`jnp.stack()` is the inverse operation of `jnp.unstack()`."
    ],
    "code_examples": [
      {
        "description": "Demonstrates unstacking a 2D array along the default axis (axis=0).",
        "code": "arr = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\narrs = jnp.unstack(arr)\nprint(*arrs)"
      },
      {
        "description": "Demonstrates the inverse relationship between `jnp.unstack()` and `jnp.stack()`.",
        "code": "arrs = (jnp.array([1, 2, 3]), jnp.array([4, 5, 6]))\njnp.stack(arrs)"
      }
    ]
  },
  {
    "title": "Unsigned Integer Scalar Type Overview",
    "concepts": [
      "This is an abstract base class for all unsigned integer scalar types.",
      "It defines the common interface for these types."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "__init__(): Initialization method.",
      "Many methods are scalar equivalents of array attributes (e.g., all, any, argmax, astype).",
      "Methods like is_integer() check for integral values.",
      "Methods for manipulating the scalar, such as copy, clip, and round.",
      "Methods for array-like operations such as cumsum, searchsorted, and transpose.",
      "Methods related to file I/O, such as tofile."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "Attributes provide access to scalar properties.",
      "T: Scalar attribute identical to the corresponding array attribute.",
      "data: Pointer to the start of the data.",
      "denominator: Denominator of the value (1).",
      "dtype: Array data-descriptor.",
      "flags: The integer value of flags.",
      "flat: A 1-D view of the scalar.",
      "imag: The imaginary part of the scalar.",
      "itemsize: The length of one element in bytes.",
      "nbytes: Total bytes consumed by the elements.",
      "ndim: The number of array dimensions.",
      "numerator: Numerator of the value (the value itself).",
      "real: The real part of the scalar.",
      "shape: Tuple of array dimensions.",
      "size: The number of elements.",
      "strides: Tuple of bytes steps in each dimension."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Signal Unwrapping",
    "concepts": [
      "The document describes a JAX implementation of numpy.unwrap().",
      "The function unwraps a periodic signal.",
      "It takes an input array p, maximum discontinuity discont, axis, and period as input.",
      "It returns an unwrapped copy of p.",
      "The default discontinuity is period / 2.",
      "The default axis is -1.",
      "The default period is 2\u03c0."
    ],
    "code_examples": []
  },
  {
    "title": "Example: Recovering Angle from Sine and Cosine Observations",
    "concepts": [
      "The example demonstrates how to recover an always-increasing angle from x and y coordinates derived from sine and cosine.",
      "The initial angle theta is generated using cumsum of random integers.",
      "The x and y coordinates are calculated using jnp.sin() and jnp.cos() applied to theta.",
      "The atan2() function is used to estimate the original angle from x and y.",
      "The unwrap() function is then applied to the estimated angle to recover the original signal."
    ],
    "code_examples": [
      {
        "description": "Generate the original angle theta as a cumulative sum of random integers between 0 and 90.",
        "code": "rng = np.random.default_rng(0)\ntheta = rng.integers(0, 90, size=(20,)).cumsum()\nprint(theta)"
      },
      {
        "description": "Calculate the x and y coordinates from the angle theta using sine and cosine functions.",
        "code": "x, y = jnp.sin(jnp.deg2rad(theta)), jnp.cos(jnp.deg2rad(theta))\nprint(x,y)"
      },
      {
        "description": "Estimate the angle theta_out using atan2 from the calculated x and y coordinates.",
        "code": "theta_out = jnp.rad2deg(jnp.atan2(x, y)).round()\nprint(theta_out)"
      },
      {
        "description": "Unwrap the estimated angle theta_out using jnp.unwrap() to recover the original angle theta.",
        "code": "import numpy as np\nimport jax.numpy as jnp\nimport jax\n\nrng = np.random.default_rng(0)\ntheta = rng.integers(0, 90, size=(20,)).cumsum()\nx, y = jnp.sin(jnp.deg2rad(theta)), jnp.cos(jnp.deg2rad(theta))\ntheta_out = jnp.rad2deg(jnp.atan2(x, y)).round()\nprint(jnp.unwrap(theta_out, period=360))"
      }
    ]
  },
  {
    "title": "Unwrap Function's Behavior and Assumptions",
    "concepts": [
      "unwrap() assumes the true underlying sequence doesn't change by more than discont within a single step.",
      "If a larger discontinuity is encountered, factors of the period are added to the data.",
      "For periodic signals satisfying this assumption, unwrap() recovers the original phased signal."
    ],
    "code_examples": []
  },
  {
    "title": "Vandermonde Matrix Generation",
    "concepts": [
      "The function generates a Vandermonde matrix from an input array.",
      "The input array must be one-dimensional.",
      "The number of columns in the output matrix can be specified.",
      "The order of powers in the columns can be increasing or decreasing."
    ],
    "code_examples": [
      {
        "description": "Generates a Vandermonde matrix with default parameters (N = len(x), increasing=False).",
        "code": "x = jnp.array([1, 2, 3, 4])\njnp.vander(x)"
      },
      {
        "description": "Generates a Vandermonde matrix with N = 2.",
        "code": "jnp.vander(x, N=2)"
      },
      {
        "description": "Generates a Vandermonde matrix with increasing powers.",
        "code": "jnp.vander(x, increasing=True)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.var",
    "concepts": [
      "Computes the variance along a specified axis of an array.",
      "It is a JAX implementation of numpy.var().",
      "Various parameters such as axis, dtype, ddof, keepdims, and where can be used to control the computation.",
      "The function returns an array containing the variance along the specified axis."
    ],
    "code_examples": []
  },
  {
    "title": "Variance Calculation Along All Axes",
    "concepts": [
      "By default, jnp.var computes the variance along all axes of the input array."
    ],
    "code_examples": [
      {
        "description": "This example demonstrates the default behavior of jnp.var, which calculates the variance across all elements of the array.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 4, 2, 9]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    jnp.var(x)"
      },
      {
        "description": "This example demonstrates the default behavior of jnp.var, which calculates the variance across all elements of the array.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 4, 2, 9]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    jnp.var(x)"
      }
    ]
  },
  {
    "title": "Variance Calculation Along a Specific Axis",
    "concepts": [
      "The axis parameter can be used to specify the axis along which the variance is computed.",
      "axis=1 computes the variance along the rows (axis 1)."
    ],
    "code_examples": [
      {
        "description": "This example calculates the variance along axis 1 (rows) of the input array.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 4, 2, 9]])\n\njnp.var(x, axis=1)"
      },
      {
        "description": "This example calculates the variance along axis 1 (rows) of the input array.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 4, 2, 9]])\n\njnp.var(x, axis=1)"
      }
    ]
  },
  {
    "title": "Preserving Dimensions with keepdims",
    "concepts": [
      "Setting keepdims=True preserves the dimensions of the input array in the output, with reduced axes having size 1."
    ],
    "code_examples": [
      {
        "description": "This example demonstrates how to preserve the dimensions of the input array by setting keepdims=True when calculating the variance along axis 1.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 4, 2, 9]])\n\njnp.var(x, axis=1, keepdims=True)"
      },
      {
        "description": "This example demonstrates how to preserve the dimensions of the input array by setting keepdims=True when calculating the variance along axis 1.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 4, 2, 9]])\n\njnp.var(x, axis=1, keepdims=True)"
      }
    ]
  },
  {
    "title": "Degrees of Freedom (ddof)",
    "concepts": [
      "The ddof parameter controls the degrees of freedom used in the variance calculation (N - ddof).",
      "Setting ddof=1 uses N-1 as the divisor."
    ],
    "code_examples": [
      {
        "description": "This example sets ddof=1 to calculate the sample variance along axis 1, preserving the dimensions of the input array.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 4, 2, 9]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.var(x, axis=1, keepdims=True, ddof=1))"
      },
      {
        "description": "This example sets ddof=1 to calculate the sample variance along axis 1, preserving the dimensions of the input array.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 4, 2, 9]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.var(x, axis=1, keepdims=True, ddof=1))"
      }
    ]
  },
  {
    "title": "Selective Variance Calculation with where",
    "concepts": [
      "The where parameter allows you to specify which elements of the array to include in the variance calculation.",
      "The where array should be boolean and broadcast compatible with the input array."
    ],
    "code_examples": [
      {
        "description": "This example demonstrates how to use the where parameter to calculate the variance along axis 1, only considering elements where the corresponding value in the where array is True.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 4, 2, 9]])\n\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 1, 1, 0],\n                   [1, 1, 1, 0]], dtype=bool)\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.var(x, axis=1, keepdims=True, where=where))"
      },
      {
        "description": "This example demonstrates how to use the where parameter to calculate the variance along axis 1, only considering elements where the corresponding value in the where array is True.",
        "code": "x = jnp.array([[1, 3, 4, 2],\n               [5, 2, 6, 3],\n               [8, 4, 2, 9]])\n\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 1, 1, 0],\n                   [1, 1, 1, 0]], dtype=bool)\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.var(x, axis=1, keepdims=True, where=where))"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.vecmat",
    "concepts": [
      "jax.numpy.vecmat() is a JAX implementation of numpy.vecmat().",
      "It computes the batched conjugate vector-matrix product.",
      "The first input `x1` is an array of shape (..., M).",
      "The second input `x2` is an array of shape (..., M, N).",
      "Leading dimensions of `x1` and `x2` must be broadcast-compatible.",
      "The output is an array of shape (..., N)."
    ],
    "code_examples": []
  },
  {
    "title": "Simple Vector-Matrix Product Example",
    "concepts": [
      "Demonstrates a simple vector-matrix product using jax.numpy.vecmat().",
      "Shows the use of jnp.array() to define the input arrays.",
      "Illustrates the output shape and data type."
    ],
    "code_examples": [
      {
        "description": "Simple vector-matrix product example",
        "code": "x1 = jnp.array([[1, 2, 3]])\nx2 = jnp.array([[4, 5],\n                [6, 7],\n                [8, 9]])\njnp.vecmat(x1, x2)"
      },
      {
        "description": "Simple vector-matrix product example",
        "code": "x1 = jnp.array([[1, 2, 3]])\nx2 = jnp.array([[4, 5],\n                [6, 7],\n                [8, 9]])\njnp.vecmat(x1, x2)"
      }
    ]
  },
  {
    "title": "Batched Vector-Matrix Product Example",
    "concepts": [
      "Demonstrates a batched vector-matrix product using jax.numpy.vecmat().",
      "Illustrates the use of jnp.array() to define the batched input arrays.",
      "Shows how jax.numpy.vecmat() handles batched computations."
    ],
    "code_examples": [
      {
        "description": "Batched vector-matrix product example",
        "code": "x1 = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\njnp.vecmat(x1, x2)"
      },
      {
        "description": "Batched vector-matrix product example",
        "code": "x1 = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\njnp.vecmat(x1, x2)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.vsplit",
    "concepts": [
      "jnp.vsplit is a JAX implementation of numpy.vsplit.",
      "jnp.vsplit splits an array into sub-arrays vertically.",
      "jnp.vsplit is equivalent to jnp.split with axis=0."
    ],
    "code_examples": []
  },
  {
    "title": "Splitting a 1D Array",
    "concepts": [
      "Splitting a 1D JAX array into two sub-arrays using jnp.vsplit."
    ],
    "code_examples": [
      {
        "description": "Splits a 1D jax array into two sub-arrays",
        "code": "x = jnp.array([1, 2, 3, 4, 5, 6])\nx1, x2 = jnp.vsplit(x, 2)\nprint(x1, x2)"
      },
      {
        "description": "Splits a 1D jax array into two sub-arrays",
        "code": "x = jnp.array([1, 2, 3, 4, 5, 6])\nx1, x2 = jnp.vsplit(x, 2)\nprint(x1, x2)"
      }
    ]
  },
  {
    "title": "Splitting a 2D Array",
    "concepts": [
      "Splitting a 2D JAX array into two sub-arrays using jnp.vsplit."
    ],
    "code_examples": [
      {
        "description": "Splits a 2D jax array into two sub-arrays",
        "code": "x = jnp.array([[1, 2, 3, 4],\n              [5, 6, 7, 8]])\nx1, x2 = jnp.vsplit(x, 2)\nprint(x1, x2)"
      },
      {
        "description": "Splits a 2D jax array into two sub-arrays",
        "code": "x = jnp.array([[1, 2, 3, 4],\n              [5, 6, 7, 8]])\nx1, x2 = jnp.vsplit(x, 2)\nprint(x1, x2)"
      }
    ]
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.split: split an array along any axis.",
      "jax.numpy.hsplit: split horizontally, i.e. along axis=1",
      "jax.numpy.dsplit: split depth-wise, i.e. along axis=2",
      "jax.numpy.array_split: like split , but allows indices_or_sections to be an integer that does not evenly divide the size of the array."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.vstack",
    "concepts": [
      "vstack stacks arrays vertically.",
      "For arrays with two or more dimensions, vstack is equivalent to concatenate with axis=0.",
      "The input 'tup' is a sequence of arrays with the same shape along all but the first axis.",
      "The 'dtype' argument specifies the data type of the resulting array."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of vstack with Scalar Values",
    "concepts": [
      "vstack can be used to stack scalar values into a 2D array.",
      "Scalar values are converted to a column vector before being stacked."
    ],
    "code_examples": [
      {
        "description": "Stacking scalar values 1, 2, and 3 vertically.",
        "code": "jnp.vstack([1, 2, 3])"
      },
      {
        "description": "Stacking scalar values 1, 2, and 3 vertically.",
        "code": "jnp.vstack([1, 2, 3])"
      }
    ]
  },
  {
    "title": "Examples of vstack with 1D Arrays",
    "concepts": [
      "vstack can be used to stack 1D arrays vertically.",
      "The resulting array has the shape (number of arrays, length of each array)."
    ],
    "code_examples": [
      {
        "description": "Stacking 1D arrays x and y vertically, where x is arange(4) and y is ones(4).",
        "code": "x = jnp.arange(4)\ny = jnp.ones(4)\njnp.vstack([x, y])"
      },
      {
        "description": "Stacking 1D arrays x and y vertically, where x is arange(4) and y is ones(4).",
        "code": "x = jnp.arange(4)\ny = jnp.ones(4)\njnp.vstack([x, y])"
      }
    ]
  },
  {
    "title": "Examples of vstack with 2D Arrays",
    "concepts": [
      "vstack can be used to stack 2D arrays vertically.",
      "Arrays must have compatible shapes for stacking along the first axis.",
      "In this example, 1D arrays are reshaped to 2D arrays before stacking."
    ],
    "code_examples": [
      {
        "description": "Stacking 2D arrays x and y vertically, where x and y are reshaped from 1D arrays.",
        "code": "x = x.reshape(1, 4)\ny = y.reshape(1, 4)\njnp.vstack([x, y])"
      },
      {
        "description": "Stacking 2D arrays x and y vertically, where x and y are reshaped from 1D arrays.",
        "code": "x = x.reshape(1, 4)\ny = y.reshape(1, 4)\njnp.vstack([x, y])"
      }
    ]
  },
  {
    "title": "Introduction to jnp.where",
    "concepts": [
      "jnp.where() is a JAX implementation of numpy.where().",
      "When only a condition is provided, jnp.where(condition) is equivalent to jnp.nonzero(condition).",
      "The three-term version of jnp.where lowers to jax.lax.select().",
      "The function returns an array with values from x where the condition is True, and from y where the condition is False.",
      "Special care is needed when x or y input could have a NaN value."
    ],
    "code_examples": []
  },
  {
    "title": "jnp.where() with only a condition",
    "concepts": [
      "When only a condition is provided, jnp.where behaves like jnp.nonzero.",
      "It returns the indices where the condition is true."
    ],
    "code_examples": [
      {
        "description": "Demonstrates jnp.where with only a condition, equivalent to jnp.nonzero.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(10)\nprint(jnp.where(x > 4))\nprint(jnp.nonzero(x > 4))"
      },
      {
        "description": "Demonstrates jnp.where with only a condition, equivalent to jnp.nonzero.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(10)\nprint(jnp.where(x > 4))\nprint(jnp.nonzero(x > 4))"
      }
    ]
  },
  {
    "title": "jnp.where() with condition, x, and y",
    "concepts": [
      "When x and y are provided, jnp.where selects elements from x where the condition is True and from y where the condition is False."
    ],
    "code_examples": [
      {
        "description": "Demonstrates jnp.where with a condition, x, and y to select elements based on the condition.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(10)\nprint(jnp.where(x > 4, x, 0))"
      },
      {
        "description": "Demonstrates jnp.where with a condition, x, and y to select elements based on the condition.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(10)\nprint(jnp.where(x > 4, x, 0))"
      }
    ]
  },
  {
    "title": "Creating Zero-Filled Arrays with jax.numpy.zeros",
    "concepts": [
      "Creates an array filled with zeros using JAX.",
      "The shape of the array is specified using an integer or a sequence of integers.",
      "The data type (dtype) of the array elements can be specified; defaults to floating point.",
      "The device or sharding to which the created array will be committed can be specified.",
      "The function returns an array of the specified shape and dtype, filled with zeros."
    ],
    "code_examples": [
      {
        "description": "Create a 1D array of shape (4) filled with zeros (float32 by default).",
        "code": ">>> jnp.zeros(4)\nArray([0., 0., 0., 0.], dtype=float32)"
      },
      {
        "description": "Create a 2D array of shape (2, 3) filled with boolean zeros.",
        "code": ">>> jnp.zeros((2, 3), dtype=bool)\nArray([[False, False, False],\n       [False, False, False]], dtype=bool)"
      },
      {
        "description": "Create a 1D array of shape (4) filled with zeros (float32 by default).",
        "code": ">>> jnp.zeros(4)\nArray([0., 0., 0., 0.], dtype=float32)"
      },
      {
        "description": "Create a 2D array of shape (2, 3) filled with boolean zeros.",
        "code": ">>> jnp.zeros((2, 3), dtype=bool)\nArray([[False, False, False],\n       [False, False, False]], dtype=bool)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.fft.fft",
    "concepts": [
      "Computes a one-dimensional discrete Fourier transform along a given axis.",
      "It is a JAX implementation of numpy.fft.fft().",
      "The input is an array-like object.",
      "The transform can be computed along a specified axis, defaulting to the last axis.",
      "The size of the output array along the transformed axis can be specified.",
      "Normalization modes are supported (backward, ortho, and forward)."
    ],
    "code_examples": []
  },
  {
    "title": "Default Usage of jnp.fft.fft",
    "concepts": [
      "Demonstrates the default behavior of jnp.fft.fft, which computes the transform along the last axis (axis -1).",
      "Shows the output of the FFT on a 2D array."
    ],
    "code_examples": [
      {
        "description": "Computes the FFT of a 2D array along the default axis (-1).",
        "code": "x = jnp.array([[1, 2, 4, 7],\n               [5, 3, 1, 9]])\njnp.fft.fft(x)"
      },
      {
        "description": "Computes the FFT of a 2D array along the default axis (-1).",
        "code": "x = jnp.array([[1, 2, 4, 7],\n               [5, 3, 1, 9]])\njnp.fft.fft(x)"
      }
    ]
  },
  {
    "title": "Specifying the Transform Size with 'n'",
    "concepts": [
      "Illustrates how to specify the size of the output array along the transformed axis using the 'n' parameter.",
      "Demonstrates that the dimensions along other axes remain the same as the input array."
    ],
    "code_examples": [
      {
        "description": "Computes the FFT with a specified output size (n=3) along the default axis (-1).  The jnp.printoptions context manager is used for cleaner output.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.fft(x, n=3))"
      },
      {
        "description": "Computes the FFT with a specified output size (n=3) along the default axis (-1).  The jnp.printoptions context manager is used for cleaner output.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.fft(x, n=3))"
      }
    ]
  },
  {
    "title": "Specifying the Axis and Transform Size",
    "concepts": [
      "Demonstrates how to specify both the axis along which the transform is computed and the size of the output array.",
      "Shows the impact on the output shape when transforming along a different axis (axis=0)."
    ],
    "code_examples": [
      {
        "description": "Computes the FFT with a specified output size (n=3) along axis 0.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.fft(x, n=3, axis=0))"
      },
      {
        "description": "Computes the FFT with a specified output size (n=3) along axis 0.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.fft(x, n=3, axis=0))"
      }
    ]
  },
  {
    "title": "Reconstructing the Input with jnp.fft.ifft",
    "concepts": [
      "Illustrates the use of jnp.fft.ifft to reconstruct the original input array from its FFT.",
      "Demonstrates the use of jnp.allclose to verify the reconstruction."
    ],
    "code_examples": [
      {
        "description": "Computes the FFT, then the inverse FFT, and verifies that the result is close to the original input.",
        "code": "x_fft = jnp.fft.fft(x)\njnp.allclose(x, jnp.fft.ifft(x_fft))"
      },
      {
        "description": "Computes the FFT, then the inverse FFT, and verifies that the result is close to the original input.",
        "code": "x_fft = jnp.fft.fft(x)\njnp.allclose(x, jnp.fft.ifft(x_fft))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.fft.fft2",
    "concepts": [
      "Computes a two-dimensional discrete Fourier transform along specified axes.",
      "JAX implementation of numpy.fft.fft2().",
      "The input array must have at least two dimensions.",
      "The size of the output along each specified axis can be controlled using the 's' parameter.",
      "The 'axes' parameter specifies the axes along which the transform is computed, defaulting to the last two axes.",
      "The 'norm' parameter specifies the normalization mode.",
      "Returns an array containing the two-dimensional discrete Fourier transform."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.fft.fft2",
    "concepts": [
      "jnp.fft.fft2 computes the transform along the last two axes by default."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the default behavior of jnp.fft.fft2, computing the 2D DFT along the last two axes of the input array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3],\n              [2, 4]],\n             [[5, 7],\n              [6, 8]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.fft.fft2(x))"
      }
    ]
  },
  {
    "title": "Specifying Output Size with the 's' Parameter",
    "concepts": [
      "The 's' parameter allows specifying the output size along the transformed axes.",
      "If s=[2, 3], the transform along axes (-2, -1) will have shape (2, 3)."
    ],
    "code_examples": [
      {
        "description": "Shows how to use the 's' parameter to specify the output size of the DFT along the last two axes.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3],\n              [2, 4]],\n             [[5, 7],\n              [6, 8]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.fft.fft2(x, s=[2, 3]))"
      }
    ]
  },
  {
    "title": "Specifying Axes with the 'axes' Parameter",
    "concepts": [
      "The 'axes' parameter determines which axes are transformed.",
      "When s=[2, 3] and axes=(0, 1), the transform along axes (0, 1) will be (2, 3)."
    ],
    "code_examples": [
      {
        "description": "Illustrates the use of both 's' and 'axes' parameters to control the output size and the transformed axes.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3],\n              [2, 4]],\n             [[5, 7],\n              [6, 8]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.fft.fft2(x, s=[2, 3], axes=(0, 1)))"
      }
    ]
  },
  {
    "title": "Reconstructing with jnp.fft.ifft2",
    "concepts": [
      "jnp.fft.ifft2 can reconstruct the original array from the result of jnp.fft.fft2."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the inverse transform to reconstruct the original array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3],\n              [2, 4]],\n             [[5, 7],\n              [6, 8]]])\n\nx_fft2 = jnp.fft.fft2(x)\n\nprint(jnp.allclose(x, jnp.fft.ifft2(x_fft2)))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.fft.fftfreq()",
    "concepts": [
      "Returns sample frequencies for the discrete Fourier transform.",
      "JAX implementation of numpy.fft.fftfreq().",
      "Returns frequencies appropriate for use with the outputs of fft() and ifft().",
      "n is the length of the FFT window.",
      "d is an optional scalar sample spacing (default: 1.0).",
      "dtype is optional dtype of returned frequencies; if not specified, JAX\u2019s default floating point dtype will be used.",
      "device is the optional Device or Sharding to which the created array will be committed.",
      "Returns an array of sample frequencies, length n."
    ],
    "code_examples": []
  },
  {
    "title": "Description of fftshift",
    "concepts": [
      "fftshift shifts the zero-frequency component to the center of the spectrum.",
      "It's a JAX implementation of numpy.fft.fftshift().",
      "The input is an N-dimensional array of frequencies.",
      "The axes argument specifies which axes to shift; if None, all axes are shifted.",
      "It returns a shifted copy of the input array."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.fft.ifftshift() is the inverse of fftshift.",
      "jax.numpy.fft.fftfreq() generates FFT frequencies."
    ],
    "code_examples": []
  },
  {
    "title": "FFT Frequencies Generation and Shifting",
    "concepts": [
      "fftfreq() is used to generate FFT frequencies.",
      "fftshift() shifts the zero-frequency entry to the middle of the array.",
      "ifftshift() unshifts the shifted frequencies to recover the original frequencies."
    ],
    "code_examples": [
      {
        "description": "Generate FFT frequencies using jnp.fft.fftfreq(5).",
        "code": "freq = jnp.fft.fftfreq(5)\nfreq"
      },
      {
        "description": "Shift the zero-frequency entry to the middle of the array using jnp.fft.fftshift(freq).",
        "code": "shifted_freq = jnp.fft.fftshift(freq)\nshifted_freq"
      },
      {
        "description": "Unshift the shifted frequencies to recover the original frequencies using jnp.fft.ifftshift(shifted_freq).",
        "code": "jnp.fft.ifftshift(shifted_freq)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.fft.ifft",
    "concepts": [
      "Computes the one-dimensional inverse discrete Fourier transform.",
      "JAX implementation of numpy.fft.ifft().",
      "The input is an ArrayLike object.",
      "The parameter n specifies the dimension of the result along axis.",
      "The parameter axis specifies the axis along which the transform is computed.",
      "The parameter norm specifies the normalization mode.",
      "Returns an array containing the one-dimensional discrete Fourier transform of a."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.fft.ifft",
    "concepts": [
      "Demonstrates basic usage of jnp.fft.ifft with a 2D array.",
      "The transform is computed along the default axis (-1).",
      "Shows the output as a complex array.",
      "Illustrates the default behavior without specifying additional parameters."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the default usage of jnp.fft.ifft along axis -1.",
        "code": "x = jnp.array([[3, 1, 4, 6],\n               [2, 5, 7, 1]])\njnp.fft.ifft(x)"
      },
      {
        "description": "Demonstrates the default usage of jnp.fft.ifft along axis -1.",
        "code": "x = jnp.array([[3, 1, 4, 6],\n               [2, 5, 7, 1]])\njnp.fft.ifft(x)"
      }
    ]
  },
  {
    "title": "Specifying the Transform Length (n)",
    "concepts": [
      "Demonstrates how to specify the length of the transform using the 'n' parameter.",
      "Shows how the output changes when n is different from the input size.",
      "Illustrates the use of jnp.printoptions for formatting the output.",
      "The transform is computed along the default axis (-1)."
    ],
    "code_examples": [
      {
        "description": "Illustrates how to use the 'n' parameter to specify the transform length.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.ifft(x, n=5))"
      },
      {
        "description": "Illustrates how to use the 'n' parameter to specify the transform length.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.ifft(x, n=5))"
      }
    ]
  },
  {
    "title": "Specifying the Axis and Transform Length",
    "concepts": [
      "Demonstrates how to specify both the axis and the length of the transform.",
      "Shows the effect of changing the axis on which the transform is computed.",
      "The dimension of the transform along the specified axis is determined by 'n'."
    ],
    "code_examples": [
      {
        "description": "Illustrates how to use both 'n' and 'axis' parameters.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.ifft(x, n=3, axis=0))"
      },
      {
        "description": "Illustrates how to use both 'n' and 'axis' parameters.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.ifft(x, n=3, axis=0))"
      }
    ]
  },
  {
    "title": "Description of jnp.fft.ifft2",
    "concepts": [
      "Computes a two-dimensional inverse discrete Fourier transform.",
      "It's a JAX implementation of numpy.fft.ifft2().",
      "The input array must have a dimension of at least 2.",
      "The 's' parameter specifies the size of the output in each specified axis; defaults to the input size if not given.",
      "The 'axes' parameter specifies the axes along which the transform is computed; defaults to (-2, -1).",
      "The 'norm' parameter specifies the normalization mode ('backward', 'ortho', 'forward')."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.fft.ifft2",
    "concepts": [
      "The function computes the transform along the last two axes by default.",
      "Demonstrates the output of the function with a default configuration."
    ],
    "code_examples": [
      {
        "description": "Computes the 2D inverse FFT on the last two axes of the input array x.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3],\n                [2, 4]],\n\n               [[5, 7],\n                [6, 8]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.fft.ifft2(x))"
      },
      {
        "description": "Computes the 2D inverse FFT on the last two axes of the input array x. This example duplicates the previous.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3],\n                [2, 4]],\n\n               [[5, 7],\n                [6, 8]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.fft.ifft2(x))"
      }
    ]
  },
  {
    "title": "Using the 's' Parameter",
    "concepts": [
      "The 's' parameter defines the output shape of the transform.",
      "The dimension of the transform along axes (-2, -1) will be defined by 's', dimension along other axes will remain unchanged."
    ],
    "code_examples": [
      {
        "description": "Computes the 2D inverse FFT, specifying the output size to be (2, 3) using the 's' parameter.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3],\n                [2, 4]],\n\n               [[5, 7],\n                [6, 8]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.fft.ifft2(x, s=[2, 3]))"
      },
      {
        "description": "Computes the 2D inverse FFT, specifying the output size to be (2, 3) using the 's' parameter.  This example duplicates the previous one.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3],\n                [2, 4]],\n\n               [[5, 7],\n                [6, 8]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.fft.ifft2(x, s=[2, 3]))"
      }
    ]
  },
  {
    "title": "Using the 's' and 'axes' Parameters",
    "concepts": [
      "The 'axes' parameter changes the axes along which the FFT is computed.",
      "Demonstrates using both 's' and 'axes' to control the output shape and transform axes."
    ],
    "code_examples": [
      {
        "description": "Computes the 2D inverse FFT, specifying the output size to be (2, 3) and using axes (0, 1) for the transform.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3],\n                [2, 4]],\n\n               [[5, 7],\n                [6, 8]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.fft.ifft2(x, s=[2, 3], axes=(0, 1)))"
      },
      {
        "description": "Computes the 2D inverse FFT, specifying the output size to be (2, 3) and using axes (0, 1) for the transform.  This example duplicates the previous one.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3],\n                [2, 4]],\n\n               [[5, 7],\n                [6, 8]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jnp.fft.ifft2(x, s=[2, 3], axes=(0, 1)))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.fft.ifftn",
    "concepts": [
      "Computes the multidimensional inverse discrete Fourier transform.",
      "JAX implementation of numpy.fft.ifftn().",
      "The input is an array-like object.",
      "The shape of the result can be specified using the 's' parameter.",
      "The axes along which the transform is computed can be specified using the 'axes' parameter.",
      "The normalization mode can be specified using the 'norm' parameter, with 'backward', 'ortho', and 'forward' supported.",
      "If axes is None, computes the transform along all the axes."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.fft.ifftn usage",
    "concepts": [
      "jnp.fft.ifftn computes the transform along all the axes by default when axes argument is None.",
      "The 's' parameter defines the shape of the transformed array.",
      "The 'axes' parameter specifies the axes along which the inverse DFT is computed.",
      "The shape of the transform will be (2, 3) when s=[2, 3]."
    ],
    "code_examples": [
      {
        "description": "Compute the ifftn of a 2D array using default parameters.",
        "code": "x = jnp.array([[1, 2, 5, 3],\n               [4, 1, 2, 6],\n               [5, 3, 2, 1]])\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.ifftn(x))"
      },
      {
        "description": "Compute the ifftn of a 2D array specifying s=[3].",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.ifftn(x, s=[3]))"
      },
      {
        "description": "Compute the ifftn of a 2D array specifying s=[2] and axes=[0].",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.ifftn(x, s=[2], axes=[0]))"
      },
      {
        "description": "Compute the ifftn of a 2D array specifying s=[2, 3].",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.ifftn(x, s=[2, 3]))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.fft.ifftshift()",
    "concepts": [
      "jax.numpy.fft.ifftshift() is the inverse of jax.numpy.fft.fftshift().",
      "It's a JAX implementation of numpy.fft.ifftshift().",
      "The function shifts an array of frequencies.",
      "The axes to shift can be specified, or all axes are shifted by default.",
      "It returns a shifted copy of the input array."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of using jax.numpy.fft.ifftshift()",
    "concepts": [
      "Generating FFT frequencies using jnp.fft.fftfreq().",
      "Shifting the zero-frequency entry to the middle of the array using jnp.fft.fftshift().",
      "Recovering the original frequencies using jnp.fft.ifftshift()."
    ],
    "code_examples": [
      {
        "description": "Generate FFT frequencies with fftfreq()",
        "code": "freq = jnp.fft.fftfreq(5)\nfreq\nfreq = jnp.fft.fftfreq(5)\nfreq"
      },
      {
        "description": "Use fftshift() to shift the zero-frequency entry to the middle of the array:",
        "code": "shifted_freq = jnp.fft.fftshift(freq)\nshifted_freq\nshifted_freq = jnp.fft.fftshift(freq)\nshifted_freq"
      },
      {
        "description": "Unshift with ifftshift to recover the original frequencies:",
        "code": "jnp.fft.ifftshift(shifted_freq)\njnp.fft.ifftshift(shifted_freq)"
      }
    ]
  },
  {
    "title": "Introduction to ihfft",
    "concepts": [
      "ihfft computes the inverse FFT of an array with Hermitian symmetry.",
      "It's a JAX implementation of numpy.fft.ihfft().",
      "The input array 'a' is transformed.",
      "The parameter 'n' specifies the effective dimension of the input along the given axis.",
      "The parameter 'axis' specifies the axis along which the transform is computed (default is -1).",
      "The parameter 'norm' specifies the normalization mode ('backward', 'ortho', 'forward').",
      "The output array contains the 1D discrete Fourier transform exploiting Hermitian symmetry.",
      "The dimension of the output array along the specified axis depends on whether n is even or odd."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Examples",
    "concepts": [
      "Demonstrates basic usage of jnp.fft.ihfft with a 2D array.",
      "The function computes the inverse FFT along the default axis (-1).",
      "Shows the complex output array resulting from the ihfft operation."
    ],
    "code_examples": [
      {
        "description": "Computes the inverse FFT of a 2D array using jnp.fft.ihfft.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 5, 7],\n               [2, 4, 6, 8]])\n\njnp.fft.ihfft(x)"
      },
      {
        "description": "Computes the inverse FFT of a 2D array using jnp.fft.ihfft.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 5, 7],\n               [2, 4, 6, 8]])\n\njnp.fft.ihfft(x)"
      }
    ]
  },
  {
    "title": "ihfft with n and axis parameters",
    "concepts": [
      "Demonstrates the usage of jnp.fft.ihfft with specified 'n' and 'axis' parameters.",
      "The 'n' parameter determines the length of the transform.",
      "The 'axis' parameter specifies the axis along which the transform is applied.",
      "Illustrates how these parameters affect the shape and values of the output array."
    ],
    "code_examples": [
      {
        "description": "Computes the inverse FFT of a 2D array along axis 0, specifying n=4.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 5, 7],\n               [2, 4, 6, 8]])\n\njnp.fft.ihfft(x, n=4, axis=0)"
      },
      {
        "description": "Computes the inverse FFT of a 2D array along axis 0, specifying n=4.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 3, 5, 7],\n               [2, 4, 6, 8]])\n\njnp.fft.ihfft(x, n=4, axis=0)"
      }
    ]
  },
  {
    "title": "Description of jnp.fft.irfft",
    "concepts": [
      "Computes a real-valued one-dimensional inverse discrete Fourier transform.",
      "JAX implementation of numpy.fft.irfft().",
      "The input is an array-like object.",
      "The parameter 'n' specifies the dimension of the result along the given axis. If not specified, n = 2*(m-1) , where m is the dimension of a along axis.",
      "The parameter 'axis' specifies the axis along which the transform is computed; defaults to -1.",
      "The parameter 'norm' specifies the normalization mode.",
      "Returns a real-valued array containing the one-dimensional inverse discrete Fourier transform."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.fft.irfft",
    "concepts": [
      "jnp.fft.irfft computes the transform along axis -1 by default.",
      "Demonstrates the basic usage of irfft with a 2D array."
    ],
    "code_examples": [
      {
        "description": "Computes the inverse real FFT of a 2D array along the default axis (-1).",
        "code": "x = jnp.array([[1, 3, 5],\n               [2, 4, 6]])\njnp.fft.irfft(x)"
      },
      {
        "description": "Computes the inverse real FFT of a 2D array along the default axis (-1).",
        "code": "x = jnp.array([[1, 3, 5],\n               [2, 4, 6]])\njnp.fft.irfft(x)"
      }
    ]
  },
  {
    "title": "Specifying the Length of the Output",
    "concepts": [
      "Demonstrates how to use the 'n' parameter to specify the output length.",
      "When n=3, the dimension of the transform along axis -1 will be 3 and dimension along other axes will be the same as that of input."
    ],
    "code_examples": [
      {
        "description": "Computes the inverse real FFT of a 2D array with a specified output length (n=3) along the default axis.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  jnp.fft.irfft(x, n=3)"
      },
      {
        "description": "Computes the inverse real FFT of a 2D array with a specified output length (n=3) along the default axis.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  jnp.fft.irfft(x, n=3)"
      }
    ]
  },
  {
    "title": "Specifying the Axis",
    "concepts": [
      "Demonstrates how to use the 'axis' parameter to specify the axis along which to compute the transform.",
      "When n=4 and axis=0, dimension of the transform along axis 0 will be 4 and dimension along other axes will be the same as that of input."
    ],
    "code_examples": [
      {
        "description": "Computes the inverse real FFT of a 2D array with a specified output length (n=4) along axis 0.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  jnp.fft.irfft(x, n=4, axis=0)"
      },
      {
        "description": "Computes the inverse real FFT of a 2D array with a specified output length (n=4) along axis 0.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  jnp.fft.irfft(x, n=4, axis=0)"
      }
    ]
  },
  {
    "title": "Introduction to irfft2",
    "concepts": [
      "Computes a real-valued two-dimensional inverse discrete Fourier transform.",
      "JAX implementation of numpy.fft.irfft2().",
      "The input array must have a.ndim >= 2.",
      "The 's' parameter specifies the size of the output in each specified axis.",
      "The 'axes' parameter specifies the axes along which the transform is computed (default is (-2, -1)).",
      "The 'norm' parameter specifies the normalization mode ('backward', 'ortho', 'forward')."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of irfft2",
    "concepts": [
      "Demonstrates the basic usage of jnp.fft.irfft2 with a sample array.",
      "The transform is computed along the last two axes by default."
    ],
    "code_examples": [
      {
        "description": "Computes the inverse real FFT of a 3D array along the last two axes.",
        "code": "x = jnp.array([[[1, 3, 5],\n[2, 4, 6]],\n[[7, 9, 11],\n[8, 10, 12]]])\njnp.fft.irfft2(x)"
      },
      {
        "description": "Computes the inverse real FFT of a 3D array along the last two axes. (duplicate)",
        "code": "x = jnp.array([[[1, 3, 5],\n[2, 4, 6]],\n[[7, 9, 11],\n[8, 10, 12]]])\njnp.fft.irfft2(x)"
      }
    ]
  },
  {
    "title": "irfft2 with specified output shape (s)",
    "concepts": [
      "Illustrates the use of the 's' parameter to specify the output shape.",
      "The shape of the transformed array is determined by the 's' parameter along the specified axes."
    ],
    "code_examples": [
      {
        "description": "Computes the inverse real FFT with a specified output shape s=[3, 3].",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  jnp.fft.irfft2(x, s=[3, 3])"
      },
      {
        "description": "Computes the inverse real FFT with a specified output shape s=[3, 3] (duplicate).",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  jnp.fft.irfft2(x, s=[3, 3])"
      }
    ]
  },
  {
    "title": "irfft2 with specified output shape (s) and axes",
    "concepts": [
      "Demonstrates the use of both 's' and 'axes' parameters.",
      "Allows specifying both the output shape and the axes along which the transform is computed."
    ],
    "code_examples": [
      {
        "description": "Computes the inverse real FFT with s=[2, 3] and axes=(0, 1).",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  jnp.fft.irfft2(x, s=[2, 3], axes=(0, 1))"
      },
      {
        "description": "Computes the inverse real FFT with s=[2, 3] and axes=(0, 1) (duplicate).",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  jnp.fft.irfft2(x, s=[2, 3], axes=(0, 1))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.fft.rfft2",
    "concepts": [
      "Computes a two-dimensional discrete Fourier transform of a real-valued array.",
      "It is a JAX implementation of numpy.fft.rfft2().",
      "The input array must be real-valued and have at least two dimensions.",
      "The function allows specifying the output size along each axis using the 's' parameter.",
      "The 'axes' parameter allows specifying the axes along which the transform is computed.",
      "The 'norm' parameter specifies the normalization mode.",
      "The output array contains the two-dimensional discrete Fourier transform of the input.",
      "The size of the output along the last axis is determined by whether s[1] is even or odd.",
      "It relates to jax.numpy.fft.rfft, jax.numpy.fft.rfftn, and jax.numpy.fft.irfft2."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.fft.rfft2",
    "concepts": [
      "jnp.fft.rfft2 computes the transform along the last two axes by default.",
      "The example demonstrates the basic usage of the function with a 3D input array.",
      "The output is a complex array representing the Fourier transform.",
      "The example uses jnp.printoptions to format the output for better readability."
    ],
    "code_examples": [
      {
        "description": "Demonstrates basic usage of jnp.fft.rfft2 with a 3D input array. The transform is computed along the last two axes by default.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3, 5],\n                [2, 4, 6]],\n               [[7, 9, 11],\n                [8, 10, 12]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.rfft2(x))"
      },
      {
        "description": "Demonstrates basic usage of jnp.fft.rfft2 with a 3D input array. The transform is computed along the last two axes by default.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3, 5],\n                [2, 4, 6]],\n               [[7, 9, 11],\n                [8, 10, 12]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.rfft2(x))"
      }
    ]
  },
  {
    "title": "Using the 's' parameter to specify output shape",
    "concepts": [
      "The 's' parameter allows specifying the effective size of the output along each axis.",
      "This example shows how the 's' parameter affects the output shape.",
      "When s=[2, 4], the dimension of the transform along axis -2 will be 2, and along axis -1 will be (4/2)+1 = 3."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of the 's' parameter to specify the output shape.  The transform dimension along the last two axes are set to 2 and 4 respectively.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3, 5],\n                [2, 4, 6]],\n               [[7, 9, 11],\n                [8, 10, 12]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.rfft2(x, s=[2, 4]))"
      },
      {
        "description": "Demonstrates the usage of the 's' parameter to specify the output shape.  The transform dimension along the last two axes are set to 2 and 4 respectively.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3, 5],\n                [2, 4, 6]],\n               [[7, 9, 11],\n                [8, 10, 12]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.rfft2(x, s=[2, 4]))"
      }
    ]
  },
  {
    "title": "Using 's' and 'axes' parameters",
    "concepts": [
      "This example demonstrates using both the 's' and 'axes' parameters.",
      "When s=[3, 5] and axes=(0, 1), the shape of the transform along axis 0 will be 3, and along axis 1 will be (5+1)/2 = 3.",
      "The dimensions along the other axes will be the same as the input."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of the 's' and 'axes' parameters. The transform dimensions along the first two axes are set to 3 and 5, respectively.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3, 5],\n                [2, 4, 6]],\n               [[7, 9, 11],\n                [8, 10, 12]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.rfft2(x, s=[3, 5], axes=(0, 1)))"
      },
      {
        "description": "Demonstrates the usage of the 's' and 'axes' parameters. The transform dimensions along the first two axes are set to 3 and 5, respectively.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[[1, 3, 5],\n                [2, 4, 6]],\n               [[7, 9, 11],\n                [8, 10, 12]]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.fft.rfft2(x, s=[3, 5], axes=(0, 1)))"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.fft.rfftn",
    "concepts": [
      "Computes the multidimensional discrete Fourier transform of a real-valued array.",
      "It's a JAX implementation of numpy.fft.rfftn().",
      "The input array should be real-valued.",
      "The transform is computed along specified axes.",
      "The normalization mode can be 'backward', 'ortho', or 'forward'.",
      "The size of the output along the last axis is s[-1]//2+1.",
      "Related functions include jax.numpy.fft.rfft(), jax.numpy.fft.rfft2(), and jax.numpy.fft.irfftn()."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of rfftn with a 3D Array",
    "concepts": [
      "Demonstrates rfftn on a 3D array without specifying 's' or 'axes'.",
      "The output is a complex array.",
      "jnp.printoptions is used for controlling the output precision and suppression of small values."
    ],
    "code_examples": [
      {
        "description": "Computes the real-to-complex FFT of a 3D array 'x' without specifying shape or axes. jnp.printoptions is used for controlling the output precision and suppression of small values.",
        "code": "x = jnp.array([[[1, 3, 5],\n[2, 4, 6]],\n[[7, 9, 11],\n[8, 10, 12]]])\nwith jnp.printoptions(precision=2, suppress=True):\n    jnp.fft.rfftn(x)"
      },
      {
        "description": "Computes the real-to-complex FFT of a 3D array 'x' without specifying shape or axes. jnp.printoptions is used for controlling the output precision and suppression of small values.",
        "code": "x = jnp.array([[[1, 3, 5],\n[2, 4, 6]],\n[[7, 9, 11],\n[8, 10, 12]]])\nwith jnp.printoptions(precision=2, suppress=True):\n    jnp.fft.rfftn(x)"
      }
    ]
  },
  {
    "title": "Example Usage of rfftn with Specified Shape 's'",
    "concepts": [
      "Shows how to use the 's' parameter to control the output size along specified axes.",
      "The output size along the last axis is calculated as s[-1]//2+1."
    ],
    "code_examples": [
      {
        "description": "Computes the real-to-complex FFT of a 3D array 'x' with specified shape s=[3, 3, 4]. jnp.printoptions is used for controlling the output precision and suppression of small values.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n    jnp.fft.rfftn(x, s=[3, 3, 4])"
      },
      {
        "description": "Computes the real-to-complex FFT of a 3D array 'x' with specified shape s=[3, 3, 4]. jnp.printoptions is used for controlling the output precision and suppression of small values.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n    jnp.fft.rfftn(x, s=[3, 3, 4])"
      }
    ]
  },
  {
    "title": "Example Usage of rfftn with Specified Shape 's' and Axes",
    "concepts": [
      "Demonstrates using both 's' and 'axes' parameters.",
      "The 'axes' parameter specifies the axes along which the transform is computed.",
      "The 's' parameter specifies the size of the transform along the specified axes."
    ],
    "code_examples": [
      {
        "description": "Computes the real-to-complex FFT of a 3D array 'x' with specified shape s=[3, 5] and axes=[0, 1]. jnp.printoptions is used for controlling the output precision and suppression of small values.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n    jnp.fft.rfftn(x, s=[3, 5], axes=[0, 1])"
      },
      {
        "description": "Computes the real-to-complex FFT of a 3D array 'x' with specified shape s=[3, 5] and axes=[0, 1]. jnp.printoptions is used for controlling the output precision and suppression of small values.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n    jnp.fft.rfftn(x, s=[3, 5], axes=[0, 1])"
      }
    ]
  },
  {
    "title": "Example Usage of rfftn with a 1D Array",
    "concepts": [
      "Illustrates rfftn on a 1D array.",
      "The output is a complex array."
    ],
    "code_examples": [
      {
        "description": "Computes the real-to-complex FFT of a 1D array 'x1'.",
        "code": "x1 = jnp.array([1, 2, 3, 4])\njnp.fft.rfftn(x1)"
      },
      {
        "description": "Computes the real-to-complex FFT of a 1D array 'x1'.",
        "code": "x1 = jnp.array([1, 2, 3, 4])\njnp.fft.rfftn(x1)"
      }
    ]
  },
  {
    "title": "Cholesky Decomposition Overview",
    "concepts": [
      "Cholesky decomposition expresses a Hermitian positive-definite matrix A as A = U^H U or A = L L^H.",
      "U is an upper-triangular matrix, and L is a lower-triangular matrix.",
      "X^H is the Hermitian transpose of X."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage with JAX",
    "concepts": [
      "The jnp.linalg.cholesky() function computes the Cholesky decomposition using JAX.",
      "The input array must be a (batched) positive-definite Hermitian matrix of shape (..., N, N).",
      "The upper parameter determines whether to compute the upper (True) or lower (False) Cholesky decomposition."
    ],
    "code_examples": [
      {
        "description": "Example of computing the lower Cholesky factorization of a 2x2 matrix.",
        "code": "x =\njnp.array([[2., 1.],\n[1., 2.]])\njnp.linalg.cholesky(x)"
      },
      {
        "description": "Example of computing the upper Cholesky factorization of a 2x2 matrix.",
        "code": "x =\njnp.array([[2., 1.],\n[1., 2.]])\njnp.linalg.cholesky(x, upper=True)"
      },
      {
        "description": "Example of reconstructing the original matrix from its lower Cholesky factorization.",
        "code": "x =\njnp.array([[2., 1.],\n[1., 2.]])\nL = jnp.linalg.cholesky(x)\njnp.allclose(x, L @ L.T)"
      }
    ]
  },
  {
    "title": "Condition Number Definition",
    "concepts": [
      "The condition number of a matrix is defined as norm(x, p) * norm(inv(x), p).",
      "For p = 2 (the default), the condition number is the ratio of the largest to the smallest singular value.",
      "The function computes the condition number of a matrix using JAX.",
      "The input 'x' is an array of shape (..., M, N).",
      "The parameter 'p' specifies the order of the norm to use, defaulting to 2.",
      "If 'p' is not in {None, 2, -2}, the input matrix 'x' must be square (M = N)."
    ],
    "code_examples": []
  },
  {
    "title": "Well-conditioned Matrix Example",
    "concepts": [
      "This example demonstrates the computation of the condition number for a well-conditioned matrix.",
      "The example uses jax.numpy to define the matrix and compute the condition number."
    ],
    "code_examples": [
      {
        "description": "Compute the condition number of a well-conditioned matrix.",
        "code": "x = jnp.array([[1, 2],\n              [2, 1]])\njnp.linalg.cond(x)"
      },
      {
        "description": "Compute the condition number of a well-conditioned matrix.",
        "code": "x = jnp.array([[1, 2],\n              [2, 1]])\njnp.linalg.cond(x)"
      }
    ]
  },
  {
    "title": "Ill-conditioned Matrix Example",
    "concepts": [
      "This example demonstrates the computation of the condition number for an ill-conditioned matrix.",
      "The example uses jax.numpy to define the matrix and compute the condition number.",
      "The resulting condition number is infinity."
    ],
    "code_examples": [
      {
        "description": "Compute the condition number of an ill-conditioned matrix.",
        "code": "x = jnp.array([[1, 2],\n              [0, 0]])\njnp.linalg.cond(x)"
      },
      {
        "description": "Compute the condition number of an ill-conditioned matrix.",
        "code": "x = jnp.array([[1, 2],\n              [0, 0]])\njnp.linalg.cond(x)"
      }
    ]
  },
  {
    "title": "Determinant Computation with JAX",
    "concepts": [
      "Computes the determinant of a square matrix using JAX.",
      "The input array has shape (..., M, M).",
      "The output is an array of determinants with shape a.shape[:-2]."
    ],
    "code_examples": [
      {
        "description": "Computes the determinant of a 2x2 matrix.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\njnp.linalg.det(a)"
      },
      {
        "description": "Computes the determinant of a 2x2 matrix. (Repeated example)",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\njnp.linalg.det(a)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.linalg.diagonal",
    "concepts": [
      "Extracts the diagonal of a matrix or stack of matrices.",
      "It's a JAX implementation of numpy.linalg.diagonal().",
      "The input array can be multi-dimensional.",
      "The offset parameter allows extracting diagonals above or below the main diagonal."
    ],
    "code_examples": []
  },
  {
    "title": "Examples with a single matrix",
    "concepts": [
      "Demonstrates extracting the main diagonal of a matrix.",
      "Demonstrates extracting diagonals with positive and negative offsets."
    ],
    "code_examples": [
      {
        "description": "Extracting diagonals from a single matrix with different offsets.",
        "code": "x = jnp.array([[1, 2, 3, 4],\n               [5, 6, 7, 8],\n               [9, 10, 11, 12]])\n\nprint(jnp.linalg.diagonal(x))\nprint(jnp.linalg.diagonal(x, offset=1))\nprint(jnp.linalg.diagonal(x, offset=-1))"
      },
      {
        "description": "Extracting diagonals from a single matrix with different offsets.",
        "code": "x = jnp.array([[1, 2, 3, 4],\n               [5, 6, 7, 8],\n               [9, 10, 11, 12]])\n\nprint(jnp.linalg.diagonal(x))\nprint(jnp.linalg.diagonal(x, offset=1))\nprint(jnp.linalg.diagonal(x, offset=-1))"
      }
    ]
  },
  {
    "title": "Examples with batched diagonals",
    "concepts": [
      "Demonstrates extracting diagonals from a stack of matrices.",
      "The input array has shape (..., M, N).",
      "The output array has shape (..., K) where K is the length of the specified diagonal."
    ],
    "code_examples": [
      {
        "description": "Extracting diagonals from a batched matrix.",
        "code": "x = jnp.arange(24).reshape(2, 3, 4)\nprint(jnp.linalg.diagonal(x))"
      },
      {
        "description": "Extracting diagonals from a batched matrix.",
        "code": "x = jnp.arange(24).reshape(2, 3, 4)\nprint(jnp.linalg.diagonal(x))"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.linalg.eig",
    "concepts": [
      "Computes eigenvalues and eigenvectors of a square array.",
      "JAX implementation of numpy.linalg.eig().",
      "Input is an array of shape (..., M, M).",
      "Returns a tuple of eigenvalues and eigenvectors.",
      "Eigenvalues are an array of shape (..., M).",
      "Eigenvectors are an array of shape (..., M, M), where column v[:, i] is the eigenvector corresponding to the eigenvalue w[i]."
    ],
    "code_examples": []
  },
  {
    "title": "Return Type and Backend Limitations",
    "concepts": [
      "Return type is always complex64 for 32-bit input and complex128 for 64-bit input.",
      "Non-symmetric eigendecomposition is only implemented on CPU and GPU backends."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.linalg.eigh() computes eigenvalues and eigenvectors of a Hermitian matrix.",
      "jax.numpy.linalg.eigvals() computes eigenvalues only."
    ],
    "code_examples": []
  },
  {
    "title": "Eigenvalue and Eigenvector Computation Example",
    "concepts": [
      "Example demonstrating the computation of eigenvalues and eigenvectors using jax.numpy.linalg.eig().",
      "Uses jax.numpy.printoptions to control the precision of the output."
    ],
    "code_examples": [
      {
        "description": "Computes the eigenvalues and eigenvectors of a 2x2 matrix using jax.numpy.linalg.eig and prints the results with a specified precision.",
        "code": "import jax.numpy as jnp\nimport jax\n\na = jnp.array([[1., 2.], [2., 1.]])\nw, v = jnp.linalg.eig(a)\n\nwith jax.numpy.printoptions(precision=4):\n    print(w)\n    print(v)"
      },
      {
        "description": "Computes the eigenvalues and eigenvectors of a 2x2 matrix using jax.numpy.linalg.eig and prints the results with a specified precision.",
        "code": "import jax.numpy as jnp\nimport jax\n\na = jnp.array([[1., 2.], [2., 1.]])\nw, v = jnp.linalg.eig(a)\n\nwith jax.numpy.printoptions(precision=4):\n    print(w)\n    print(v)"
      }
    ]
  },
  {
    "title": "Overview",
    "concepts": [
      "Computes eigenvalues of a general matrix.",
      "JAX implementation of numpy.linalg.eigvals().",
      "Input array 'a' should have shape (..., M, M).",
      "Returns an array of shape (..., M) containing the eigenvalues.",
      "The return type is always complex64 for 32-bit input and complex128 for 64-bit input.",
      "Non-symmetric eigendecomposition is only implemented on the CPU backend."
    ],
    "code_examples": []
  },
  {
    "title": "Examples",
    "concepts": [
      "Demonstrates how to compute the eigenvalues of a 2x2 matrix using jnp.linalg.eigvals().",
      "Shows how to format the output using jnp.printoptions()."
    ],
    "code_examples": [
      {
        "description": "Computes the eigenvalues of a 2x2 matrix and prints the result with specified precision.",
        "code": "a = jnp.array([[1., 2.],[2., 1.]])\nw = jnp.linalg.eigvals(a)\nwith jnp.printoptions(precision=2):\n  w"
      },
      {
        "description": "Computes the eigenvalues of a 2x2 matrix and prints the result with specified precision.",
        "code": "a = jnp.array([[1., 2.],[2., 1.]])\nw = jnp.linalg.eigvals(a)\nwith jnp.printoptions(precision=2):\n  w"
      }
    ]
  },
  {
    "title": "Eigenvalue Computation of Hermitian Matrix using JAX",
    "concepts": [
      "Computes eigenvalues of a Hermitian matrix.",
      "JAX implementation of numpy.linalg.eigvalsh().",
      "The input 'a' is an array of shape (..., M, M) containing the Hermitian or symmetric matrix.",
      "The UPLO parameter specifies whether the lower ('L') or upper ('U') triangular part of 'a' is used.",
      "Returns an array of shape (..., M) containing the eigenvalues, sorted in ascending order."
    ],
    "code_examples": [
      {
        "description": "Compute eigenvalues of a complex Hermitian matrix.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, -2j],\n               [2j, 1]])\n\nw = jnp.linalg.eigvalsh(a)\n\nprint(w)"
      },
      {
        "description": "Another example to compute eigenvalues of a complex Hermitian matrix.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, -2j],\n               [2j, 1]])\n\nw = jnp.linalg.eigvalsh(a)\n\nprint(w)"
      }
    ]
  },
  {
    "title": "Overview",
    "concepts": [
      "The document describes the JAX implementation of calculating the inverse of a square matrix.",
      "The function `jax.numpy.linalg.inv()` computes the inverse of a square matrix.",
      "It's generally more performant and numerically precise to use direct solvers like `jax.scipy.linalg.solve()` instead of explicitly computing the inverse, especially for solving linear systems.",
      "The input `a` is an array of shape (..., N, N) representing the square matrix/matrices to be inverted.",
      "The output is an array of shape (..., N, N) containing the inverse of the input matrix/matrices."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage: Computing the Inverse of a Matrix",
    "concepts": [
      "This section demonstrates how to compute the inverse of a 3x3 matrix using `jnp.linalg.inv()`.",
      "The example initializes a JAX array `a` representing the matrix.",
      "The inverse is calculated and stored in `a_inv`."
    ],
    "code_examples": [
      {
        "description": "Compute the inverse of a 3x3 matrix using jax.numpy.linalg.inv",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1., 2., 3.],\n               [2., 4., 2.],\n               [3., 2., 1.]])\n\na_inv = jnp.linalg.inv(a)\n\nprint(a_inv)"
      }
    ]
  },
  {
    "title": "Verification: Multiplying by the Inverse",
    "concepts": [
      "This section demonstrates how to verify that the computed inverse is correct.",
      "It checks if multiplying the original matrix `a` by its inverse `a_inv` results in an identity matrix.",
      "The `jnp.allclose()` function is used for approximate comparison due to floating-point precision."
    ],
    "code_examples": [
      {
        "description": "Check if a matrix multiplied by its inverse results in an identity matrix.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1., 2., 3.],\n               [2., 4., 2.],\n               [3., 2., 1.]])\n\na_inv = jnp.linalg.inv(a)\n\nprint(jnp.allclose(a @ a_inv, jnp.eye(3), atol=1E-5))"
      }
    ]
  },
  {
    "title": "Solving Linear Equations",
    "concepts": [
      "This section demonstrates solving a linear system a @ x = b by multiplying the inverse of `a` with `b`.",
      "A vector `b` is initialized.",
      "The solution `x` is obtained by computing `a_inv @ b`.",
      "However, using `jnp.linalg.solve()` is recommended for better performance and precision."
    ],
    "code_examples": [
      {
        "description": "Solve a linear equation a @ x = b by multiplying the inverse of a with b, but recommend using jnp.linalg.solve instead.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1., 2., 3.],\n               [2., 4., 2.],\n               [3., 2., 1.]])\n\na_inv = jnp.linalg.inv(a)\n\nb = jnp.array([1., 4., 2.])\n\nprint(a_inv @ b)\nprint(jnp.linalg.solve(a, b))"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.linalg.lstsq",
    "concepts": [
      "The function returns the least-squares solution to a linear equation.",
      "It is a JAX implementation of numpy.linalg.lstsq().",
      "It accepts coefficient matrix 'a' and right-hand side 'b' as input.",
      "It uses a cut-off ratio 'rcond' for small singular values.",
      "The numpy_resid flag can be set to replicate NumPy's behavior precisely.",
      "It returns a tuple containing the solution 'x', residual 'resid', rank 'rank', and singular values 's'."
    ],
    "code_examples": []
  },
  {
    "title": "lstsq example",
    "concepts": [
      "Demonstrates how to use jnp.linalg.lstsq to solve a linear equation system.",
      "The example shows how to define the coefficient matrix 'a' and the right-hand side vector 'b'.",
      "The output 'x' represents the least-squares solution.",
      "The code uses jnp.printoptions to format the output with a specified precision."
    ],
    "code_examples": [
      {
        "description": "Computes the least-squares solution and prints the result with specified precision.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\nb = jnp.array([5, 6])\nx, _, _, _ = jnp.linalg.lstsq(a, b)\nwith jnp.printoptions(precision=3):\n  print(x)"
      },
      {
        "description": "Computes the least-squares solution and prints the result with specified precision (duplicated example).",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\nb = jnp.array([5, 6])\nx, _, _, _ = jnp.linalg.lstsq(a, b)\nwith jnp.printoptions(precision=3):\n  print(x)"
      }
    ]
  },
  {
    "title": "Introduction to Matrix Multiplication with JAX",
    "concepts": [
      "This section describes the JAX implementation of matrix multiplication.",
      "It is equivalent to numpy.linalg.matmul().",
      "The function takes two array-like inputs, x1 and x2.",
      "The shape of x2 can be either (N,) or (..., N, M).",
      "Leading dimensions of x1 and x2 must be broadcast-compatible.",
      "Precision can be specified for the computation.",
      "A preferred element type for accumulation can be specified.",
      "The function returns the matrix product of the inputs as an array.",
      "jax.numpy.matmul() is the NumPy API for this function.",
      "jax.numpy.linalg.vecdot() calculates the batched vector product.",
      "jax.numpy.linalg.tensordot() calculates the batched tensor product."
    ],
    "code_examples": []
  },
  {
    "title": "Vector Dot Product Examples",
    "concepts": [
      "Demonstrates how to compute the dot product of two vectors using jnp.linalg.matmul()."
    ],
    "code_examples": [
      {
        "description": "Calculates the dot product of two vectors x1 and x2.",
        "code": "x1 = jnp.array([1, 2, 3])\nx2 = jnp.array([4, 5, 6])\njnp.linalg.matmul(x1, x2)"
      },
      {
        "description": "Calculates the dot product of two vectors x1 and x2 (repeated example).",
        "code": "x1 = jnp.array([1, 2, 3])\nx2 = jnp.array([4, 5, 6])\njnp.linalg.matmul(x1, x2)"
      }
    ]
  },
  {
    "title": "Matrix Dot Product Examples",
    "concepts": [
      "Demonstrates matrix multiplication using jnp.linalg.matmul()."
    ],
    "code_examples": [
      {
        "description": "Calculates the matrix product of two matrices x1 and x2.",
        "code": "x1 = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\nx2 = jnp.array([[1, 2],\n                [3, 4],\n                [5, 6]])\njnp.linalg.matmul(x1, x2)"
      },
      {
        "description": "Calculates the matrix product of two matrices x1 and x2 (repeated example).",
        "code": "x1 = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\nx2 = jnp.array([[1, 2],\n                [3, 4],\n                [5, 6]])\njnp.linalg.matmul(x1, x2)"
      }
    ]
  },
  {
    "title": "Matrix Multiplication with the @ Operator",
    "concepts": [
      "Demonstrates using the @ operator as a shorthand for matrix multiplication.",
      "The @ operator provides a more concise way to express matrix multiplication."
    ],
    "code_examples": [
      {
        "description": "Calculates the matrix product of x1 and x2 using the @ operator.",
        "code": "x1 @ x2"
      },
      {
        "description": "Calculates the matrix product of x1 and x2 using the @ operator (repeated example).",
        "code": "x1 @ x2"
      }
    ]
  },
  {
    "title": "Matrix Norm Calculation in JAX",
    "concepts": [
      "Computes the norm of a matrix or a stack of matrices using JAX.",
      "It is a JAX implementation of numpy.linalg.matrix_norm().",
      "The input 'x' is an array of shape (..., M, N).",
      "The 'keepdims' parameter determines whether to keep the reduced dimensions in the output.",
      "The 'ord' parameter specifies the type of norm, defaulting to the Frobenius norm.",
      "The function returns an array containing the norm of x.",
      "See numpy.linalg.norm() for details on available options."
    ],
    "code_examples": [
      {
        "description": "Compute the matrix norm of a 2D array using jax.numpy.linalg.matrix_norm.",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\njnp.linalg.matrix_norm(x)"
      },
      {
        "description": "Compute the matrix norm of a 2D array using jax.numpy.linalg.matrix_norm (repeated example).",
        "code": "x = jnp.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\njnp.linalg.matrix_norm(x)"
      }
    ]
  },
  {
    "title": "Matrix Power Function",
    "concepts": [
      "The function raises a square matrix to an integer power.",
      "It's a JAX implementation of numpy.linalg.matrix_power().",
      "It uses repeated squarings for computation.",
      "The input 'a' is an array of shape (..., M, M).",
      "The input 'n' is the integer exponent.",
      "The output is an array of shape (..., M, M) representing the matrix power."
    ],
    "code_examples": []
  },
  {
    "title": "Positive Integer Power Example",
    "concepts": [
      "Demonstrates raising a matrix to a positive integer power (3).",
      "Shows the equivalence of using jnp.linalg.matrix_power and repeated matrix multiplication using the @ operator."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating raising a matrix to the power of 3.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1., 2.],\n               [3., 4.]])\n\njnp.linalg.matrix_power(a, 3)"
      },
      {
        "description": "Demonstrates the equivalent calculation using matrix multiplication.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1., 2.],\n               [3., 4.]])\n\na @ a @ a"
      }
    ]
  },
  {
    "title": "Zero Power Example",
    "concepts": [
      "Demonstrates raising a matrix to the power of zero.",
      "Raising a matrix to the power of zero results in the identity matrix."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating raising a matrix to the power of 0.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1., 2.],\n               [3., 4.]])\n\njnp.linalg.matrix_power(a, 0)"
      }
    ]
  },
  {
    "title": "Negative Power Example",
    "concepts": [
      "Demonstrates raising a matrix to a negative integer power (-2).",
      "Shows the equivalence of raising to a negative power and repeated multiplication of the inverse matrix.",
      "Uses jnp.printoptions to control the precision of the output."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating raising a matrix to the power of -2 with print precision.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1., 2.],\n               [3., 4.]])\n\nwith jnp.printoptions(precision=3):\n  jnp.linalg.matrix_power(a, -2)"
      },
      {
        "description": "Demonstrates the equivalent calculation using the inverse matrix.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1., 2.],\n               [3., 4.]])\n\ninv_a = jnp.linalg.inv(a)\n\nwith jnp.printoptions(precision=3):\n  inv_a @ inv_a"
      }
    ]
  },
  {
    "title": "Introduction to Matrix Rank Computation",
    "concepts": [
      "The document describes a JAX implementation of numpy.linalg.matrix_rank().",
      "The rank is computed using Singular Value Decomposition (SVD).",
      "The rank is determined by counting singular values greater than a specified tolerance.",
      "The function accepts an array `M` and an optional relative tolerance `rtol`.",
      "A deprecated alias `tol` exists for the `rtol` argument.",
      "The function returns an array representing the matrix rank.",
      "The rank calculation may be inaccurate for ill-conditioned matrices or those with very small singular values."
    ],
    "code_examples": []
  },
  {
    "title": "Matrix Rank Examples",
    "concepts": [
      "Demonstrates how to compute the rank of a matrix using `jnp.linalg.matrix_rank()`.",
      "Illustrates the computation of the rank of a full-rank matrix.",
      "Illustrates the computation of the rank of a rank-deficient matrix."
    ],
    "code_examples": [
      {
        "description": "Computes the rank of a 2x2 full-rank matrix.",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\njnp.linalg.matrix_rank(a)"
      },
      {
        "description": "Computes the rank of a 2x2 full-rank matrix (duplicate example).",
        "code": "a = jnp.array([[1, 2],\n               [3, 4]])\njnp.linalg.matrix_rank(a)"
      },
      {
        "description": "Computes the rank of a 2x2 rank-deficient matrix.",
        "code": "b = jnp.array([[1, 0],\n               [0, 0]]) # Rank-deficient matrix\njnp.linalg.matrix_rank(b)"
      },
      {
        "description": "Computes the rank of a 2x2 rank-deficient matrix (duplicate example).",
        "code": "b = jnp.array([[1, 0],\n               [0, 0]]) # Rank-deficient matrix\njnp.linalg.matrix_rank(b)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.linalg.matrix_transpose",
    "concepts": [
      "The function `jax.numpy.linalg.matrix_transpose()` transposes a matrix or a stack of matrices.",
      "It is a JAX implementation of `numpy.linalg.matrix_transpose()`.",
      "The input `x` is an array of shape (..., M, N).",
      "The output is an array of shape (..., N, M).",
      "The function returns the matrix transpose of the input array `x`.",
      "See also `jax.numpy.transpose()` for a more general transpose operation."
    ],
    "code_examples": []
  },
  {
    "title": "Transpose of a Single Matrix",
    "concepts": [
      "Demonstrates how to transpose a single matrix using `jax.numpy.linalg.matrix_transpose()`."
    ],
    "code_examples": [
      {
        "description": "Transposes a 2x3 matrix.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\njnp.linalg.matrix_transpose(x)"
      },
      {
        "description": "Transposes a 2x3 matrix.",
        "code": "x = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\njnp.linalg.matrix_transpose(x)"
      }
    ]
  },
  {
    "title": "Transpose of a Stack of Matrices",
    "concepts": [
      "Demonstrates how to transpose a stack of matrices using `jax.numpy.linalg.matrix_transpose()`."
    ],
    "code_examples": [
      {
        "description": "Transposes a stack of 2x2 matrices.",
        "code": "x = jnp.array([[[1, 2],\n              [3, 4]],\n\n             [[5, 6],\n              [7, 8]]])\njnp.linalg.matrix_transpose(x)"
      },
      {
        "description": "Transposes a stack of 2x2 matrices.",
        "code": "x = jnp.array([[[1, 2],\n              [3, 4]],\n\n             [[5, 6],\n              [7, 8]]])\njnp.linalg.matrix_transpose(x)"
      }
    ]
  },
  {
    "title": "Using the mT Property for Matrix Transpose",
    "concepts": [
      "Demonstrates using the `.mT` property of JAX array objects as a convenient way to compute the matrix transpose."
    ],
    "code_examples": [
      {
        "description": "Transposes a stack of 2x2 matrices using the .mT property.",
        "code": "x = jnp.array([[[1, 2],\n              [3, 4]],\n\n             [[5, 6],\n              [7, 8]]])\nx.mT"
      },
      {
        "description": "Transposes a stack of 2x2 matrices using the .mT property.",
        "code": "x = jnp.array([[[1, 2],\n              [3, 4]],\n\n             [[5, 6],\n              [7, 8]]])\nx.mT"
      }
    ]
  },
  {
    "title": "Overview of jnp.linalg.norm()",
    "concepts": [
      "Computes the norm of a matrix or vector.",
      "JAX implementation of numpy.linalg.norm().",
      "The norm can be computed over specified axes.",
      "The type of norm is determined by the 'ord' parameter.",
      "Supports vector norms and matrix norms.",
      "The output array can keep the same number of dimensions as the input if keepdims is True."
    ],
    "code_examples": []
  },
  {
    "title": "Vector Norms",
    "concepts": [
      "Calculates the norm of a vector.",
      "Different 'ord' values result in different vector norms (2-norm, max, min, sum).",
      "ord=None computes the 2-norm",
      "ord=inf computes max(abs(x))",
      "ord=-inf computes min(abs(x))",
      "ord=0 computes sum(x!=0)",
      "for other numerical values, computes sum(abs(x) ** ord)**(1/ord)"
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of jnp.linalg.norm() to compute different vector norms.",
        "code": "x = jnp.array([3., 4., 12.])\n\nprint(jnp.linalg.norm(x))\n# Array(13., dtype=float32)\n\nprint(jnp.linalg.norm(x, ord=1))\n# Array(19., dtype=float32)\n\nprint(jnp.linalg.norm(x, ord=0))\n# Array(3., dtype=float32)"
      },
      {
        "description": "Demonstrates the use of jnp.linalg.norm() to compute different vector norms.",
        "code": "x = jnp.array([3., 4., 12.])\n\nprint(jnp.linalg.norm(x))\n# Array(13., dtype=float32)\n\nprint(jnp.linalg.norm(x, ord=1))\n# Array(19., dtype=float32)\n\nprint(jnp.linalg.norm(x, ord=0))\n# Array(3., dtype=float32)"
      }
    ]
  },
  {
    "title": "Matrix Norms",
    "concepts": [
      "Calculates the norm of a matrix.",
      "Different 'ord' values result in different matrix norms (Frobenius, nuclear, 1-norm, 2-norm).",
      "ord='fro' or ord=None (default) computes the Frobenius norm",
      "ord='nuc' computes the nuclear norm, or the sum of the singular values",
      "ord=1 computes max(abs(x).sum(0))",
      "ord=-1 computes min(abs(x).sum(0))",
      "ord=2 computes the 2-norm, i.e. the largest singular value",
      "ord=-2 computes the smallest singular value"
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of jnp.linalg.norm() to compute different matrix norms.",
        "code": "x = jnp.array([[1., 2., 3.],\n               [4., 5., 7.]])\n\nprint(jnp.linalg.norm(x)) # Frobenius norm\n# Array(10.198039, dtype=float32)\n\nprint(jnp.linalg.norm(x, ord='nuc')) # nuclear norm\n# Array(10.762535, dtype=float32)\n\nprint(jnp.linalg.norm(x, ord=1)) # 1-norm\n# Array(10., dtype=float32)"
      },
      {
        "description": "Demonstrates the use of jnp.linalg.norm() to compute different matrix norms.",
        "code": "x = jnp.array([[1., 2., 3.],\n               [4., 5., 7.]])\n\nprint(jnp.linalg.norm(x)) # Frobenius norm\n# Array(10.198039, dtype=float32)\n\nprint(jnp.linalg.norm(x, ord='nuc')) # nuclear norm\n# Array(10.762535, dtype=float32)\n\nprint(jnp.linalg.norm(x, ord=1)) # 1-norm\n# Array(10., dtype=float32)"
      }
    ]
  },
  {
    "title": "Batched Vector Norm",
    "concepts": [
      "Calculates the vector norm along a specified axis.",
      "Demonstrates how to compute norms for multiple vectors in a batch."
    ],
    "code_examples": [
      {
        "description": "Demonstrates calculating the norm of a matrix along axis 1, which results in a vector of norms.",
        "code": "x = jnp.array([[1., 2., 3.],\n               [4., 5., 7.]])\n\nprint(jnp.linalg.norm(x, axis=1))\n# Array([3.7416575, 9.486833 ], dtype=float32)"
      },
      {
        "description": "Demonstrates calculating the norm of a matrix along axis 1, which results in a vector of norms.",
        "code": "x = jnp.array([[1., 2., 3.],\n               [4., 5., 7.]])\n\nprint(jnp.linalg.norm(x, axis=1))\n# Array([3.7416575, 9.486833 ], dtype=float32)"
      }
    ]
  },
  {
    "title": "Outer Product Computation",
    "concepts": [
      "Computes the outer product of two 1-dimensional arrays.",
      "JAX implementation of numpy.linalg.outer()."
    ],
    "code_examples": [
      {
        "description": "Compute the outer product of two jax.numpy arrays.",
        "code": "x1 = jnp.array([1, 2, 3])\nx2 = jnp.array([4, 5, 6])\njnp.linalg.outer(x1, x2)"
      },
      {
        "description": "Compute the outer product of two jax.numpy arrays. (Duplicated Example)",
        "code": "x1 = jnp.array([1, 2, 3])\nx2 = jnp.array([4, 5, 6])\njnp.linalg.outer(x1, x2)"
      }
    ]
  },
  {
    "title": "Introduction to Pseudo-Inverse Computation",
    "concepts": [
      "The document describes the JAX implementation of the Moore-Penrose pseudo-inverse.",
      "The function `jax.numpy.linalg.pinv()` computes the pseudo-inverse of a matrix.",
      "The pseudo-inverse is an approximation of the inverse for non-square or singular matrices."
    ],
    "code_examples": []
  },
  {
    "title": "Arguments of `jax.numpy.linalg.pinv()`",
    "concepts": [
      "The `a` argument is the input array, which contains matrices to be pseudo-inverted.",
      "The `rtol` argument specifies the cutoff for small singular values.",
      "Singular values smaller than `rtol * largest_singular_value` are treated as zero.",
      "The `hermitian` argument indicates whether the input matrix is Hermitian.",
      "The `rcond` argument is a deprecated alias of the `rtol` argument."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "The function returns an array containing the pseudo-inverse of the input matrix `a`."
    ],
    "code_examples": []
  },
  {
    "title": "Differences from NumPy's `pinv()`",
    "concepts": [
      "JAX's `pinv()` differs from NumPy's `pinv()` in the default value of `rcond`.",
      "In NumPy, the default `rcond` is 1e-15.",
      "In JAX, the default `rcond` is `10 * max(num_rows, num_cols) * jnp.finfo(dtype).eps`."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of using `jax.numpy.linalg.pinv()`",
    "concepts": [
      "The example demonstrates how to compute the pseudo-inverse of a matrix using `jax.numpy.linalg.pinv()`.",
      "It also verifies that the pseudo-inverse acts as a multiplicative inverse for non-rank-deficient outputs."
    ],
    "code_examples": [
      {
        "description": "Compute the pseudo-inverse of a sample matrix `a`.",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 2],\n               [3, 4],\n               [5, 6]])\n\na_pinv = jnp.linalg.pinv(a)\n\nprint(a_pinv)"
      },
      {
        "description": "Verify that the pseudo-inverse acts as a multiplicative inverse (within a tolerance).",
        "code": "import jax.numpy as jnp\n\na = jnp.array([[1, 2],\n               [3, 4],\n               [5, 6]])\n\na_pinv = jnp.linalg.pinv(a)\n\nprint(jnp.allclose(a_pinv @ a, jnp.eye(2), atol=1E-4))"
      }
    ]
  },
  {
    "title": "QR Decomposition Overview",
    "concepts": [
      "QR decomposition expresses a matrix A as the product of a unitary matrix Q and an upper-triangular matrix R.",
      "Q is a unitary matrix (Q^H * Q = I).",
      "R is an upper-triangular matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Function Definition and Parameters",
    "concepts": [
      "The function computes the QR decomposition of an array using JAX.",
      "The input 'a' is an array of shape (..., M, N).",
      "The 'mode' parameter controls the shape of the output matrices Q and R.",
      "Mode 'reduced' returns Q of shape (..., M, K) and R of shape (..., K, N), where K = min(M, N).",
      "Mode 'complete' returns Q of shape (..., M, M) and R of shape (..., M, N).",
      "Mode 'raw' returns LAPACK-internal representations of shape (..., M, N) and (..., K).",
      "Mode 'r' returns R only."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "The function returns a tuple (Q, R) if mode is not 'r', otherwise, it returns the array R.",
      "Q is an orthogonal matrix.",
      "R is an upper-triangular matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of QR Decomposition",
    "concepts": [
      "Demonstrates how to compute the QR decomposition of a matrix using jnp.linalg.qr().",
      "Shows how to verify that Q is orthonormal.",
      "Shows how to reconstruct the original matrix A from Q and R."
    ],
    "code_examples": [
      {
        "description": "Compute the QR decomposition of a matrix and print Q and R.",
        "code": "a = jnp.array([[1., 2., 3., 4.],\n               [5., 4., 2., 1.],\n               [6., 3., 1., 5.]])\nQ, R = jnp.linalg.qr(a)\nprint(Q)\nprint(R)"
      },
      {
        "description": "Check that Q is orthonormal.",
        "code": "jnp.allclose(Q.T @ Q, jnp.eye(3), atol=1E-5)"
      },
      {
        "description": "Reconstruct the input matrix from Q and R.",
        "code": "jnp.allclose(Q @ R, a)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.linalg.slogdet()",
    "concepts": [
      "Computes the sign and natural logarithm of the determinant of an array.",
      "JAX implementation of numpy.linalg.slogdet().",
      "The input array `a` should have shape (..., M, M).",
      "The function returns a tuple of arrays (sign, logabsdet).",
      "The `sign` array contains the sign of the determinant.",
      "The `logabsdet` array contains the natural log of the determinant\u2019s absolute value.",
      "Available methods for computation: 'lu' (default) and 'qr'."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Example of slogdet()",
    "concepts": [
      "Demonstrates how to compute the sign and log absolute value of the determinant using `jnp.linalg.slogdet()`.",
      "Shows how to interpret the sign and log absolute value.",
      "Uses jnp.exp() to compute absolute value of determinant from its logarithm."
    ],
    "code_examples": [
      {
        "description": "Compute sign and logabsdet of a 2x2 matrix and compute the absolute value of the determinant.",
        "code": "a = jnp.array([[1, 2],\n              [3, 4]])\nsign, logabsdet = jnp.linalg.slogdet(a)\nprint(sign)\n# -1 indicates negative determinant\nprint(jnp.exp(logabsdet))\n# Absolute value of determinant"
      },
      {
        "description": "Compute sign and logabsdet of a 2x2 matrix and compute the absolute value of the determinant.",
        "code": "a = jnp.array([[1, 2],\n              [3, 4]])\nsign, logabsdet = jnp.linalg.slogdet(a)\nprint(sign)\n# -1 indicates negative determinant\nprint(jnp.exp(logabsdet))\n# Absolute value of determinant"
      }
    ]
  },
  {
    "title": "Introduction to Linear System Solving with JAX",
    "concepts": [
      "The document describes a JAX implementation of numpy.linalg.solve().",
      "It solves the linear system of equations a @ x = b for x, given a and b.",
      "If 'a' is singular, the function returns nan or inf values.",
      "The function supports batched linear systems.",
      "Input 'a' should have shape (..., N, N).",
      "Input 'b' can have shape (N,) or (..., N, M).",
      "The output shape is (..., N) when b is (N,) and (..., N, M) otherwise.",
      "It uses JAX arrays for computation."
    ],
    "code_examples": []
  },
  {
    "title": "Simple Linear System Example",
    "concepts": [
      "Demonstrates solving a 3x3 linear system using jnp.linalg.solve().",
      "Defines matrix A and vector b.",
      "Solves for x in the equation A @ x = b.",
      "Verifies the solution by checking if A @ x is close to b using jnp.allclose()."
    ],
    "code_examples": [
      {
        "description": "Defines matrix A and vector b, then solves the linear system A @ x = b using jnp.linalg.solve(). Finally, it verifies the solution.",
        "code": "A = jnp.array([[1., 2., 3.], [2., 4., 2.], [3., 2., 1.]])\nb = jnp.array([14., 16., 10.])\nx = jnp.linalg.solve(A, b)\nprint(x) # Output: Array([1., 2., 3.], dtype=float32)\n\njnp.allclose(A @ x, b) # Output: Array(True, dtype=bool)"
      },
      {
        "description": "Demonstrates solving a 3x3 linear system.",
        "code": "A = jnp.array([[1., 2., 3.], [2., 4., 2.], [3., 2., 1.]])\nb = jnp.array([14., 16., 10.])\nx = jnp.linalg.solve(A, b)\nprint(x) # Output: Array([1., 2., 3.], dtype=float32)\n\njnp.allclose(A @ x, b) # Output: Array(True, dtype=bool)"
      }
    ]
  },
  {
    "title": "Singular Value Decomposition (SVD) Overview",
    "concepts": [
      "SVD decomposes a matrix A into U, \u03a3, and V^H such that A = U\u03a3V^H.",
      "U contains left singular vectors (U^HU = I).",
      "V contains right singular vectors (V^HV = I).",
      "\u03a3 is a diagonal matrix of singular values.",
      "The function is a JAX implementation of numpy.linalg.svd.",
      "The function is implemented in terms of jax.lax.linalg.svd()."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters and Return Values",
    "concepts": [
      "a (ArrayLike): input array of shape (..., N, M).",
      "full_matrices (bool): if True, compute full matrices U (N, N) and V^H (M, M); if False, compute reduced matrices U (N, K) and V^H (K, M) where K = min(N, M).",
      "compute_uv (bool): if True, return (U, s, V^H); if False, return only singular values s.",
      "hermitian (bool): If True, assumes the matrix is Hermitian for efficiency.",
      "subset_by_index (tuple[int, int] | None): Optional 2-tuple [start, end] indicating the range of singular values to compute. Only compatible with full_matrices=False.",
      "The function returns a tuple of arrays (u, s, vh) if compute_uv is True, otherwise the array s.",
      "u : left singular vectors of shape (..., N, N) if full_matrices is True or (..., N, K) otherwise.",
      "s : singular values of shape (..., K)",
      "vh : conjugate-transposed right singular vectors of shape (..., M, M) if full_matrices is True or (..., K, M) otherwise."
    ],
    "code_examples": []
  },
  {
    "title": "SVD Example",
    "concepts": [
      "Demonstrates how to compute the SVD of a small real-valued array.",
      "Shows how to access singular values.",
      "Demonstrates that the singular vectors are orthonormal.",
      "Shows how to reconstruct the original matrix from the SVD components."
    ],
    "code_examples": [
      {
        "description": "Computes the SVD of a 2x3 matrix and prints the singular values.  `full_matrices=False` is used to compute the reduced SVD.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., 3.],\n               [6., 5., 4.]])\n\nu, s, vt = jnp.linalg.svd(x, full_matrices=False)\n\nprint(s)"
      },
      {
        "description": "Checks that the left and right singular vectors are orthonormal by computing U.T @ U and V.T @ V, and comparing the result to the identity matrix.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., 3.],\n               [6., 5., 4.]])\n\nu, s, vt = jnp.linalg.svd(x, full_matrices=False)\n\nprint(jnp.allclose(u.T @ u, jnp.eye(2), atol=1E-5))\nv = vt.T\nprint(jnp.allclose(v.T @ v, jnp.eye(2), atol=1E-5))"
      },
      {
        "description": "Reconstructs the original matrix from its SVD components and checks that the reconstructed matrix is close to the original matrix.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1., 2., 3.],\n               [6., 5., 4.]])\n\nu, s, vt = jnp.linalg.svd(x, full_matrices=False)\n\nx_reconstructed = u @ jnp.diag(s) @ vt\n\nprint(jnp.allclose(x_reconstructed, x))"
      }
    ]
  },
  {
    "title": "Singular Value Computation with JAX",
    "concepts": [
      "Computes the singular values of a matrix using JAX.",
      "Implements the functionality of numpy.linalg.svdvals() in JAX.",
      "The input array 'x' should have shape (..., M, N).",
      "The output array of singular values has shape (..., K) where K = min(M, N)."
    ],
    "code_examples": [
      {
        "description": "Compute the singular values of a 2x3 matrix using jax.numpy.linalg.svdvals().",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\njnp.linalg.svdvals(x)"
      },
      {
        "description": "Compute the singular values of a 2x3 matrix using jax.numpy.linalg.svdvals().",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\njnp.linalg.svdvals(x)"
      }
    ]
  },
  {
    "title": "Introduction to Tensor Dot Product",
    "concepts": [
      "Computes the tensor dot product of two N-dimensional arrays.",
      "JAX implementation of numpy.linalg.tensordot().",
      "x1 is an N-dimensional array.",
      "x2 is an M-dimensional array.",
      "The axes argument specifies the axes to sum over.",
      "Precision can be specified for x1 and x2.",
      "The function returns an array containing the tensor dot product."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Tensor Dot Product Example",
    "concepts": [
      "Demonstrates a basic tensor dot product between two 3D arrays.",
      "The default axes are used, summing over the last axes of x1 and the first axes of x2."
    ],
    "code_examples": [
      {
        "description": "Computes the tensor dot product of two 3D arrays using default axes.",
        "code": "x1 = jnp.arange(24.).reshape(2, 3, 4)\nx2 = jnp.ones((3, 4, 5))\njnp.linalg.tensordot(x1, x2)"
      },
      {
        "description": "Computes the tensor dot product of two 3D arrays using default axes.",
        "code": "x1 = jnp.arange(24.).reshape(2, 3, 4)\nx2 = jnp.ones((3, 4, 5))\njnp.linalg.tensordot(x1, x2)"
      }
    ]
  },
  {
    "title": "Specifying Axes for Tensor Dot Product",
    "concepts": [
      "Demonstrates specifying the axes to sum over as explicit sequences.",
      "Axes are defined as a tuple, where the first element is a list of axes from the first tensor, and the second element is a list of axes from the second tensor."
    ],
    "code_examples": [
      {
        "description": "Computes the tensor dot product with specified axes.",
        "code": "x1 = jnp.arange(24.).reshape(2, 3, 4)\nx2 = jnp.ones((3, 4, 5))\njnp.linalg.tensordot(x1, x2, axes=([1, 2], [0, 1]))"
      },
      {
        "description": "Computes the tensor dot product with specified axes.",
        "code": "x1 = jnp.arange(24.).reshape(2, 3, 4)\nx2 = jnp.ones((3, 4, 5))\njnp.linalg.tensordot(x1, x2, axes=([1, 2], [0, 1]))"
      }
    ]
  },
  {
    "title": "Equivalence with einsum",
    "concepts": [
      "Demonstrates the equivalence between jnp.linalg.tensordot() and jnp.einsum() for specific tensor contractions.",
      "The einsum notation provides a more general way to specify tensor contractions."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the equivalent result using jnp.einsum().",
        "code": "x1 = jnp.arange(24.).reshape(2, 3, 4)\nx2 = jnp.ones((3, 4, 5))\njnp.einsum('ijk,jkm->im', x1, x2)"
      },
      {
        "description": "Demonstrates the equivalent result using jnp.einsum().",
        "code": "x1 = jnp.arange(24.).reshape(2, 3, 4)\nx2 = jnp.ones((3, 4, 5))\njnp.einsum('ijk,jkm->im', x1, x2)"
      }
    ]
  },
  {
    "title": "Matrix Multiplication Equivalence",
    "concepts": [
      "Demonstrates that setting axes=1 for two-dimensional inputs is equivalent to matrix multiplication.",
      "The @ operator is used for matrix multiplication in JAX."
    ],
    "code_examples": [
      {
        "description": "Shows equivalence between tensordot with axes=1 and matrix multiplication.",
        "code": "x1 = jnp.array([[1, 2],\n              [3, 4]])\nx2 = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\njnp.linalg.tensordot(x1, x2, axes=1)"
      },
      {
        "description": "Shows equivalence between tensordot with axes=1 and matrix multiplication.",
        "code": "x1 = jnp.array([[1, 2],\n              [3, 4]])\nx2 = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\nx1 @ x2"
      },
      {
        "description": "Shows equivalence between tensordot with axes=1 and matrix multiplication.",
        "code": "x1 = jnp.array([[1, 2],\n              [3, 4]])\nx2 = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\njnp.linalg.tensordot(x1, x2, axes=1)"
      },
      {
        "description": "Shows equivalence between tensordot with axes=1 and matrix multiplication.",
        "code": "x1 = jnp.array([[1, 2],\n              [3, 4]])\nx2 = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\nx1 @ x2"
      }
    ]
  },
  {
    "title": "Outer Product Equivalence",
    "concepts": [
      "Shows that setting axes=0 for one-dimensional inputs is equivalent to jax.numpy.linalg.outer().",
      "jax.numpy.linalg.outer() computes the outer product of two arrays."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the equivalence between tensordot with axes=0 and jnp.linalg.outer().",
        "code": "x1 = jnp.array([1, 2])\nx2 = jnp.array([1, 2, 3])\njnp.linalg.tensordot(x1, x2, axes=0)"
      },
      {
        "description": "Demonstrates the equivalence between tensordot with axes=0 and jnp.linalg.outer().",
        "code": "x1 = jnp.array([1, 2])\nx2 = jnp.array([1, 2, 3])\njnp.linalg.outer(x1, x2)"
      },
      {
        "description": "Demonstrates the equivalence between tensordot with axes=0 and jnp.linalg.outer().",
        "code": "x1 = jnp.array([1, 2])\nx2 = jnp.array([1, 2, 3])\njnp.linalg.tensordot(x1, x2, axes=0)"
      },
      {
        "description": "Demonstrates the equivalence between tensordot with axes=0 and jnp.linalg.outer().",
        "code": "x1 = jnp.array([1, 2])\nx2 = jnp.array([1, 2, 3])\njnp.linalg.outer(x1, x2)"
      }
    ]
  },
  {
    "title": "Introduction to tensorsolve",
    "concepts": [
      "Solves the tensor equation a x = b for x.",
      "JAX implementation of numpy.linalg.tensorsolve().",
      "The input array 'a' after reordering via axes must have shape (*b.shape, *x.shape).",
      "The function returns an array x such that after reordering of axes of a, tensordot(a, x, x.ndim) is equivalent to b."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Example",
    "concepts": [
      "Demonstrates how to use jnp.linalg.tensorsolve() with randomly generated arrays 'a' and 'b'.",
      "Shows how to find 'x' such that a x = b.",
      "Verifies the shape of the resulting array 'x'."
    ],
    "code_examples": [
      {
        "description": "Solve for x in the tensor equation a x = b, and check the shape of x.",
        "code": "key1 , key2 = jax . random . split ( jax . random . key ( 8675309 ))\na = jax . random . normal ( key1 , shape = ( 2 , 2 , 4 ))\nb = jax . random . normal ( key2 , shape = ( 2 , 2 ))\nx = jnp . linalg . tensorsolve ( a , b )\nx . shape\n(4,)"
      },
      {
        "description": "Solve for x in the tensor equation a x = b, and check the shape of x (repeated for demonstration).",
        "code": "key1 , key2 = jax . random . split ( jax . random . key ( 8675309 ))\na = jax . random . normal ( key1 , shape = ( 2 , 2 , 4 ))\nb = jax . random . normal ( key2 , shape = ( 2 , 2 ))\nx = jnp . linalg . tensorsolve ( a , b )\nx . shape\n(4,)"
      }
    ]
  },
  {
    "title": "Reconstructing b with tensordot",
    "concepts": [
      "Demonstrates how the solution 'x' can be used with jnp.linalg.tensordot() to reconstruct the original array 'b'.",
      "Uses jnp.allclose() to verify that the reconstructed array is close to the original 'b'."
    ],
    "code_examples": [
      {
        "description": "Reconstruct b using tensordot and verify the result with allclose.",
        "code": "b_reconstructed = jnp . linalg . tensordot ( a , x , axes = x . ndim )\njnp . allclose ( b , b_reconstructed )\nArray(True, dtype=bool)"
      },
      {
        "description": "Reconstruct b using tensordot and verify the result with allclose (repeated for demonstration).",
        "code": "b_reconstructed = jnp . linalg . tensordot ( a , x , axes = x . ndim )\njnp . allclose ( b , b_reconstructed )\nArray(True, dtype=bool)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.linalg.trace",
    "concepts": [
      "Computes the trace of a matrix.",
      "JAX implementation of numpy.linalg.trace().",
      "The input 'x' is an array of shape (..., M, N).",
      "The 'offset' parameter allows specifying an offset from the main diagonal.",
      "The 'dtype' parameter specifies the data type of the returned array.",
      "Returns an array of batched traces with shape x.shape[:-2]."
    ],
    "code_examples": []
  },
  {
    "title": "Trace of a Single Matrix",
    "concepts": [
      "Demonstrates calculating the trace of a single matrix using jnp.linalg.trace().",
      "Shows how to use the 'offset' parameter to calculate the trace of a diagonal offset from the main diagonal.",
      "Demonstrates changing the output dtype using the 'dtype' parameter."
    ],
    "code_examples": [
      {
        "description": "Calculating the trace of a 3x4 matrix.",
        "code": "x = jnp.array([[1, 2, 3, 4],\n               [5, 6, 7, 8],\n               [9, 10, 11, 12]])\n\njnp.linalg.trace(x)"
      },
      {
        "description": "Calculating the trace with an offset of 1.",
        "code": "jnp.linalg.trace(x, offset=1)"
      },
      {
        "description": "Calculating the trace with an offset of -1 and specifying the dtype.",
        "code": "jnp.linalg.trace(x, offset=-1, dtype=\"float32\")"
      }
    ]
  },
  {
    "title": "Batched Traces",
    "concepts": [
      "Demonstrates calculating batched traces of a 3D array using jnp.linalg.trace()."
    ],
    "code_examples": [
      {
        "description": "Calculating batched traces of a 2x3x4 array.",
        "code": "x = jnp.arange(24).reshape(2, 3, 4)\njnp.linalg.trace(x)"
      }
    ]
  },
  {
    "title": "Introduction to vector_norm",
    "concepts": [
      "The function computes the vector norm of a vector or batch of vectors.",
      "It is a JAX implementation of numpy.linalg.vector_norm().",
      "The 'axis' parameter specifies the axis along which to compute the norm.",
      "The 'keepdims' parameter keeps the reduced dimensions in the output if True.",
      "The 'ord' parameter specifies the type of norm (default is the 2-norm)."
    ],
    "code_examples": []
  },
  {
    "title": "Examples: Norm of a Single Vector",
    "concepts": [
      "Demonstrates how to calculate the norm of a single vector using jnp.linalg.vector_norm()."
    ],
    "code_examples": [
      {
        "description": "Calculates the norm of the vector [1., 2., 3.].",
        "code": "x = jnp.array([1., 2., 3.])\njnp.linalg.vector_norm(x)"
      },
      {
        "description": "Calculates the norm of the vector [1., 2., 3.] (duplicate example).",
        "code": "x = jnp.array([1., 2., 3.])\njnp.linalg.vector_norm(x)"
      }
    ]
  },
  {
    "title": "Examples: Norm of a Batch of Vectors",
    "concepts": [
      "Demonstrates how to calculate the norm of a batch of vectors along a specified axis using jnp.linalg.vector_norm()."
    ],
    "code_examples": [
      {
        "description": "Calculates the norm of a batch of vectors ([[1., 2., 3.], [4., 5., 7.]]) along axis 1.",
        "code": "x = jnp.array([[1., 2., 3.],[4., 5., 7.]])\njnp.linalg.vector_norm(x, axis=1)"
      },
      {
        "description": "Calculates the norm of a batch of vectors ([[1., 2., 3.], [4., 5., 7.]]) along axis 1 (duplicate example).",
        "code": "x = jnp.array([[1., 2., 3.],[4., 5., 7.]])\njnp.linalg.vector_norm(x, axis=1)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.linalg.vecdot",
    "concepts": [
      "Computes the (batched) vector conjugate dot product of two arrays.",
      "JAX implementation of numpy.linalg.vecdot().",
      "The size of x2[axis] must match the size of x1[axis].",
      "Remaining dimensions must be broadcast-compatible.",
      "The non-contracted dimensions are broadcast together.",
      "It accepts arguments such as x1, x2, axis, precision and preferred_element_type."
    ],
    "code_examples": []
  },
  {
    "title": "Vector dot product of two 1D arrays",
    "concepts": [
      "Demonstrates the use of jnp.linalg.vecdot with two 1D arrays.",
      "The arrays x1 and x2 are defined using jnp.array().",
      "The dot product is computed and returned as a JAX array."
    ],
    "code_examples": [
      {
        "description": "Computes the dot product of two 1D arrays using jnp.linalg.vecdot",
        "code": "x1 = jnp.array([1, 2, 3])\nx2 = jnp.array([4, 5, 6])\njnp.linalg.vecdot(x1, x2)"
      },
      {
        "description": "Computes the dot product of two 1D arrays using jnp.linalg.vecdot",
        "code": "x1 = jnp.array([1, 2, 3])\nx2 = jnp.array([4, 5, 6])\njnp.linalg.vecdot(x1, x2)"
      }
    ]
  },
  {
    "title": "Batched vector dot product of two 2D arrays",
    "concepts": [
      "Demonstrates the use of jnp.linalg.vecdot with two 2D arrays to compute a batched vector dot product.",
      "The arrays x1 and x2 are defined using jnp.array().",
      "The axis along which the dot product is computed is specified using the axis parameter.",
      "The dot product is computed for each batch and returned as a JAX array."
    ],
    "code_examples": [
      {
        "description": "Computes the batched vector dot product of two 2D arrays using jnp.linalg.vecdot.",
        "code": "x1 = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\nx2 = jnp.array([[2, 3, 4]])\njnp.linalg.vecdot(x1, x2, axis=-1)"
      },
      {
        "description": "Computes the batched vector dot product of two 2D arrays using jnp.linalg.vecdot.",
        "code": "x1 = jnp.array([[1, 2, 3],\n                [4, 5, 6]])\nx2 = jnp.array([[2, 3, 4]])\njnp.linalg.vecdot(x1, x2, axis=-1)"
      }
    ]
  },
  {
    "title": "Vector Quantization and Discrete Cosine Transform",
    "concepts": [
      "Assigning codes from a code book to observations using vector quantization.",
      "Computing the discrete cosine transform (DCT) of the input signal.",
      "Computing the multidimensional discrete cosine transform.",
      "Computing the inverse discrete cosine transform (IDCT).",
      "Computing the multidimensional inverse discrete cosine transform."
    ],
    "code_examples": []
  },
  {
    "title": "Integration and Interpolation",
    "concepts": [
      "Integrating along an axis using the composite trapezoidal rule.",
      "Interpolating points on a regular rectangular grid."
    ],
    "code_examples": []
  },
  {
    "title": "Linear Algebra: Matrix Operations and Decompositions",
    "concepts": [
      "Creating a block diagonal matrix from input arrays.",
      "Performing Cholesky factorization for linear solves.",
      "Solving a linear system using Cholesky factorization.",
      "Computing the Cholesky decomposition of a matrix.",
      "Computing the determinant of a matrix.",
      "Computing eigenvalues and eigenvectors for a Hermitian matrix.",
      "Solving the eigenvalue problem for a symmetric real tridiagonal matrix.",
      "Computing the matrix exponential.",
      "Computing the Frechet derivative of the matrix exponential.",
      "Evaluating a matrix-valued function.",
      "Computing the Hessenberg form of a matrix.",
      "Creating a Hilbert matrix.",
      "Returning the inverse of a square matrix.",
      "Computing the LU decomposition.",
      "Factorization for LU-based linear solves.",
      "Solving a linear system using an LU factorization.",
      "Computing the polar decomposition.",
      "Computing the QR decomposition of an array.",
      "Converting real Schur form to complex Schur form.",
      "Computing the Schur decomposition.",
      "Solving a linear system of equations.",
      "Solving a triangular linear system of equations.",
      "Computing the matrix square root.",
      "Computing the singular value decomposition.",
      "Constructing a Toeplitz matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Signal Processing: Convolution and Transforms",
    "concepts": [
      "Mapping an input array to new coordinates using interpolation.",
      "Minimizing a scalar function of one or more variables.",
      "Representing optimization results.",
      "Convolving two N-dimensional arrays using FFT.",
      "Convolving two N-dimensional arrays.",
      "Convolving two 2-dimensional arrays.",
      "Cross-correlating two N-dimensional arrays.",
      "Cross-correlating two 2-dimensional arrays.",
      "Estimating cross power spectral density (CSD) using Welch's method.",
      "Removing linear or piecewise linear trends from data.",
      "Performing the inverse short-time Fourier transform (ISTFT).",
      "Computing the short-time Fourier transform (STFT).",
      "Estimating power spectral density (PSD) using Welch's method."
    ],
    "code_examples": []
  },
  {
    "title": "Rotations and Iterative Solvers",
    "concepts": [
      "Representing a rotation in 3 dimensions.",
      "Performing Spherical Linear Interpolation of Rotations.",
      "Solving Ax = b using Bi-Conjugate Gradient Stable iteration.",
      "Solving Ax = b using Conjugate Gradient iteration.",
      "Solving the linear system A x = b for x using GMRES."
    ],
    "code_examples": []
  },
  {
    "title": "Special Functions",
    "concepts": [
      "Generating the first N Bernoulli numbers.",
      "Calculating the beta function.",
      "Calculating the regularized incomplete beta function.",
      "Calculating the natural log of the absolute value of the beta function.",
      "Calculating the digamma function.",
      "Calculating the entropy function.",
      "Calculating the error function.",
      "Calculating the complement of the error function.",
      "Calculating the inverse of the error function.",
      "Calculating the exponential integral function.",
      "Calculating the logistic sigmoid (expit) function.",
      "Calculating the generalized exponential integral function.",
      "Calculating the factorial function.",
      "Calculating the Fresnel integrals.",
      "Calculating the gamma function.",
      "Calculating the regularized lower incomplete gamma function.",
      "Calculating the regularized upper incomplete gamma function.",
      "Calculating the natural log of the absolute value of the gamma function.",
      "Calculating the sign of the gamma function.",
      "Calculating the 1F1 hypergeometric function.",
      "Calculating the modified Bessel function of zeroth order.",
      "Calculating the exponentially scaled modified Bessel function of zeroth order.",
      "Calculating the modified Bessel function of first order.",
      "Calculating the exponentially scaled modified Bessel function of first order.",
      "Calculating the Kullback-Leibler divergence.",
      "Calculating the Log Normal distribution function.",
      "Calculating the Log-Softmax function.",
      "Calculating the logit function.",
      "Calculating Log-sum-exp reduction.",
      "Calculating the associated Legendre functions (ALFs) of the first kind.",
      "Calculating the natural log of the multivariate gamma function.",
      "Calculating the Normal distribution function.",
      "Calculating the inverse of the CDF of the Normal distribution function.",
      "Calculating the Pochammer symbol.",
      "Calculating the polygamma function.",
      "Calculating the relative entropy function.",
      "Calculating the Softmax function.",
      "Calculating Spence's function, also known as the dilogarithm for real values.",
      "Computing the spherical harmonics.",
      "Compute x*log(1 + y), returning 0 for x=0.",
      "Compute x*log(y), returning 0 for x=0.",
      "Calculating the Hurwitz zeta function."
    ],
    "code_examples": []
  },
  {
    "title": "Statistical Measures",
    "concepts": [
      "Computing the mode (most common value) along an axis of an array.",
      "Computing the rank of data along an array axis.",
      "Computing the standard error of the mean."
    ],
    "code_examples": []
  },
  {
    "title": "Probability Distributions",
    "concepts": [
      "Bernoulli log probability mass function.",
      "Bernoulli probability mass function.",
      "Bernoulli cumulative distribution function.",
      "Bernoulli percent point function.",
      "Beta log probability distribution function.",
      "Beta probability distribution function.",
      "Beta cumulative distribution function",
      "Beta log cumulative distribution function.",
      "Beta distribution survival function.",
      "Beta distribution log survival function.",
      "Beta-binomial log probability mass function.",
      "Beta-binomial probability mass function.",
      "Binomial log probability mass function.",
      "Binomial probability mass function.",
      "Cauchy log probability distribution function.",
      "Cauchy probability distribution function.",
      "Cauchy cumulative distribution function.",
      "Cauchy log cumulative distribution function.",
      "Cauchy distribution log survival function.",
      "Cauchy distribution log survival function.",
      "Cauchy distribution inverse survival function.",
      "Cauchy distribution percent point function.",
      "Chi-square log probability distribution function.",
      "Chi-square probability distribution function.",
      "Chi-square cumulative distribution function.",
      "Chi-square log cumulative distribution function.",
      "Chi-square survival function.",
      "Chi-square log survival function.",
      "Dirichlet log probability distribution function.",
      "Dirichlet probability distribution function.",
      "Exponential log probability distribution function.",
      "Exponential probability distribution function.",
      "Exponential log cumulative density function.",
      "Exponential cumulative density function.",
      "Exponential log survival function.",
      "Exponential survival function.",
      "Exponential survival function.",
      "Gamma log probability distribution function.",
      "Gamma probability distribution function.",
      "Gamma cumulative distribution function.",
      "Gamma log cumulative distribution function.",
      "Gamma survival function.",
      "Gamma log survival function.",
      "Generalized normal cumulative distribution function.",
      "Generalized normal log probability distribution function.",
      "Generalized normal probability distribution function.",
      "Geometric log probability mass function.",
      "Geometric probability mass function.",
      "Laplace cumulative distribution function.",
      "Laplace log probability distribution function.",
      "Laplace probability distribution function.",
      "Logistic cumulative distribution function.",
      "Logistic distribution inverse survival function.",
      "Logistic log probability distribution function.",
      "Logistic probability distribution function.",
      "Logistic distribution percent point function.",
      "Logistic distribution survival function.",
      "Multinomial log probability mass function.",
      "Multinomial probability mass function.",
      "Multivariate normal log probability distribution function.",
      "Multivariate normal probability distribution function.",
      "Negative-binomial log probability mass function.",
      "Negative-binomial probability mass function.",
      "Normal log probability distribution function.",
      "Normal probability distribution function.",
      "Normal cumulative distribution function.",
      "Normal log cumulative distribution function.",
      "Normal distribution percent point function.",
      "Normal distribution survival function.",
      "Normal distribution log survival function.",
      "Normal distribution inverse survival function.",
      "Pareto log probability distribution function.",
      "Pareto probability distribution function.",
      "Poisson log probability mass function.",
      "Poisson probability mass function.",
      "Poisson cumulative distribution function.",
      "Student's T log probability distribution function.",
      "Student's T probability distribution function.",
      "Truncated normal cumulative distribution function.",
      "Truncated normal log cumulative distribution function.",
      "Truncated normal log probability distribution function.",
      "Truncated normal distribution log survival function.",
      "Truncated normal probability distribution function.",
      "Truncated normal distribution log survival function.",
      "Uniform log probability distribution function.",
      "Uniform probability distribution function.",
      "Uniform cumulative distribution function.",
      "Uniform distribution percent point function."
    ],
    "code_examples": []
  },
  {
    "title": "Kernel Density Estimation",
    "concepts": [
      "Gaussian Kernel Density Estimator",
      "Evaluate the Gaussian KDE on the given points.",
      "Integrate the distribution weighted by a Gaussian.",
      "Integrate the distribution over the given limits.",
      "Integrate the product of two Gaussian KDE distributions.",
      "Randomly sample a dataset from the estimated pdf",
      "Probability density function",
      "Log probability density function"
    ],
    "code_examples": []
  },
  {
    "title": "Circular Distributions",
    "concepts": [
      "von Mises log probability distribution function.",
      "von Mises probability distribution function.",
      "Wrapped Cauchy log probability distribution function.",
      "Wrapped Cauchy probability distribution function."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of Vector Quantization with JAX",
    "concepts": [
      "Assign codes from a code book to observations.",
      "JAX implementation of scipy.cluster.vq.vq()",
      "Assigns each observation vector to a code based on Euclidean distance.",
      "The function takes observation vectors and a code book as input.",
      "The function returns the closest code indices and distances."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use jax.scipy.cluster.vq.vq to assign codes to observations from a code book and calculate the Euclidean distances.",
        "code": "import jax.numpy as jnp\nimport jax\n\nobs = jnp.array([[1.1, 2.1, 3.1],\n               [5.9, 4.8, 6.2]])\ncode_book = jnp.array([[1., 2., 3.],\n                    [2., 3., 4.],\n                    [3., 4., 5.],\n                    [4., 5., 6.]])\ncodes, distances = jax.scipy.cluster.vq.vq(obs, code_book)\nprint(codes)\nprint(distances)"
      },
      {
        "description": "Demonstrates how to use jax.scipy.cluster.vq.vq to assign codes to observations from a code book and calculate the Euclidean distances.",
        "code": "import jax.numpy as jnp\nimport jax\n\nobs = jnp.array([[1.1, 2.1, 3.1],\n               [5.9, 4.8, 6.2]])\ncode_book = jnp.array([[1., 2., 3.],\n                    [2., 3., 4.],\n                    [3., 4., 5.],\n                    [4., 5., 6.]])\ncodes, distances = jax.scipy.cluster.vq.vq(obs, code_book)\nprint(codes)\nprint(distances)"
      }
    ]
  },
  {
    "title": "Introduction to dctn",
    "concepts": [
      "Computes the multidimensional discrete cosine transform of the input array.",
      "JAX implementation of scipy.fft.dctn().",
      "Only DCT type 2 is supported.",
      "The `s` parameter specifies the shape of the result. Defaults to the shape of x along the specified axes.",
      "The `axes` parameter specifies the axes along which the transform will be computed.",
      "The `norm` parameter specifies the normalization mode, with options None, 'backward', or 'ortho'. Defaults to None (equivalent to 'backward')."
    ],
    "code_examples": []
  },
  {
    "title": "dctn Examples",
    "concepts": [
      "Demonstrates the use of jax.scipy.fft.dctn with default parameters.",
      "Demonstrates the use of jax.scipy.fft.dctn with the `s` parameter to specify the output shape along a single axis.",
      "Demonstrates the use of jax.scipy.fft.dctn with both `s` and `axes` parameters to control the output shape and the axis of transformation.",
      "Demonstrates the use of jax.scipy.fft.dctn with the `s` parameter to specify the output shape along multiple axes."
    ],
    "code_examples": [
      {
        "description": "Computes the DCT along both axes by default when the `axes` argument is None.",
        "code": "x = jax.random.normal(jax.random.key(0), (3, 3))\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.dctn(x))"
      },
      {
        "description": "Specifies the dimension of the transform along axis 0 to be 2, while axis 1 remains the same.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.dctn(x, s=[2]))"
      },
      {
        "description": "Specifies the dimension of the transform along axis 1 to be 2, while axis 0 remains the same. Transform will be computed only along axis 1.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.dctn(x, s=[2], axes=[1]))"
      },
      {
        "description": "Sets the shape of the transform to (2, 4).",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.dctn(x, s=[2, 4]))"
      }
    ]
  },
  {
    "title": "Introduction to Inverse Discrete Cosine Transform (IDCT)",
    "concepts": [
      "Computes the inverse discrete cosine transform of the input array.",
      "JAX implementation of scipy.fft.idct().",
      "Currently, only type 2 IDCT is supported.",
      "The length of the transform can be specified, which can lead to zero-padding or truncation.",
      "The axis along which the IDCT is performed can be specified.",
      "Normalization mode can be specified as None, backward, or ortho; None is equivalent to backward."
    ],
    "code_examples": []
  },
  {
    "title": "Basic IDCT Example",
    "concepts": [
      "Demonstrates a basic application of the `jax.scipy.fft.idct` function with a randomly generated array.",
      "Illustrates the use of `jnp.printoptions` to control the output precision."
    ],
    "code_examples": [
      {
        "description": "Computes and prints the IDCT of a 3x3 random array using default parameters.",
        "code": "x = jax.random.normal(jax.random.key(0), (3, 3))\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.idct(x))"
      },
      {
        "description": "Computes and prints the IDCT of a 3x3 random array using default parameters (repeated example).",
        "code": "x = jax.random.normal(jax.random.key(0), (3, 3))\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.idct(x))"
      }
    ]
  },
  {
    "title": "IDCT with Truncation (n < x.shape[axis])",
    "concepts": [
      "Shows the effect of specifying a transform length (`n`) smaller than the input array's shape along the default axis.",
      "Illustrates truncation of the input array."
    ],
    "code_examples": [
      {
        "description": "Computes the IDCT with n=2, truncating the input.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.idct(x, n=2))"
      },
      {
        "description": "Computes the IDCT with n=2, truncating the input (repeated example).",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.idct(x, n=2))"
      }
    ]
  },
  {
    "title": "IDCT with Truncation and Specified Axis",
    "concepts": [
      "Shows the effect of specifying a transform length (`n`) smaller than the input array's shape along a specified axis (axis=0).",
      "Illustrates truncation of the input array along the specified axis."
    ],
    "code_examples": [
      {
        "description": "Computes the IDCT with n=2 and axis=0, truncating the input along the first axis.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.idct(x, n=2, axis=0))"
      },
      {
        "description": "Computes the IDCT with n=2 and axis=0, truncating the input along the first axis (repeated example).",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.idct(x, n=2, axis=0))"
      }
    ]
  },
  {
    "title": "IDCT with Zero-Padding (n > x.shape[axis])",
    "concepts": [
      "Shows the effect of specifying a transform length (`n`) larger than the input array's shape along a specified axis (axis=0).",
      "Illustrates zero-padding of the input array along the specified axis."
    ],
    "code_examples": [
      {
        "description": "Computes the IDCT with n=4 and axis=0, zero-padding the input along the first axis.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.idct(x, n=4, axis=0))"
      },
      {
        "description": "Computes the IDCT with n=4 and axis=0, zero-padding the input along the first axis (repeated example).",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jax.scipy.fft.idct(x, n=4, axis=0))"
      }
    ]
  },
  {
    "title": "Reconstructing Input from DCT using IDCT",
    "concepts": [
      "Demonstrates how to reconstruct the original input array `x` from its DCT using the IDCT.",
      "Verifies the reconstruction using `jnp.allclose`."
    ],
    "code_examples": [
      {
        "description": "Calculates the DCT of x, then calculates the IDCT of the result, and checks if it's close to the original x.",
        "code": "x_dct = jax.scipy.fft.dct(x)\njnp.allclose(x, jax.scipy.fft.idct(x_dct))"
      },
      {
        "description": "Calculates the DCT of x, then calculates the IDCT of the result, and checks if it's close to the original x (repeated example).",
        "code": "x_dct = jax.scipy.fft.dct(x)\njnp.allclose(x, jax.scipy.fft.idct(x_dct))"
      }
    ]
  },
  {
    "title": "Description and Usage of idctn",
    "concepts": [
      "Computes the multidimensional inverse discrete cosine transform.",
      "JAX implementation of scipy.fft.idctn().",
      "Supports type 2 DCT only.",
      "The `s` parameter specifies the shape of the result.",
      "The `axes` parameter specifies the axes along which to compute the transform.",
      "The `norm` parameter specifies the normalization mode.",
      "The default normalization mode is None, equivalent to backward.",
      "It can be used to reconstruct the original array from its DCT."
    ],
    "code_examples": []
  },
  {
    "title": "Basic idctn Example",
    "concepts": [
      "Shows how to compute idctn on a 3x3 array.",
      "Demonstrates the default behavior of computing the transform along both axes when `axes` is None."
    ],
    "code_examples": [
      {
        "description": "Computes the idctn of a 3x3 array using default parameters, which applies the transform along all axes.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jax.random.normal(jax.random.key(0), (3, 3))\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.fft.idctn(x))\n"
      },
      {
        "description": "Computes the idctn of a 3x3 array using default parameters, which applies the transform along all axes.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jax.random.normal(jax.random.key(0), (3, 3))\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.fft.idctn(x))\n"
      }
    ]
  },
  {
    "title": "idctn with Shape Parameter",
    "concepts": [
      "Shows how to use the `s` parameter to specify the output shape.",
      "Demonstrates how the output shape is affected when `s` is used."
    ],
    "code_examples": [
      {
        "description": "Computes the idctn with the shape `s` set to [2], resulting in the transform having a dimension of 2 along axis 0, while axis 1 remains the same as input.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jax.random.normal(jax.random.key(0), (3, 3))\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.fft.idctn(x, s=[2]))\n"
      },
      {
        "description": "Computes the idctn with the shape `s` set to [2], resulting in the transform having a dimension of 2 along axis 0, while axis 1 remains the same as input.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jax.random.normal(jax.random.key(0), (3, 3))\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.fft.idctn(x, s=[2]))\n"
      }
    ]
  },
  {
    "title": "idctn with Shape and Axes Parameters",
    "concepts": [
      "Shows how to use both the `s` and `axes` parameters.",
      "Demonstrates how the output shape and transform axes are affected."
    ],
    "code_examples": [
      {
        "description": "Computes the idctn with shape `s` set to [2] and axes set to [1], resulting in a transform with dimension 2 along axis 1, while axis 0 remains the same. The transform is only computed along axis 1.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jax.random.normal(jax.random.key(0), (3, 3))\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.fft.idctn(x, s=[2], axes=[1]))\n"
      },
      {
        "description": "Computes the idctn with shape `s` set to [2] and axes set to [1], resulting in a transform with dimension 2 along axis 1, while axis 0 remains the same. The transform is only computed along axis 1.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jax.random.normal(jax.random.key(0), (3, 3))\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.fft.idctn(x, s=[2], axes=[1]))\n"
      }
    ]
  },
  {
    "title": "idctn with Multidimensional Shape Parameter",
    "concepts": [
      "Shows how to use a sequence of integers for the `s` parameter to specify the output shape in multiple dimensions."
    ],
    "code_examples": [
      {
        "description": "Computes the idctn with shape `s` set to [2, 4], resulting in a transform with shape (2, 4).",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jax.random.normal(jax.random.key(0), (3, 3))\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.fft.idctn(x, s=[2, 4]))\n"
      },
      {
        "description": "Computes the idctn with shape `s` set to [2, 4], resulting in a transform with shape (2, 4).",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jax.random.normal(jax.random.key(0), (3, 3))\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.fft.idctn(x, s=[2, 4]))\n"
      }
    ]
  },
  {
    "title": "Reconstructing with idctn after dctn",
    "concepts": [
      "Demonstrates the use of `idctn` to reconstruct the original input after applying `dctn`.",
      "Shows that `idctn(dctn(x))` is approximately equal to `x`."
    ],
    "code_examples": [
      {
        "description": "Verifies that the original array can be reconstructed from its DCT using idctn.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jax.random.normal(jax.random.key(0), (3, 3))\nx_dctn = jax.scipy.fft.dctn(x)\n\njnp.allclose(x, jax.scipy.fft.idctn(x_dctn))\n"
      },
      {
        "description": "Verifies that the original array can be reconstructed from its DCT using idctn.",
        "code": "import jax\nimport jax.numpy as jnp\n\nx = jax.random.normal(jax.random.key(0), (3, 3))\nx_dctn = jax.scipy.fft.dctn(x)\n\njnp.allclose(x, jax.scipy.fft.idctn(x_dctn))\n"
      }
    ]
  },
  {
    "title": "Introduction to Trapezoidal Rule Integration in JAX",
    "concepts": [
      "The trapezoidal rule approximates the definite integral.",
      "It sums the areas of trapezoids formed by adjacent data points.",
      "The function integrates along a specified axis of an array.",
      "It is a JAX implementation of SciPy's trapezoid function."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Trapezoidal Integration with Regular Grid Spacing",
    "concepts": [
      "Integration with default x spacing (dx=1.0).",
      "Demonstrates the basic usage of the trapezoid function with only the y values."
    ],
    "code_examples": [
      {
        "description": "Integrates the provided y values using the trapezoidal rule with a default spacing of 1.0.",
        "code": "y = jnp.array([1, 2, 3, 2, 3, 2, 1])\njax.scipy.integrate.trapezoid(y, dx=1.0)"
      },
      {
        "description": "Integrates the provided y values using the trapezoidal rule with a default spacing of 1.0. (Duplicated example)",
        "code": "y = jnp.array([1, 2, 3, 2, 3, 2, 1])\njax.scipy.integrate.trapezoid(y, dx=1.0)"
      }
    ]
  },
  {
    "title": "Trapezoidal Integration with Irregular Grid Spacing",
    "concepts": [
      "Integration with explicitly defined x values.",
      "Demonstrates how to use the trapezoid function with both x and y values to account for irregular spacing."
    ],
    "code_examples": [
      {
        "description": "Integrates the provided y values with corresponding x values using the trapezoidal rule.",
        "code": "x = jnp.array([0, 2, 5, 7, 10, 15, 20])\ny = jnp.array([1, 2, 3, 2, 3, 2, 1])\njax.scipy.integrate.trapezoid(y, x)"
      },
      {
        "description": "Integrates the provided y values with corresponding x values using the trapezoidal rule. (Duplicated example)",
        "code": "x = jnp.array([0, 2, 5, 7, 10, 15, 20])\ny = jnp.array([1, 2, 3, 2, 3, 2, 1])\njax.scipy.integrate.trapezoid(y, x)"
      }
    ]
  },
  {
    "title": "Application: Approximating the Integral of sin^2(x)",
    "concepts": [
      "Approximating a definite integral using the trapezoidal rule.",
      "Comparing the approximation to the known value of the integral.",
      "Using jnp.linspace to create an array of evenly spaced points."
    ],
    "code_examples": [
      {
        "description": "Approximates the integral of sin^2(x) from 0 to 2*pi using the trapezoidal rule and verifies the result against the expected value of pi.",
        "code": "x = jnp.linspace(0, 2 * jnp.pi, 1000)\ny = jnp.sin(x)**2\nresult = jax.scipy.integrate.trapezoid(y, x)\njnp.allclose(result, jnp.pi)"
      },
      {
        "description": "Approximates the integral of sin^2(x) from 0 to 2*pi using the trapezoidal rule and verifies the result against the expected value of pi. (Duplicated example)",
        "code": "x = jnp.linspace(0, 2 * jnp.pi, 1000)\ny = jnp.sin(x)**2\nresult = jax.scipy.integrate.trapezoid(y, x)\njnp.allclose(result, jnp.pi)"
      }
    ]
  },
  {
    "title": "Introduction to RegularGridInterpolator",
    "concepts": [
      "Interpolates points on a regular rectangular grid.",
      "JAX implementation of scipy.interpolate.RegularGridInterpolator().",
      "points: sequence of arrays specifying the grid coordinates.",
      "values: N-dimensional array specifying the grid values.",
      "method: interpolation method, either 'linear' or 'nearest'.",
      "bounds_error: not implemented by JAX.",
      "fill_value: value returned for points outside the grid, defaults to NaN."
    ],
    "code_examples": []
  },
  {
    "title": "RegularGridInterpolator Example",
    "concepts": [
      "Demonstrates how to use RegularGridInterpolator with linear interpolation."
    ],
    "code_examples": [
      {
        "description": "Illustrates the usage of RegularGridInterpolator with sample points and values, performing linear interpolation to find values at specified query points.",
        "code": "points = (jnp.array([1, 2, 3]), jnp.array([4, 5, 6]))\nvalues = jnp.array([[10, 20, 30],\n                       [40, 50, 60],\n                       [70, 80, 90]])\ninterpolate = RegularGridInterpolator(points, values, method='linear')\nquery_points = jnp.array([[1.5, 4.5], [2.2, 5.8]])\ninterpolate(query_points)"
      }
    ]
  },
  {
    "title": "Methods",
    "concepts": [
      "__init__(points, values[, method, ...])"
    ],
    "code_examples": []
  },
  {
    "title": "Block Diagonal Matrix Creation",
    "concepts": [
      "Creation of a block diagonal matrix from input arrays using JAX.",
      "The input arrays should be at most two dimensions.",
      "The function jax.scipy.linalg.block_diag() is used."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the creation of a block diagonal matrix from three input arrays A, B, and C, which are 1x1, 2x2, and 3x3 matrices of ones respectively.",
        "code": "A = jnp.ones((1, 1))\nB = jnp.ones((2, 2))\nC = jnp.ones((3, 3))\nresult = jax.scipy.linalg.block_diag(A, B, C)\nprint(result)"
      },
      {
        "description": "Another example demonstrating the creation of a block diagonal matrix from three input arrays A, B, and C, which are 1x1, 2x2, and 3x3 matrices of ones respectively.",
        "code": "A = jnp.ones((1, 1))\nB = jnp.ones((2, 2))\nC = jnp.ones((3, 3))\nresult = jax.scipy.linalg.block_diag(A, B, C)\nprint(result)"
      }
    ]
  },
  {
    "title": "Factorization for Cholesky-based linear solves",
    "concepts": [
      "JAX implementation of scipy.linalg.cho_factor().",
      "Returns a result suitable for use with jax.scipy.linalg.cho_solve().",
      "For direct Cholesky decompositions, prefer jax.scipy.linalg.cholesky().",
      "Input array `a` represents a (batched) positive-definite hermitian matrix with shape (..., N, N).",
      "`lower` boolean determines if the lower triangular Cholesky decomposition is computed.",
      "Returns `(c, lower)` where `c` is the Cholesky decomposition and `lower` indicates if it's lower or upper.",
      "See also jax.scipy.linalg.cholesky() and jax.scipy.linalg.cho_solve()."
    ],
    "code_examples": [
      {
        "description": "Compute the cholesky factorization via cho_factor() , and use it to solve a linear equation via cho_solve() .",
        "code": "x = jnp.array([[2., 1.],\n               [1., 2.]])\nb = jnp.array([3., 4.])\ncfac = jax.scipy.linalg.cho_factor(x)\ny = jax.scipy.linalg.cho_solve(cfac, b)\nprint(y)\n#Array([0.6666666, 1.6666666], dtype=float32)"
      },
      {
        "description": "Check that the result is consistent.",
        "code": "x = jnp.array([[2., 1.],\n               [1., 2.]])\nb = jnp.array([3., 4.])\ncfac = jax.scipy.linalg.cho_factor(x)\ny = jax.scipy.linalg.cho_solve(cfac, b)\njnp.allclose(x @ y, b)\n#Array(True, dtype=bool)"
      }
    ]
  },
  {
    "title": "Introduction to cho_solve",
    "concepts": [
      "This section describes how to solve a linear system using a Cholesky factorization with JAX.",
      "It uses the output of jax.scipy.linalg.cho_factor().",
      "The function jax.scipy.linalg.cho_solve() is a JAX implementation of scipy.linalg.cho_solve().",
      "The input 'c_and_lower' is a tuple containing the Cholesky decomposition and a boolean indicating whether it's lower or upper.",
      "The input 'b' is the right-hand side of the linear system.",
      "The function returns the solution of the linear system."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of cho_solve",
    "concepts": [
      "This example demonstrates solving a linear equation using cho_factor() and cho_solve().",
      "It initializes a Hermitian positive-definite matrix x.",
      "It computes the Cholesky factorization of x using cho_factor().",
      "It solves the linear system with cho_solve().",
      "It verifies the result by checking if x @ y is close to b."
    ],
    "code_examples": [
      {
        "description": "Compute the Cholesky factorization via cho_factor(), and use it to solve a linear equation via cho_solve().",
        "code": "x = jnp.array([[2., 1.],\n              [1., 2.]])\nb = jnp.array([3., 4.])\ncfac = jax.scipy.linalg.cho_factor(x)\ny = jax.scipy.linalg.cho_solve(cfac, b)\nprint(y)"
      },
      {
        "description": "Check that the result is consistent.",
        "code": "x = jnp.array([[2., 1.],\n              [1., 2.]])\nb = jnp.array([3., 4.])\ncfac = jax.scipy.linalg.cho_factor(x)\ny = jax.scipy.linalg.cho_solve(cfac, b)\nprint(jnp.allclose(x @ y, b))"
      }
    ]
  },
  {
    "title": "Cholesky Decomposition",
    "concepts": [
      "The Cholesky decomposition of a matrix A is A = U^H * U = L * L^H, where U is an upper-triangular matrix and L is a lower-triangular matrix.",
      "The input array 'a' must be a (batched) positive-definite Hermitian matrix with shape (..., N, N).",
      "The 'lower' parameter determines whether to compute the lower (L) or upper (U) Cholesky decomposition.",
      "'overwrite_a' and 'check_finite' parameters are unused by JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of Cholesky Decomposition with JAX",
    "concepts": [
      "Demonstrates the use of jax.scipy.linalg.cholesky() to compute the Cholesky decomposition of a matrix.",
      "Shows how to compute both the upper and lower Cholesky factorizations.",
      "Illustrates how to reconstruct the original matrix from its lower Cholesky factorization."
    ],
    "code_examples": [
      {
        "description": "Define a small, real, Hermitian positive-definite matrix.",
        "code": "x = jnp.array([[2., 1.], [1., 2.]])"
      },
      {
        "description": "Compute the upper Cholesky factorization of the matrix.",
        "code": "jax.scipy.linalg.cholesky(x)"
      },
      {
        "description": "Compute the lower Cholesky factorization of the matrix.",
        "code": "jax.scipy.linalg.cholesky(x, lower=True)"
      },
      {
        "description": "Reconstruct the original matrix from its lower Cholesky factorization and verify using jnp.allclose().",
        "code": "L = jax.scipy.linalg.cholesky(x, lower=True)\njnp.allclose(x, L @ L.T)"
      }
    ]
  },
  {
    "title": "Determinant Calculation with JAX",
    "concepts": [
      "JAX implementation of the determinant function.",
      "Input array 'a' should have a shape of (..., N, N).",
      "The function returns the determinant with shape a.shape[:-2].",
      "The function mimics scipy.linalg.det() API.",
      "The parameters overwrite_a and check_finite are unused by JAX."
    ],
    "code_examples": [
      {
        "description": "Calculate the determinant of a 2x2 matrix using jax.scipy.linalg.det().",
        "code": "x = jnp.array([[1., 2.],\n              [3., 4.]])\njax.scipy.linalg.det(x)\n"
      },
      {
        "description": "Calculate the determinant of a batch of 2x2 matrices using jax.scipy.linalg.det().",
        "code": "x = jnp.array([[[1., 2.],\n              [3., 4.]],\n             [[8., 5.],\n              [7., 9.]]])\njax.scipy.linalg.det(x)"
      }
    ]
  },
  {
    "title": "Overview",
    "concepts": [
      "This section describes the JAX implementation of scipy.linalg.eigh().",
      "The function computes eigenvalues and eigenvectors for a Hermitian matrix.",
      "It also supports the generalized eigenvalue problem.",
      "The function has parameters for specifying the portion of the matrix to access (lower or upper), whether to compute only eigenvalues, the type of generalized eigenvalue problem, and which eigenvalues to compute."
    ],
    "code_examples": []
  },
  {
    "title": "Eigenvalue Decomposition Example",
    "concepts": [
      "Demonstrates computation of eigenvalues and eigenvectors for a simple 2x2 matrix using jax.scipy.linalg.eigh().",
      "Shows how to define a matrix using jnp.array().",
      "Illustrates the use of the eigh function to obtain eigenvalues and eigenvectors.",
      "Shows that eigenvectors are orthonormal.",
      "Solution satisfies the eigenvalue problem."
    ],
    "code_examples": [
      {
        "description": "Compute eigenvalues and eigenvectors of a 2x2 matrix.",
        "code": "a = jnp.array([[2., 1.], [1., 2.]])\neigvals, eigvecs = jax.scipy.linalg.eigh(a)\nprint(eigvals)\nprint(eigvecs)"
      },
      {
        "description": "Verify that eigenvectors are orthonormal.",
        "code": "jnp.allclose(eigvecs.T @ eigvecs, jnp.eye(2), atol=1E-5)"
      },
      {
        "description": "Verify that solution satisfies the eigenvalue problem.",
        "code": "jnp.allclose(a @ eigvecs, eigvecs @ jnp.diag(eigvals))"
      }
    ]
  },
  {
    "title": "Eigenvalue Problem for Symmetric Real Tridiagonal Matrix",
    "concepts": [
      "Solving eigenvalue problem for a symmetric real tridiagonal matrix using JAX.",
      "The function leverages JAX implementation of scipy.linalg.eigh_tridiagonal().",
      "It calculates eigenvalues of a tridiagonal matrix defined by its diagonal and off-diagonal elements.",
      "Computation of eigenvectors is not yet implemented, so eigvals_only must be set to True."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of jax.scipy.linalg.eigh_tridiagonal to calculate eigenvalues of a tridiagonal matrix.",
        "code": ">>> d = jnp.array([1., 2., 3., 4.])\n>>> e = jnp.array([1., 1., 1.])\n>>> eigvals = jax.scipy.linalg.eigh_tridiagonal(d, e, eigvals_only=True)\n>>> eigvals\nArray([0.2547188, 1.8227171, 3.1772828, 4.745281 ], dtype=float32)"
      },
      {
        "description": "Constructs the full matrix from the diagonal and off-diagonal elements and computes the eigenvalues using jax.scipy.linalg.eigh.",
        "code": ">>> A = jnp.diag(d) + jnp.diag(e, 1) + jnp.diag(e, -1)\n>>> A\nArray([[1., 1., 0., 0.],\n       [1., 2., 1., 0.],\n       [0., 1., 3., 1.],\n       [0., 0., 1., 4.]], dtype=float32)\n>>> eigvals_full = jax.scipy.linalg.eigh(A, eigvals_only=True)\n>>> jnp.allclose(eigvals, eigvals_full)\nArray(True, dtype=bool)"
      }
    ]
  },
  {
    "title": "Matrix Exponential Overview",
    "concepts": [
      "The document describes a JAX implementation of the matrix exponential function, equivalent to scipy.linalg.expm().",
      "The function computes the matrix exponential of an array.",
      "It uses a scaling-and-squaring approximation method.",
      "The method's computational complexity is controlled by the `max_squarings` argument."
    ],
    "code_examples": []
  },
  {
    "title": "Properties and Examples of Matrix Exponential",
    "concepts": [
      "For matrices, expm(A+B) = expm(A) @ expm(B) only holds when A and B commute (AB = BA).",
      "If a matrix X is invertible, then expm(X @ A @ inv(X)) = X @ expm(A) @ inv(X)."
    ],
    "code_examples": [
      {
        "description": "Demonstrates that expm(A+B) = expm(A) @ expm(B) when A and B commute. A and B are diagonal matrices in this case.",
        "code": "A = jnp.array([[2, 0],\n                [0, 1]])\nB = jnp.array([[3, 0],\n                [0, 4]])\njnp.allclose(jax.scipy.linalg.expm(A + B),\n             jax.scipy.linalg.expm(A) @ jax.scipy.linalg.expm(B),\n             rtol=0.0001)"
      },
      {
        "description": "Demonstrates the property expm(X @ A @ inv(X)) = X @ expm(A) @ inv(X), where X is an invertible matrix.",
        "code": "X = jnp.array([[3, 1],\n                [2, 5]])\nX_inv = jax.scipy.linalg.inv(X)\njnp.allclose(jax.scipy.linalg.expm(X @ A @ X_inv),\n             X @ jax.scipy.linalg.expm(A) @ X_inv)"
      }
    ]
  },
  {
    "title": "Introduction to Frechet Derivative of Matrix Exponential",
    "concepts": [
      "The document describes a JAX implementation of scipy.linalg.expm_frechet().",
      "The function computes the Frechet derivative of the matrix exponential.",
      "It takes two matrices, A and E, as input, where E specifies the direction of the derivative.",
      "It can optionally compute and return expm(A) along with the Frechet derivative.",
      "The function returns a tuple (expm_A, expm_frechet_AE) if compute_expm is True, else the array expm_frechet_AE."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of expm_frechet",
    "concepts": [
      "Demonstrates how to compute the matrix exponential of A and its derivative in the direction E using jax.scipy.linalg.expm_frechet()."
    ],
    "code_examples": [
      {
        "description": "Compute the matrix exponential and its derivative using jax.scipy.linalg.expm_frechet.",
        "code": "key1 , key2 = jax . random . split ( jax . random . key ( 3372 ))\nA = jax . random . normal ( key1 , ( 3 , 3 ))\nE = jax . random . normal ( key2 , ( 3 , 3 ))\nexpmA , expm_frechet_AE = jax . scipy . linalg . expm_frechet ( A , E )"
      },
      {
        "description": "Compute the matrix exponential and its derivative using jax.scipy.linalg.expm_frechet.",
        "code": "key1 , key2 = jax . random . split ( jax . random . key ( 3372 ))\nA = jax . random . normal ( key1 , ( 3 , 3 ))\nE = jax . random . normal ( key2 , ( 3 , 3 ))\nexpmA , expm_frechet_AE = jax . scipy . linalg . expm_frechet ( A , E )"
      }
    ]
  },
  {
    "title": "Verification using Automatic Differentiation",
    "concepts": [
      "Shows how to compute the derivative of expm() in the direction of E using jax.jvp().",
      "Verifies that the results from jax.scipy.linalg.expm_frechet() and jax.jvp() are equivalent."
    ],
    "code_examples": [
      {
        "description": "Compute the derivative of the matrix exponential using jax.jvp and compare results with jax.scipy.linalg.expm_frechet.",
        "code": "expmA2 , expm_frechet_AE2 = jax . jvp ( jax . scipy . linalg . expm , ( A ,), ( E ,))\njnp . allclose ( expmA , expmA2 )\njnp . allclose ( expm_frechet_AE , expm_frechet_AE2 )"
      },
      {
        "description": "Compute the derivative of the matrix exponential using jax.jvp and compare results with jax.scipy.linalg.expm_frechet.",
        "code": "expmA2 , expm_frechet_AE2 = jax . jvp ( jax . scipy . linalg . expm , ( A ,), ( E ,))\njnp . allclose ( expmA , expmA2 )\njnp . allclose ( expm_frechet_AE , expm_frechet_AE2 )"
      }
    ]
  },
  {
    "title": "Overview of jax.scipy.linalg.funm",
    "concepts": [
      "This section describes the JAX implementation of the SciPy function `scipy.linalg.funm()`.",
      "It evaluates a matrix-valued function.",
      "The function takes a square matrix and a callable function as input.",
      "The callable function is applied to the eigenvalues of the matrix.",
      "The function can optionally take a precomputed matrix exponential for improved efficiency when calculating the exponential function.",
      "The return type might differ from SciPy, returning complex-valued arrays even when SciPy returns real-valued arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Applying an Arbitrary Matrix Function",
    "concepts": [
      "Demonstrates how to apply an arbitrary function to a matrix using `jax.scipy.linalg.funm()`.",
      "Defines a simple function `func` that calculates `sin(x) + 2*cos(x)`.",
      "Applies the defined function to a matrix A."
    ],
    "code_examples": [
      {
        "description": "Applying an arbitrary matrix function using jax.scipy.linalg.funm.",
        "code": "import jax.numpy as jnp\nimport jax\n\nA = jnp.array([[1., 2.], [3., 4.]])\n\ndef func(x):\n    return jnp.sin(x) + 2 * jnp.cos(x)\n\nresult = jax.scipy.linalg.funm(A, func)\nprint(result)"
      },
      {
        "description": "Applying an arbitrary matrix function using jax.scipy.linalg.funm.",
        "code": "import jax.numpy as jnp\nimport jax\n\nA = jnp.array([[1., 2.], [3., 4.]])\n\ndef func(x):\n    return jnp.sin(x) + 2 * jnp.cos(x)\n\nresult = jax.scipy.linalg.funm(A, func)\nprint(result)"
      }
    ]
  },
  {
    "title": "Comparing funm with expm for Matrix Exponentiation",
    "concepts": [
      "Compares two ways of computing the matrix exponential: using `jax.scipy.linalg.funm()` with `jnp.exp` and using `jax.scipy.linalg.expm()` directly.",
      "Demonstrates that both methods produce similar results within a certain tolerance.",
      "Uses `jnp.allclose()` to verify the similarity between the two results."
    ],
    "code_examples": [
      {
        "description": "Comparing jax.scipy.linalg.funm with jax.scipy.linalg.expm for calculating matrix exponent.",
        "code": "import jax.numpy as jnp\nimport jax\n\nA = jnp.array([[1., 2.], [3., 4.]])\nexpA_1 = jax.scipy.linalg.funm(A, jnp.exp)\nexpA_2 = jax.scipy.linalg.expm(A)\n\nclose = jnp.allclose(expA_1, expA_2, rtol=1E-4)\nprint(close)"
      },
      {
        "description": "Comparing jax.scipy.linalg.funm with jax.scipy.linalg.expm for calculating matrix exponent.",
        "code": "import jax.numpy as jnp\nimport jax\n\nA = jnp.array([[1., 2.], [3., 4.]])\nexpA_1 = jax.scipy.linalg.funm(A, jnp.exp)\nexpA_2 = jax.scipy.linalg.expm(A)\n\nclose = jnp.allclose(expA_1, expA_2, rtol=1E-4)\nprint(close)"
      }
    ]
  },
  {
    "title": "Hessenberg Form",
    "concepts": [
      "The Hessenberg form of a matrix A is defined as A = Q H Q^H, where Q is unitary and H is zero below the first subdiagonal.",
      "The function jax.scipy.linalg.hessenberg() computes the Hessenberg form of a matrix.",
      "The function can optionally compute the unitary matrix Q.",
      "The function returns either the Hessenberg form H or a tuple (H, Q) if calc_q is True."
    ],
    "code_examples": []
  },
  {
    "title": "Hessenberg Form Computation and Reconstruction",
    "concepts": [
      "Demonstrates how to compute the Hessenberg form of a matrix using jax.scipy.linalg.hessenberg.",
      "Shows how to reconstruct the original matrix from the Hessenberg form and the unitary matrix Q.",
      "Illustrates the use of jnp.allclose to verify the reconstruction."
    ],
    "code_examples": [
      {
        "description": "Computes the Hessenberg form of a 4x4 matrix and prints the Hessenberg matrix.",
        "code": "import jax\nimport jax.numpy as jnp\n\na = jnp.array([[1., 2., 3., 4.],\n               [1., 4., 2., 3.],\n               [3., 2., 1., 4.],\n               [2., 3., 2., 2.]])\n\nH, Q = jax.scipy.linalg.hessenberg(a, calc_q=True)\n\nwith jnp.printoptions(suppress=True, precision=3):\n  print(H)"
      },
      {
        "description": "Reconstructs the original matrix from the Hessenberg form and the unitary matrix, then verifies the reconstruction.",
        "code": "import jax\nimport jax.numpy as jnp\n\na = jnp.array([[1., 2., 3., 4.],\n               [1., 4., 2., 3.],\n               [3., 2., 1., 4.],\n               [2., 3., 2., 2.]])\n\nH, Q = jax.scipy.linalg.hessenberg(a, calc_q=True)\n\na_reconstructed = Q @ H @ Q.conj().T\n\nprint(jnp.allclose(a_reconstructed, a))"
      }
    ]
  },
  {
    "title": "Hilbert Matrix Definition",
    "concepts": [
      "A Hilbert matrix of order n is created.",
      "The Hilbert matrix is defined by H_{ij} = 1 / (i + j + 1) for 1 <= i <= n and 1 <= j <= n.",
      "The input 'n' determines the size of the matrix.",
      "The output is a Hilbert matrix of shape (n, n)."
    ],
    "code_examples": []
  },
  {
    "title": "Hilbert Matrix Examples",
    "concepts": [
      "Demonstration of creating Hilbert matrices of order 2 and 3 using jax.scipy.linalg.hilbert()."
    ],
    "code_examples": [
      {
        "description": "Example of creating a Hilbert matrix of order 2.",
        "code": "jax.scipy.linalg.hilbert(2)\nArray([[1.        , 0.5       ],\n[0.5       , 0.33333334]], dtype=float32)"
      },
      {
        "description": "Example of creating a Hilbert matrix of order 3.",
        "code": "jax.scipy.linalg.hilbert(3)\nArray([[1.        , 0.5       , 0.33333334],\n[0.5       , 0.33333334, 0.25      ],\n[0.33333334, 0.25      , 0.2       ]], dtype=float32)"
      },
      {
        "description": "Example of creating a Hilbert matrix of order 2.",
        "code": "jax.scipy.linalg.hilbert(2)\nArray([[1.        , 0.5       ],\n[0.5       , 0.33333334]], dtype=float32)"
      },
      {
        "description": "Example of creating a Hilbert matrix of order 3.",
        "code": "jax.scipy.linalg.hilbert(3)\nArray([[1.        , 0.5       , 0.33333334],\n[0.5       , 0.33333334, 0.25      ],\n[0.33333334, 0.25      , 0.2       ]], dtype=float32)"
      }
    ]
  },
  {
    "title": "Introduction to LU Solve with JAX",
    "concepts": [
      "Solving a linear system using LU factorization.",
      "JAX implementation of scipy.linalg.lu_solve().",
      "Utilizes the output of jax.scipy.linalg.lu_factor().",
      "lu_and_piv is a tuple containing the LU decomposition and pivots from lu_factor().",
      "b represents the right-hand-side of the linear system.",
      "trans specifies the type of system to solve (Ax=b, A^Tx=b, or A^Hx=b).",
      "The function returns the solution of the linear system.",
      "See also jax.scipy.linalg.lu() and jax.scipy.linalg.lu_factor()"
    ],
    "code_examples": []
  },
  {
    "title": "Solving a Linear System using LU Factorization in JAX",
    "concepts": [
      "Demonstrates solving a linear system using lu_factor and lu_solve.",
      "The example first defines a matrix 'a' and a vector 'b'.",
      "It then computes the LU factorization of 'a' using lu_factor.",
      "Finally, it solves the linear system using lu_solve with the LU factorization and 'b'."
    ],
    "code_examples": [
      {
        "description": "Solving a small linear system using LU factorization.",
        "code": "a = jnp.array([[2., 1.],\n              [1., 2.]])\nb = jnp.array([3., 4.])\nlufac = jax.scipy.linalg.lu_factor(a)\ny = jax.scipy.linalg.lu_solve(lufac, b)\nprint(y)"
      }
    ]
  },
  {
    "title": "Verifying the Solution",
    "concepts": [
      "This section shows how to verify if the computed solution is consistent.",
      "It uses jnp.allclose() to check if a @ y is approximately equal to b."
    ],
    "code_examples": [
      {
        "description": "Checking that the result is consistent.",
        "code": "a = jnp.array([[2., 1.],\n              [1., 2.]])\nb = jnp.array([3., 4.])\nlufac = jax.scipy.linalg.lu_factor(a)\ny = jax.scipy.linalg.lu_solve(lufac, b)\n\nprint(jnp.allclose(a @ y, b))"
      }
    ]
  },
  {
    "title": "Polar Decomposition Definition",
    "concepts": [
      "The polar decomposition of a matrix A decomposes it into a unitary matrix U and a positive semidefinite matrix P such that A = UP (right polar decomposition) or A = PU (left polar decomposition).",
      "If A is nonsingular, P is positive definite, and the decomposition is unique.",
      "The unitary factor U has orthonormal columns or rows depending on the dimensions of A.",
      "The unitary factor U can be constructed from the SVD of A as  U = U_svd @ V^H_svd."
    ],
    "code_examples": []
  },
  {
    "title": "Polar Decomposition Methods",
    "concepts": [
      "Two methods are supported for computing the polar decomposition: SVD and QDWH.",
      "The 'svd' method computes the SVD of A and then forms U = U_svd @ V^H_svd.",
      "The 'qdwh' method applies the QR-based Dynamically Weighted Halley algorithm."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters and Return Value",
    "concepts": [
      "The function takes an input matrix A, the side ('right' or 'left'), the method ('svd' or 'qdwh'), precision, eps (for QDWH), and max_iterations (for QDWH).",
      "The function returns a tuple containing the unitary factor U and the positive semidefinite factor P."
    ],
    "code_examples": []
  },
  {
    "title": "Polar Decomposition Example",
    "concepts": [
      "Demonstrates how to compute the polar decomposition of a 3x3 matrix using jax.scipy.linalg.polar.",
      "Shows how to verify that U is a unitary matrix.",
      "Illustrates how to verify that P is a positive semi-definite matrix.",
      "Demonstrates how to reconstruct the original matrix by multiplying U and P."
    ],
    "code_examples": [
      {
        "description": "Computes the polar decomposition of a 3x3 matrix and checks that U is unitary.",
        "code": "a = jnp.array([[1., 2., 3.], [5., 4., 2.], [3., 2., 1.]])\nU, P = jax.scipy.linalg.polar(a)\nprint(jnp.round(U.T @ U))"
      },
      {
        "description": "Verifies that P is positive semi-definite.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n    print(P)"
      },
      {
        "description": "Reconstructs the original matrix from the polar factors.",
        "code": "a_reconstructed = U @ P\nprint(jnp.allclose(a, a_reconstructed))"
      }
    ]
  },
  {
    "title": "QR Decomposition Overview",
    "concepts": [
      "The QR decomposition of a matrix A is given by A = QR.",
      "Q is a unitary matrix (Q^H * Q = I).",
      "R is an upper-triangular matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters and Return Values",
    "concepts": [
      "The input 'a' is the array to be decomposed, with shape (\u2026, M, N).",
      "The 'mode' parameter controls the shape of Q and R: 'full', 'r', or 'economic'.",
      "'full' returns Q of shape (M, M) and R of shape (M, N).",
      "'r' returns only R.",
      "'economic' returns Q of shape (M, K) and R of shape (K, N), where K = min(M, N).",
      "The 'pivoting' parameter allows for rank-revealing decomposition.",
      "The function returns a tuple (Q, R) or (Q, R, P) depending on 'mode' and 'pivoting'.",
      "Q is an orthogonal matrix.",
      "R is an upper-triangular matrix.",
      "P is an index vector when pivoting is enabled."
    ],
    "code_examples": []
  },
  {
    "title": "Notes and See Also",
    "concepts": [
      "Pivoting is only implemented on CPU and GPU backends.",
      "jax.numpy.linalg.qr() is a NumPy-style QR decomposition API.",
      "jax.lax.linalg.qr() is an XLA-style QR decomposition API."
    ],
    "code_examples": []
  },
  {
    "title": "QR Decomposition Example",
    "concepts": [
      "Compute the QR decomposition of a matrix using jax.scipy.linalg.qr().",
      "Verify that Q is an orthonormal matrix.",
      "Reconstruct the original matrix from Q and R."
    ],
    "code_examples": [
      {
        "description": "Compute the QR decomposition of a matrix and print Q and R.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\na = jnp.array([[1., 2., 3., 4.],\n               [5., 4., 2., 1.],\n               [6., 3., 1., 5.]])\n\nQ, R = jax.scipy.linalg.qr(a)\n\nprint(Q)\nprint(R)"
      },
      {
        "description": "Check that Q is orthonormal.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\na = jnp.array([[1., 2., 3., 4.],\n               [5., 4., 2., 1.],\n               [6., 3., 1., 5.]])\n\nQ, R = jax.scipy.linalg.qr(a)\n\nprint(jnp.allclose(Q.T @ Q, jnp.eye(3), atol=1E-5))"
      },
      {
        "description": "Reconstruct the input matrix from Q and R.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\na = jnp.array([[1., 2., 3., 4.],\n               [5., 4., 2., 1.],\n               [6., 3., 1., 5.]])\n\nQ, R = jax.scipy.linalg.qr(a)\n\nprint(jnp.allclose(Q @ R, a))"
      }
    ]
  },
  {
    "title": "Schur Decomposition Overview",
    "concepts": [
      "The Schur decomposition of a matrix A is A = Z T Z^H, where Z is unitary.",
      "T is upper-triangular for the complex-valued Schur decomposition.",
      "T is quasi-upper-triangular for the real-valued Schur decomposition.",
      "In the quasi-triangular case, the diagonal may include 2x2 blocks associated with complex-valued eigenvalue pairs of A."
    ],
    "code_examples": []
  },
  {
    "title": "Function Definition and Parameters",
    "concepts": [
      "The function takes an input array 'a' of shape (..., N, N).",
      "The 'output' parameter specifies whether to compute the 'real' (default) or 'complex' Schur decomposition."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "The function returns a tuple of arrays (T, Z).",
      "T is a shape (..., N, N) array containing the upper-triangular Schur form of the input.",
      "Z is a shape (..., N, N) array containing the unitary Schur transformation matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Schur Decomposition Example",
    "concepts": [
      "Demonstrates how to compute the Schur decomposition of a 3x3 matrix using jax.scipy.linalg.schur().",
      "Shows that the transformation matrix Z is unitary.",
      "Demonstrates how to reconstruct the original matrix from the Schur decomposition."
    ],
    "code_examples": [
      {
        "description": "Compute the Schur decomposition of a 3x3 matrix.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\nimport jax\n\na = jnp.array([[1., 2., 3.],\n               [1., 4., 2.],\n               [3., 2., 1.]])\n\nT, Z = jax.scipy.linalg.schur(a)"
      },
      {
        "description": "Verify that the Schur form T is quasi-upper-triangular (or upper-triangular in this case).",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\nimport jax\n\na = jnp.array([[1., 2., 3.],\n               [1., 4., 2.],\n               [3., 2., 1.]])\n\nT, Z = jax.scipy.linalg.schur(a)\n\nT"
      },
      {
        "description": "Verify that the transformation matrix Z is unitary.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\nimport jax\n\na = jnp.array([[1., 2., 3.],\n               [1., 4., 2.],\n               [3., 2., 1.]])\n\nT, Z = jax.scipy.linalg.schur(a)\n\njnp.allclose(Z.T @ Z, jnp.eye(3), atol=1E-5)"
      },
      {
        "description": "Reconstruct the original matrix from the Schur decomposition.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\nimport jax\n\na = jnp.array([[1., 2., 3.],\n               [1., 4., 2.],\n               [3., 2., 1.]])\n\nT, Z = jax.scipy.linalg.schur(a)\n\njnp.allclose(Z @ T @ Z.T, a)"
      }
    ]
  },
  {
    "title": "Overview of jax.scipy.linalg.solve",
    "concepts": [
      "Solves a linear system of equations a @ x = b for x.",
      "JAX implementation of scipy.linalg.solve().",
      "If 'a' is singular, returns nan or inf values.",
      "Accepts batched linear systems.",
      "Can assume properties of 'a' such as symmetric, hermitian, or positive-definite."
    ],
    "code_examples": []
  },
  {
    "title": "Solving a Simple Linear System",
    "concepts": [
      "Demonstrates solving a 3x3 linear system using jax.scipy.linalg.solve.",
      "Shows how to define the matrix 'A' and the vector 'b' using jnp.array.",
      "Illustrates the use of jax.scipy.linalg.solve to find the solution 'x'."
    ],
    "code_examples": [
      {
        "description": "Solving a basic 3x3 linear system with jax.scipy.linalg.solve.",
        "code": "A = jnp.array([[1., 2., 3.],\n              [2., 4., 2.],\n              [3., 2., 1.]])\nb = jnp.array([14., 16., 10.])\nx = jax.scipy.linalg.solve(A, b)\nprint(x)"
      }
    ]
  },
  {
    "title": "Verifying the Solution",
    "concepts": [
      "Confirms that the calculated solution 'x' satisfies the equation A @ x = b.",
      "Uses jnp.allclose to compare A @ x and b, accounting for potential floating-point inaccuracies."
    ],
    "code_examples": [
      {
        "description": "Validating the solution by checking if A @ x is approximately equal to b.",
        "code": "jnp.allclose(A @ x, b)"
      }
    ]
  },
  {
    "title": "Solving Triangular Linear Systems with JAX",
    "concepts": [
      "The function `jax.scipy.linalg.solve_triangular` solves a linear system of equations `a @ x = b` where `a` is a triangular matrix.",
      "The `lower` argument specifies whether to use the lower or upper triangle of the matrix `a`.",
      "The `unit_diagonal` argument indicates whether the diagonal elements of `a` are assumed to be 1.",
      "The `trans` argument specifies whether to solve `Ax=b`, `A^Tx=b`, or `A^Hx=b`.",
      "The function returns the solution `x` to the linear system."
    ],
    "code_examples": [
      {
        "description": "Solves a simple 3x3 triangular linear system.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\nA = jnp.array([[1., 2., 3.], [0., 3., 2.], [0., 0., 5.]])\nb = jnp.array([10., 8., 5.])\nx = jax.scipy.linalg.solve_triangular(A, b)\nprint(x)"
      },
      {
        "description": "Confirms that the result solves the system.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\nA = jnp.array([[1., 2., 3.], [0., 3., 2.], [0., 0., 5.]])\nb = jnp.array([10., 8., 5.])\nx = jax.scipy.linalg.solve_triangular(A, b)\nprint(jnp.allclose(A @ x, b))"
      },
      {
        "description": "Computing the transposed problem.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\nA = jnp.array([[1., 2., 3.], [0., 3., 2.], [0., 0., 5.]])\nb = jnp.array([10., 8., 5.])\nx = jax.scipy.linalg.solve_triangular(A, b, trans='T')\nprint(x)"
      },
      {
        "description": "Confirming that the result solves the transposed system.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\nA = jnp.array([[1., 2., 3.], [0., 3., 2.], [0., 0., 5.]])\nb = jnp.array([10., 8., 5.])\nx = jax.scipy.linalg.solve_triangular(A, b, trans='T')\nprint(jnp.allclose(A.T @ x, b))"
      }
    ]
  },
  {
    "title": "Matrix Square Root Computation with JAX",
    "concepts": [
      "JAX implementation of scipy.linalg.sqrtm().",
      "Computes the matrix square root of a given array.",
      "Blocksize parameter is not supported in JAX; JAX always uses blocksize=1.",
      "The function uses the complex Schur method.",
      "Matrix multiplication of the matrix square root with itself equals the input matrix."
    ],
    "code_examples": [
      {
        "description": "Compute the matrix square root of a 3x3 matrix and print the result with specified precision.",
        "code": "a = jnp.array([[1., 2., 3.],\n               [2., 4., 2.],\n               [3., 2., 1.]])\nsqrt_a = jax.scipy.linalg.sqrtm(a)\nwith jnp.printoptions(precision=2, suppress=True):\n  print(sqrt_a)"
      },
      {
        "description": "Verify that the matrix square root multiplied by itself equals the original matrix.",
        "code": "jnp.allclose(a, sqrt_a @ sqrt_a)"
      }
    ]
  },
  {
    "title": "Introduction to map_coordinates",
    "concepts": [
      "The function interpolates values from an input array at specified coordinates.",
      "It is a JAX implementation of scipy.ndimage.map_coordinates().",
      "It takes an input array and a sequence of coordinate arrays as input.",
      "It returns the interpolated values at the given coordinates."
    ],
    "code_examples": []
  },
  {
    "title": "Arguments of map_coordinates",
    "concepts": [
      "input: N-dimensional input array from which values are interpolated.",
      "coordinates: length-N sequence of arrays specifying the coordinates at which to evaluate the interpolated values.",
      "order: The order of interpolation (0: Nearest-neighbor, 1: Linear).",
      "mode: Specifies how to handle points outside the input boundaries ('constant', 'nearest', 'mirror', 'wrap', 'reflect').",
      "cval: Value used for points outside the boundaries when mode='constant'.",
      "The 'wrap' mode in JAX behaves as 'grid-wrap' mode in SciPy.",
      "The 'constant' mode in JAX behaves as 'grid-constant' mode in SciPy."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Examples",
    "concepts": [
      "Demonstrates how to use jax.scipy.ndimage.map_coordinates with a 2D input array and coordinate arrays.",
      "Shows the result of linear interpolation (order=1) at the specified coordinates."
    ],
    "code_examples": [
      {
        "description": "Demonstrates map_coordinates with a sample input array and coordinates using linear interpolation.",
        "code": "import jax.numpy as jnp\nimport jax\n\ninput_array = jnp.arange(12.0).reshape(3, 4)\ncoordinates = [jnp.array([0.5, 1.5]), jnp.array([1.5, 2.5])]\n\nresult = jax.scipy.ndimage.map_coordinates(input_array, coordinates, order=1)\n\nprint(\"Input array:\\n\", input_array)\nprint(\"Coordinates:\", coordinates)\nprint(\"Result:\", result)"
      },
      {
        "description": "Demonstrates map_coordinates with a sample input array and coordinates using linear interpolation.",
        "code": "import jax.numpy as jnp\nimport jax\n\ninput_array = jnp.arange(12.0).reshape(3, 4)\ncoordinates = [jnp.array([0.5, 1.5]), jnp.array([1.5, 2.5])]\n\nresult = jax.scipy.ndimage.map_coordinates(input_array, coordinates, order=1)\n\nprint(\"Input array:\\n\", input_array)\nprint(\"Coordinates:\", coordinates)\nprint(\"Result:\", result)"
      }
    ]
  },
  {
    "title": "Important Notes",
    "concepts": [
      "Interpolation near boundaries may differ from the SciPy implementation due to a bug fix in JAX.",
      "The mode argument is interpreted as documented by SciPy, but not necessarily as implemented by SciPy."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of Minimization Function",
    "concepts": [
      "The API for minimization functions is similar to SciPy's.",
      "Gradients are calculated automatically using JAX's autodiff.",
      "The 'method' argument is mandatory; a solver must be specified.",
      "Not all optional arguments from SciPy are implemented.",
      "Optimization results may differ from SciPy due to line search differences.",
      "minimize supports jit() compilation.",
      "Differentiation and multi-dimensional arrays are planned features."
    ],
    "code_examples": []
  },
  {
    "title": "Function Arguments",
    "concepts": [
      "fun: The objective function to be minimized.",
      "fun must accept x (a 1-D array) and *args as input.",
      "fun must return a float.",
      "fun must support differentiation.",
      "x0: The initial guess, an array of real elements.",
      "args: Extra arguments passed to the objective function.",
      "method: Solver type. Currently, only 'BFGS' is supported.",
      "tol: Tolerance for termination.",
      "options: Solver options, including maxiter (maximum iterations)."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "The function returns an OptimizeResults object."
    ],
    "code_examples": []
  },
  {
    "title": "Optimization Results Object",
    "concepts": [
      "The object holds the results of an optimization process.",
      "x: Represents the final solution (jax.Array).",
      "success: Indicates if the optimization was successful (bool | jax.Array).",
      "status: Integer code representing the solver-specific return status (int | jax.Array).",
      "fun: Represents the final function value (jax.Array).",
      "jac: Represents the final Jacobian array (jax.Array).",
      "hess_inv: Represents the final inverse Hessian estimate (jax.Array | None).",
      "nfev: Number of function calls used (int | jax.Array).",
      "njev: Number of gradient evaluations (int | jax.Array).",
      "nit: Number of iterations of the optimization algorithm (int | jax.Array)."
    ],
    "code_examples": []
  },
  {
    "title": "Methods of the Object",
    "concepts": [
      "__init__(): Initialization method of the object.",
      "count(value, /): Returns the number of occurrences of a value.",
      "index(value[, start, stop]): Returns the first index of a value."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes of the Object",
    "concepts": [
      "fun: Alias for field number 3",
      "hess_inv: Alias for field number 5",
      "jac: Alias for field number 4",
      "nfev: Alias for field number 6",
      "nit: Alias for field number 8",
      "njev: Alias for field number 7",
      "status: Alias for field number 2",
      "success: Alias for field number 1",
      "x: Alias for field number 0"
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to fftconvolve",
    "concepts": [
      "fftconvolve is a JAX implementation of scipy.signal.fftconvolve().",
      "It convolves two N-dimensional arrays using Fast Fourier Transform (FFT).",
      "The inputs must have the same number of dimensions.",
      "The 'mode' parameter controls the size of the output.",
      "Available modes are 'full', 'same', and 'valid'."
    ],
    "code_examples": []
  },
  {
    "title": "Mode Parameter Explanation",
    "concepts": [
      "mode='full' outputs the full convolution of the inputs, using implicit zero-padding at the edges.",
      "mode='same' returns a centered portion of the 'full' output, the same size as the first input.",
      "mode='valid' returns the portion of the 'full' output where the two arrays fully overlap, not depending on padding."
    ],
    "code_examples": []
  },
  {
    "title": "1D Convolution Examples",
    "concepts": [
      "Demonstrates the use of fftconvolve with different 'mode' parameters.",
      "Uses jax.numpy.printoptions() to adjust printing precision.",
      "Illustrates the 'full', 'same', and 'valid' modes."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the 'full' mode convolution.",
        "code": "x = jnp.array([1, 2, 3, 2, 1])\ny = jnp.array([1, 1, 1])\nwith jax.numpy.printoptions(precision=3):\n  print(jax.scipy.signal.fftconvolve(x, y, mode='full'))"
      },
      {
        "description": "Demonstrates the 'same' mode convolution.",
        "code": "x = jnp.array([1, 2, 3, 2, 1])\ny = jnp.array([1, 1, 1])\nwith jax.numpy.printoptions(precision=3):\n  print(jax.scipy.signal.fftconvolve(x, y, mode='same'))"
      },
      {
        "description": "Demonstrates the 'valid' mode convolution.",
        "code": "x = jnp.array([1, 2, 3, 2, 1])\ny = jnp.array([1, 1, 1])\nwith jax.numpy.printoptions(precision=3):\n  print(jax.scipy.signal.fftconvolve(x, y, mode='valid'))"
      }
    ]
  },
  {
    "title": "Convolution Function Description",
    "concepts": [
      "The function performs N-dimensional convolution of two arrays.",
      "It's a JAX implementation of scipy.signal.convolve().",
      "The inputs, `in1` and `in2`, must have the same number of dimensions.",
      "The `mode` parameter controls the output size ('full', 'same', 'valid').",
      "The `method` parameter controls the computation method ('auto', 'direct', 'fft').",
      "`precision` specifies the computation precision.",
      "The function returns the convolved result as an array."
    ],
    "code_examples": []
  },
  {
    "title": "1D Convolution Examples",
    "concepts": [
      "Demonstrates 1D convolution using jax.scipy.signal.convolve.",
      "Shows the effect of different 'mode' parameters: 'full', 'same', and 'valid'."
    ],
    "code_examples": [
      {
        "description": "Defines two 1D arrays, x and y, for convolution.",
        "code": "x = jnp.array([1, 2, 3, 2, 1])\ny = jnp.array([1, 1, 1])"
      },
      {
        "description": "Performs full convolution with implicit zero-padding.",
        "code": "jax.scipy.signal.convolve(x, y, mode='full')\n# Output: Array([1., 3., 6., 7., 6., 3., 1.], dtype=float32)"
      },
      {
        "description": "Performs convolution with mode='same', returning a centered convolution with the same size as the first input.",
        "code": "jax.scipy.signal.convolve(x, y, mode='same')\n# Output: Array([3., 6., 7., 6., 3.], dtype=float32)"
      },
      {
        "description": "Performs convolution with mode='valid', returning only the portion where the two arrays fully overlap.",
        "code": "jax.scipy.signal.convolve(x, y, mode='valid')\n# Output: Array([6., 7., 6.], dtype=float32)"
      }
    ]
  },
  {
    "title": "Introduction to 2D Convolution",
    "concepts": [
      "The function implements 2D convolution using JAX.",
      "It is a JAX implementation of scipy.signal.convolve2d().",
      "The function takes two 2D arrays as input: in1 and in2.",
      "The function uses the jax.lax.conv_general_dilated() function.",
      "The function can compute the result via a fast Fourier transform (FFT)."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Convolution",
    "concepts": [
      "in1 is the left-hand input to the convolution, must have ndim == 2.",
      "in2 is the right-hand input to the convolution, must have ndim == 2.",
      "mode controls the size of the output ('full', 'same', 'valid').",
      "'full' outputs the full convolution (default).",
      "'same' returns a centered portion of the 'full' output with the same size as in1.",
      "'valid' returns the portion of the 'full' output without padding.",
      "boundary only supports 'fill'.",
      "fillvalue only supports 0.",
      "method controls the computation method ('auto', 'direct', 'fft').",
      "'auto' always uses the 'direct' method (default).",
      "'direct' lowers to jax.lax.conv_general_dilated().",
      "'fft' computes the result via FFT.",
      "precision specifies the computation precision."
    ],
    "code_examples": []
  },
  {
    "title": "Convolution Examples",
    "concepts": [
      "Demonstrates 2D convolution with different modes.",
      "Illustrates 'full' mode with implicit zero-padding.",
      "Illustrates 'same' mode returning the centered portion.",
      "Illustrates 'valid' mode returning only fully overlapped portions."
    ],
    "code_examples": [
      {
        "description": "Defining two input arrays for convolution.",
        "code": "x = jnp.array([[1, 2],\n               [3, 4]])\ny = jnp.array([[2, 1, 1],\n               [4, 3, 4],\n               [1, 3, 2]])"
      },
      {
        "description": "Full 2D convolution example.",
        "code": "jax.scipy.signal.convolve2d(x, y, mode='full')\n# Output:\n# Array([[ 2.,  5.,  3.,  2.],\n#        [10., 22., 17., 12.],\n#        [13., 30., 32., 20.],\n#        [ 3., 13., 18.,  8.]], dtype=float32)"
      },
      {
        "description": "Same mode 2D convolution example.",
        "code": "jax.scipy.signal.convolve2d(x, y, mode='same')\n# Output:\n# Array([[22., 17.],\n#        [30., 32.]], dtype=float32)"
      },
      {
        "description": "Valid mode 2D convolution example.",
        "code": "jax.scipy.signal.convolve2d(x, y, mode='valid')\n# Output:\n# Array([[22., 17.],\n#        [30., 32.]], dtype=float32)"
      }
    ]
  },
  {
    "title": "Overview",
    "concepts": [
      "The document describes a JAX implementation of the scipy.signal.correlate2d() function for cross-correlation of two 2-dimensional arrays.",
      "The function takes two arrays, in1 and in2, as input, both of which must have a dimension of 2.",
      "The 'mode' parameter controls the size of the output, with options 'full', 'same', and 'valid'.",
      "The 'boundary' parameter only supports 'fill'.",
      "The 'fillvalue' parameter only supports 0.",
      "The 'method' parameter controls the computation method, with options 'auto', 'direct', and 'fft'.",
      "The 'precision' parameter specifies the precision of the computation."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Examples",
    "concepts": [
      "Demonstrates how to use `jax.scipy.signal.correlate2d` with different modes.",
      "Shows the effect of 'full', 'same', and 'valid' modes on the output array.",
      "Illustrates the implicit zero-padding used in 'full' mode."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the 'full' mode in `correlate2d`.",
        "code": "x = jnp.array([[2, 1, 3],\n               [1, 3, 1],\n               [4, 1, 2]])\ny = jnp.array([[1, 3],\n               [4, 2]])\njax.scipy.signal.correlate2d(x, y, mode='full')\n"
      },
      {
        "description": "Example demonstrating the 'same' mode in `correlate2d`.",
        "code": "x = jnp.array([[2, 1, 3],\n               [1, 3, 1],\n               [4, 1, 2]])\ny = jnp.array([[1, 3],\n               [4, 2]])\njax.scipy.signal.correlate2d(x, y, mode='same')\n"
      },
      {
        "description": "Example demonstrating the 'valid' mode in `correlate2d`.",
        "code": "x = jnp.array([[2, 1, 3],\n               [1, 3, 1],\n               [4, 1, 2]])\ny = jnp.array([[1, 3],\n               [4, 2]])\njax.scipy.signal.correlate2d(x, y, mode='valid')\n"
      }
    ]
  },
  {
    "title": "Introduction to Detrending with JAX",
    "concepts": [
      "The document describes a JAX implementation of scipy.signal.detrend().",
      "The function removes linear or piecewise linear trends from data.",
      "The data argument is the input array.",
      "The axis argument specifies the axis along which to detrend.",
      "The type argument specifies the type of detrending ('linear' or 'constant').",
      "The bp argument specifies breakpoints for piecewise linear detrending.",
      "The overwrite_data argument is not supported."
    ],
    "code_examples": []
  },
  {
    "title": "Linear Detrending Example",
    "concepts": [
      "Demonstrates removing a linear trend from a 1D array using jax.scipy.signal.detrend().",
      "The example shows the detrended data and the underlying linear trend that was removed."
    ],
    "code_examples": [
      {
        "description": "Define an example array.",
        "code": "data = jnp.array([1., 4., 8., 8., 9.])"
      },
      {
        "description": "Remove a linear trend from the data and print the results.",
        "code": "detrended = jax.scipy.signal.detrend(data)\nwith jnp.printoptions(precision=3, suppress=True):\n    # suppress float error\n    print(\"Detrended:\", detrended)\n    print(\"Underlying trend:\", data - detrended)"
      },
      {
        "description": "Remove a linear trend from the data and print the results.",
        "code": "detrended = jax.scipy.signal.detrend(data)\nwith jnp.printoptions(precision=3, suppress=True):\n    # suppress float error\n    print(\"Detrended:\", detrended)\n    print(\"Underlying trend:\", data - detrended)"
      }
    ]
  },
  {
    "title": "Constant Detrending Example",
    "concepts": [
      "Demonstrates removing a constant trend (mean) from a 1D array.",
      "The example shows the detrended data and the constant value that was removed."
    ],
    "code_examples": [
      {
        "description": "Remove a constant trend from the data and print the results.",
        "code": "detrended = jax.scipy.signal.detrend(data, type='constant')\nwith jnp.printoptions(precision=3):\n    # suppress float error\n    print(\"Detrended:\", detrended)\n    print(\"Underlying trend:\", data - detrended)"
      },
      {
        "description": "Remove a constant trend from the data and print the results.",
        "code": "detrended = jax.scipy.signal.detrend(data, type='constant')\nwith jnp.printoptions(precision=3):\n    # suppress float error\n    print(\"Detrended:\", detrended)\n    print(\"Underlying trend:\", data - detrended)"
      }
    ]
  },
  {
    "title": "Inverse Short-Time Fourier Transform (ISTFT) Overview",
    "concepts": [
      "The function performs the inverse short-time Fourier transform (ISTFT).",
      "It is a JAX implementation of scipy.signal.istft().",
      "It computes the inverse of jax.scipy.signal.stft().",
      "The function returns a tuple containing the time array and the reconstructed time series.",
      "It takes the STFT of the signal (Zxx), sampling frequency (fs), window type, number of points per segment (nperseg), overlap (noverlap), number of FFT points (nfft), a boolean to indicate if the input is one-sided, a boundary boolean, and the time/frequency axis as input parameters."
    ],
    "code_examples": []
  },
  {
    "title": "ISTFT Example",
    "concepts": [
      "Demonstrates the inverse relationship between STFT and ISTFT.",
      "Shows how to reconstruct a signal using ISTFT after applying STFT."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of jax.scipy.signal.stft and jax.scipy.signal.istft to reconstruct a signal.",
        "code": "x = jnp.array([1., 2., 3., 2., 1., 0., 1., 2.])\nf, t, Zxx = jax.scipy.signal.stft(x, nperseg=4)\nprint(Zxx)\nt, x_reconstructed = jax.scipy.signal.istft(Zxx)\nprint(x_reconstructed)"
      },
      {
        "description": "Demonstrates the usage of jax.scipy.signal.stft and jax.scipy.signal.istft to reconstruct a signal.",
        "code": "x = jnp.array([1., 2., 3., 2., 1., 0., 1., 2.])\nf, t, Zxx = jax.scipy.signal.stft(x, nperseg=4)\nprint(Zxx)\nt, x_reconstructed = jax.scipy.signal.istft(Zxx)\nprint(x_reconstructed)"
      }
    ]
  },
  {
    "title": "Short-Time Fourier Transform (STFT) Overview",
    "concepts": [
      "STFT is computed using JAX, mirroring scipy.signal.stft().",
      "Input signal is a time series represented by an array.",
      "Sampling frequency is a key parameter.",
      "A data tapering window is applied to each segment.",
      "Segment length and overlap are configurable.",
      "FFT length can be specified for zero-padding.",
      "Segment detrending can be applied.",
      "Output can be one-sided or two-sided.",
      "Signal boundary extension can be specified.",
      "Zero-padding at the end of the signal is an option.",
      "The axis along which the STFT is computed can be selected.",
      "Returns a tuple of frequency array, time array, and the STFT of the input."
    ],
    "code_examples": []
  },
  {
    "title": "STFT Parameters",
    "concepts": [
      "x: Input time series array.",
      "fs: Sampling frequency (default is 1.0).",
      "window: Data tapering window (default is 'hann').",
      "nperseg: Length of each segment (default is 256).",
      "noverlap: Number of overlapping points between segments (default is nperseg // 2).",
      "nfft: Length of the FFT. If None, the FFT length is nperseg.",
      "detrend: Specifies how to detrend each segment (default is False).",
      "return_onesided: If True, return a one-sided spectrum for real inputs (default is True).",
      "boundary: Specifies whether the input signal is extended at both ends (default is 'zeros').",
      "padded: Specifies whether the input signal is zero-padded at the end (default is True).",
      "axis: Axis along which the STFT is computed (default is -1)."
    ],
    "code_examples": []
  },
  {
    "title": "STFT Return Value",
    "concepts": [
      "Returns a tuple containing the frequency array (f), the segment times array (t), and the STFT array (Zxx)."
    ],
    "code_examples": []
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.scipy.signal.istft(): Inverse short-time Fourier transform."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to 3D Rotation with JAX",
    "concepts": [
      "The document describes a JAX implementation of scipy.spatial.transform.Rotation.",
      "It provides examples of how to construct and manipulate Rotation objects in 3D space.",
      "The implementation focuses on using JAX arrays for representing rotations."
    ],
    "code_examples": []
  },
  {
    "title": "Constructing Rotation Objects",
    "concepts": [
      "Rotation objects can be created from Euler angles using `Rotation.from_euler()`.",
      "The 'z' parameter specifies the rotation axis.",
      "The `degrees` parameter indicates whether the angles are in degrees or radians."
    ],
    "code_examples": [
      {
        "description": "Creates a rotation object representing a 90-degree rotation about the z-axis.",
        "code": "from jax.scipy.spatial.transform import Rotation\n\nr = Rotation.from_euler('z', 90, degrees=True)"
      },
      {
        "description": "Creates a rotation object representing a 90-degree rotation about the z-axis (repeated example).",
        "code": "from jax.scipy.spatial.transform import Rotation\n\nr = Rotation.from_euler('z', 90, degrees=True)"
      }
    ]
  },
  {
    "title": "Converting Rotation Representations",
    "concepts": [
      "Rotation objects can be converted to rotation vectors using `as_rotvec()`.",
      "Rotation objects can be converted to rotation matrices using `as_matrix()`."
    ],
    "code_examples": [
      {
        "description": "Converts the rotation object to a rotation vector.",
        "code": "r.as_rotvec()"
      },
      {
        "description": "Converts the rotation object to a rotation vector (repeated example).",
        "code": "r.as_rotvec()"
      },
      {
        "description": "Converts the rotation object to a rotation matrix.",
        "code": "r.as_matrix()"
      },
      {
        "description": "Converts the rotation object to a rotation matrix (repeated example).",
        "code": "r.as_matrix()"
      }
    ]
  },
  {
    "title": "Composing Rotations",
    "concepts": [
      "Rotations can be composed using the `*` operator.",
      "The order of rotations matters in composition.",
      "The composed rotation can then be converted to other representations like a rotation matrix."
    ],
    "code_examples": [
      {
        "description": "Composes two rotations: a 90-degree rotation about the z-axis (r) and a 90-degree rotation about the x-axis (r2).",
        "code": "r2 = Rotation.from_euler('x', 90, degrees=True)\nr3 = r * r2\n\nr3.as_matrix()"
      },
      {
        "description": "Composes two rotations: a 90-degree rotation about the z-axis (r) and a 90-degree rotation about the x-axis (r2) (repeated example).",
        "code": "r2 = Rotation.from_euler('x', 90, degrees=True)\nr3 = r * r2\n\nr3.as_matrix()"
      }
    ]
  },
  {
    "title": "Rotation Methods and Attributes",
    "concepts": [
      "The document lists available methods for the Rotation class.",
      "These include methods for initialization, conversion, application, inversion, and more.",
      "The document also lists available attributes, such as 'quat' and 'single'."
    ],
    "code_examples": []
  },
  {
    "title": "Bi-Conjugate Gradient Stable (BiCGStab) Solver in JAX",
    "concepts": [
      "BiCGStab is used to solve linear systems of equations Ax = b.",
      "JAX's BiCGStab implementation aims for numerical consistency with SciPy's but has a slightly different interface.",
      "The linear operator A is supplied as a function that computes the matrix-vector product A(x) or A @ x.",
      "Derivatives are computed via implicit differentiation.",
      "Accuracy of derivatives depends on the convergence of both the forward and backward solves."
    ],
    "code_examples": []
  },
  {
    "title": "Arguments for the BiCGStab Solver",
    "concepts": [
      "A represents the linear operator (matrix) as a 2D array, function, or matmul-compatible object.",
      "b is the right-hand side vector of the linear system, as an array or tree of arrays.",
      "x0 is the initial guess for the solution, with the same structure as b.",
      "tol is the tolerance for convergence, based on the norm of the residual.",
      "atol is the absolute tolerance for convergence.",
      "maxiter is the maximum number of iterations allowed.",
      "M is the preconditioner for A, approximating the inverse of A for faster convergence."
    ],
    "code_examples": []
  },
  {
    "title": "Returns of the BiCGStab Solver",
    "concepts": [
      "x is the converged solution, with the same structure as b.",
      "info is a placeholder for convergence information (number of iterations in the future)."
    ],
    "code_examples": []
  },
  {
    "title": "See Also",
    "concepts": [
      "scipy.sparse.linalg.bicgstab is the SciPy equivalent.",
      "jax.lax.custom_linear_solve is a related JAX function."
    ],
    "code_examples": []
  },
  {
    "title": "Conjugate Gradient Method Overview",
    "concepts": [
      "The Conjugate Gradient (CG) method solves linear systems of the form Ax = b.",
      "JAX's cg implementation aims to match SciPy's cg numerically.",
      "The interface requires the linear operator A to be supplied as a function.",
      "Derivatives are computed using implicit differentiation with another CG solve.",
      "Accuracy of derivatives depends on convergence of both CG solves."
    ],
    "code_examples": []
  },
  {
    "title": "Arguments of the CG Function",
    "concepts": [
      "A represents the linear operator as a 2D array or function.",
      "A must be hermitian and positive definite.",
      "b is the right-hand side vector.",
      "x0 is the initial guess for the solution.",
      "tol is the relative tolerance for convergence.",
      "atol is the absolute tolerance for convergence.",
      "maxiter is the maximum number of iterations.",
      "M is a preconditioner for A."
    ],
    "code_examples": []
  },
  {
    "title": "Return Values of the CG Function",
    "concepts": [
      "x is the converged solution.",
      "info is a placeholder for convergence information (number of iterations)."
    ],
    "code_examples": []
  },
  {
    "title": "See Also",
    "concepts": [
      "References to scipy.sparse.linalg.cg and jax.lax.custom_linear_solve."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to GMRES",
    "concepts": [
      "GMRES solves the linear system A x = b for x.",
      "A can be a function performing A(vi) -> vf = A @ vi.",
      "Convergence can be slow for nearly symmetric operators."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters: A and b",
    "concepts": [
      "A is a 2D array, function, or matmul-compatible object representing the linear map A(x) or A @ x.",
      "A must return array(s) with the same structure and shape as its argument.",
      "b is the right-hand side of the linear system.",
      "b can be an array or Python container of array(s) with any shape."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters: x0, tol, and atol",
    "concepts": [
      "x0 is the starting guess for the solution and must have the same structure as b.",
      "If x0 is unspecified, zeroes are used.",
      "tol is the tolerance for convergence.",
      "atol is the absolute tolerance for convergence.",
      "JAX's tolerance will differ from SciPy unless you explicitly pass atol to SciPy's gmres."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters: restart and maxiter",
    "concepts": [
      "restart is the size of the Krylov subspace built between restarts.",
      "It bounds the maximum accuracy achievable from any guess solution.",
      "Larger values increase iteration count and cost but may be necessary for convergence.",
      "maxiter is the maximum number of times to rebuild the size-restart Krylov space.",
      "Decreasing maxiter may help if GMRES halts or is very slow."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters: M and solve_method",
    "concepts": [
      "M is the preconditioner for A, approximating the inverse of A.",
      "Effective preconditioning dramatically improves convergence.",
      "solve_method can be 'incremental' or 'batched'.",
      "The 'incremental' solve method builds a QR decomposition incrementally using Givens rotations, improving stability and giving a residual norm estimate for early termination.",
      "The 'batched' solve method solves the least squares problem from scratch, without early termination, but with less overhead on GPUs."
    ],
    "code_examples": []
  },
  {
    "title": "Returns: x and info",
    "concepts": [
      "x is the converged solution and has the same structure as b.",
      "info is a placeholder for convergence information.",
      "JAX will report the number of iterations when convergence is not achieved, like SciPy, in the future."
    ],
    "code_examples": []
  },
  {
    "title": "See Also",
    "concepts": [
      "Related functions include scipy.sparse.linalg.gmres and jax.lax.custom_linear_solve."
    ],
    "code_examples": []
  },
  {
    "title": "Bernoulli Number Generation",
    "concepts": [
      "Generate the first N Bernoulli numbers.",
      "JAX implementation of scipy.special.bernoulli().",
      "The function takes an integer 'n' as input, representing the number of Bernoulli terms to generate.",
      "The function returns an array containing the first n Bernoulli numbers.",
      "Bernoulli numbers are generated using the \\(B_n^-\\) convention, where \\(B_1=-1/2\\)."
    ],
    "code_examples": []
  },
  {
    "title": "Beta Function Definition",
    "concepts": [
      "The beta function is defined as the ratio of gamma functions.",
      "The formula is beta(a, b) = Gamma(a) * Gamma(b) / Gamma(a + b).",
      "The beta function relates to the gamma function.",
      "Input 'a' is a real-valued array-like parameter.",
      "Input 'b' is a real-valued array-like parameter.",
      "The function returns an array containing the beta function values."
    ],
    "code_examples": []
  },
  {
    "title": "Description of the Regularized Incomplete Beta Function",
    "concepts": [
      "The document describes the regularized incomplete beta function.",
      "It provides a JAX implementation of scipy.special.betainc.",
      "The function is defined as an integral involving parameters a and b, and integration limit x.",
      "B(a, b) is the beta function.",
      "Parameters a, b, and x are array-like and real-valued.",
      "The function returns an array containing values of the betainc function.",
      "Related functions are jax.scipy.special.beta() and jax.scipy.special.betaln()."
    ],
    "code_examples": []
  },
  {
    "title": "Digamma Function Definition",
    "concepts": [
      "The digamma function is defined as the derivative of the logarithm of the gamma function.",
      "digamma(z) = \u03c8(z) = d/dz log \u0393(z)",
      "\u0393(z) is the gamma function."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Implementation and Usage",
    "concepts": [
      "JAX provides an implementation of the digamma function.",
      "The input 'x' is an array-like, real-valued number.",
      "The output is an array containing the values of the digamma function.",
      "The JAX version of digamma accepts real-valued inputs.",
      "Related functions include gamma() and polygamma()."
    ],
    "code_examples": []
  },
  {
    "title": "Entropy Function Definition",
    "concepts": [
      "The entropy function is defined piecewise.",
      "For x > 0, entropy(x) = -x * log(x).",
      "For x = 0, entropy(x) = 0.",
      "Otherwise, entropy(x) = -infinity."
    ],
    "code_examples": []
  },
  {
    "title": "Error Function Overview",
    "concepts": [
      "JAX implementation of the error function (erf).",
      "The error function is defined as an integral.",
      "Input x should be array-like and real-valued.",
      "The function returns an array containing values of the error function.",
      "JAX version only supports real-valued inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Error Function Complement (erfc)",
    "concepts": [
      "erfc(x) is the complement of the error function erf(x).",
      "erfc(x) is defined as (2/sqrt(pi)) * integral from x to infinity of exp(-t^2) dt.",
      "The JAX implementation of erfc supports real-valued inputs.",
      "erfc(x) = 1 - erf(x)."
    ],
    "code_examples": []
  },
  {
    "title": "Inverse Error Function in JAX",
    "concepts": [
      "Implementation of the inverse error function using JAX.",
      "Returns the inverse of the error function erf().",
      "Input x is an array-like, real-valued data structure.",
      "The function returns an array containing values of the inverse error function.",
      "The JAX version only supports real-valued inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Integral Function exp1",
    "concepts": [
      "Definition of the exponential integral function exp1.",
      "exp1(x) is equivalent to E_1(x).",
      "exp1(x) is defined by the integral of e^{-t}/t from x to infinity.",
      "Input x is a real-valued array-like object.",
      "The function returns an array of exp1 values."
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Integral Function (expi)",
    "concepts": [
      "Definition of the exponential integral function expi(x).",
      "expi(x) is the integral of e^t/t from negative infinity to x.",
      "The input x should be a real-valued array-like object.",
      "The output is an array of expi values.",
      "The implementation is based on JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Log Normal Distribution Function",
    "concepts": [
      "JAX implementation of scipy.special.log_ndtr.",
      "Calculates log(ndtr(x)).",
      "Uses different methods based on the value of x: asymptotic series for x <= lower_segment, existing ndtr technique and take a log for lower_segment < x <= upper_segment, and approximation -ndtr(-x) for x > upper_segment.",
      "Defines lower_segment and upper_segment based on the dtype of the input x (float32 or float64).",
      "Uses an asymptotic series approximation for ndtr(x) when x < lower_segment."
    ],
    "code_examples": []
  },
  {
    "title": "Asymptotic Series Approximation",
    "concepts": [
      "When x < lower_segment, the ndtr asymptotic series approximation is used.",
      "The approximation involves calculating a scale factor and a sum of terms.",
      "The scale factor is e^(-0.5 * x^2) / (-x * sqrt(2 * pi)).",
      "The sum is a series of terms involving the double-factorial operator.",
      "(2n-1)!! = (2n-1) * (2n-3) * (2n-5) ... (3) * (1)."
    ],
    "code_examples": []
  },
  {
    "title": "Arguments and Return Value",
    "concepts": [
      "x: ArrayLike, an array of type float32 or float64.",
      "series_order: int, a positive Python integer representing the maximum depth to evaluate the asymptotic expansion.",
      "Returns an array with dtype=x.dtype.",
      "TypeError is raised if x.dtype is not handled or if series_order is not a Python integer.",
      "ValueError is raised if series_order is not in [0, 30]."
    ],
    "code_examples": []
  },
  {
    "title": "Normal Distribution Function Overview",
    "concepts": [
      "JAX implementation of scipy.special.ndtr",
      "Calculates the area under the Gaussian probability density function from minus infinity to x",
      "The function is equivalent to 1/2 * (1 + erf(x/sqrt(2)))",
      "The function is equivalent to 1/2 * erfc(x/sqrt(2))",
      "Input x is an array of type float32 or float64",
      "Output is an array with the same dtype as x",
      "Raises TypeError if x is not a floating-type"
    ],
    "code_examples": []
  },
  {
    "title": "xlog1py Function",
    "concepts": [
      "Computes x*log(1 + y), returning 0 for x=0.",
      "JAX implementation of scipy.special.xlog1py.",
      "Returns 0 when (x, y) = (0, -1).",
      "Includes a custom derivative rule for automatic differentiation."
    ],
    "code_examples": []
  },
  {
    "title": "Bernoulli Cumulative Distribution Function",
    "concepts": [
      "The Bernoulli cumulative distribution function (CDF) is defined as the sum of the probability mass function (PMF) from 0 to k.",
      "jax.scipy.stats.bernoulli.cdf is a JAX implementation of the Bernoulli CDF.",
      "k is the value at which to evaluate the CDF.",
      "p is the distribution shape parameter.",
      "The function returns an array of CDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Binomial Probability Mass Function",
    "concepts": [
      "Definition of the binomial probability mass function.",
      "Formula: f(k, n, p) = {n choose k} * p^k * (1-p)^(n-k)",
      "k: value at which to evaluate the PMF.",
      "n: distribution shape parameter.",
      "p: distribution shape parameter.",
      "loc: distribution offset parameter.",
      "Returns an array of PMF values."
    ],
    "code_examples": []
  },
  {
    "title": "Chi-square Survival Function",
    "concepts": [
      "The survival function (SF) is defined as 1 - CDF.",
      "JAX implements the chi-square survival function.",
      "The function takes x, df, loc, and scale as input parameters.",
      "df represents degrees of freedom.",
      "The function returns an array of SF values."
    ],
    "code_examples": []
  },
  {
    "title": "Gamma Log Cumulative Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.gamma logcdf.",
      "The cdf is defined as the integral of the pdf from negative infinity to x.",
      "x is the value at which to evaluate the CDF.",
      "a is the distribution shape parameter.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "The function returns an array of logcdf values.",
      "Related functions: jax.scipy.stats.gamma.cdf(), jax.scipy.stats.gamma.pdf(), jax.scipy.stats.gamma.sf(), jax.scipy.stats.gamma.logpdf(), jax.scipy.stats.gamma.logsf()"
    ],
    "code_examples": []
  },
  {
    "title": "Multinomial Log Probability Mass Function",
    "concepts": [
      "This section describes the multinomial log probability mass function.",
      "It provides a JAX implementation of the scipy.stats.multinomial logpdf.",
      "The multinomial probability distribution is defined as  f(x, n, p) = n! * product(p_i^{x_i} / x_i!) where n is the sum of x_i.",
      "x is the value at which to evaluate the PMF.",
      "n is the distribution shape parameter.",
      "p is the distribution shape parameter.",
      "The function returns an array of logpmf values."
    ],
    "code_examples": []
  },
  {
    "title": "Normal Log Probability Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.norm logpdf.",
      "The normal distribution pdf is defined as f(x) = (1 / sqrt(2*pi)) * e^(-x^2 / 2).",
      "The function calculates the log of the probability density function (PDF) of a normal distribution.",
      "It takes x, loc (mean), and scale (standard deviation) as input parameters.",
      "It returns an array of logpdf values.",
      "Related functions include cdf, pdf, sf, logcdf, logsf, isf, and ppf."
    ],
    "code_examples": []
  },
  {
    "title": "Uniform Log Probability Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.uniform logpdf.",
      "The uniform distribution pdf is 1 when 0 <= x <= 1, and 0 otherwise.",
      "x is the value at which to evaluate the PDF.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "The function returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Two-Term Arc Tangent",
    "concepts": [
      "Calculates the elementwise arc tangent of x/y.",
      "Lowers directly to the stablehlo.atan2 operation.",
      "Input arrays x and y must have matching floating-point or complex dtypes.",
      "If x and y are not scalars, they must have the same number of dimensions and be broadcast-compatible.",
      "Returns an array of the same shape and dtype as x and y.",
      "The returned array contains the element-wise arc tangent of x/y, respecting the quadrant indicated by the sign of each input."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise AND",
    "concepts": [
      "Computes the elementwise AND of two arrays.",
      "Lowers directly to the stablehlo.and operation.",
      "Input arrays must have matching boolean or integer dtypes.",
      "If neither array is a scalar, they must have the same number of dimensions and be broadcast compatible.",
      "Returns an array containing the bitwise AND of each pair of broadcasted entries."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Cube Root",
    "concepts": [
      "Calculates the elementwise cube root of an array.",
      "Lowers directly to the stablehlo.cbrt operation.",
      "Input array must have floating or complex dtype.",
      "Returns an array of the same shape and dtype as input."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Complex Conjugate",
    "concepts": [
      "Calculates the elementwise complex conjugate of an array.",
      "The input array must have a complex dtype.",
      "The output array has the same shape and dtype as the input array.",
      "The function lowers to stablehlo.real, stablehlo.imag, and stablehlo.complex.",
      "Related functions include jax.lax.complex(), jax.lax.real(), jax.lax.imag(), and jax.lax.abs()."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Exponential Function",
    "concepts": [
      "The function computes the elementwise exponential of an input array (e^x).",
      "It directly lowers to the stablehlo.exponential operation.",
      "The input array must have a floating-point or complex type.",
      "The output is an array with the same shape and dtype as the input array, containing the elementwise exponential values."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Base-2 Exponential",
    "concepts": [
      "Computes the elementwise base-2 exponential (2^x) of an array.",
      "Implemented using stablehlo.exponential and stablehlo.multiply operations.",
      "Input array must have a floating-point or complex type.",
      "Output is an array with the same shape and dtype as the input array."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Exponential Minus One",
    "concepts": [
      "The function computes the elementwise exponential minus one (e^x - 1).",
      "It lowers directly to the stablehlo.exponential_minus_one operation.",
      "It is more accurate than lax.exp(x) - 1 for x near zero.",
      "The input array must have floating-point or complex type.",
      "The output is an array of the same shape and dtype as the input, containing the element-wise exponential minus 1."
    ],
    "code_examples": []
  },
  {
    "title": "Array Creation with Specified Value",
    "concepts": [
      "Creates a full array similar to np.full using an example array for shape and dtype.",
      "Uses an example array (x) for shape and dtype information.",
      "Fills the array with a specified scalar value (fill_value).",
      "Accepts optional dtype, shape and sharding parameters.",
      "Sharding defaults to the input array's sharding, with limitations during tracing and for weakly typed/uncommitted arrays.",
      "Returns an ndarray with the same shape as x, filled with fill_value."
    ],
    "code_examples": []
  },
  {
    "title": "XLA Reduce Operator",
    "concepts": [
      "Wraps XLA's Reduce operator.",
      "init_values and computation must form a monoid for correctness.",
      "init_values must be an identity of computation.",
      "computation must be associative.",
      "XLA may exploit monoid properties during code generation.",
      "Violating monoid properties leads to undefined results."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Tangent Function",
    "concepts": [
      "Calculates the element-wise tangent of an input array.",
      "Lowers directly to the stablehlo.tangent operation.",
      "Input array must be floating-point or complex.",
      "Output array has the same shape and dtype as the input.",
      "Related functions include cosine, sine, arctangent, and 2-term arctangent."
    ],
    "code_examples": []
  },
  {
    "title": "Conditional Application of Functions",
    "concepts": [
      "The cond function conditionally applies either true_fun or false_fun based on the value of pred.",
      "It is equivalent to XLA's Conditional operator.",
      "It mimics a Python if/else statement where pred is a scalar.",
      "Only one branch is executed, but both branches are traced.",
      "When transformed with vmap(), cond is converted to select()."
    ],
    "code_examples": [
      {
        "description": "Python implementation of the cond function's semantics.",
        "code": "def cond(pred, true_fun, false_fun, *operands):\n    if pred:\n        return true_fun(*operands)\n    else:\n        return false_fun(*operands)"
      },
      {
        "description": "Redundant definition of the cond function's semantics.",
        "code": "def cond(pred, true_fun, false_fun, *operands):\n    if pred:\n        return true_fun(*operands)\n    else:\n        return false_fun(*operands)"
      }
    ]
  },
  {
    "title": "Select Operation Overview",
    "concepts": [
      "Select operation chooses between two branches based on a boolean predicate.",
      "It wraps XLA's Select operator.",
      "Both branches are generally evaluated, but the compiler may elide computations if possible.",
      "Use cond() for a similar function that usually evaluates only a single branch."
    ],
    "code_examples": []
  },
  {
    "title": "Select Parameters",
    "concepts": [
      "pred: A boolean array that determines which branch to select.",
      "on_true: Array containing entries to return where pred is True. Must have the same shape as pred, and the same shape and dtype as on_false.",
      "on_false: Array containing entries to return where pred is False. Must have the same shape as pred, and the same shape and dtype as on_true.",
      "The function returns an array with the same shape and dtype as on_true and on_false."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to psum_scatter",
    "concepts": [
      "psum_scatter is similar to psum, but each device retains only part of the result.",
      "psum_scatter is more efficient than psum when only part of the result is needed on each device.",
      "psum_scatter can be considered the first half of a psum operation, often followed by an all_gather.",
      "The result of psum is scattered along the mapped axis.",
      "x is an array with a mapped axis named axis_name.",
      "axis_name is a hashable Python object used to name a mapped axis.",
      "scatter_dimension is a positional axis where the all-reduce result will be scattered.",
      "axis_index_groups is an optional list of lists of integers containing axis indices used for reduce-scatter operations over specific groups of indices.",
      "The parameter 'tiled' represents whether to use rank-preserving \u2018tiled\u2019 behavior.",
      "When tiled=False, the size of dimension in scatter_dimension must match the size of axis axis_name. The output is squeezed by removing scatter_dimension so the result has lower rank than the input.",
      "When tiled=True, the size of dimension in scatter_dimension must be divisible by the size of axis axis_name, and the scatter_dimension axis is preserved, so the result has the same rank as the input.",
      "The output array has a similar shape as the input array x.",
      "When tiled=True, the size of the dimension in position scatter_dimension is divided by the size of axis axis_name.",
      "When tiled=False, the dimension in position scatter_dimension is eliminated."
    ],
    "code_examples": []
  },
  {
    "title": "psum_scatter Examples",
    "concepts": [
      "Examples demonstrating the usage of psum_scatter with and without tiling.",
      "Examples demonstrating the usage of axis_index_groups."
    ],
    "code_examples": [
      {
        "description": "Example of psum_scatter without tiling.",
        "code": "x = np.arange(16).reshape(4, 4)\nprint(x)\n\ny = jax.pmap(lambda x: jax.lax.psum_scatter(x, 'i'), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "Example of psum_scatter with tiling.",
        "code": "y = jax.pmap(lambda x: jax.lax.psum_scatter(x, 'i', tiled=True), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "Example of psum_scatter with axis_index_groups and tiling.",
        "code": "def f(x):\n    return jax.lax.psum_scatter(\n        x, 'i', axis_index_groups=[[0, 2], [3, 1]], tiled=True\n    )\ny = jax.pmap(f, axis_name='i')(x)\nprint(y)"
      }
    ]
  },
  {
    "title": "All-Reduce Mean Computation with pmean",
    "concepts": [
      "Computes an all-reduce mean on an array over a pmapped axis.",
      "The input can be a pytree.",
      "axis_name specifies the pmapped axis.",
      "axis_index_groups allows for averaging over subgroups of replicas.",
      "Returns an array with the same shape as the input, representing the all-reduce mean."
    ],
    "code_examples": [
      {
        "description": "Computes the pmean of an array across a pmapped axis named 'i'.",
        "code": "x = np.arange(4)\ny = jax.pmap(lambda x: jax.lax.pmean(x, 'i'), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "Divides an array by its pmean across the pmapped axis 'i'.",
        "code": "x = np.arange(4)\ny = jax.pmap(lambda x: x / jax.lax.pmean(x, 'i'), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "Computes the pmean of an array across a pmapped axis named 'i'.",
        "code": "x = np.arange(4)\ny = jax.pmap(lambda x: jax.lax.pmean(x, 'i'), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "Divides an array by its pmean across the pmapped axis 'i'.",
        "code": "x = np.arange(4)\ny = jax.pmap(lambda x: x / jax.lax.pmean(x, 'i'), axis_name='i')(x)\nprint(y)"
      }
    ]
  },
  {
    "title": "Hessenberg Reduction",
    "concepts": [
      "Reduces a square matrix to upper Hessenberg form.",
      "Implemented on CPU only.",
      "Input is a floating point or complex square matrix or batch of matrices.",
      "Output is a pair containing the upper Hessenberg matrix and Householder reflectors."
    ],
    "code_examples": []
  },
  {
    "title": "Pivot Conversion to Permutation",
    "concepts": [
      "Converts pivots (row swaps) from LU decomposition to a permutation.",
      "Builds a permutation instead of directly applying pivots to rows for differentiability.",
      "Input: pivots - an int32 array of row swaps.",
      "Input: permutation_size - the size of the output permutation, must be >= k.",
      "Output: an int32 array representing the permutation."
    ],
    "code_examples": []
  },
  {
    "title": "QR-based dynamically weighted Halley iteration for polar decomposition Parameters",
    "concepts": [
      "x is a full-rank matrix with shape M x N.",
      "The matrix x may be padded up to that size from a smaller true shape (dynamic_shape).",
      "is_hermitian is a boolean indicating if x is Hermitian (currently unused).",
      "eps is a float that defines the convergence criterion.",
      "max_iterations is an integer defining the maximum number of iterations.",
      "dynamic_shape is a tuple representing the unpadded shape of the matrix x."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value of QR-based Halley Iteration",
    "concepts": [
      "The function returns a four-tuple: (u, h, num_iters, is_converged).",
      "u is the unitary/orthogonal matrix from the polar decomposition.",
      "h is the Hermitian/symmetric positive-definite matrix from the polar decomposition.",
      "num_iters is the number of iterations required to compute u.",
      "is_converged indicates whether the convergence criterion was met within the maximum number of iterations."
    ],
    "code_examples": []
  },
  {
    "title": "Symmetric/Hermitian Matrix Reduction to Tridiagonal Form",
    "concepts": [
      "Reduces a symmetric/Hermitian matrix to tridiagonal form.",
      "Implemented on CPU and GPU.",
      "Accepts a floating-point or complex matrix or batch of matrices as input.",
      "The 'lower' parameter specifies which triangle of the input matrix to use.",
      "Returns a tuple containing the tridiagonal representation, Householder reflectors, diagonal, first sub/superdiagonal, and scalar factors of the Householder reflectors."
    ],
    "code_examples": []
  },
  {
    "title": "Categorical Distribution Sampling",
    "concepts": [
      "Sampling random values from categorical distributions.",
      "Using a PRNG key for random number generation.",
      "Specifying unnormalized log probabilities (logits) as input.",
      "Defining the axis along which logits belong to the same categorical distribution.",
      "Optionally specifying the output shape, which must be broadcast-compatible.",
      "The default output shape is derived from the logits shape by deleting the specified axis.",
      "The output is an array with integer dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Description of Truncated Standard Normal Random Values",
    "concepts": [
      "Generates sample truncated standard normal random values.",
      "Values are returned according to the probability density function f(x) \u221d e^{-x^2/2}.",
      "The domain is lower < x < upper.",
      "key is a PRNG key used as the random key.",
      "lower is a float or array of floats representing the lower bound for truncation.",
      "upper is a float or array of floats representing the upper bound for truncation.",
      "shape is an optional tuple of nonnegative integers specifying the result shape.",
      "dtype is an optional float dtype for the returned values (default float64 if jax_enable_x64 is true, otherwise float32).",
      "Returns values in the open interval (lower, upper)."
    ],
    "code_examples": []
  },
  {
    "title": "Wald Random Value Generation",
    "concepts": [
      "Generates Wald-distributed random values with a given shape and float dtype.",
      "The Wald distribution is defined by a probability density function.",
      "The distribution is defined on the domain of all real numbers.",
      "The distribution has a location parameter \u03bc > 0.",
      "The function takes a PRNG key, mean, shape, and dtype as input.",
      "The shape parameter determines the shape of the output array.",
      "The dtype parameter determines the data type of the output array."
    ],
    "code_examples": []
  },
  {
    "title": "FFI Target Call",
    "concepts": [
      "Calls a foreign function interface (FFI) target.",
      "The behavior of ffi_call under vmap() depends on the value of vmap_method.",
      "vmap_method's default will be to raise a NotImplementedError unless explicitly specified in the future.",
      "target_name specifies the name of the registered XLA FFI custom call target.",
      "result_shape_dtypes specifies the shape and dtype of the custom call output(s).",
      "has_side_effect specifies whether the custom call has side effects.",
      "input_layouts specifies the layouts for each input argument, including None for default row-major order, DeviceLocalLayout, or a sequence of integers for major-to-minor axis ordering.",
      "output_layouts specifies the required layouts for the output arrays.",
      "input_output_aliases specifies which output arrays alias specific input arrays.",
      "custom_call_api_version specifies the version number of the custom call API implemented by the FFI target.",
      "legacy_backend_config is used for legacy targets implemented using custom_call_api_version<4.",
      "The function can be called with the input arrays as positional arguments."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of JAX Device Memory Profiling",
    "concepts": [
      "Captures a JAX device memory profile as a pprof-format protocol buffer.",
      "The profile is a snapshot of memory state, describing JAX Arrays and executables.",
      "It includes allocation sites for objects in memory.",
      "Profiling is done by instrumenting on-device allocations and capturing Python stack traces.",
      "The instrumentation is always enabled.",
      "device_memory_profile() provides an API to capture the profile.",
      "Output is a binary protocol buffer that can be interpreted by pprof.",
      "The `backend` parameter specifies the JAX backend to profile."
    ],
    "code_examples": []
  },
  {
    "title": "Gradient Checking with Finite Differences",
    "concepts": [
      "Gradients from automatic differentiation are checked against finite differences.",
      "A single randomly chosen direction is used for checking gradients to optimize the finite difference calculation cost.",
      "The function `f` is checked at `f(*args)`.",
      "Arguments are provided as a tuple `args`.",
      "Gradients up to a specified `order` are checked.",
      "Gradient modes such as 'fwd' (forward) and/or 'rev' (reverse) can be selected.",
      "`atol` sets the absolute tolerance for gradient comparison.",
      "`rtol` sets the relative tolerance for gradient comparison.",
      "`eps` determines the step size for finite differences.",
      "An `AssertionError` is raised if gradients do not match."
    ],
    "code_examples": []
  },
  {
    "title": "VJP Check with Finite Differences",
    "concepts": [
      "Verifying VJP (Vector-Jacobian Product) against finite differences.",
      "Gradient checking is performed in a single random direction for efficiency.",
      "The function `f` represents the function to be checked.",
      "The function `f_vjp` calculates the jax.vjp applied to `f`.",
      "`args` is a tuple of argument values passed to `f`.",
      "`atol` is the absolute tolerance for gradient comparison.",
      "`rtol` is the relative tolerance for gradient comparison.",
      "`eps` is the step size used in the finite difference approximation.",
      "`err_msg` is an optional error message for assertion failures.",
      "AssertionError is raised if gradients do not match within specified tolerances."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of checkify.checkify",
    "concepts": [
      "Wraps a function to add runtime error checks.",
      "Returns an Error object along with the original function's output.",
      "Errors can be user-defined checks or automatic checks (NaN, division by zero, index out-of-bounds).",
      "The `errors` argument controls which automatic checks are enabled.",
      "`err.get()` returns None if no error occurred, otherwise an error message.",
      "`err.throw()` raises a ValueError with the error message if an error occurred.",
      "User-added checks are enabled by default."
    ],
    "code_examples": []
  },
  {
    "title": "Enabling Different Error Checks",
    "concepts": [
      "Error categories include user_checks, nan_checks, div_checks, and index_checks.",
      "Multiple categories can be enabled using a set (e.g., errors=nan_checks).",
      "Sets can be combined using set operations (e.g., errors=float_checks | user_checks)."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage with NaN Checks",
    "concepts": [
      "Shows how to use `checkify.checkify` to detect NaN values.",
      "Demonstrates the use of `float_checks` to enable NaN checks.",
      "Illustrates how `err.throw()` raises an error when a NaN is detected.",
      "The example includes importing jax and jax.numpy, and using jax.jit."
    ],
    "code_examples": [
      {
        "description": "Example of using checkify to detect NaN values when calculating the sine of infinity.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.experimental import checkify\n\n@jax.jit\ndef f(x):\n  y = jnp.sin(x)\n  return x + y\n\nerr, out = checkify.checkify(f, errors=checkify.float_checks)(jnp.inf)\nerr.throw()"
      },
      {
        "description": "Example of using checkify to detect NaN values when calculating the sine of infinity.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.experimental import checkify\n\n@jax.jit\ndef f(x):\n  y = jnp.sin(x)\n  return x + y\n\nerr, out = checkify.checkify(f, errors=checkify.float_checks)(jnp.inf)\nerr.throw()"
      }
    ]
  },
  {
    "title": "Data Gathering and Processing",
    "concepts": [
      "Data is gathered from multiple processes.",
      "The input data is expected to be a PyTree of arrays with the same shape across hosts.",
      "The `in_tree` parameter represents the PyTree of arrays.",
      "The `tiled` parameter determines whether to stack or concatenate the output arrays.",
      "If `tiled` is False (default), the arrays are stacked, adding a new positional axis at index 0.",
      "If the input is a non-fully addressable jax.Array, the data is fully replicated.",
      "If the input is a NumPy array or fully addressable jax.Array, the output shape depends on the `tiled` argument.",
      "If the input is a scalar, then the output will be stacked."
    ],
    "code_examples": []
  },
  {
    "title": "Array Slicing and Kernel Invocation",
    "concepts": [
      "Defines how an array is sliced for kernel invocations.",
      "Related to BlockSpec for input chunking.",
      "Includes parameters like block_shape, index_map, pipeline_mode, memory_space, and indexing_mode."
    ],
    "code_examples": []
  },
  {
    "title": "Class Attributes and Methods",
    "concepts": [
      "Class has attributes: block_shape, index_map, indexing_mode, memory_space, pipeline_mode.",
      "Class has methods: __init__, replace, to_block_mapping.",
      "replace method: Returns a new object with specified fields replaced."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of Grid Parameters Encoding",
    "concepts": [
      "Encodes grid parameters for jax.experimental.pallas.pallas_call().",
      "Refer to the documentation for jax.experimental.pallas.pallas_call() for more information.",
      "Grids and BlockSpecs provide a detailed description of the parameters."
    ],
    "code_examples": []
  },
  {
    "title": "Grid Parameters",
    "concepts": [
      "grid (TupleGrid)",
      "in_specs (BlockSpecTree)",
      "out_specs (BlockSpecTree)",
      "scratch_shapes (ScratchShapeTree)",
      "grid (Grid)",
      "in_specs (BlockSpecTree)",
      "out_specs (BlockSpecTree)",
      "scratch_shapes (ScratchShapeTree)"
    ],
    "code_examples": []
  },
  {
    "title": "Methods and Attributes",
    "concepts": [
      "__init__([grid, in_specs, out_specs, ...])",
      "Attributes: scratch_shapes, grid, grid_names, in_specs, out_specs"
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for jax_check_tracer_leaks",
    "concepts": [
      "Context manager for controlling the jax_check_tracer_leaks configuration.",
      "Enables checking for leaked tracers immediately after a trace completes.",
      "Enabling leak checking may impact performance due to disabled caching and added overheads.",
      "Python debuggers can cause false positives when leak checking is enabled.",
      "It is recommended to disable debuggers while leak checking is enabled.",
      "The new_val parameter represents the new value to set the jax_check_tracer_leaks config option to."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Jaxpr",
    "concepts": [
      "The make_jaxpr function produces a Jaxpr representation of a function given example arguments.",
      "Jaxpr is JAX's intermediate representation for program traces.",
      "The jaxpr language is based on the simply-typed first-order lambda calculus with let-bindings.",
      "make_jaxpr adapts a function to return its jaxpr, which we can inspect to understand what JAX is doing internally.",
      "The jaxpr returned is a trace of fun abstracted to ShapedArray level."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of make_jaxpr",
    "concepts": [
      "Importing jax is necessary.",
      "Defining a simple function using jax.numpy.",
      "Calling make_jaxpr on the function with an example argument returns a Jaxpr representation.",
      "The Jaxpr represents the operations performed by the function."
    ],
    "code_examples": [
      {
        "description": "Defines a function f that computes sin(cos(x)) using jax.numpy, then uses jax.make_jaxpr to print the jaxpr of the function evaluated at x=3.0.",
        "code": "import jax\n\ndef f(x):\n    return jax.numpy.sin(jax.numpy.cos(x))\n\nprint(f(3.0))\njax.make_jaxpr(f)(3.0)"
      },
      {
        "description": "Defines a function f that computes sin(cos(x)) using jax.numpy, then uses jax.make_jaxpr to print the jaxpr of the gradient of the function evaluated at x=3.0.",
        "code": "import jax\n\ndef f(x):\n    return jax.numpy.sin(jax.numpy.cos(x))\n\nprint(f(3.0))\njax.make_jaxpr(jax.grad(f))(3.0)"
      },
      {
        "description": "Defines a function f that computes sin(cos(x)) using jax.numpy, then uses jax.make_jaxpr to print the jaxpr of the function evaluated at x=3.0.",
        "code": "import jax\n\ndef f(x):\n    return jax.numpy.sin(jax.numpy.cos(x))\n\nprint(f(3.0))\njax.make_jaxpr(f)(3.0)"
      },
      {
        "description": "Defines a function f that computes sin(cos(x)) using jax.numpy, then uses jax.make_jaxpr to print the jaxpr of the gradient of the function evaluated at x=3.0.",
        "code": "import jax\n\ndef f(x):\n    return jax.numpy.sin(jax.numpy.cos(x))\n\nprint(f(3.0))\njax.make_jaxpr(jax.grad(f))(3.0)"
      }
    ]
  },
  {
    "title": "Jacobian Computation with Forward-Mode AD",
    "concepts": [
      "Computes the Jacobian of a function column-by-column.",
      "Uses forward-mode automatic differentiation.",
      "Accepts a callable function as input.",
      "Specifies arguments to differentiate with respect to using argnums.",
      "Indicates if the function returns auxiliary data via the has_aux parameter.",
      "Specifies if the function is holomorphic.",
      "Returns a function that evaluates the Jacobian."
    ],
    "code_examples": [
      {
        "description": "Calculates the Jacobian of a function `f` at a given point using `jax.jacfwd`.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x):\n  return jnp.asarray(\n  [x[0], 5 * x[2], 4 * x[1]**2 - 2 * x[2], x[2] * jnp.sin(x[0])])\n\nprint(jax.jacfwd(f)(jnp.array([1., 2., 3.])))\n"
      },
      {
        "description": "Calculates the Jacobian of a function `f` at a given point using `jax.jacfwd`.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x):\n  return jnp.asarray(\n  [x[0], 5 * x[2], 4 * x[1]**2 - 2 * x[2], x[2] * jnp.sin(x[0])])\n\nprint(jax.jacfwd(f)(jnp.array([1., 2., 3.])))\n"
      }
    ]
  },
  {
    "title": "Introduction to Closure Conversion",
    "concepts": [
      "Closure conversion is a technique for making values in a function's closure accessible to custom derivative rules.",
      "Higher-order functions can hide values in the closure of functions passed as arguments, causing issues with automatic differentiation.",
      "Closure conversion extracts these values and passes them as explicit arguments.",
      "The function to be converted must be a pure function.",
      "The conversion process specializes the function to the types of the example arguments provided."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of Closure Conversion with custom_vjp",
    "concepts": [
      "The example demonstrates how to use closure conversion with `jax.custom_vjp` to define custom derivatives for a minimization function.",
      "The `closure_convert` function is used to prepare the objective function for use with custom derivatives.",
      "The converted function and auxiliary arguments are then passed to an internal minimization function.",
      "Custom forward and reverse mode AD are defined for the internal minimization function."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating closure conversion usage in a minimization context with custom VJP.",
        "code": "def minimize(objective_fn, x0):\n    converted_fn, aux_args = closure_convert(objective_fn, x0)\n    return _minimize(converted_fn, x0, *aux_args)\n\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef _minimize(objective_fn, x0, *args):\n    z = objective_fn(x0, *args)\n    # ... find minimizer x_opt ...\n    return x_opt\n\ndef fwd(objective_fn, x0, *args):\n    y = _minimize(objective_fn, x0, *args)\n    return y, (y, args)\n\ndef rev(objective_fn, res, g):\n    y, args = res\n    y_bar = g\n    # ... custom reverse-mode AD ...\n    return x0_bar, *args_bars\n\n_minimize.defvjp(fwd, rev)\ndef minimize(objective_fn, x0):\n    converted_fn, aux_args = closure_convert(objective_fn, x0)\n    return _minimize(converted_fn, x0, *aux_args)\n\n@partial(custom_vjp, nondiff_argnums=(0,))\ndef _minimize(objective_fn, x0, *args):\n    z = objective_fn(x0, *args)\n    # ... find minimizer x_opt ...\n    return x_opt\n\ndef fwd(objective_fn, x0, *args):\n    y = _minimize(objective_fn, x0, *args)\n    return y, (y, args)\n\ndef rev(objective_fn, res, g):\n    y, args = res\n    y_bar = g\n    # ... custom reverse-mode AD ...\n    return x0_bar, *args_bars\n\n_minimize.defvjp(fwd, rev)"
      }
    ]
  },
  {
    "title": "Function Signature and Return Value",
    "concepts": [
      "The `closure_convert` function takes a callable `fun` and example arguments `example_args` as input.",
      "`example_args` are used to determine the types of arguments to `fun`.",
      "It returns a pair: a converted callable and a list of hoisted values from the closure."
    ],
    "code_examples": []
  },
  {
    "title": "Custom VJP Rule Definition using jax.custom_vjp",
    "concepts": [
      "The jax.custom_vjp decorator allows defining custom VJP rules for functions.",
      "It provides control over reverse-mode differentiation.",
      "The decorator precludes the use of forward-mode automatic differentiation.",
      "The defvjp() method is used to define the custom VJP rule."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating how to define a custom VJP rule for a function using jax.custom_vjp.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.custom_vjp\ndef f(x, y):\n  return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)"
      },
      {
        "description": "Example demonstrating how to define a custom VJP rule for a function using jax.custom_vjp.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.custom_vjp\ndef f(x, y):\n  return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)"
      }
    ]
  },
  {
    "title": "Methods of custom_vjp",
    "concepts": [
      "__init__(fun[, nondiff_argnums]): Initializes the custom_vjp object.",
      "defvjp(fwd, bwd[, symbolic_zeros, ...]): Defines a custom VJP rule for the function."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.make_array_from_callback",
    "concepts": [
      "Creates a jax.Array using data fetched from a callback function.",
      "The data_callback fetches data for each addressable shard of the returned jax.Array.",
      "The data_callback function must return concrete arrays.",
      "make_array_from_callback has limited compatibility with JAX transformations like jit() or vmap().",
      "The shape parameter defines the shape of the jax.Array.",
      "The sharding parameter describes how the array is laid out across devices.",
      "The data_callback takes indices into the global array and returns the corresponding data as an array-like object."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of jax.make_array_from_callback",
    "concepts": [
      "Demonstrates how to use jax.make_array_from_callback with a custom callback function and sharding.",
      "The example uses a numpy array as the underlying data.",
      "It creates a Mesh and NamedSharding to distribute the array across devices.",
      "The callback function retrieves data from the numpy array based on the provided index.",
      "Finally, it creates a jax.Array from the callback, sharding, and shape.",
      "Accessing the addressable data allows us to inspect the shape of the individual shards."
    ],
    "code_examples": [
      {
        "description": "Creates a jax.Array from a callback function that fetches data from a numpy array, demonstrating sharding and device distribution.",
        "code": "import math\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\ninput_shape = (8, 8)\nglobal_input_data = np.arange(math.prod(input_shape)).reshape(input_shape)\nglobal_mesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))\ninp_sharding = jax.sharding.NamedSharding(global_mesh, P('x', 'y'))\n\ndef cb(index):\n  return global_input_data[index]\n\narr = jax.make_array_from_callback(input_shape, inp_sharding, cb)\nprint(arr.addressable_data(0).shape)"
      },
      {
        "description": "Creates a jax.Array from a callback function that fetches data from a numpy array, demonstrating sharding and device distribution.",
        "code": "import math\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\ninput_shape = (8, 8)\nglobal_input_data = np.arange(math.prod(input_shape)).reshape(input_shape)\nglobal_mesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))\ninp_sharding = jax.sharding.NamedSharding(global_mesh, P('x', 'y'))\n\ndef cb(index):\n  return global_input_data[index]\n\narr = jax.make_array_from_callback(input_shape, inp_sharding, cb)\nprint(arr.addressable_data(0).shape)"
      }
    ]
  },
  {
    "title": "Overview of Distributed Tensor Creation",
    "concepts": [
      "This function creates a distributed tensor using data available in a process.",
      "It's a special case of `make_array_from_callback`.",
      "It handles index wrangling for data available in the process.",
      "The most common use case is sharding across the batch dimension.",
      "Supports mixed multi-host and multi-axis replication and sharding.",
      "If any two hosts are replicas, `host_local_data` should be identical.",
      "The `global_shape` is optional and can be inferred from `local_data` and `sharding`.",
      "Explicitly setting `global_shape` allows for finer-grained control, especially with non-uniform shardings."
    ],
    "code_examples": []
  },
  {
    "title": "Shape Matching and Data Placement",
    "concepts": [
      "Each dimension of `global_shape` must match `host_local_data` or the inferred global shape of the sharding.",
      "If a dimension is fully sharded, its size is `per_device_shape[i] * jax.local_device_count()`.",
      "Each device is mapped into a local slice of the `local_data` array.",
      "If `global_shape` matches `local_data.shape`, the local data is assumed to be the target array.",
      "When `global_shape` equals `local_data.shape`, the data must be the same across all hosts."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage with JAX Sharding",
    "concepts": [
      "Demonstrates creating a distributed array using `jax.make_array_from_process_local_data`.",
      "Shows how to define a mesh, sharding, and local data.",
      "Illustrates how to specify `global_shape` and verify the shape of the resulting array.",
      "The example showcases how the local data is used to construct the global array with the specified sharding."
    ],
    "code_examples": [
      {
        "description": "Demonstrates creating a distributed array using `jax.make_array_from_process_local_data`.",
        "code": "from\njax.sharding\nimport\nPartitionSpec\nas\nP\n>>>\nmesh_rows\n=\n2\n>>>\nmesh_cols\n=\njax\n.\ndevice_count\n()\n//\n2\n...\n>>>\nmesh\n=\njax\n.\nsharding\n.\nMesh\n(\nnp\n.\narray\n(\njax\n.\ndevices\n())\n.\nreshape\n(\nmesh_rows\n,\nmesh_cols\n),\n(\n'x'\n,\n'y'\n))\n>>>\nfrom\njax.sharding\nimport\nPartitionSpec\nas\nP\n>>>\nmesh_rows\n=\n2\n>>>\nmesh_cols\n=\njax\n.\ndevice_count\n()\n//\n2\n...\n>>>\nmesh\n=\njax\n.\nsharding\n.\nMesh\n(\nnp\n.\narray\n(\njax\n.\ndevices\n())\n.\nreshape\n(\nmesh_rows\n,\nmesh_cols\n),\n(\n'x'\n,\n'y'\n))\n>>>\nsharding\n=\njax\n.\nsharding\n.\nNamedSharding\n(\nmesh\n,\nP\n((\n'x'\n,\n'y'\n),))\n>>>\nrows_per_device\n=\n2\n>>>\nfeature_length\n=\n32\n>>>\nper_device_shape\n=\n(\nrows_per_device\n,\nfeature_length\n)\n>>>\nper_host_shape\n=\n(\nrows_per_device\n*\nlen\n(\nmesh\n.\nlocal_devices\n),\nfeature_length\n)\n>>>\nper_host_generator\n=\nlambda\n:\nnp\n.\narange\n(\nnp\n.\nprod\n(\nper_host_shape\n))\n.\nreshape\n(\nper_host_shape\n)\n>>>\nper_host_data\n=\nper_host_generator\n()\n# replace with your own per-host data pipeline that outputs numpy arrays\n>>>\nglobal_shape\n=\n(\nrows_per_device\n*\nlen\n(\nsharding\n.\ndevice_set\n),\n)\n+\nper_device_shape\n[\n1\n:]\n>>>\noutput_global_array\n=\njax\n.\nmake_array_from_process_local_data\n(\nsharding\n,\nper_host_data\n,\nglobal_shape\n)\n...\n>>>\nassert\noutput_global_array\n.\naddressable_data\n(\n0\n)\n.\nshape\n==\nper_device_shape\n>>>\nassert\noutput_global_array\n.\nshape\n==\nglobal_shape\n>>>\nsharding\n=\njax\n.\nsharding\n.\nNamedSharding\n(\nmesh\n,\nP\n((\n'x'\n,\n'y'\n),))\n>>>\nrows_per_device\n=\n2\n>>>\nfeature_length\n=\n32\n>>>\nper_device_shape\n=\n(\nrows_per_device\n,\nfeature_length\n)\n>>>\nper_host_shape\n=\n(\nrows_per_device\n*\nlen\n(\nmesh\n.\nlocal_devices\n),\nfeature_length\n)\n>>>\nper_host_generator\n=\nlambda\n:\nnp\n.\narange\n(\nnp\n.\nprod\n(\nper_host_shape\n))\n.\nreshape\n(\nper_host_shape\n)\n>>>\nper_host_data\n=\nper_host_generator\n()\n# replace with your own per-host data pipeline that outputs numpy arrays\n>>>\nglobal_shape\n=\n(\nrows_per_device\n*\nlen\n(\nsharding\n.\ndevice_set\n),\n)\n+\nper_device_shape\n[\n1\n:]\n>>>\noutput_global_array\n=\njax\n.\nmake_array_from_process_local_data\n(\nsharding\n,\nper_host_data\n,\nglobal_shape\n)\n...\n>>>\nassert\noutput_global_array\n.\naddressable_data\n(\n0\n)\n.\nshape\n==\nper_device_shape\n>>>\nassert\noutput_global_array\n.\nshape\n==\nglobal_shape"
      }
    ]
  },
  {
    "title": "Non-Uniform Sharding Considerations",
    "concepts": [
      "Non-uniform sharding occurs when each process's devices are arranged in a non-grid pattern or indices overlap non-trivially.",
      "In non-uniform dimensions, the global shape must match the local shape.",
      "The user must explicitly provide `global_shape` for non-uniform shardings.",
      "Even if each host can fit into a smaller shape, the local data must reflect the overlap in the non-uniform dimension."
    ],
    "code_examples": []
  },
  {
    "title": "Parameter Definitions and Return Value",
    "concepts": [
      "`sharding` (Sharding): Sharding of the global array.",
      "`local_data` (np.ndarray): Data on the host to be placed on local devices; each dimension should either match `global_shape` or `num_addressable_indices(dim)`.",
      "`global_shape` (Shape | None): The target shape of the global array; if None, it will be inferred from `local_data` and `sharding`.",
      "Returns: Tensor that will have `sharding=sharding` and of shape `global_shape` (ArrayImpl)."
    ],
    "code_examples": []
  },
  {
    "title": "Asynchronous Array Copy to Host",
    "concepts": [
      "Arrays residing on accelerators (GPU, TPU) may be cached on the host by JAX.",
      "Normally, device-to-host copies occur when the user requests the array's value, causing blocking.",
      "copy_to_host_async requests that JAX populate its on-host cache without waiting for completion.",
      "This can speed up future on-host access to the array's content."
    ],
    "code_examples": []
  },
  {
    "title": "Generalized Exponential Integral Function",
    "concepts": [
      "Describes the generalized exponential integral function, expn(x) or En(x).",
      "Defines the mathematical formula for expn(x).",
      "Specifies the input parameters 'n' and 'x' as array-like, real-valued.",
      "Indicates that the function returns an array of expn values.",
      "References related functions jax.scipy.special.expi() and jax.scipy.special.exp1()."
    ],
    "code_examples": []
  },
  {
    "title": "Log-Softmax Function Definition",
    "concepts": [
      "Log-Softmax is a rescaling function that maps elements to the range (-inf, 0).",
      "Log-Softmax is the logarithm of the softmax function.",
      "The formula for Log-Softmax is provided.",
      "Input array `x` and axis `axis` are key parameters.",
      "The output is an array of the same shape as the input array `x`."
    ],
    "code_examples": []
  },
  {
    "title": "Notes on Log-Softmax Behavior",
    "concepts": [
      "Input values of +inf result in all NaN values in the output.",
      "This behavior reflects the undefined nature of inf/inf in floating-point math."
    ],
    "code_examples": []
  },
  {
    "title": "Logit Function Definition",
    "concepts": [
      "The logit function is defined as log(p / (1 - p)).",
      "The input 'x' is an array-like, real-valued object.",
      "The output is an array containing the logit values."
    ],
    "code_examples": []
  },
  {
    "title": "Log-Sum-Exp Reduction",
    "concepts": [
      "Log-sum-exp is a method for computing the logarithm of a sum of exponentials.",
      "It is a JAX implementation of scipy.special.logsumexp().",
      "The function calculates logsumexp(a) = log(sum_j b * exp(a_ij)), where j is the reduced dimension.",
      "a is the input array.",
      "axis specifies the axis or axes along which the sum is computed.",
      "b represents scaling factors for exp(a).",
      "keepdims determines whether reduced axes are kept in the output as dimensions of size 1.",
      "return_sign specifies whether the output should include the sign of the sums.",
      "where specifies elements to include in reduction"
    ],
    "code_examples": []
  },
  {
    "title": "Associated Legendre Functions (ALFs)",
    "concepts": [
      "Associated Legendre functions of the first kind are computed.",
      "m represents the maximum order of the ALFs.",
      "n represents the maximum degree of the ALFs (often denoted as l).",
      "Both degrees and orders range from 0 to l_max.",
      "z is a vector containing sampling points.",
      "The function returns a 2-tuple of 3D arrays containing values and derivatives of ALFs.",
      "The return type matches the type of z (float32 or float64)."
    ],
    "code_examples": []
  },
  {
    "title": "Multivariate Gamma Function",
    "concepts": [
      "Definition of the multivariate gamma function: multigammaln(a, d) = log(\u0393d(a)).",
      "\u0393d(a) is defined as pi^(d(d-1)/4) * product from i=1 to d of Gamma(a-(i-1)/2).",
      "The gamma() function is used in the definition.",
      "Input 'a' is array-like and real-valued.",
      "Input 'd' is an integer representing the dimension of the integration space.",
      "The function returns an array containing the values of the log-multigamma function."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of the Inverse Normal CDF",
    "concepts": [
      "Calculates the inverse of the Cumulative Distribution Function (CDF) for a Normal distribution.",
      "Provides a JAX implementation of scipy.special.ndtri.",
      "Finds the value x such that the area under the Normal PDF from negative infinity to x equals p.",
      "Uses a piece-wise rational approximation method.",
      "Implementation is based on netlib."
    ],
    "code_examples": []
  },
  {
    "title": "Input and Output",
    "concepts": [
      "The input 'p' is an array of type float32 or float64.",
      "The output is an array with the same dtype as 'p'.",
      "A TypeError is raised if 'p' is not a floating-type array."
    ],
    "code_examples": []
  },
  {
    "title": "Pochhammer Symbol Definition",
    "concepts": [
      "The Pochhammer symbol is defined as (z)_m = Gamma(z + m) / Gamma(z).",
      "Gamma(z) is the gamma function."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Implementation of Pochhammer",
    "concepts": [
      "JAX implements the scipy.special.poch function.",
      "The input 'z' is an array-like, real-valued number.",
      "The input 'm' is an array-like, real-valued number.",
      "The output is an array of Pochhammer values.",
      "The JAX version only supports real-valued inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Polygamma Function Definition",
    "concepts": [
      "The polygamma function is the nth derivative of the digamma function.",
      "The polygamma function is defined as the nth derivative of the logarithm of the gamma function.",
      "The gamma function is denoted by \u0393(x).",
      "The variable 'n' represents the order of the derivative and should be integer-valued.",
      "The variable 'x' represents the value at which the function is evaluated and should be real-valued."
    ],
    "code_examples": []
  },
  {
    "title": "Relative Entropy Function",
    "concepts": [
      "The relative entropy function is defined as p*log(p/q) when p>0 and q>0.",
      "The relative entropy function is defined as 0 when p=0 and q>=0.",
      "The relative entropy function is defined as infinity otherwise."
    ],
    "code_examples": []
  },
  {
    "title": "Softmax Function Overview",
    "concepts": [
      "The softmax function rescales elements to the range [0, 1].",
      "Elements along the specified axis sum to 1 after applying softmax.",
      "The softmax function is defined as exp(x_i) / sum_j exp(x_j).",
      "The input is an array, and the output is an array of the same shape.",
      "If any input values are +inf, the result will be all NaN."
    ],
    "code_examples": []
  },
  {
    "title": "Spence's Function (Dilogarithm)",
    "concepts": [
      "Spence's function is also known as the dilogarithm.",
      "The JAX implementation is based on scipy.special.spence.",
      "Spence's function is defined as the integral of log(t)/(1-t) from 1 to x.",
      "The JAX implementation is only defined for positive real values of z; NaN is returned for negative values.",
      "The input 'z' should be an array of type float32 or float64.",
      "The function returns an array with dtype=z.dtype containing the computed values of Spence's function.",
      "A TypeError is raised if the elements of the input array 'z' are not float32 or float64.",
      "An alternative definition of Spence's function involves the integral of -log(1-t)/t from 0 to z, which corresponds to spence(1 - z)."
    ],
    "code_examples": []
  },
  {
    "title": "Spherical Harmonics Computation",
    "concepts": [
      "This function is deprecated, use sph_harm_y() instead.",
      "The JAX version requires an extra argument n_max.",
      "Spherical harmonics are defined by degree (n) and order (m).",
      "The formula for spherical harmonics is Y_n^m(theta, phi) = N_n^m * P_n^m(cos phi) * exp(i m theta).",
      "N_n^m is the normalization factor.",
      "phi is the colatitude and theta is the longitude.",
      "Spherical harmonics form an orthonormal basis of L^2(S^2)."
    ],
    "code_examples": []
  },
  {
    "title": "Input Parameters",
    "concepts": [
      "m is the order of the harmonic, with |m| <= n.",
      "n is the degree of the harmonic, with n >= 0.",
      "theta is the azimuthal (longitudinal) coordinate, in [0, 2*pi].",
      "phi is the polar (colatitudinal) coordinate, in [0, pi].",
      "n_max is the maximum degree allowed, and results are clipped if n exceeds n_max."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the n_max parameter, which clips the 'n' argument.",
        "code": "sph_harm(m=jnp.array([2]), n=jnp.array([10]), theta, phi, n_max=6) actually returns sph_harm(m=jnp.array([2]), n=jnp.array([6]), theta, phi, n_max=6)"
      }
    ]
  },
  {
    "title": "Output",
    "concepts": [
      "The function returns a 1D array.",
      "The array contains the spherical harmonics values at the given (m, n, theta, phi) coordinates."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to xlogy Function",
    "concepts": [
      "Computes x*log(y), returning 0 for x=0.",
      "JAX implementation of scipy.special.xlogy.",
      "Returns zero when (x, y) = (0, 0).",
      "Includes a custom derivative rule for automatic differentiation.",
      "x is an array-like, real-valued input.",
      "y is an array-like, real-valued input.",
      "Returns an array containing xlogy values."
    ],
    "code_examples": []
  },
  {
    "title": "Hurwitz Zeta Function Definition",
    "concepts": [
      "The document defines the Hurwitz zeta function.",
      "The JAX implementation is described as equivalent to scipy.special.zeta().",
      "JAX does not implement the Riemann zeta function (q = None).",
      "The Hurwitz zeta function is defined by the summation formula: \u03b6(x, q) = \u2211 (1/(n + q)^x) from n=0 to infinity.",
      "x is an array-like, real-valued input.",
      "q is an array-like, real-valued input."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.scipy.stats.mode",
    "concepts": [
      "Computes the mode (most common value) along an axis of an array.",
      "JAX implementation of scipy.stats.mode().",
      "Returns a tuple of arrays: (mode, count).",
      "mode is the array of modal values.",
      "count is the number of times each value appears in the input array."
    ],
    "code_examples": []
  },
  {
    "title": "One-dimensional Array Mode Calculation",
    "concepts": [
      "Example demonstrates how to calculate the mode and its count for a 1D JAX array."
    ],
    "code_examples": [
      {
        "description": "Calculates the mode of a 1D JAX array.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx = jnp.array([2, 4, 1, 1, 3, 4, 4, 2, 3])\nmode, count = jax.scipy.stats.mode(x)\nprint(mode, count)"
      },
      {
        "description": "Calculates the mode of a 1D JAX array.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx = jnp.array([2, 4, 1, 1, 3, 4, 4, 2, 3])\nmode, count = jax.scipy.stats.mode(x)\nprint(mode, count)"
      }
    ]
  },
  {
    "title": "Multi-dimensional Array Mode Calculation along axis=0",
    "concepts": [
      "Example shows how to calculate the mode along axis 0 for a multi-dimensional JAX array."
    ],
    "code_examples": [
      {
        "description": "Calculates the mode along the default axis (0) for a 2D JAX array.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\nmode, count = jax.scipy.stats.mode(x1)\nprint(mode, count)"
      },
      {
        "description": "Calculates the mode along the default axis (0) for a 2D JAX array.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\nmode, count = jax.scipy.stats.mode(x1)\nprint(mode, count)"
      }
    ]
  },
  {
    "title": "Multi-dimensional Array Mode Calculation along axis=1",
    "concepts": [
      "Example demonstrates calculating the mode along axis 1 for a multi-dimensional JAX array."
    ],
    "code_examples": [
      {
        "description": "Calculates the mode along axis 1 for a 2D JAX array.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\nmode, count = jax.scipy.stats.mode(x1, axis=1)\nprint(mode, count)"
      },
      {
        "description": "Calculates the mode along axis 1 for a 2D JAX array.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\nmode, count = jax.scipy.stats.mode(x1, axis=1)\nprint(mode, count)"
      }
    ]
  },
  {
    "title": "Keeping Dimensions with keepdims=True",
    "concepts": [
      "Demonstrates the use of the keepdims argument to preserve the input array's dimensions in the output."
    ],
    "code_examples": [
      {
        "description": "Calculates the mode along axis 1 and keeps the dimensions of the input array using keepdims=True.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\nmode, count = jax.scipy.stats.mode(x1, axis=1, keepdims=True)\nprint(mode, count)"
      },
      {
        "description": "Calculates the mode along axis 1 and keeps the dimensions of the input array using keepdims=True.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\nmode, count = jax.scipy.stats.mode(x1, axis=1, keepdims=True)\nprint(mode, count)"
      }
    ]
  },
  {
    "title": "Rank Data Computation",
    "concepts": [
      "Computes the rank of data along an array axis using JAX.",
      "Ranks start at 1.",
      "The `method` argument controls tie handling.",
      "Supported methods include 'average', 'min', 'max', 'dense', and 'ordinal'.",
      "The `axis` argument specifies the axis along which to compute ranks.",
      "If axis is not specified, the array is flattened.",
      "The `nan_policy` argument only supports 'propagate'."
    ],
    "code_examples": [
      {
        "description": "Example of ranking a simple array.",
        "code": "x = jnp.array([10, 30, 20])\nrankdata(x)\n#Array([1., 3., 2.], dtype=float32)"
      },
      {
        "description": "Example of ranking an array with ties using the default 'average' method.",
        "code": "x = jnp.array([1, 3, 2, 3])\nrankdata(x)\n#Array([1. , 3.5, 2. , 3.5], dtype=float32)"
      }
    ]
  },
  {
    "title": "Introduction to jax.scipy.stats.sem",
    "concepts": [
      "The document describes the JAX implementation of the standard error of the mean (SEM).",
      "The function computes the standard error of the mean of an array.",
      "The function mirrors the functionality of `scipy.stats.sem()`.",
      "It supports specifying the axis along which to compute the SEM.",
      "It includes handling of NaN values via the `nan_policy` parameter.",
      "The `ddof` parameter controls the degrees of freedom.",
      "The `keepdims` parameter specifies whether to preserve the original dimensions."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage with a 1D Array",
    "concepts": [
      "Demonstrates calculating the SEM for a one-dimensional JAX array.",
      "Shows how to use `jnp.printoptions` to control the output precision.",
      "The default `axis` is None for 1D arrays."
    ],
    "code_examples": [
      {
        "description": "Calculates the SEM of a 1D JAX array and prints the result with specified precision.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx = jnp.array([2, 4, 1, 1, 3, 4, 4, 2, 3])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x))"
      },
      {
        "description": "Repeats the SEM calculation for a 1D JAX array to demonstrate consistent behavior.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx = jnp.array([2, 4, 1, 1, 3, 4, 4, 2, 3])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x))"
      }
    ]
  },
  {
    "title": "SEM Calculation with Multi-Dimensional Arrays",
    "concepts": [
      "Illustrates SEM computation for multi-dimensional arrays.",
      "The default behavior calculates SEM along the first axis (axis=0).",
      "Example shows SEM calculated along axis 0 for a 2D array."
    ],
    "code_examples": [
      {
        "description": "Computes the SEM along the default axis (axis=0) of a 2D array.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x1))"
      },
      {
        "description": "Repeats the SEM calculation along axis 0 for a 2D JAX array.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x1))"
      }
    ]
  },
  {
    "title": "Specifying the Axis for SEM Calculation",
    "concepts": [
      "Demonstrates specifying the `axis` parameter to compute SEM along a different axis.",
      "Example calculates SEM along axis 1.",
      "Shows how SEM can be calculated along all axes by setting axis to None."
    ],
    "code_examples": [
      {
        "description": "Calculates the SEM along axis 1 of a 2D JAX array.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x1, axis=1))"
      },
      {
        "description": "Repeats SEM calculation along axis 1.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x1, axis=1))"
      },
      {
        "description": "Calculates the SEM along all axes of the array.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x1, axis=None))"
      },
      {
        "description": "Repeats the SEM calculation along all axes using axis=None.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x1, axis=None))"
      }
    ]
  },
  {
    "title": "Preserving Dimensions with keepdims",
    "concepts": [
      "Demonstrates the use of the `keepdims` parameter.",
      "Setting `keepdims=True` preserves the dimensions of the input array in the output.",
      "The reduced axes will have size 1."
    ],
    "code_examples": [
      {
        "description": "Calculates SEM along axis 1 and keeps the original dimensions using keepdims=True.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x1, axis=1, keepdims=True))"
      },
      {
        "description": "Repeats the SEM calculation with keepdims=True.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nx1 = jnp.array([[1, 2, 1, 3, 2, 1],\n                [3, 1, 3, 2, 1, 3],\n                [1, 2, 2, 3, 1, 2]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x1, axis=1, keepdims=True))"
      }
    ]
  },
  {
    "title": "Handling NaN Values with nan_policy",
    "concepts": [
      "Illustrates the use of the `nan_policy` parameter to handle NaN values.",
      "The default `nan_policy='propagate'` propagates NaN values to the result.",
      "Setting `nan_policy='omit'` omits NaN values from the calculation.",
      "The 'omit' policy computes the error for the remaining values along the specified axis."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the default nan_policy='propagate', where NaN values in the input array result in NaN values in the output.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nnan = jnp.nan\nx2 = jnp.array([[1, 2, 3, nan, 4, 2],\n                [4, 5, 4, 3, nan, 1],\n                [7, nan, 8, 7, 9, nan]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x2))"
      },
      {
        "description": "Repeats the demonstration of nan_policy='propagate'.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nnan = jnp.nan\nx2 = jnp.array([[1, 2, 3, nan, 4, 2],\n                [4, 5, 4, 3, nan, 1],\n                [7, nan, 8, 7, 9, nan]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x2))"
      },
      {
        "description": "Demonstrates the use of nan_policy='omit', where NaN values are omitted from the calculation.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nnan = jnp.nan\nx2 = jnp.array([[1, 2, 3, nan, 4, 2],\n                [4, 5, 4, 3, nan, 1],\n                [7, nan, 8, 7, 9, nan]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x2, nan_policy='omit'))"
      },
      {
        "description": "Repeats the demonstration of nan_policy='omit'.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.stats\n\nnan = jnp.nan\nx2 = jnp.array([[1, 2, 3, nan, 4, 2],\n                [4, 5, 4, 3, nan, 1],\n                [7, nan, 8, 7, 9, nan]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n    print(jax.scipy.stats.sem(x2, nan_policy='omit'))"
      }
    ]
  },
  {
    "title": "Bernoulli Log Probability Mass Function",
    "concepts": [
      "The Bernoulli PMF is defined as f(k) = 1-p if k=0, and p if k=1, and 0 otherwise.",
      "k is the value at which to evaluate the PMF.",
      "p is the distribution shape parameter.",
      "loc is the distribution offset.",
      "The function returns an array of logpmf values."
    ],
    "code_examples": []
  },
  {
    "title": "Bernoulli Percent Point Function (PPF)",
    "concepts": [
      "The percent point function is the inverse of the cumulative distribution function (CDF).",
      "The function calculates the PPF for a Bernoulli distribution.",
      "It uses the shape parameter 'p' and input value 'k' to compute the PPF value.",
      "jax.scipy.stats.bernoulli.cdf() can be used to calculate the CDF.",
      "jax.scipy.stats.bernoulli.logpmf() can be used to calculate the log PMF.",
      "jax.scipy.stats.bernoulli.pmf() can be used to calculate the PMF."
    ],
    "code_examples": []
  },
  {
    "title": "Beta Log Probability Density Function",
    "concepts": [
      "Describes the beta log probability density function.",
      "Implements scipy.stats.beta logpdf using JAX.",
      "The beta function PDF is defined as f(x, a, b) = (Gamma(a + b) / (Gamma(a)Gamma(b))) * x^(a-1) * (1-x)^(b-1).",
      "The function is defined for 0 <= x <= 1 and b > 0.",
      "It takes x, a, b, loc, and scale as input parameters.",
      "Returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Beta Probability Distribution Function",
    "concepts": [
      "Describes the Beta probability distribution function.",
      "JAX implementation of scipy.stats.beta pdf.",
      "The probability density function (PDF) of the beta distribution is defined by the formula: f(x, a, b) = (Gamma(a + b) / (Gamma(a) * Gamma(b))) * x^(a-1) * (1-x)^(b-1).",
      "Gamma represents the gamma function.",
      "The beta distribution is defined for 0 <= x <= 1 and b > 0.",
      "Parameters: x (value at which to evaluate the PDF), a (distribution shape parameter), b (distribution shape parameter), loc (distribution offset parameter), scale (distribution scale parameter).",
      "Returns an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Beta Cumulative Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.beta cdf.",
      "The cdf is the integral of the beta distribution pdf from negative infinity to x.",
      "The function takes x, a, b, loc, and scale as input parameters.",
      "It returns an array of cdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Beta Log Cumulative Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.beta logcdf.",
      "The CDF is defined as the integral of the PDF from negative infinity to x.",
      "x is the value at which to evaluate the CDF.",
      "a is the distribution shape parameter.",
      "b is the distribution shape parameter.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "The function returns an array of logcdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Beta Distribution Survival Function",
    "concepts": [
      "The survival function (SF) is defined as 1 - CDF.",
      "The SF is evaluated at a given value x, with shape parameters a and b.",
      "The function may also take location and scale parameters."
    ],
    "code_examples": []
  },
  {
    "title": "Beta Distribution Log Survival Function",
    "concepts": [
      "The document describes the log survival function of the beta distribution.",
      "It is a JAX implementation of scipy.stats.beta logsf.",
      "The survival function is defined as 1 - cdf, where cdf is the cumulative distribution function.",
      "It provides information on the input parameters: x, a, b, loc, and scale.",
      "The function returns an array of logsf values."
    ],
    "code_examples": []
  },
  {
    "title": "Beta-binomial Log Probability Mass Function",
    "concepts": [
      "The beta-binomial distribution's probability mass function is defined.",
      "The formula for the beta-binomial PMF is provided.",
      "Definition of the parameters k, n, a, b, and loc.",
      "The function returns an array of logpmf values.",
      "The function is related to jax.scipy.stats.betabinom.pmf()."
    ],
    "code_examples": []
  },
  {
    "title": "Beta-binomial Probability Mass Function",
    "concepts": [
      "The beta-binomial distribution's probability mass function is defined by a specific formula involving binomial coefficients and the beta function.",
      "The parameters of the beta-binomial distribution are n, a, and b, with constraints n >= 0, a > 0, and b > 0.",
      "The pmf is evaluated at non-negative integers k."
    ],
    "code_examples": []
  },
  {
    "title": "Binomial Log Probability Mass Function",
    "concepts": [
      "Defines the binomial log probability mass function.",
      "Provides a JAX implementation of scipy.stats.binom logpmf.",
      "The binomial probability mass function is defined as f(k, n, p) = {n choose k}p^k(1-p)^{n-k} for 0 <= p <= 1 and non-negative integers k.",
      "k is the value at which to evaluate the PMF.",
      "n is the distribution shape parameter.",
      "p is the distribution shape parameter.",
      "loc is the distribution offset parameter.",
      "Returns an array of logpmf values."
    ],
    "code_examples": []
  },
  {
    "title": "Cauchy Log Probability Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.cauchy logpdf.",
      "The Cauchy probability distribution function is defined as f(x) = 1 / (\u03c0(1 + x^2)).",
      "x is the value at which to evaluate the PDF.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "The function returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters and Return Value",
    "concepts": [
      "x is an array-like value at which to evaluate the PDF.",
      "loc is an array-like distribution offset parameter.",
      "scale is an array-like distribution scale parameter.",
      "The function returns an array of logpdf values of type Array."
    ],
    "code_examples": []
  },
  {
    "title": "See Also",
    "concepts": [
      "Related functions: jax.scipy.stats.cauchy.cdf(), jax.scipy.stats.cauchy.pdf(), jax.scipy.stats.cauchy.sf(), jax.scipy.stats.cauchy.logcdf(), jax.scipy.stats.cauchy.logsf(), jax.scipy.stats.cauchy.isf(), jax.scipy.stats.cauchy.ppf()."
    ],
    "code_examples": []
  },
  {
    "title": "Cauchy Probability Density Function",
    "concepts": [
      "The Cauchy distribution PDF is defined as f(x) = 1 / (\u03c0 * (1 + x^2)).",
      "The function evaluates the PDF at a given value x.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "The function returns an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Cauchy Log Cumulative Distribution Function Overview",
    "concepts": [
      "Describes the JAX implementation of the Cauchy log cumulative distribution function.",
      "Relates to scipy.stats.cauchy logcdf.",
      "The CDF is defined as the integral of the PDF from negative infinity to x.",
      "Links to the Cauchy probability distribution function, jax.scipy.stats.cauchy.pdf().",
      "Defines input parameters: x (value at which to evaluate the CDF), loc (distribution offset), and scale (distribution scale)."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "References other related JAX functions for the Cauchy distribution.",
      "Links to cdf, pdf, sf, logpdf, logsf, isf, and ppf functions."
    ],
    "code_examples": []
  },
  {
    "title": "Cauchy Distribution Log Survival Function",
    "concepts": [
      "The document describes the log survival function of the Cauchy distribution.",
      "It provides a JAX implementation of the scipy.stats.cauchy sf.",
      "The survival function is defined as 1 - cdf(x).",
      "It specifies the input parameters x, loc, and scale.",
      "It returns an array of sf values."
    ],
    "code_examples": []
  },
  {
    "title": "Cauchy Distribution Log Survival Function",
    "concepts": [
      "The document describes the log survival function (logsf) of the Cauchy distribution.",
      "The logsf is defined as the logarithm of the survival function (sf).",
      "The survival function is the complement of the cumulative distribution function (cdf): sf(x) = 1 - cdf(x).",
      "The function jax.scipy.stats.cauchy.logsf calculates the logsf of the Cauchy distribution.",
      "The function takes the value at which to evaluate the SF (x), the distribution offset parameter (loc), and the distribution scale parameter (scale) as inputs.",
      "The function returns an array of logsf values."
    ],
    "code_examples": []
  },
  {
    "title": "Cauchy Distribution Inverse Survival Function",
    "concepts": [
      "JAX implementation of the inverse survival function (ISF) for the Cauchy distribution.",
      "It computes the inverse of the survival function, which is the value at which the survival function equals a given probability.",
      "The ISF is calculated based on the quantile (q), location (loc), and scale parameters of the Cauchy distribution.",
      "The function returns an array of ISF values."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "q: arraylike, value at which to evaluate the ISF.",
      "loc: arraylike, distribution offset parameter.",
      "scale: arraylike, distribution scale parameter."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "Array of isf values."
    ],
    "code_examples": []
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.scipy.stats.cauchy.cdf(): Cauchy cumulative distribution function.",
      "jax.scipy.stats.cauchy.pdf(): Cauchy probability density function.",
      "jax.scipy.stats.cauchy.sf(): Cauchy survival function.",
      "jax.scipy.stats.cauchy.logcdf(): Cauchy log cumulative distribution function.",
      "jax.scipy.stats.cauchy.logpdf(): Cauchy log probability density function.",
      "jax.scipy.stats.cauchy.logsf(): Cauchy log survival function.",
      "jax.scipy.stats.cauchy.ppf(): Cauchy percent point function (quantile function)."
    ],
    "code_examples": []
  },
  {
    "title": "Cauchy Distribution Percent Point Function (PPF)",
    "concepts": [
      "The percent point function (PPF) is the inverse of the cumulative distribution function (CDF).",
      "This document describes the JAX implementation of the Cauchy distribution PPF, analogous to scipy.stats.cauchy.ppf.",
      "The function takes a quantile `q`, location parameter `loc`, and scale parameter `scale` as input.",
      "It returns an array of PPF values."
    ],
    "code_examples": []
  },
  {
    "title": "Chi-Square Log Probability Distribution Function",
    "concepts": [
      "Describes the chi-square log probability distribution function.",
      "JAX implementation of scipy.stats.chi2 logpdf.",
      "The chi-square PDF is defined by a specific formula involving x, k (degrees of freedom), and the gamma function.",
      "JAX uses 'df' to denote degrees of freedom, following the SciPy convention.",
      "The function takes x (value at which to evaluate the PDF), df (shape parameter), loc (offset parameter), and scale (scale parameter) as inputs.",
      "The function returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Chi-Square Probability Distribution Function",
    "concepts": [
      "Describes the chi-square probability distribution function.",
      "The function calculates the probability density for a given value.",
      "It is defined by the formula involving the gamma function.",
      "It uses degrees of freedom (k) as a parameter.",
      "JAX follows the scipy convention using df for degrees of freedom.",
      "The function accepts x (value), df (degrees of freedom), loc (offset), and scale as inputs.",
      "It returns an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Chi-square Cumulative Distribution Function",
    "concepts": [
      "JAX implementation of the chi2 cdf.",
      "The cdf is the integral of the pdf from negative infinity to x.",
      "jax.scipy.stats.chi2.cdf() computes the cumulative distribution function for the chi-square distribution.",
      "df represents degrees of freedom.",
      "x is the value at which to evaluate the CDF.",
      "df is the distribution shape parameter.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter."
    ],
    "code_examples": []
  },
  {
    "title": "Chi-square Log Cumulative Distribution Function Overview",
    "concepts": [
      "This section describes the Chi-square log cumulative distribution function.",
      "It provides a JAX implementation of scipy.stats.chi2 logcdf.",
      "The cumulative distribution function (CDF) is the integral of the probability density function (PDF).",
      "The probability density function is referenced as jax.scipy.stats.chi2.pdf().",
      "df is used to denote degrees of freedom, following the scipy convention.",
      "The function calculates the logcdf value for a given x, df, loc, and scale."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Chi-square Log CDF",
    "concepts": [
      "x: Value at which to evaluate the CDF (array-like).",
      "df: Distribution shape parameter (array-like), representing degrees of freedom.",
      "loc: Distribution offset parameter (array-like).",
      "scale: Distribution scale parameter (array-like)."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.scipy.stats.chi2.cdf(): Chi-square cumulative distribution function.",
      "jax.scipy.stats.chi2.pdf(): Chi-square probability density function.",
      "jax.scipy.stats.chi2.sf(): Chi-square survival function.",
      "jax.scipy.stats.chi2.logpdf(): Chi-square log probability density function.",
      "jax.scipy.stats.chi2.logsf(): Chi-square log survival function."
    ],
    "code_examples": []
  },
  {
    "title": "Chi-square Log Survival Function Definition",
    "concepts": [
      "The log survival function (logsf) is the logarithm of the survival function.",
      "The survival function (sf) is defined as 1 - cdf, where cdf is the cumulative distribution function.",
      "JAX follows the scipy convention of using df to denote degrees of freedom.",
      "The function calculates the logsf of the chi-square distribution.",
      "It uses x, df, loc, and scale as parameters, similar to scipy.stats.chi2."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters and Return Value",
    "concepts": [
      "x represents the value at which to evaluate the survival function.",
      "df represents the degrees of freedom, which is the shape parameter of the chi-square distribution.",
      "loc is the location parameter (offset).",
      "scale is the scale parameter.",
      "The function returns an array of logsf values."
    ],
    "code_examples": []
  },
  {
    "title": "Dirichlet Log Probability Density Function",
    "concepts": [
      "The Dirichlet distribution's log probability density function (log PDF) is described.",
      "The formula for the Dirichlet PDF is provided.",
      "Input 'x' represents the value at which to evaluate the PDF.",
      "Input 'alpha' represents the distribution's shape parameter.",
      "The function returns an array of log PDF values.",
      "It is a JAX implementation of scipy.stats.dirichlet logpdf."
    ],
    "code_examples": []
  },
  {
    "title": "Dirichlet Probability Density Function",
    "concepts": [
      "Definition of the Dirichlet probability density function.",
      "f(x) = (1 / B(alpha)) * product(x_i^(alpha_i - 1))",
      "B(alpha) is the beta function.",
      "x is the value at which to evaluate the PDF.",
      "alpha is the distribution shape parameter.",
      "The function returns an array of PDF values.",
      "See also jax.scipy.stats.dirichlet.logpdf()"
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Log Probability Distribution Function",
    "concepts": [
      "The Exponential probability distribution function is defined as f(x) = e^{-x} for x >= 0, and 0 otherwise.",
      "The function calculates the logpdf of the exponential distribution.",
      "The function accepts parameters x (value at which to evaluate the PDF), loc (distribution offset), and scale (distribution scale)."
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Probability Distribution Function",
    "concepts": [
      "Describes the Exponential probability distribution function.",
      "Provides a JAX implementation.",
      "Defines the formula for the probability density function f(x).",
      "Explains the input parameter x: value at which to evaluate the PDF.",
      "Explains the input parameter loc: distribution offset parameter.",
      "Explains the input parameter scale: distribution scale parameter.",
      "The output is an array of PDF values.",
      "References related JAX functions: cdf, pdf, ppf, sf, logcdf, logpdf, logsf."
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Log Cumulative Density Function",
    "concepts": [
      "The document describes the exponential log cumulative density function.",
      "It is a JAX implementation of scipy.stats.expon logcdf.",
      "The cdf is the integral of the pdf from negative infinity to x.",
      "It relates to jax.scipy.stats.expon.pdf().",
      "The function takes x, loc, and scale as parameters.",
      "x is the value at which to evaluate the PDF.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "It returns an array of pdf values.",
      "Related functions include cdf, pdf, ppf, sf, logcdf, logpdf, and logsf."
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Cumulative Density Function",
    "concepts": [
      "JAX implementation of the exponential CDF.",
      "The CDF is the integral of the PDF from negative infinity to x.",
      "The exponential distribution's PDF is given by jax.scipy.stats.expon.pdf().",
      "The function evaluates the CDF at a given value x.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "The function returns an array of CDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Log Survival Function Overview",
    "concepts": [
      "The document describes the exponential log survival function.",
      "It's a JAX implementation of scipy.stats.expon logsf.",
      "The survival function is defined as 1 - cdf(x).",
      "It utilizes the exponential cumulative distribution function jax.scipy.stats.expon.cdf().",
      "The function takes x, loc, and scale as input parameters.",
      "It returns an array of log survival function values.",
      "It relates to other exponential functions like cdf, pdf, ppf, sf, logcdf, logpdf, and logsf."
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Survival Function Definition",
    "concepts": [
      "The survival function is the complement of the cumulative distribution function.",
      "The survival function is defined as f_sf(x) = 1 - f_cdf(x).",
      "jax.scipy.stats.expon.cdf() can be used to calculate the cumulative distribution function.",
      "The function takes x, loc, and scale as input parameters.",
      "x is the value at which to evaluate the survival function.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter."
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Survival Function and Percent Point Function",
    "concepts": [
      "Exponential survival function is discussed.",
      "JAX implementation of scipy.stats.expon ppf is provided.",
      "Percent point function (PPF) is the inverse of the cumulative distribution function (CDF).",
      "Parameters: x, loc, and scale are used for evaluation.",
      "The output is an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Gamma Log Probability Distribution Function",
    "concepts": [
      "Describes the Gamma log probability distribution function.",
      "JAX implementation of scipy.stats.gamma logpdf.",
      "The Gamma probability distribution is defined by the formula: f(x, a) = (1/\u0393(a)) * x^(a-1) * e^(-x).",
      "\u0393(a) is the gamma() function.",
      "The distribution is defined for x >= 0 and a > 0.",
      "The function takes x (value), a (shape), loc (offset), and scale as parameters.",
      "It returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Gamma Probability Distribution Function",
    "concepts": [
      "The Gamma probability distribution is defined by the formula: f(x, a) = (1/Gamma(a)) * x^(a-1) * e^(-x).",
      "The distribution is defined for x >= 0 and a > 0.",
      "The function calculates the probability density function (PDF) of the Gamma distribution.",
      "The function takes x (value at which to evaluate the PDF), a (distribution shape parameter), loc (distribution offset parameter), and scale (distribution scale parameter) as inputs.",
      "It returns an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Gamma Cumulative Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.gamma cdf.",
      "The cdf is defined as the integral of the pdf from negative infinity to x: f_cdf(x, a) = integral[-inf to x] f_pdf(y, a) dy.",
      "x is the value at which to evaluate the CDF.",
      "a is the distribution shape parameter.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "Returns an array of cdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Gamma Survival Function Definition",
    "concepts": [
      "The gamma survival function (sf) is defined as 1 - cdf, where cdf is the cumulative distribution function.",
      "jax.scipy.stats.gamma.sf is a JAX implementation of the gamma survival function.",
      "The function takes parameters x, a, loc, and scale as inputs.",
      "It returns an array of sf values."
    ],
    "code_examples": []
  },
  {
    "title": "Gamma Log Survival Function",
    "concepts": [
      "The gamma log survival function (logsf) is implemented in JAX.",
      "The survival function is the complement of the cumulative distribution function (cdf).",
      "The formula for the survival function is  f_sf(x, k) = 1 - f_cdf(x, k).",
      "The function takes x, a, loc, and scale as input parameters.",
      "x is the value at which to evaluate the SF.",
      "a is the distribution shape parameter.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "The function returns an array of logsf values."
    ],
    "code_examples": []
  },
  {
    "title": "Generalized Normal Cumulative Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.gennorm cdf.",
      "The cdf is defined as the integral of the pdf from negative infinity to x.",
      "x is the value at which to evaluate the CDF.",
      "beta is the distribution shape parameter.",
      "Returns an array of cdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Generalized Normal Probability Distribution Function",
    "concepts": [
      "Defines the generalized normal probability distribution function.",
      "The function is parameterized by a shape parameter beta.",
      "The function utilizes the gamma function.",
      "The function returns the probability density function (PDF) value at a given point x."
    ],
    "code_examples": []
  },
  {
    "title": "Mathematical Definition",
    "concepts": [
      "The generalized normal probability distribution function is defined as f(x, beta) = (beta / (2*Gamma(1/beta))) * exp(-|x|^beta)."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "x represents the value at which to evaluate the PDF.",
      "beta represents the shape parameter of the distribution, which must be greater than 0."
    ],
    "code_examples": []
  },
  {
    "title": "Geometric Log Probability Mass Function",
    "concepts": [
      "Describes the geometric log probability mass function (logpmf).",
      "Provides a JAX implementation of scipy.stats.geom logpmf.",
      "Defines the formula for the Geometric probability mass function: f(k) = (1 - p)^(k-1)p.",
      "Specifies the range of k as k >= 1 and p as 0 <= p <= 1.",
      "Defines the input parameter 'k' as the value at which to evaluate the PMF.",
      "Defines the input parameter 'p' as the distribution shape parameter.",
      "Defines the input parameter 'loc' as the distribution offset parameter.",
      "The output is an array of logpmf values."
    ],
    "code_examples": []
  },
  {
    "title": "Geometric Probability Mass Function",
    "concepts": [
      "Describes the Geometric probability mass function.",
      "The formula for the Geometric PMF is f(k) = (1 - p)^(k-1)p.",
      "k is the value at which to evaluate the PMF.",
      "p is the distribution shape parameter.",
      "loc is the distribution offset parameter.",
      "The function returns an array of PMF values."
    ],
    "code_examples": []
  },
  {
    "title": "Laplace Cumulative Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.laplace cdf.",
      "The cdf is defined as the integral of the pdf from negative infinity to x.",
      "x is the value at which to evaluate the CDF.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "Returns an array of cdf values.",
      "Related functions are jax.scipy.stats.laplace.pdf() and jax.scipy.stats.laplace.logpdf()."
    ],
    "code_examples": []
  },
  {
    "title": "Laplace Log Probability Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.laplace logpdf.",
      "The Laplace probability distribution function is defined as f(x) = (1/2) * e^(-|x|).",
      "The function evaluates the PDF at a given value x.",
      "The function accepts loc as the distribution offset parameter.",
      "The function accepts scale as the distribution scale parameter.",
      "The function returns an array of logpdf values.",
      "See also jax.scipy.stats.laplace.cdf()",
      "See also jax.scipy.stats.laplace.pdf()"
    ],
    "code_examples": []
  },
  {
    "title": "Laplace Probability Distribution Function",
    "concepts": [
      "The document describes the Laplace probability distribution function.",
      "The function is implemented in JAX.",
      "The probability density function (PDF) is defined as f(x) = (1/2) * e^(-|x|).",
      "The function takes x, loc (offset), and scale parameters as input.",
      "It returns an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Logistic Cumulative Distribution Function (CDF)",
    "concepts": [
      "JAX implementation of scipy.stats.logistic cdf.",
      "The CDF is the integral of the probability density function (PDF) from negative infinity to x.",
      "x: value at which to evaluate the CDF.",
      "loc: distribution offset parameter.",
      "scale: distribution scale parameter.",
      "Returns an array of CDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Logistic Distribution Inverse Survival Function",
    "concepts": [
      "Calculates the inverse of the survival function for the logistic distribution.",
      "Uses the jax.scipy.stats.logistic.sf() function.",
      "x represents the value at which to evaluate the ISF.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "Returns an array of ISF values.",
      "Utilizes JAX for implementation."
    ],
    "code_examples": []
  },
  {
    "title": "Logistic Log Probability Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.logistic logpdf.",
      "The logistic probability distribution function is defined as f(x) = e^{-x} / (1 + e^{-x})^2.",
      "x is the value at which to evaluate the PDF.",
      "a is the distribution shape parameter.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "The function returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Logistic Probability Distribution Function",
    "concepts": [
      "The document describes the logistic probability distribution function.",
      "It provides a JAX implementation of the scipy.stats.logistic pdf.",
      "The logistic PDF is defined by the formula f(x) = e^{-x} / (1 + e^{-x})^2.",
      "The function takes x, loc, and scale as input parameters.",
      "It returns an array of PDF values.",
      "Related functions include cdf, sf, isf, logpdf, and ppf."
    ],
    "code_examples": []
  },
  {
    "title": "Logistic Distribution Percent Point Function",
    "concepts": [
      "The percent point function (PPF) is the inverse of the cumulative distribution function (CDF).",
      "The function evaluates the PPF for a logistic distribution.",
      "The function takes x, loc (offset), and scale parameters as input.",
      "It returns an array of PPF values.",
      "The function is a JAX implementation of the scipy.stats.logistic PPF."
    ],
    "code_examples": []
  },
  {
    "title": "Logistic Distribution Survival Function",
    "concepts": [
      "The survival function (SF) is defined as 1 - CDF.",
      "The SF is evaluated at a given value (x) with location (loc) and scale (scale) parameters.",
      "It returns an array of SF values.",
      "The implementation uses JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Multinomial Probability Mass Function",
    "concepts": [
      "The multinomial probability distribution is defined.",
      "The formula for the multinomial PMF is provided: f(x, n, p) = n! * product(p_i^x_i / x_i!)",
      "x represents the value at which to evaluate the PMF.",
      "n represents the distribution shape parameter, which is the sum of x_i.",
      "p represents the distribution shape parameter.",
      "The function returns an array of PMF values.",
      "jax.scipy.stats.multinomial.logpmf() is related function."
    ],
    "code_examples": []
  },
  {
    "title": "Multivariate Normal Log Probability Distribution Function",
    "concepts": [
      "Implements the logpdf of a multivariate normal distribution.",
      "Uses JAX to implement scipy.stats.multivariate_normal logpdf.",
      "The formula for the multivariate normal PDF is provided.",
      "Defines the parameters: x (value), mean (centroid), and cov (covariance matrix).",
      "allow_singular is not supported."
    ],
    "code_examples": []
  },
  {
    "title": "Multivariate Normal PDF Definition",
    "concepts": [
      "Definition of the multivariate normal probability density function (PDF).",
      "Formula for calculating the multivariate normal PDF.",
      "Explanation of the parameters: mean (\u03bc), covariance matrix (\u03a3), and rank (k).",
      "Description of the input variable x as the point at which to evaluate the PDF."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters Description",
    "concepts": [
      "x is the arraylike value at which to evaluate the PDF.",
      "mean is the arraylike centroid of distribution.",
      "cov is the arraylike covariance matrix of distribution.",
      "allow_singular is not supported.",
      "The function returns an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Negative-Binomial Log Probability Mass Function",
    "concepts": [
      "Describes the negative-binomial log probability mass function.",
      "It is a JAX implementation of scipy.stats.nbinom logpmf.",
      "Formula: f(k) = {{k+n-1} choose {n-1}}p^n(1-p)^k for k >= 0 and 0 <= p <= 1.",
      "k: value at which to evaluate the PMF.",
      "n: distribution shape parameter.",
      "p: distribution shape parameter.",
      "loc: distribution offset parameter.",
      "Returns an array of logpdf values.",
      "See also jax.scipy.stats.nbinom.pmf()"
    ],
    "code_examples": []
  },
  {
    "title": "Negative-binomial Probability Mass Function",
    "concepts": [
      "The document describes the negative-binomial probability mass function (PMF).",
      "It provides a JAX implementation of the SciPy stats.nbinom PMF.",
      "The PMF formula is f(k) = ((k+n-1) choose (n-1)) * p^n * (1-p)^k.",
      "k is the value at which to evaluate the PMF.",
      "n is the distribution shape parameter.",
      "p is the distribution shape parameter and must be between 0 and 1.",
      "The function returns an array of PMF values.",
      "See also jax.scipy.stats.nbinom.logpmf()."
    ],
    "code_examples": []
  },
  {
    "title": "Normal Cumulative Distribution Function (CDF)",
    "concepts": [
      "JAX implementation of SciPy's norm CDF.",
      "The CDF is the integral of the PDF from negative infinity to x.",
      "The CDF is defined as the integral of the probability density function (PDF).",
      "Input 'x' is the value at which to evaluate the CDF.",
      "Input 'loc' is the distribution offset parameter.",
      "Input 'scale' is the distribution scale parameter.",
      "The output is an array of CDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Normal Log Cumulative Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.norm logcdf.",
      "The CDF is defined as the integral of the PDF from negative infinity to x.",
      "jax.scipy.stats.norm.pdf() is the probability density function.",
      "x is the value at which to evaluate the CDF.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "Returns an array of logcdf values.",
      "Related functions: jax.scipy.stats.norm.cdf(), jax.scipy.stats.norm.pdf(), jax.scipy.stats.norm.sf(), jax.scipy.stats.norm.logpdf(), jax.scipy.stats.norm.logsf(), jax.scipy.stats.norm.isf(), jax.scipy.stats.norm.ppf()"
    ],
    "code_examples": []
  },
  {
    "title": "Normal Distribution Percent Point Function",
    "concepts": [
      "JAX implementation of scipy.stats.norm ppf.",
      "The percent point function (PPF) is the inverse of the cumulative distribution function (CDF).",
      "The PPF is evaluated at a given value 'q'.",
      "The PPF takes 'loc' and 'scale' parameters for distribution offset and scale, respectively.",
      "The function returns an array of PPF values."
    ],
    "code_examples": []
  },
  {
    "title": "Normal Distribution Survival Function",
    "concepts": [
      "Defines the survival function (SF) for a normal distribution.",
      "The survival function is the complement of the cumulative distribution function (CDF): SF(x) = 1 - CDF(x).",
      "It uses jax.scipy.stats.norm.cdf() to calculate the CDF.",
      "x is the value at which to evaluate the SF",
      "loc is the distribution offset parameter",
      "scale is the distribution scale parameter"
    ],
    "code_examples": []
  },
  {
    "title": "Normal Distribution Log Survival Function",
    "concepts": [
      "JAX implementation of scipy.stats.norm logsf.",
      "The survival function is defined as 1 - cdf(x).",
      "cdf(x) is the cumulative distribution function.",
      "The function calculates the log of the survival function for a normal distribution.",
      "x is the value at which to evaluate the SF.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter."
    ],
    "code_examples": []
  },
  {
    "title": "Normal Distribution Inverse Survival Function",
    "concepts": [
      "JAX implementation of scipy.stats.norm isf.",
      "Returns the inverse of the survival function, jax.scipy.stats.norm.sf().",
      "The function takes x, loc, and scale as input parameters.",
      "The function returns an array of isf values."
    ],
    "code_examples": []
  },
  {
    "title": "Pareto Log Probability Distribution Function",
    "concepts": [
      "Describes the Pareto log probability density function.",
      "Provides a JAX implementation of scipy.stats.pareto logpdf.",
      "The Pareto probability density function is defined as  f(x, b) = bx^{-(b+1)} for x >= 1 and 0 for x < 1, where b > 0.",
      "x is the value at which to evaluate the PDF.",
      "b is the distribution shape parameter.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "Returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Pareto Probability Density Function",
    "concepts": [
      "The Pareto probability density function is defined.",
      "The formula for the Pareto PDF is provided.",
      "The Pareto PDF is defined for b > 0.",
      "The function takes parameters x, b, loc and scale.",
      "The function returns an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Poisson Log Probability Mass Function",
    "concepts": [
      "The Poisson probability mass function is defined as f(k) = e^{-\\mu} * (\\mu^k) / k!.",
      "The function is defined for k >= 0 and mu >= 0.",
      "k represents the value at which to evaluate the PMF.",
      "mu represents the distribution shape parameter.",
      "loc represents the distribution offset parameter.",
      "The function returns an array of logpmf values."
    ],
    "code_examples": []
  },
  {
    "title": "Poisson Probability Mass Function",
    "concepts": [
      "JAX implementation of the Poisson PMF.",
      "The Poisson PMF formula is f(k) = exp(-mu) * (mu^k) / k!.",
      "The PMF is defined for k >= 0 and mu >= 0.",
      "k is the value at which to evaluate the PMF.",
      "mu is the distribution shape parameter.",
      "loc is the distribution offset parameter."
    ],
    "code_examples": []
  },
  {
    "title": "Poisson Cumulative Distribution Function",
    "concepts": [
      "JAX implementation of SciPy's Poisson CDF.",
      "The CDF is the sum of the PMF from 0 to k.",
      "k is the value at which to evaluate the CDF.",
      "mu is the distribution shape parameter.",
      "loc is the distribution offset parameter.",
      "The function returns an array of CDF values.",
      "See also: jax.scipy.stats.poisson.pmf() and jax.scipy.stats.poisson.logpmf()."
    ],
    "code_examples": []
  },
  {
    "title": "Student's T Log Probability Distribution Function",
    "concepts": [
      "Implements the logpdf of the Student's T distribution using JAX.",
      "The probability distribution function is defined by a specific formula involving the gamma function.",
      "df represents degrees of freedom, following the scipy convention.",
      "The function takes x, df, loc, and scale as input parameters.",
      "Returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Student's T Probability Distribution Function",
    "concepts": [
      "Describes the Student's T probability distribution function.",
      "Provides the mathematical formula for the PDF.",
      "Explains the parameters: x, df (degrees of freedom), loc (offset), and scale.",
      "Refers to the gamma function.",
      "Mentions the JAX implementation of scipy.stats.t pdf."
    ],
    "code_examples": []
  },
  {
    "title": "Truncated Normal Cumulative Distribution Function",
    "concepts": [
      "Defines the truncated normal cumulative distribution function (CDF).",
      "Provides a JAX implementation of the scipy.stats.truncnorm CDF.",
      "The CDF is defined as the integral of the probability distribution function (PDF) from negative infinity to x.",
      "Uses jax.scipy.stats.truncnorm.pdf() as the PDF.",
      "x is the value at which to evaluate the CDF.",
      "a and b are distribution shape parameters.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "Returns an array of CDF values.",
      "References other related functions such as pdf, sf, logcdf, logpdf and logsf."
    ],
    "code_examples": []
  },
  {
    "title": "Truncated Normal Log Cumulative Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.truncnorm logcdf.",
      "The cdf is defined as the integral of the pdf from negative infinity to x.",
      "x is the value at which to evaluate the CDF.",
      "a and b are distribution shape parameters.",
      "loc is the distribution offset parameter.",
      "scale is the distribution scale parameter.",
      "The function returns an array of logcdf values.",
      "Related functions include cdf, pdf, sf, logpdf, and logsf."
    ],
    "code_examples": []
  },
  {
    "title": "Truncated Normal Log Probability Distribution Function",
    "concepts": [
      "JAX implements the truncated normal logpdf.",
      "The truncated normal distribution is defined as a normal distribution within the bounds [a, b] and 0 otherwise.",
      "a and b are specified in number of standard deviations from zero.",
      "JAX uses 'loc' for the centroid and 'scale' for the standard deviation.",
      "The function calculates the logpdf values for given inputs x, a, b, loc, and scale."
    ],
    "code_examples": []
  },
  {
    "title": "Truncated Normal Distribution Log Survival Function",
    "concepts": [
      "Defines the truncated normal distribution log survival function.",
      "Provides a JAX implementation of scipy.stats.truncnorm logsf.",
      "The survival function is defined as 1 - cdf(x).",
      "The function takes x, a, b, loc, and scale as input.",
      "Returns the array of logsf values."
    ],
    "code_examples": []
  },
  {
    "title": "Truncated Normal Probability Distribution Function",
    "concepts": [
      "The document describes the JAX implementation of the truncated normal probability distribution function (PDF).",
      "The truncated normal PDF is defined as a normal distribution between the bounds a and b, and 0 otherwise.",
      "a and b are specified in number of standard deviations from the centroid.",
      "JAX uses the scipy nomenclature of loc for the centroid and scale for the standard deviation.",
      "The function calculates the PDF values for given input x, shape parameters a and b, location parameter loc, and scale parameter scale."
    ],
    "code_examples": []
  },
  {
    "title": "Truncated Normal Distribution Log Survival Function",
    "concepts": [
      "The log survival function (logsf) of a truncated normal distribution is implemented in JAX.",
      "The survival function is defined as 1 - cdf(x), where cdf(x) is the cumulative distribution function.",
      "The function `jax.scipy.stats.truncnorm.logsf()` calculates the log survival function.",
      "The input parameters are x (value at which to evaluate the SF), a, b (shape parameters), loc (offset parameter), and scale (scale parameter)."
    ],
    "code_examples": []
  },
  {
    "title": "Uniform Probability Distribution Function",
    "concepts": [
      "JAX implementation of scipy.stats.uniform pdf.",
      "The uniform distribution pdf is defined as 1 for 0 <= x <= 1 and 0 otherwise.",
      "x: Value at which to evaluate the PDF.",
      "loc: Distribution offset parameter.",
      "scale: Distribution scale parameter.",
      "Returns an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Uniform Cumulative Distribution Function (CDF) Definition",
    "concepts": [
      "The CDF is the integral of the PDF from negative infinity to x.",
      "jax.scipy.stats.uniform.cdf is a JAX implementation of the SciPy uniform CDF.",
      "The CDF describes the probability that a real-valued random variable with a given probability distribution will be found at a value less than or equal to x.",
      "The PDF refers to probability distribution function, jax.scipy.stats.uniform.pdf()."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Uniform CDF",
    "concepts": [
      "x: Value at which to evaluate the CDF. It can be an array-like object of numbers or booleans.",
      "loc: Distribution offset parameter. It can be an array-like object of numbers or booleans.",
      "scale: Distribution scale parameter. It can be an array-like object of numbers or booleans."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value and Related Functions",
    "concepts": [
      "The function returns an array of CDF values.",
      "Related functions include jax.scipy.stats.uniform.pdf(), jax.scipy.stats.uniform.logpdf(), and jax.scipy.stats.uniform.ppf()."
    ],
    "code_examples": []
  },
  {
    "title": "Gaussian Kernel Density Estimator Overview",
    "concepts": [
      "JAX implementation of Gaussian KDE, similar to scipy.stats.gaussian_kde.",
      "Estimates probability density from a dataset.",
      "Supports 1D and 2D datasets.",
      "Allows for specifying bandwidth method (scott, silverman, scalar, or callable).",
      "Supports weights for the dataset."
    ],
    "code_examples": []
  },
  {
    "title": "Constructor (__init__) and Main Methods",
    "concepts": [
      "__init__(dataset, bw_method, weights): Initializes the Gaussian KDE.",
      "evaluate(points): Evaluates the KDE at given points.",
      "integrate_box(low_bounds, high_bounds, maxpts): Integrates the distribution over a box (not implemented in JAX version).",
      "integrate_box_1d(low, high): Integrates the distribution over given 1D limits.",
      "integrate_gaussian(mean, cov): Integrates the distribution weighted by a Gaussian.",
      "integrate_kde(other): Integrates the product of two Gaussian KDE distributions.",
      "logpdf(x): Calculates the log probability density function.",
      "pdf(x): Calculates the probability density function.",
      "resample(key, shape): Randomly samples a dataset from the estimated PDF.",
      "set_bandwidth(bw_method): Sets the bandwidth (not implemented in JAX version).",
      "tree_flatten(): Flattens the tree structure.",
      "tree_unflatten(aux_data, children): Unflattens the tree structure."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "d: Represents the dimensionality of the data.",
      "n: Represents the number of data points.",
      "neff: Represents the effective sample size.",
      "dataset: Stores the input dataset.",
      "weights: Stores the weights of the data points.",
      "covariance: Stores the covariance matrix.",
      "inv_cov: Stores the inverse of the covariance matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Gaussian KDE Evaluation",
    "concepts": [
      "Evaluate the Gaussian Kernel Density Estimation (KDE) on a set of points."
    ],
    "code_examples": []
  },
  {
    "title": "Gaussian Distribution Integration",
    "concepts": [
      "Integrate a distribution weighted by a Gaussian."
    ],
    "code_examples": []
  },
  {
    "title": "Integration Overview",
    "concepts": [
      "Integrating a distribution over specified limits."
    ],
    "code_examples": []
  },
  {
    "title": "Gaussian KDE Integration",
    "concepts": [
      "Integrating the product of two Gaussian Kernel Density Estimation (KDE) distributions."
    ],
    "code_examples": []
  },
  {
    "title": "Dataset Sampling",
    "concepts": [
      "Sampling a dataset from a probability density function (PDF).",
      "Using a PRNG key for random sampling.",
      "Specifying the batch shape of the sampled dataset."
    ],
    "code_examples": []
  },
  {
    "title": "Probability Density Function",
    "concepts": [
      "The document mentions the concept of a probability density function."
    ],
    "code_examples": []
  },
  {
    "title": "Log Probability Density Function",
    "concepts": [
      "The document mentions the concept of log probability density function."
    ],
    "code_examples": []
  },
  {
    "title": "Von Mises Probability Distribution Function",
    "concepts": [
      "The von Mises distribution is implemented in JAX.",
      "The probability density function (PDF) is defined as f(x, \u03ba) = (1 / (2\u03c0I0(\u03ba))) * e^(\u03bacos x).",
      "I0 represents the modified Bessel function.",
      "kappa (\u03ba) is the distribution shape parameter and must be greater than or equal to 0.",
      "The distribution is normalized within the interval -\u03c0 \u2264 x \u2264 \u03c0.",
      "x is the value at which to evaluate the PDF.",
      "The function returns an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Wrapped Cauchy Log Probability Distribution Function",
    "concepts": [
      "The document describes the Wrapped Cauchy log probability distribution function.",
      "It is a JAX implementation of scipy.stats.wrapcauchy logpdf.",
      "The formula for the wrapped Cauchy probability distribution function is provided: f(x, c) = (1-c^2)/(2\u03c0(1+c^2-2c cos x)).",
      "The shape parameter 'c' must be between 0 and 1 (0 < c < 1).",
      "The domain of normalization is 0 <= x <= 2\u03c0.",
      "The function takes 'x' (value at which to evaluate the PDF) and 'c' (distribution shape parameter) as inputs.",
      "It returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Wrapped Cauchy Probability Distribution Function",
    "concepts": [
      "The wrapped Cauchy probability distribution function is defined.",
      "The formula for the wrapped Cauchy PDF is provided: f(x, c) = (1-c^2) / (2*pi*(1+c^2-2c*cos(x))).",
      "The parameter 'c' must be between 0 and 1.",
      "Normalization is on the domain 0 <= x <= 2*pi.",
      "The function takes 'x' as the value at which to evaluate the PDF.",
      "The function takes 'c' as the distribution shape parameter.",
      "The function returns an array of PDF values.",
      "Reference is made to the jax.scipy.stats.wrapcauchy.logpdf() function."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.lax",
    "concepts": [
      "jax.lax is a library of primitive operations that underpins libraries such as jax.numpy.",
      "Transformation rules are defined on jax.lax primitives.",
      "Many jax.lax primitives are thin wrappers around equivalent XLA operations.",
      "JAX diverges from XLA in some cases to ensure closure under JVP and transpose rules.",
      "It is recommended to use libraries like jax.numpy instead of jax.lax directly.",
      "The jax.numpy API is more stable and less likely to change than the jax.lax API."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Mathematical Operations",
    "concepts": [
      "abs(x): Elementwise absolute value.",
      "acos(x): Elementwise arc cosine.",
      "acosh(x): Elementwise inverse hyperbolic cosine.",
      "add(x, y): Elementwise addition.",
      "asin(x): Elementwise arc sine.",
      "asinh(x): Elementwise inverse hyperbolic sine.",
      "atan(x): Elementwise arc tangent.",
      "atan2(x, y): Elementwise two-term arc tangent.",
      "atanh(x): Elementwise inverse hyperbolic tangent.",
      "bessel_i0e(x): Exponentially scaled modified Bessel function of order 0.",
      "bessel_i1e(x): Exponentially scaled modified Bessel function of order 1.",
      "betainc(a, b, x): Elementwise regularized incomplete beta integral.",
      "cbrt(x): Elementwise cube root.",
      "ceil(x): Elementwise ceiling.",
      "clamp(min, x, max): Elementwise clamp.",
      "complex(x, y): Elementwise make complex number.",
      "conj(x): Elementwise complex conjugate function.",
      "cos(x): Elementwise cosine.",
      "cosh(x): Elementwise hyperbolic cosine.",
      "digamma(x): Elementwise digamma.",
      "div(x, y): Elementwise division.",
      "eq(x, y): Elementwise equals.",
      "erf(x): Elementwise error function.",
      "erfc(x): Elementwise complementary error function.",
      "erf_inv(x): Elementwise inverse error function.",
      "exp(x): Elementwise exponential.",
      "exp2(x): Elementwise base-2 exponential.",
      "expm1(x): Elementwise e^x - 1.",
      "floor(x): Elementwise floor.",
      "ge(x, y): Elementwise greater-than-or-equals.",
      "gt(x, y): Elementwise greater-than.",
      "igamma(a, x): Elementwise regularized incomplete gamma function.",
      "igammac(a, x): Elementwise complementary regularized incomplete gamma function.",
      "imag(x): Elementwise extract imaginary part.",
      "integer_pow(x, y): Elementwise power where y is a static integer.",
      "is_finite(x): Elementwise isfinite.",
      "le(x, y): Elementwise less-than-or-equals.",
      "lgamma(x): Elementwise log gamma.",
      "log(x): Elementwise natural logarithm.",
      "log1p(x): Elementwise log(1 + x).",
      "logistic(x): Elementwise logistic (sigmoid) function.",
      "lt(x, y): Elementwise less-than.",
      "max(x, y): Elementwise maximum.",
      "min(x, y): Elementwise minimum.",
      "mul(x, y): Elementwise multiplication.",
      "ne(x, y): Elementwise not-equals.",
      "neg(x): Elementwise negation.",
      "nextafter(x1, x2): Returns the next representable value after x1 in the direction of x2.",
      "polygamma(m, x): Elementwise polygamma.",
      "pow(x, y): Elementwise power.",
      "random_gamma_grad(a, x): Elementwise derivative of samples from Gamma(a, 1).",
      "real(x): Elementwise extract real part.",
      "reciprocal(x): Elementwise reciprocal.",
      "rem(x, y): Elementwise remainder.",
      "round(x): Elementwise round.",
      "rsqrt(x): Elementwise reciprocal square root.",
      "sign(x): Elementwise sign.",
      "sin(x): Elementwise sine.",
      "sinh(x): Elementwise hyperbolic sine.",
      "sqrt(x): Elementwise square root.",
      "square(x): Elementwise square.",
      "sub(x, y): Elementwise subtraction.",
      "tan(x): Elementwise tangent.",
      "tanh(x): Elementwise hyperbolic tangent.",
      "zeta(x, q): Elementwise Hurwitz zeta function."
    ],
    "code_examples": []
  },
  {
    "title": "Array Manipulation Operations",
    "concepts": [
      "broadcast(operand, sizes): Broadcasts an array, adding new leading dimensions.",
      "broadcast_in_dim(operand, shape, broadcast_dimensions): Wraps XLA's BroadcastInDim operator.",
      "broadcast_shapes(*shapes): Returns the shape that results from NumPy broadcasting of shapes.",
      "broadcast_to_rank(x, rank): Adds leading dimensions of 1 to give x rank rank.",
      "collapse(operand, start_dimension, length): Collapses dimensions of an array into a single dimension.",
      "concatenate(operands, dimension): Concatenates a sequence of arrays along dimension.",
      "conv(lhs, rhs, window_strides, padding): Convenience wrapper around conv_general_dilated.",
      "conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type): General n-dimensional convolution operator, with optional dilation.",
      "conv_transpose(lhs, rhs, strides, padding): Convenience wrapper for calculating the N-d convolution \"transpose\".",
      "dynamic_slice(operand, start_indices, slice_sizes): Wraps XLA's DynamicSlice operator.",
      "dynamic_update_slice(operand, update, start_indices): Wraps XLA's DynamicUpdateSlice operator.",
      "expand_dims(array, dimensions): Insert any number of size 1 dimensions into an array.",
      "pad(operand, padding_value, padding_config): Applies low, high, and/or interior padding to an array.",
      "reshape(operand, new_sizes, dimensions): Wraps XLA's Reshape operator.",
      "rev(operand, dimensions): Wraps XLA's Rev operator.",
      "slice(operand, start_indices, limit_indices): Wraps XLA's Slice operator.",
      "split(operand, sizes, axis): Splits an array along axis.",
      "squeeze(array, dimensions): Squeeze any number of size 1 dimensions from an array.",
      "transpose(operand, permutation): Wraps XLA's Transpose operator."
    ],
    "code_examples": []
  },
  {
    "title": "Reduction Operations",
    "concepts": [
      "cumlogsumexp(operand, axis, reverse): Computes a cumulative logsumexp along axis.",
      "cummax(operand, axis, reverse): Computes a cumulative maximum along axis.",
      "cummin(operand, axis, reverse): Computes a cumulative minimum along axis.",
      "cumprod(operand, axis, reverse): Computes a cumulative product along axis.",
      "cumsum(operand, axis, reverse): Computes a cumulative sum along axis.",
      "reduce(operands, init_values, computation, dimensions): Wraps XLA's Reduce operator.",
      "reduce_and(operand, axes): Compute the bitwise AND of elements over one or more array axes.",
      "reduce_max(operand, axes): Compute the maximum of elements over one or more array axes.",
      "reduce_min(operand, axes): Compute the minimum of elements over one or more array axes.",
      "reduce_or(operand, axes): Compute the bitwise OR of elements over one or more array axes.",
      "reduce_prod(operand, axes): Compute the product of elements over one or more array axes.",
      "reduce_sum(operand, axes): Compute the sum of elements over one or more array axes.",
      "reduce_window(operand, init_value, computation, window_dimensions, window_strides, padding): Wraps XLA's ReduceWindowWithGeneralPadding operator.",
      "reduce_xor(operand, axes): Compute the bitwise XOR of elements over one or more array axes."
    ],
    "code_examples": []
  },
  {
    "title": "Linear Algebra Operations",
    "concepts": [
      "batch_matmul(lhs, rhs, precision): Batch matrix multiplication.",
      "dot(lhs, rhs, precision, preferred_element_type): Vector/vector, matrix/vector, and matrix/matrix multiplication.",
      "dot_general(lhs, rhs, dimension_numbers, precision, preferred_element_type): General dot product/contraction operator.",
      "cholesky(x, symmetrize_input): Cholesky decomposition.",
      "eig(x, compute_left_eigenvectors, compute_right_eigenvectors): Eigendecomposition of a general matrix.",
      "eigh(x, lower, symmetrize_input): Eigendecomposition of a Hermitian matrix.",
      "lu(x): LU decomposition with partial pivoting.",
      "qr(): QR decomposition.",
      "svd(): Singular value decomposition.",
      "triangular_solve(a, b, left_side, lower, unit_diagonal, transpose_a): Triangular solve."
    ],
    "code_examples": []
  },
  {
    "title": "Control Flow Operations",
    "concepts": [
      "associative_scan(fn, elems, reverse, axis): Performs a scan with an associative binary operation, in parallel.",
      "cond(pred, true_fun, false_fun, operands, linear_solve_fun): Conditionally apply true_fun or false_fun.",
      "fori_loop(lower, upper, body_fun, init_val): Loop from lower to upper by reduction to jax.lax.while_loop().",
      "map(f, xs, batch_size): Map a function over leading array axes.",
      "scan(f, init, xs, length, reverse, unroll, num_consts): Scan a function over leading array axes while carrying along state.",
      "select(pred, on_true, on_false): Selects between two branches based on a boolean predicate.",
      "select_n(which, cases): Selects array values from multiple cases.",
      "switch(index, branches, operands, operand): Apply exactly one of the branches given by index.",
      "while_loop(cond_fun, body_fun, init_val): Call body_fun repeatedly in a loop while cond_fun is True.",
      "stop_gradient(x): Stops gradient computation."
    ],
    "code_examples": []
  },
  {
    "title": "Custom Derivatives",
    "concepts": [
      "custom_linear_solve(matvec, b, solve, transpose_matvec, implicit_diff): Perform a matrix-free linear solve with implicitly defined gradients.",
      "custom_root(f, initial_guess, solve, tangent_solve, implicit_diff): Differentiably solve for the roots of a function."
    ],
    "code_examples": []
  },
  {
    "title": "Parallel Communication Primitives",
    "concepts": [
      "all_gather(x, axis_name, tiled): Gather values of x across all replicas.",
      "all_to_all(x, axis_name, split_axis, concat_axis, tiled): Materialize the mapped axis and map a different axis.",
      "psum(x, axis_name, axis_index_groups): Compute an all-reduce sum on x over the pmapped axis axis_name.",
      "psum_scatter(x, axis_name, scatter_axis, tiled): Like psum(x, axis_name) but each device retains only part of the result.",
      "pmax(x, axis_name, axis_index_groups): Compute an all-reduce max on x over the pmapped axis axis_name.",
      "pmin(x, axis_name, axis_index_groups): Compute an all-reduce min on x over the pmapped axis axis_name.",
      "pmean(x, axis_name, axis_index_groups): Compute an all-reduce mean on x over the pmapped axis axis_name.",
      "ppermute(x, axis_name, perm): Perform a collective permutation according to the permutation perm.",
      "pshuffle(x, axis_name, perm): Convenience wrapper of jax.lax.ppermute with alternate permutation encoding",
      "pswapaxes(x, axis_name, axis, tiled): Swap the pmapped axis axis_name with the unmapped axis axis.",
      "axis_index(axis_name): Return the index along the mapped axis axis_name.",
      "with_sharding_constraint(x, shardings): Mechanism to constrain the sharding of an Array inside a jitted computation"
    ],
    "code_examples": []
  },
  {
    "title": "Helper Functions and Enums",
    "concepts": [
      "ConvDimensionNumbers: Describes batch, spatial, and feature dimensions of a convolution.",
      "DotAlgorithm: Specify the algorithm used for computing dot products.",
      "DotAlgorithmPreset: An enum of known algorithms for computing dot products.",
      "GatherDimensionNumbers: Describes the dimension number arguments to an XLA\u2019s Gather operator.",
      "GatherScatterMode: Describes how to handle out-of-bounds indices in a gather or scatter.",
      "Precision: Precision enum for lax matrix multiply related functions.",
      "RandomAlgorithm: Describes which PRNG algorithm to use for rng_bit_generator.",
      "RoundingMethod: Rounding strategies for handling halfway values (e.g., 0.5) in jax.lax.round().",
      "ScatterDimensionNumbers: Describes the dimension number arguments to an XLA\u2019s Scatter operator."
    ],
    "code_examples": [
      {
        "description": "Accumulate two 16-bit floats using a 32-bit float accumulator:",
        "code": "algorithm = DotAlgorithm(\n    lhs_precision_type = np.float16,\n    rhs_precision_type = np.float16,\n    accumulation_type = np.float32,\n)\nlhs = jnp.array([1.0, 2.0, 3.0, 4.0], dtype = np.float16)\nrhs = jnp.array([1.0, 2.0, 3.0, 4.0], dtype = np.float16)\ndot(\nlhs,\nrhs,\nprecision = algorithm\n)\n"
      },
      {
        "description": "Or, equivalently, using a preset:",
        "code": "algorithm = DotAlgorithmPreset.F16_F16_F32\ndot(\nlhs,\nrhs,\nprecision = algorithm\n)\n"
      },
      {
        "description": "Presets can also be specified by name:",
        "code": "dot(\nlhs,\nrhs,\nprecision = \"F16_F16_F32\"\n)\n"
      },
      {
        "description": "The preferred_element_type parameter can be used to return the output without downcasting the accumulation type:",
        "code": "dot(\nlhs,\nrhs,\nprecision = \"F16_F16_F32\",\npreferred_element_type = np.float32\n)\n"
      },
      {
        "description": "Users can specify the preset using this Enum directly:",
        "code": "lhs = jnp.array([1.0, 2.0, 3.0, 4.0], dtype = np.float16)\nrhs = jnp.array([1.0, 2.0, 3.0, 4.0], dtype = np.float16)\nalgorithm = DotAlgorithmPreset.F16_F16_F32\ndot(\nlhs,\nrhs,\nprecision = algorithm\n)\n"
      },
      {
        "description": "or, equivalently, they can be specified by name:",
        "code": "dot(\nlhs,\nrhs,\nprecision = \"F16_F16_F32\"\n)\n"
      }
    ]
  },
  {
    "title": "Bitwise Operations",
    "concepts": [
      "bitwise_and(x, y): Elementwise AND.",
      "bitwise_not(x): Elementwise NOT.",
      "bitwise_or(x, y): Elementwise OR.",
      "bitwise_xor(x, y): Elementwise exclusive OR.",
      "clz(x): Elementwise count-leading-zeros.",
      "population_count(x): Elementwise popcount, count the number of set bits in each element.",
      "shift_left(x, y): Elementwise left shift.",
      "shift_right_arithmetic(x, y): Elementwise arithmetic right shift.",
      "shift_right_logical(x, y): Elementwise logical right shift."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Absolute Value",
    "concepts": [
      "Calculates the elementwise absolute value of an array.",
      "Maps directly to the stablehlo.abs operation.",
      "Input array must have a signed integer, floating-point, or complex data type.",
      "For complex numbers (a + ib), the absolute value is sqrt(a^2 + b^2)."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Arc Cosine",
    "concepts": [
      "The function calculates the elementwise arc cosine of an input array.",
      "The input array must have a floating-point or complex type.",
      "The output array has the same shape and dtype as the input array.",
      "The function lowers directly to the chlo.acos operation.",
      "Related functions include jax.lax.cos(), jax.lax.asin(), and jax.lax.atan()."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Inverse Hyperbolic Cosine",
    "concepts": [
      "Computes the elementwise inverse hyperbolic cosine of an array.",
      "The function lowers directly to the chlo.acosh operation.",
      "The input array must have floating-point or complex type.",
      "The output is an array of the same shape and dtype as the input."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Addition",
    "concepts": [
      "Performs elementwise addition of two arrays.",
      "The operation is denoted as x + y.",
      "Input x is an ArrayLike.",
      "Input y is an ArrayLike.",
      "The result is an Array."
    ],
    "code_examples": []
  },
  {
    "title": "XLA Token Merging",
    "concepts": [
      "Merges one or more XLA token values.",
      "Wraps the XLA AfterAll operator.",
      "Experimental feature."
    ],
    "code_examples": []
  },
  {
    "title": "Description of approx_min_k Function",
    "concepts": [
      "The function returns the approximate min k values and their indices of the operand.",
      "It is based on the algorithm described in https://arxiv.org/abs/2206.14286.",
      "The operand must be a floating number type.",
      "The k parameter specifies the number of min-k values to find.",
      "reduction_dimension specifies the dimension along which to search (default: -1).",
      "recall_target specifies the recall target for the approximation.",
      "reduction_input_size_override can override the size determined by operand[reduction_dim] for evaluating the recall.",
      "aggregate_to_topk controls whether the results are sorted or unsorted.",
      "The function returns a tuple of two arrays: the least k values and their indices.",
      "The arrays' dimensions are the same as the input operand except for the reduction_dimension."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Example: Nearest Neighbor Search with approx_min_k",
    "concepts": [
      "The approx_min_k function is suitable for nearest neighbor search.",
      "Using jit to compile the function can improve performance.",
      "The example demonstrates nearest neighbor search over the squared l2 distance."
    ],
    "code_examples": [
      {
        "description": "Nearest neighbor search over the squared l2 distance using approx_min_k with jit.",
        "code": "import functools\nimport jax\nimport numpy as np\n\n@functools.partial(jax.jit, static_argnames=[\"k\", \"recall_target\"])\ndef l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):\n    dists = half_db_norms - jax.lax.dot(qy, db.transpose())\n    return jax.lax.approx_min_k(dists, k=k, recall_target=recall_target)\n\nqy = jax.numpy.array(np.random.rand(50, 64))\ndb = jax.numpy.array(np.random.rand(1024, 64))\nhalf_db_norm_sq = jax.numpy.linalg.norm(db, axis=1)**2 / 2\ndists, neighbors = l2_ann(qy, db, half_db_norm_sq, k=10)\nimport functools\nimport jax\nimport numpy as np\n\n@functools.partial(jax.jit, static_argnames=[\"k\", \"recall_target\"])\ndef l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):\n    dists = half_db_norms - jax.lax.dot(qy, db.transpose())\n    return jax.lax.approx_min_k(dists, k=k, recall_target=recall_target)\n\nqy = jax.numpy.array(np.random.rand(50, 64))\ndb = jax.numpy.array(np.random.rand(1024, 64))\nhalf_db_norm_sq = jax.numpy.linalg.norm(db, axis=1)**2 / 2\ndists, neighbors = l2_ann(qy, db, half_db_norm_sq, k=10)"
      }
    ]
  },
  {
    "title": "Performance Optimization",
    "concepts": [
      "Calculating db^2/2 - dot(qy, db^T) is more efficient than calculating qy^2 - 2 dot(qy, db^T) + db^2 for finding nearest neighbors.",
      "The former uses less arithmetic and produces the same set of neighbors."
    ],
    "code_examples": []
  },
  {
    "title": "Index of Maximum Element",
    "concepts": [
      "Computes the index of the maximum element along a specified axis.",
      "The input array is referred to as 'operand'.",
      "The axis along which to compute the maximum index is specified by 'axis'.",
      "The data type of the output index array is specified by 'index_dtype'."
    ],
    "code_examples": []
  },
  {
    "title": "Minimum Element Index Computation",
    "concepts": [
      "Computes the index of the minimum element along a specified axis.",
      "Takes an ArrayLike operand as input.",
      "Specifies the axis along which to compute the minimum.",
      "Defines the data type of the index."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Arc Sine Function",
    "concepts": [
      "The function calculates the element-wise arc sine of an input array.",
      "It lowers directly to the chlo.asin operation.",
      "The input array must have a floating-point or complex type.",
      "The output array has the same shape and dtype as the input array.",
      "Related functions include elementwise sine, arc cosine, and arc tangent."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Inverse Hyperbolic Sine",
    "concepts": [
      "The function calculates the elementwise inverse hyperbolic sine of an array.",
      "The function \\(\\mathrm{asinh}(x)\\) is mathematically represented.",
      "It directly lowers to the chlo.asinh operation.",
      "The input array must have a floating-point or complex type.",
      "The output is an array of the same shape and dtype as the input array."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Arc Tangent (atan(x))",
    "concepts": [
      "The function calculates the elementwise arc tangent of an input array.",
      "It lowers directly to the chlo.atan operation.",
      "The input array must have a floating-point or complex type.",
      "The output is an array of the same shape and dtype as the input, containing the element-wise arc tangent."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Inverse Hyperbolic Tangent",
    "concepts": [
      "The function calculates the elementwise inverse hyperbolic tangent of an array.",
      "The function is named `atanh(x)`.",
      "The function lowers directly to the chlo.atanh operation.",
      "The input array `x` must have a floating-point or complex type.",
      "The output is an array with the same shape and dtype as `x`.",
      "The output array contains the element-wise inverse hyperbolic tangent."
    ],
    "code_examples": []
  },
  {
    "title": "Batch Matrix Multiplication",
    "concepts": [
      "Batch matrix multiplication operation is performed.",
      "The operation takes two arrays, lhs and rhs, as input.",
      "An optional precision parameter can be specified.",
      "The result is an array."
    ],
    "code_examples": []
  },
  {
    "title": "Exponentially Scaled Modified Bessel Function of Order 0",
    "concepts": [
      "Definition of i0e(x): i0e(x) = exp(-|x|) * i0(x)",
      "x is the input argument",
      "x can be an array, ndarray, bool, number, int, float, or complex"
    ],
    "code_examples": []
  },
  {
    "title": "Definition of i1e(x)",
    "concepts": [
      "The function i1e(x) is an exponentially scaled modified Bessel function of order 1.",
      "i1e(x) is defined as e^{-|x|} * i1(x)."
    ],
    "code_examples": []
  },
  {
    "title": "Input Parameter x",
    "concepts": [
      "The input x can be an array, ndarray, bool, number, int, float, or complex type."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Regularized Incomplete Beta Integral",
    "concepts": [
      "The function calculates the elementwise regularized incomplete beta integral.",
      "The parameter 'a' represents the first shape parameter and can be an array, ndarray, boolean, number, integer, float, or complex number.",
      "The parameter 'b' represents the second shape parameter and can be an array, ndarray, boolean, number, integer, float, or complex number.",
      "The parameter 'x' represents the upper limit of integration and can be an array, ndarray, boolean, number, integer, float, or complex number.",
      "The function returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise NOT",
    "concepts": [
      "The function calculates the elementwise NOT (\\(\\neg x\\)) of an input array.",
      "It is implemented using the stablehlo.not operation.",
      "The input array must have a boolean or integer dtype.",
      "The output is an array with the same shape and dtype as the input, containing the bitwise inversion of each element.",
      "jax.numpy.invert() is a NumPy wrapper for this functionality and can also be accessed using the ~x operator on JAX arrays.",
      "Related functions include jax.lax.bitwise_and(), jax.lax.bitwise_or(), and jax.lax.bitwise_xor()."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise OR",
    "concepts": [
      "The function computes the elementwise OR of two arrays.",
      "The function lowers directly to the stablehlo.or operation.",
      "Input arrays must have matching boolean or integer dtypes.",
      "If neither input is a scalar, the inputs must have the same number of dimensions and be broadcast compatible.",
      "The function returns an array containing the bitwise OR of each pair of broadcasted entries with the same dtype as the inputs.",
      "See also: jax.numpy.invert(), jax.lax.bitwise_not(), jax.lax.bitwise_and(), jax.lax.bitwise_xor()"
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Popcount Description",
    "concepts": [
      "Elementwise popcount counts set bits in each element.",
      "The input is an ArrayLike object named 'x'."
    ],
    "code_examples": []
  },
  {
    "title": "BroadcastInDim Operator",
    "concepts": [
      "Wraps XLA\u2019s BroadcastInDim operator.",
      "The function takes an array (operand), a target shape, and broadcast dimensions as input.",
      "The broadcast_dimensions specify how dimensions of the operand map to dimensions of the result.",
      "It returns an array containing the result of the broadcast operation.",
      "jax.lax.broadcast provides a simpler interface for adding new leading dimensions."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jnp.broadcast_shapes",
    "concepts": [
      "The function returns the shape resulting from NumPy broadcasting of input shapes.",
      "It follows the rules of NumPy broadcasting.",
      "The function takes one or more tuples of integers representing array shapes.",
      "It returns a tuple of integers representing the broadcasted shape.",
      "It raises ValueError if the shapes are not broadcast-compatible."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of Compatible Shapes",
    "concepts": [
      "Demonstration of broadcasting compatible shapes using jnp.broadcast_shapes."
    ],
    "code_examples": [
      {
        "description": "Broadcasting (1,) and (4,).",
        "code": "jnp.broadcast_shapes((1,), (4,))"
      },
      {
        "description": "Broadcasting (3, 1) and (4,).",
        "code": "jnp.broadcast_shapes((3, 1), (4,))"
      },
      {
        "description": "Broadcasting (3, 1), (1, 4), and (5, 1, 1).",
        "code": "jnp.broadcast_shapes((3, 1), (1, 4), (5, 1, 1))"
      },
      {
        "description": "Broadcasting (1,) and (4,).",
        "code": "jnp.broadcast_shapes((1,), (4,))"
      },
      {
        "description": "Broadcasting (3, 1) and (4,).",
        "code": "jnp.broadcast_shapes((3, 1), (4,))"
      },
      {
        "description": "Broadcasting (3, 1), (1, 4), and (5, 1, 1).",
        "code": "jnp.broadcast_shapes((3, 1), (1, 4), (5, 1, 1))"
      }
    ]
  },
  {
    "title": "Example of Incompatible Shapes",
    "concepts": [
      "Demonstration of a ValueError raised when attempting to broadcast incompatible shapes using jnp.broadcast_shapes."
    ],
    "code_examples": [
      {
        "description": "Attempting to broadcast (3, 1) and (4, 1) results in a ValueError.",
        "code": "jnp.broadcast_shapes((3, 1), (4, 1))"
      },
      {
        "description": "Attempting to broadcast (3, 1) and (4, 1) results in a ValueError.",
        "code": "jnp.broadcast_shapes((3, 1), (4, 1))"
      }
    ]
  },
  {
    "title": "Adding Leading Dimensions",
    "concepts": [
      "Adds leading dimensions of 1 to an array.",
      "Input array is denoted as 'x'.",
      "The target rank of the array after adding dimensions is denoted as 'rank'."
    ],
    "code_examples": []
  },
  {
    "title": "iota Convenience Wrapper",
    "concepts": [
      "Provides a convenience wrapper around the iota function."
    ],
    "code_examples": []
  },
  {
    "title": "dtype",
    "concepts": [
      "dtype represents the data type of the array elements."
    ],
    "code_examples": []
  },
  {
    "title": "shape",
    "concepts": [
      "shape represents the dimensions of the array."
    ],
    "code_examples": []
  },
  {
    "title": "dimension",
    "concepts": [
      "dimension represents the number of dimensions in the array.",
      "It is an integer value."
    ],
    "code_examples": []
  },
  {
    "title": "Array",
    "concepts": [
      "This refers to the array object created."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Ceiling Function",
    "concepts": [
      "The function calculates the ceiling of each element in an array.",
      "The input array must have a floating-point type.",
      "The output array has the same shape and dtype as the input array.",
      "The function is lowered to the stablehlo.ceil operation.",
      "jax.lax.floor() rounds to the next integer toward negative infinity.",
      "jax.lax.round() rounds to the nearest integer."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Clamp",
    "concepts": [
      "Elementwise clamp function restricts values within a specified range [min, max].",
      "If x is less than min, the function returns min.",
      "If x is greater than max, the function returns max.",
      "Otherwise, the function returns x.",
      "The function takes three ArrayLike parameters: min, x, and max."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Count Leading Zeros",
    "concepts": [
      "The function calculates the number of leading zero bits for each element in an array.",
      "The input is an ArrayLike object named 'x'."
    ],
    "code_examples": []
  },
  {
    "title": "Collapsing Array Dimensions",
    "concepts": [
      "Collapses dimensions of an array into a single dimension.",
      "The collapsed dimensions are laid out major-to-minor.",
      "The lowest-numbered dimension varies the slowest."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Complex Number Creation",
    "concepts": [
      "Creates a complex number from real and imaginary parts.",
      "Lowers directly to the stablehlo.complex operation.",
      "Input arrays (x, y) must have matching floating-point dtypes.",
      "If inputs are not scalar, they must have the same number of dimensions and be broadcast-compatible.",
      "The resulting complex array has a dtype of complex64 for float32 inputs and complex128 for float64 inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Array Concatenation",
    "concepts": [
      "Concatenation of arrays along a specified dimension.",
      "Uses XLA's Concatenate operator.",
      "Operands must be a sequence of arrays with equal shapes, except in the concatenation dimension.",
      "A dimension must be specified for concatenation."
    ],
    "code_examples": []
  },
  {
    "title": "Convolution Wrapper",
    "concepts": [
      "The function is a convenience wrapper around conv_general_dilated.",
      "lhs is a rank n+2 dimensional input array.",
      "rhs is a rank n+2 dimensional array of kernel weights.",
      "window_strides is a sequence of n integers representing inter-window strides.",
      "padding can be 'SAME' or 'VALID'.",
      "precision can be None, Precision.DEFAULT, Precision.HIGH, Precision.HIGHEST, or a tuple of two Precision enums.",
      "preferred_element_type can be None or a datatype for accumulation and result type."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Cast",
    "concepts": [
      "Wraps XLA\u2019s ConvertElementType operator.",
      "Performs elementwise conversion from one type to another.",
      "Similar to a C++ static_cast.",
      "The output array has the same shape as the input array.",
      "The output array is cast elementwise to the new dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Convolution Dimension Conversion",
    "concepts": [
      "Converts convolution dimension_numbers to a ConvDimensionNumbers object.",
      "The function takes lhs_shape (input shape), rhs_shape (kernel shape), and dimension_numbers as input.",
      "dimension_numbers can be None, a tuple/list of strings, or a ConvDimensionNumbers object.",
      "The function returns a ConvDimensionNumbers object in the canonical form used by lax functions."
    ],
    "code_examples": []
  },
  {
    "title": "General N-Dimensional Convolution",
    "concepts": [
      "Describes a general n-dimensional convolution operator.",
      "It is a wrapper for XLA's Conv operator.",
      "`lhs` is the input array with rank n+2.",
      "rhs is the kernel weights array with rank n+2.",
      "`window_strides` defines the inter-window strides.",
      "`padding` specifies padding strategy such as 'SAME', 'SAME_LOWER', 'VALID', or custom (low, high) pairs.",
      "`lhs_dilation` defines dilation factors for the input array.",
      "`rhs_dilation` defines dilation factors for the kernel weights.",
      "`dimension_numbers` specifies how dimensions are mapped between input, kernel, and output.",
      "`feature_group_count` and `batch_group_count` are integers for grouping.",
      "`precision` specifies the numerical precision.",
      "`preferred_element_type` specifies the preferred data type for accumulation.",
      "The output is an array containing the convolution result."
    ],
    "code_examples": []
  },
  {
    "title": "Dimension Number Specification",
    "concepts": [
      "Describes how to specify dimension numbers for convolution.",
      "'N' represents batch dimensions.",
      "'C' represents feature dimensions in input and output.",
      "'I' and 'O' represent input and output feature dimensions in the kernel.",
      "Other distinct characters represent spatial dimension correspondences.",
      "Example: ('NCHW', 'OIHW', 'NCHW') is consistent with the conv function.",
      "Example: ('NHWC', 'HWIO', 'NHWC') is consistent with TensorFlow Conv2D.",
      "Window strides are associated with spatial dimension character labels in `rhs_spec`."
    ],
    "code_examples": []
  },
  {
    "title": "Default Dimension Numbers",
    "concepts": [
      "If `dimension_numbers` is None, the default is ('NCHW', 'OIHW', 'NCHW') for a 2D convolution."
    ],
    "code_examples": []
  },
  {
    "title": "General N-Dimensional Unshared Convolution",
    "concepts": [
      "Unshared convolution is also known as locally connected layer.",
      "It's equivalent to convolution with a separate RHS kernel used at each output spatial location.",
      "The `lhs` argument is the input array of rank n+2.",
      "The `rhs` argument is the kernel weights array of rank n+2.",
      "Unlike regular CNNs, `rhs` spatial coordinates correspond to output spatial locations.",
      "Input spatial locations are fused with input channel locations in `rhs`.",
      "The `window_strides` argument specifies inter-window strides.",
      "The `padding` argument specifies padding applied before and after each spatial dimension.",
      "The `filter_shape` argument defines the receptive window spatial shape.",
      "The `lhs_dilation` argument gives the dilation factor to apply in each spatial dimension of `lhs` (transposed convolution).",
      "The `rhs_dilation` argument gives the dilation factor to apply in each input spatial dimension of `rhs` (atrous convolution).",
      "The `dimension_numbers` argument specifies dimension correspondences between lhs, rhs, and output.",
      "The `precision` argument specifies the precision of computation.",
      "Dimension numbers can be specified using strings or a ConvDimensionNumbers object.",
      "When using strings for dimension numbers, characters represent batch, feature, input/output feature, and spatial dimensions.",
      "Window strides are associated with spatial dimensions according to their order in the `rhs_spec` string.",
      "If `dimension_numbers` is None, the default is ('NCHW', 'OIHW', 'NCHW') for a 2D convolution."
    ],
    "code_examples": []
  },
  {
    "title": "Extracting Patches with Convolution",
    "concepts": [
      "The function extracts patches using a general dilated convolution.",
      "The output channel dimension 'C' contains flattened image patches.",
      "The order of dimensions within 'C' is defined by the rhs_spec and lhs_spec.",
      "The size of the 'C' dimension is the product of the filter shape and the channel dimension of the input.",
      "The function uses jax.lax.conv_general_dilated for the convolution operation."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how the output channel dimension 'C' is constructed from the flattened image patches.",
        "code": "np.prod(filter_shape) * lhs.shape[lhs_spec.index(\u2018C\u2019)]"
      }
    ]
  },
  {
    "title": "Parameters of the Convolution Operation",
    "concepts": [
      "lhs: Input array of rank n+2.",
      "filter_shape: Receptive window shape (sequence of n integers).",
      "window_strides: Inter-window strides (sequence of n integers).",
      "padding: Padding applied before and after each spatial dimension ('SAME', 'VALID', or sequence of (low, high) pairs).",
      "lhs_dilation: Dilation factor for the input (LHS).",
      "rhs_dilation: Dilation factor for the filter (RHS).",
      "dimension_numbers: Specifies the dimension numbers for the input, filter, and output.",
      "precision: Specifies the precision of the computation.",
      "preferred_element_type: Specifies the preferred data type for accumulation and output."
    ],
    "code_examples": []
  },
  {
    "title": "Output of the Convolution Operation",
    "concepts": [
      "The function returns an array containing flattened image patches in the output channel 'C'.",
      "The output array has rank n+2.",
      "The dimension numbers determine the arrangement of the output dimensions.",
      "The size of dimension 'C' is equal to the size of each patch."
    ],
    "code_examples": []
  },
  {
    "title": "Convolution Wrapper",
    "concepts": [
      "The function is a convenience wrapper around conv_general_dilated.",
      "lhs is a rank n+2 dimensional input array.",
      "rhs is a rank n+2 dimensional array of kernel weights.",
      "window_strides is a sequence of n integers, representing the inter-window strides.",
      "padding can be 'SAME', 'VALID', or a sequence of n (low, high) integer pairs for padding before and after each spatial dimension.",
      "lhs_dilation is a sequence of n integers, giving the dilation factor to apply in each spatial dimension of lhs. It is also known as transposed convolution.",
      "rhs_dilation is a sequence of n integers, giving the dilation factor to apply in each spatial dimension of rhs. It is also known as atrous convolution.",
      "precision is an optional parameter to specify the precision of the computation.",
      "preferred_element_type is an optional parameter to specify the accumulation and result datatype.",
      "The function returns an array containing the convolution result."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Cosine Function",
    "concepts": [
      "Computes the elementwise cosine of an array.",
      "For floating-point inputs, it lowers directly to stablehlo.cosine.",
      "For complex inputs, it lowers to a sequence of HLO operations.",
      "Input must have floating-point or complex type.",
      "Returns an array of the same shape and dtype as the input.",
      "Related functions include sine, tangent, and arc cosine."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Hyperbolic Cosine",
    "concepts": [
      "Calculates the hyperbolic cosine of each element in an array.",
      "The input array must have a floating-point or complex type.",
      "The output array has the same shape and dtype as the input array.",
      "Lowers directly to the chlo.cosh operation.",
      "Related functions: jax.lax.acosh(), jax.lax.sinh(), jax.lax.tanh()"
    ],
    "code_examples": []
  },
  {
    "title": "Cumulative Logsumexp Computation",
    "concepts": [
      "Computes a cumulative logsumexp along a specified axis of an array.",
      "The input array is referred to as 'operand'.",
      "The axis along which to compute the cumulative logsumexp is specified by 'axis' (an integer).",
      "The 'reverse' parameter (boolean) controls the direction of the cumulative sum."
    ],
    "code_examples": []
  },
  {
    "title": "Cumulative Maximum Computation",
    "concepts": [
      "Computes a cumulative maximum along a specified axis of an array.",
      "The 'operand' is the input array.",
      "The 'axis' is an integer specifying the axis along which to compute the cumulative maximum.",
      "The 'reverse' parameter is a boolean that determines the direction of accumulation."
    ],
    "code_examples": []
  },
  {
    "title": "Cumulative Minimum Computation",
    "concepts": [
      "Computes the cumulative minimum of an array along a specified axis.",
      "The input array is referred to as the 'operand'.",
      "The 'axis' parameter specifies the axis along which the cumulative minimum is calculated.",
      "The 'reverse' parameter (boolean) controls the direction of the cumulative computation."
    ],
    "code_examples": []
  },
  {
    "title": "Cumulative Product Computation",
    "concepts": [
      "Computes a cumulative product along a specified axis of an array.",
      "operand is the input array.",
      "axis is the axis along which to compute the cumulative product.",
      "reverse indicates whether to perform the cumulative product in reverse order."
    ],
    "code_examples": []
  },
  {
    "title": "Cumulative Sum Computation",
    "concepts": [
      "Computes a cumulative sum along a specified axis.",
      "The input array is referred to as 'operand'.",
      "The axis along which to compute the cumulative sum is specified by 'axis' (an integer).",
      "The 'reverse' parameter (boolean) determines the direction of the cumulative sum."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Digamma Function",
    "concepts": [
      "The document introduces the elementwise digamma function, denoted as \u03c8(x).",
      "The input 'x' can be an array, ndarray, boolean, number, integer, float, or complex."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Division",
    "concepts": [
      "Elementwise division is represented as x / y.",
      "Integer division overflow can occur.",
      "Overflow happens when dividing by zero or dividing INT_SMIN by -1.",
      "Integer division overflow results in implementation-defined behavior.",
      "x and y are ArrayLike objects.",
      "The result is an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Dot Product Overview",
    "concepts": [
      "The function performs vector/vector, matrix/vector, and matrix/matrix multiplication.",
      "It wraps XLA's Dot operator.",
      "For more general contraction, see the jax.lax.dot_general() operator."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "lhs: An array of dimension 1 or 2.",
      "rhs: An array of dimension 1 or 2.",
      "precision: Controls the numerics of the computation. Can be None, a Precision enum value, a tuple of two Precision enums, a DotAlgorithm, or a DotAlgorithmPreset.",
      "preferred_element_type: Controls the data type output by the dot product. By default, the output element type will match the input element types under the usual type promotion rules.  When precision is not a DotAlgorithm or DotAlgorithmPreset , preferred_element_type provides a hint to the compiler to accumulate the dot product using this data type."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "An array containing the product."
    ],
    "code_examples": []
  },
  {
    "title": "dynamic_index_in_dim Function Overview",
    "concepts": [
      "The function is a convenience wrapper around dynamic_slice for integer indexing.",
      "It is similar to Python indexing syntax: operand[..., index].",
      "The operand is the array to slice.",
      "The index is the (possibly dynamic) start index.",
      "The axis is the axis along which to apply the slice (defaults to 0).",
      "keepdims specifies whether the output should have the same rank as the input (default = True).",
      "allow_negative_indices specifies whether negative indices are allowed; if true, they are relative to the end of the array.",
      "The function returns an array containing the slice."
    ],
    "code_examples": []
  },
  {
    "title": "One-Dimensional Examples",
    "concepts": [
      "Demonstrates the usage of dynamic_index_in_dim with a 1D array.",
      "Shows the effect of the keepdims parameter."
    ],
    "code_examples": [
      {
        "description": "Example using a 1D array and default keepdims.",
        "code": "x = jnp.arange(5)\ndynamic_index_in_dim(x, 1)"
      },
      {
        "description": "Example using a 1D array and keepdims set to False.",
        "code": "x = jnp.arange(5)\ndynamic_index_in_dim(x, 1, keepdims=False)"
      }
    ]
  },
  {
    "title": "Two-Dimensional Examples",
    "concepts": [
      "Demonstrates the usage of dynamic_index_in_dim with a 2D array.",
      "Shows the effect of the axis parameter.",
      "Shows the effect of the keepdims parameter."
    ],
    "code_examples": [
      {
        "description": "Example using a 2D array and specifying the axis.",
        "code": "x = jnp.arange(12).reshape(3, 4)\ndynamic_index_in_dim(x, 1, axis=1, keepdims=False)"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "Links to related functions: jax.numpy.ndarray.at, jax.lax.index_in_dim(), jax.lax.dynamic_slice(), jax.lax.dynamic_slice_in_dim()."
    ],
    "code_examples": []
  },
  {
    "title": "DynamicSlice Overview",
    "concepts": [
      "The function wraps XLA\u2019s DynamicSlice operator.",
      "It slices an array based on dynamic start indices and static slice sizes.",
      "Start indices can be dynamic, while slice sizes must be static inside JIT compiled functions.",
      "Negative indices can be allowed and interpreted relative to the end of the array."
    ],
    "code_examples": []
  },
  {
    "title": "DynamicSlice Examples",
    "concepts": [
      "Demonstrates a simple two-dimensional dynamic slice.",
      "Shows the behavior when the requested slice overruns the bounds of the array."
    ],
    "code_examples": [
      {
        "description": "Simple two-dimensional dynamic slice example.",
        "code": "x = jnp.arange(12).reshape(3, 4)\nprint(x)\nprint(dynamic_slice(x, (1, 1), (2, 3)))"
      },
      {
        "description": "Demonstrates behavior when the slice overruns the array bounds.",
        "code": "x = jnp.arange(12).reshape(3, 4)\nprint(x)\nprint(dynamic_slice(x, (1, 1), (2, 4)))"
      }
    ]
  },
  {
    "title": "Introduction to dynamic_slice_in_dim",
    "concepts": [
      "dynamic_slice_in_dim is a convenience wrapper around lax.dynamic_slice() applied to one dimension.",
      "It is equivalent to Python indexing syntax: operand[..., start_index:start_index + slice_size] along a specified axis.",
      "The function takes an array, a start index, a slice size, and an axis as input.",
      "It also takes a boolean flag allow_negative_indices to allow negative indices."
    ],
    "code_examples": []
  },
  {
    "title": "One-Dimensional Example",
    "concepts": [
      "Demonstrates dynamic_slice_in_dim with a 1D array.",
      "Shows how to extract a slice of size 3 starting from index 1."
    ],
    "code_examples": [
      {
        "description": "Extracts a slice of size 3 from a 1D array, starting at index 1.",
        "code": "x = jnp.arange(5)\ndynamic_slice_in_dim(x, 1, 3)"
      },
      {
        "description": "Extracts a slice of size 3 from a 1D array, starting at index 1 (repeated example).",
        "code": "x = jnp.arange(5)\ndynamic_slice_in_dim(x, 1, 3)"
      }
    ]
  },
  {
    "title": "Out-of-Bounds Clipping",
    "concepts": [
      "Illustrates how out-of-bound slices are clipped to the valid range.",
      "Shows that when the slice extends beyond the array bounds, it returns the available elements."
    ],
    "code_examples": [
      {
        "description": "Demonstrates clipping of out-of-bound slices, showing the available elements when starting at index 4 with a slice size of 3.",
        "code": "dynamic_slice_in_dim(x, 4, 3)"
      },
      {
        "description": "Demonstrates clipping of out-of-bound slices, showing the available elements when starting at index 4 with a slice size of 3 (repeated example).",
        "code": "dynamic_slice_in_dim(x, 4, 3)"
      }
    ]
  },
  {
    "title": "Two-Dimensional Example",
    "concepts": [
      "Demonstrates dynamic_slice_in_dim with a 2D array.",
      "Shows how to extract a slice of size 2 along axis 1, starting from index 1."
    ],
    "code_examples": [
      {
        "description": "Extracts a slice of size 2 along axis 1, starting from index 1 of a 2D array.",
        "code": "x = jnp.arange(12).reshape(3, 4)\nx\ndynamic_slice_in_dim(x, 1, 2, axis=1)"
      },
      {
        "description": "Extracts a slice of size 2 along axis 1, starting from index 1 of a 2D array (repeated example).",
        "code": "x = jnp.arange(12).reshape(3, 4)\nx\ndynamic_slice_in_dim(x, 1, 2, axis=1)"
      }
    ]
  },
  {
    "title": "DynamicUpdateSlice Overview",
    "concepts": [
      "Wraps XLA\u2019s DynamicUpdateSlice operator.",
      "Updates a slice of an array with new values.",
      "It takes an operand array, an update array, and start indices.",
      "The `allow_negative_indices` parameter controls how negative indices are handled."
    ],
    "code_examples": []
  },
  {
    "title": "One-Dimensional Slice Update",
    "concepts": [
      "Demonstrates updating a 1D slice using `dynamic_update_slice`.",
      "Shows how the update array is placed into the operand array at the specified start index.",
      "Illustrates the usage of `jnp.zeros` and `jnp.ones` to create example arrays."
    ],
    "code_examples": [
      {
        "description": "Updates a 1D slice with a new array.",
        "code": ">>> x = jnp.zeros(6)\n>>> y = jnp.ones(3)\n>>> dynamic_update_slice(x, y, (2,))\nArray([0., 0., 1., 1., 1., 0.], dtype=float32)"
      },
      {
        "description": "Shows how `dynamic_update_slice` automatically adjusts the index if the update slice is out of bounds.",
        "code": ">>> x = jnp.zeros(6)\n>>> y = jnp.ones(3)\n>>> dynamic_update_slice(x, y, (3,))\nArray([0., 0., 0., 1., 1., 1.], dtype=float32)\n>>> dynamic_update_slice(x, y, (5,))\nArray([0., 0., 0., 1., 1., 1.], dtype=float32)"
      }
    ]
  },
  {
    "title": "Two-Dimensional Slice Update",
    "concepts": [
      "Demonstrates updating a 2D slice using `dynamic_update_slice`.",
      "Illustrates how the update array is placed into the operand array at the specified start indices."
    ],
    "code_examples": [
      {
        "description": "Updates a 2D slice with a new array.",
        "code": ">>> x = jnp.zeros((4, 4))\n>>> y = jnp.ones((2, 2))\n>>> dynamic_update_slice(x, y, (1, 2))\nArray([[0., 0., 0., 0.],\n       [0., 0., 1., 1.],\n       [0., 0., 1., 1.],\n       [0., 0., 0., 0.]], dtype=float32)"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "References related functions for similar functionalities.",
      "jax.numpy.ndarray.at",
      "lax.dynamic_update_index_in_dim",
      "lax.dynamic_update_slice_in_dim"
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Equals: x = y",
    "concepts": [
      "The function lowers directly to the stablehlo.compare operation.",
      "Comparison direction is set to EQ.",
      "Compare type is set according to the input dtype.",
      "Input arrays must have matching dtypes.",
      "If neither input is a scalar, they must have the same number of dimensions and be broadcast compatible.",
      "Returns a boolean array of shape lax.broadcast_shapes(x.shape, y.shape) containing the elementwise equal comparison."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters and Return Value",
    "concepts": [
      "x and y are the input arrays (ArrayLike).",
      "The return value is a boolean array indicating elementwise equality."
    ],
    "code_examples": []
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.numpy.equal() : NumPy wrapper for this API, also accessible via the x == y operator on JAX arrays.",
      "jax.lax.ne() : elementwise not-equal",
      "jax.lax.ge() : elementwise greater-than-or-equal",
      "jax.lax.gt() : elementwise greater-than",
      "jax.lax.le() : elementwise less-than-or-equal",
      "jax.lax.lt() : elementwise less-than"
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Error Function",
    "concepts": [
      "The document introduces the elementwise error function, denoted as erf(x).",
      "The input 'x' can be an array, ndarray, boolean, number, integer, float, or complex number."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Complementary Error Function",
    "concepts": [
      "Definition of the complementary error function: erfc(x) = 1 - erf(x)."
    ],
    "code_examples": []
  },
  {
    "title": "Input Parameter x",
    "concepts": [
      "The input 'x' can be an array, ndarray, boolean, number, integer, float, or complex number."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Inverse Error Function",
    "concepts": [
      "The document describes the elementwise inverse error function, denoted as erf^{-1}(x).",
      "The input 'x' can be of various numerical types including Array, ndarray, bool, number, int, float, or complex."
    ],
    "code_examples": []
  },
  {
    "title": "Inserting Size 1 Dimensions into an Array",
    "concepts": [
      "The function inserts dimensions of size 1 into an array.",
      "It takes an array-like object as input.",
      "It also takes a sequence of integers representing the dimensions to be inserted."
    ],
    "code_examples": []
  },
  {
    "title": "fft_type",
    "concepts": [
      "Describes the type of FFT to be used.",
      "Can be of type FftType or a string."
    ],
    "code_examples": []
  },
  {
    "title": "fft_lengths",
    "concepts": [
      "Specifies the lengths of the FFT to be performed.",
      "It is a sequence (e.g., a list or tuple) of integers."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Floor Function",
    "concepts": [
      "The function calculates the floor of each element in an array.",
      "It lowers to the stablehlo.floor operation.",
      "The input array must have a floating-point type.",
      "The output array has the same shape and dtype as the input.",
      "Values are rounded to the next integer toward negative infinity.",
      "jax.lax.ceil() rounds to the next integer toward positive infinity.",
      "jax.lax.round() rounds to the nearest integer."
    ],
    "code_examples": []
  },
  {
    "title": "Array Creation with Fill Value",
    "concepts": [
      "Creates an array of a specified shape.",
      "Fills the array with a given fill value.",
      "Allows specifying the data type (dtype) of the array.",
      "Supports optional sharding for distributed arrays (note: sharding might be ignored in jitted mode)."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of the Gather Operator",
    "concepts": [
      "The gather() operator is a low-level operator that wraps XLA\u2019s Gather operator.",
      "Most JAX users should prefer using NumPy-style indexing or jax.numpy.ndarray.at() instead of calling gather() directly.",
      "The Gather operator takes slices from an operand array at specified start indices.",
      "It uses GatherDimensionNumbers to describe how the dimensions of the operand, start_indices, and the output relate."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Gather Operator",
    "concepts": [
      "operand: the array from which slices are taken.",
      "start_indices: the indices at which slices are taken.",
      "dimension_numbers: a GatherDimensionNumbers object that describes how dimensions of operand, start_indices and the output relate.",
      "slice_sizes: the size of each slice.",
      "indices_are_sorted: whether indices is known to be sorted.",
      "unique_indices: whether the elements gathered from operand are guaranteed not to overlap with each other.",
      "mode: how to handle indices that are out of bounds.",
      "fill_value: the fill value to return for out-of-bounds slices when mode is 'fill'."
    ],
    "code_examples": []
  },
  {
    "title": "Gather Operator Examples",
    "concepts": [
      "NumPy-style indexing is the preferred way to gather values from arrays.",
      "jax.numpy.ndarray.at syntax can be used for control over settings like indices_are_sorted, unique_indices, mode, and fill_value.",
      "Calling gather() directly is generally not recommended for typical users."
    ],
    "code_examples": [
      {
        "description": "Extracting values using NumPy-style indexing.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([10, 11, 12])\nindices = jnp.array([0, 1, 1, 2, 2, 2])\n\nx[indices]"
      },
      {
        "description": "Using jax.numpy.ndarray.at syntax for fine-grained control.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([10, 11, 12])\nindices = jnp.array([0, 1, 1, 2, 2, 2])\n\nx.at[indices].get(indices_are_sorted=True, mode=\"promise_in_bounds\")"
      },
      {
        "description": "Equivalent function call using gather() directly (not recommended).",
        "code": "import jax.numpy as jnp\nfrom jax import lax\n\nx = jnp.array([10, 11, 12])\nindices = jnp.array([0, 1, 1, 2, 2, 2])\n\nlax.gather(\n    x,\n    indices[:, None],\n    slice_sizes=(1,),\n    dimension_numbers=lax.GatherDimensionNumbers(\n        offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)\n    ),\n    indices_are_sorted=True,\n    mode=lax.GatherScatterMode.PROMISE_IN_BOUNDS,\n)"
      }
    ]
  },
  {
    "title": "Elementwise Greater-Than-Or-Equals",
    "concepts": [
      "Performs elementwise greater-than-or-equal comparison between two arrays.",
      "Lowers to the stablehlo.compare operation with comparison_direction=GE.",
      "Input arrays must have matching non-complex dtypes.",
      "If neither input is a scalar, the arrays must have the same number of dimensions and be broadcast compatible.",
      "Returns a boolean array with the result of the comparison."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Greater-Than Overview",
    "concepts": [
      "The function performs elementwise greater-than comparison (x > y).",
      "It lowers to the stablehlo.compare operation.",
      "The comparison direction is set to GT.",
      "The comparison type is set according to the input dtype.",
      "Inputs must have matching non-complex dtypes.",
      "If inputs are not scalars, they must have the same number of dimensions and be broadcast compatible.",
      "The output is a boolean array with the shape resulting from broadcasting the input shapes."
    ],
    "code_examples": []
  },
  {
    "title": "Input Parameters",
    "concepts": [
      "x is the first input array (ArrayLike).",
      "y is the second input array (ArrayLike).",
      "ArrayLike indicates that the inputs can be any JAX array-like object.",
      "Arrays must have matching non-complex dtypes.",
      "If neither input is a scalar, the arrays must have the same number of dimensions and be broadcast compatible."
    ],
    "code_examples": []
  },
  {
    "title": "Output",
    "concepts": [
      "The output is a boolean array.",
      "The shape of the output array is determined by broadcasting the shapes of the input arrays (lax.broadcast_shapes(x.shape, y.shape)).",
      "Each element in the output array indicates whether the corresponding element in x is greater than the corresponding element in y."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.greater() is the NumPy wrapper for this API.",
      "It is also accessible via the x > y operator on JAX arrays.",
      "jax.lax.eq() performs elementwise equal comparison.",
      "jax.lax.ne() performs elementwise not-equal comparison.",
      "jax.lax.ge() performs elementwise greater-than-or-equal comparison.",
      "jax.lax.le() performs elementwise less-than-or-equal comparison.",
      "jax.lax.lt() performs elementwise less-than comparison."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Regularized Incomplete Gamma Function",
    "concepts": [
      "The function computes the elementwise regularized incomplete gamma function.",
      "The function accepts `a` and `x` as input, which can be arrays, ndarrays, booleans, numbers, integers, floats, or complex numbers.",
      "The function returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Complementary Regularized Incomplete Gamma Function",
    "concepts": [
      "The function calculates the elementwise complementary regularized incomplete gamma function.",
      "The function takes two arguments: 'a' and 'x'.",
      "The arguments 'a' and 'x' can be of type Array, ndarray, bool, number, int, float, or complex.",
      "The function returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Extract Imaginary Part",
    "concepts": [
      "The function extracts the imaginary part of a complex number.",
      "The function lowers directly to the stablehlo.imag operation.",
      "The input array must have a complex dtype.",
      "The output array has the same shape as the input and contains the imaginary part.",
      "The output dtype is float32 for complex64 input and float64 for complex128 input."
    ],
    "code_examples": []
  },
  {
    "title": "Description of lax.index_in_dim",
    "concepts": [
      "lax.index_in_dim is a convenience wrapper around lax.slice() for integer indexing.",
      "It's equivalent to operand[..., index] on a specified axis.",
      "The function takes an array, an index, and an axis as input.",
      "The keepdims parameter controls whether the output preserves the input rank."
    ],
    "code_examples": []
  },
  {
    "title": "One-Dimensional Examples",
    "concepts": [
      "Demonstrates indexing a 1D JAX array using lax.index_in_dim.",
      "Illustrates the effect of the keepdims parameter."
    ],
    "code_examples": [
      {
        "description": "Indexing a 1D array with default keepdims=True.",
        "code": "x = jnp.arange(4)\nlax.index_in_dim(x, 2)"
      },
      {
        "description": "Indexing a 1D array with keepdims=False.",
        "code": "x = jnp.arange(4)\nlax.index_in_dim(x, 2, keepdims=False)"
      }
    ]
  },
  {
    "title": "Two-Dimensional Examples",
    "concepts": [
      "Demonstrates indexing a 2D JAX array using lax.index_in_dim.",
      "Illustrates indexing along different axes.",
      "Shows the impact of the keepdims parameter in higher dimensions."
    ],
    "code_examples": [
      {
        "description": "Creating a 2D array.",
        "code": "x = jnp.arange(12).reshape(3, 4)\nx"
      },
      {
        "description": "Indexing a 2D array along the default axis (axis=0) with default keepdims=True.",
        "code": "x = jnp.arange(12).reshape(3, 4)\nlax.index_in_dim(x, 1)"
      },
      {
        "description": "Indexing a 2D array along axis=1 with keepdims=False.",
        "code": "x = jnp.arange(12).reshape(3, 4)\nlax.index_in_dim(x, 1, axis=1, keepdims=False)"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "References related functions and modules for further exploration.",
      "jax.numpy.ndarray.at",
      "jax.lax.slice()",
      "jax.lax.slice_in_dim()",
      "jax.lax.dynamic_index_in_dim()"
    ],
    "code_examples": []
  },
  {
    "title": "Array Definition",
    "concepts": [
      "src is an Array.",
      "idxs is an Array.",
      "axes is a Sequence of integers."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Power with Static Integer Exponent",
    "concepts": [
      "Elementwise power operation (x^y) where y is a static integer.",
      "Lowering to a sequence of O[log2(y)] stablehlo.multiply operations.",
      "x is the input array (base) with a numerical dtype.",
      "y is a static scalar integer representing the exponent.",
      "Output array has the same shape and dtype as x."
    ],
    "code_examples": []
  },
  {
    "title": "Iota Operator Wrapper",
    "concepts": [
      "This section describes a wrapper for XLA's Iota operator.",
      "The wrapper uses `dtype` to specify the data type.",
      "The wrapper uses `size` to specify the size of the array.",
      "The wrapper returns an `Array`."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Less-Than-Or-Equals",
    "concepts": [
      "The function performs elementwise less-than-or-equals comparison (x <= y).",
      "It lowers to the stablehlo.compare operation with comparison_direction=LE.",
      "The compare_type is set according to the input dtype.",
      "Input arrays x and y must have matching non-complex dtypes.",
      "If neither x nor y is a scalar, they must have the same number of dimensions and be broadcast compatible.",
      "The output is a boolean array with shape lax.broadcast_shapes(x.shape, y.shape)."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Log Gamma Function",
    "concepts": [
      "The document introduces the elementwise log gamma function.",
      "log(\u0393(x)) represents the log gamma function.",
      "x can be an Array, ndarray, bool, number, int, float, or complex."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Natural Logarithm",
    "concepts": [
      "The function calculates the elementwise natural logarithm of an input array.",
      "The function lowers directly to the stablehlo.log operation.",
      "The input array must have a floating-point or complex type.",
      "The output is an array of the same shape and dtype as the input, containing the element-wise natural logarithm."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise log(1 + x)",
    "concepts": [
      "This function calculates the elementwise natural logarithm of x + 1.",
      "It lowers directly to the stablehlo.log_plus_one operation.",
      "It is more accurate for x near zero compared to lax.log(1 + x).",
      "The input array x must have floating-point or complex type.",
      "The output array has the same shape and dtype as x."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Less-Than Comparison",
    "concepts": [
      "The function performs an element-wise less-than comparison between two arrays.",
      "It lowers directly to the stablehlo.compare operation.",
      "The comparison_direction is set to LT.",
      "The compare_type is determined by the input dtype.",
      "Inputs must have matching non-complex dtypes.",
      "If neither input is a scalar, they must have the same number of dimensions and be broadcast compatible.",
      "The output is a boolean array.",
      "The output shape is determined by lax.broadcast_shapes(x.shape, y.shape)."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Maximum",
    "concepts": [
      "Calculates the elementwise maximum of two arrays, x and y.",
      "For complex numbers, the comparison is lexicographic based on (real, imaginary) pairs.",
      "The inputs x and y are ArrayLike objects.",
      "The output is an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Minimum",
    "concepts": [
      "Calculates the elementwise minimum between two arrays.",
      "For complex numbers, the comparison is lexicographic based on (real, imaginary) pairs."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Multiplication",
    "concepts": [
      "Elementwise multiplication of two arrays (x and y) is performed.",
      "The result is an array."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Not-Equals: x != y",
    "concepts": [
      "This function performs elementwise not-equal comparison between two arrays.",
      "It lowers directly to the stablehlo.compare operation.",
      "The comparison_direction is set to NE.",
      "The compare_type is set according to the input dtype.",
      "Input arrays must have matching dtypes.",
      "If neither input is a scalar, the arrays must have the same number of dimensions and be broadcast compatible.",
      "The output is a boolean array indicating elementwise inequality."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Negation",
    "concepts": [
      "Elementwise negation is represented as -x.",
      "It lowers to the stablehlo.negate operation.",
      "The function takes an ArrayLike input array.",
      "It returns an array of the same shape and dtype as the input, containing the element-wise negative.",
      "For unsigned integer inputs, the function returns 2 ** nbits - x, where nbits is the number of bits in the integer representation."
    ],
    "code_examples": []
  },
  {
    "title": "Description of nextafter Function",
    "concepts": [
      "The function returns the next representable value after x1 in the direction of x2.",
      "It lowers directly to the chlo.next_after operation.",
      "Inputs x1 and x2 must have matching floating-point dtypes and be broadcast-compatible.",
      "The return is an array of the same dtype and broadcasted shape as the inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Denormal Numbers and Flush-to-Zero Semantics",
    "concepts": [
      "Some environments use flush-denormal-to-zero semantics.",
      "Around zero, the function returns strictly non-zero values.",
      "These non-zero values may appear as zero in operations due to flush-to-zero.",
      "jnp.finfo.tiny can be used to get the smallest usable (normal) float."
    ],
    "code_examples": [
      {
        "description": "Demonstrates denormal numbers being representable initially but flushed to zero when multiplied by 1.",
        "code": "from jax import lax\n\nlax.nextafter(0.0, 1.0)\n# denormal numbers are representable\n# Array(1.e-45, dtype=float32, weak_type=True)\n\nlax.nextafter(0.0, 1.0) * 1\n# but are flushed to zero\n# Array(0., dtype=float32, weak_type=True)"
      },
      {
        "description": "Demonstrates denormal numbers being representable initially but flushed to zero when multiplied by 1 (repeated example).",
        "code": "from jax import lax\n\nlax.nextafter(0.0, 1.0)\n# denormal numbers are representable\n# Array(1.e-45, dtype=float32, weak_type=True)\n\nlax.nextafter(0.0, 1.0) * 1\n# but are flushed to zero\n# Array(0., dtype=float32, weak_type=True)"
      }
    ]
  },
  {
    "title": "Optimization Barriers",
    "concepts": [
      "Optimization barriers prevent the compiler from moving operations across the barrier.",
      "They ensure all inputs are evaluated before operators that depend on the barrier's outputs.",
      "Optimization barriers prevent common subexpression elimination.",
      "They prevent compiler fusions, ensuring operations before and after the barrier are not fused into the same kernel.",
      "JAX does not define derivative or batching rules for optimization barriers.",
      "Optimization barriers have no effect outside a compiled function."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how optimization_barrier prevents common-subexpression elimination between two calls to sin.",
        "code": "def f(x):\n  return jax.lax.optimization_barrier(jax.lax.sin(x)) + jax.lax.sin(x)\n\njax.jit(f)(0.)"
      },
      {
        "description": "Demonstrates how optimization_barrier prevents common-subexpression elimination between two calls to sin.",
        "code": "def f(x):\n  return jax.lax.optimization_barrier(jax.lax.sin(x)) + jax.lax.sin(x)\n\njax.jit(f)(0.)"
      }
    ]
  },
  {
    "title": "Overview of lax.pad",
    "concepts": [
      "Applies low, high, and/or interior padding to an array.",
      "Wraps XLA\u2019s Pad operator.",
      "The function accepts an array to be padded (operand), a padding value (padding_value), and a padding configuration (padding_config).",
      "The padding_config is a sequence of (low, high, interior) tuples specifying padding amounts for each dimension.",
      "The function returns the padded array."
    ],
    "code_examples": []
  },
  {
    "title": "1D Array Padding with Zeros",
    "concepts": [
      "Padding a 1-dimensional array with zeros.",
      "Specifying the number of zeros to add at the beginning and end of the array.",
      "The padding configuration is provided as a list of tuples, where each tuple corresponds to a dimension.",
      "The `lax.pad` function is used to apply the padding."
    ],
    "code_examples": [
      {
        "description": "Pads a 1D array with two zeros at the beginning and three zeros at the end.",
        "code": "from jax import lax\nimport jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4])\nresult = lax.pad(x, 0, [(2, 3, 0)])\nprint(result)"
      },
      {
        "description": "Pads a 1D array with two zeros at the beginning and three zeros at the end.",
        "code": "from jax import lax\nimport jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4])\nresult = lax.pad(x, 0, [(2, 3, 0)])\nprint(result)"
      }
    ]
  },
  {
    "title": "1D Array Padding with Interior Zeros",
    "concepts": [
      "Padding a 1-dimensional array with interior zeros.",
      "Inserting a zero between each element of the array.",
      "The interior padding is specified in the padding configuration tuple.",
      "The `lax.pad` function is used to apply the padding."
    ],
    "code_examples": [
      {
        "description": "Pads a 1D array by inserting a zero between each element.",
        "code": "from jax import lax\nimport jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4])\nresult = lax.pad(x, 0, [(0, 0, 1)])\nprint(result)"
      },
      {
        "description": "Pads a 1D array by inserting a zero between each element.",
        "code": "from jax import lax\nimport jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4])\nresult = lax.pad(x, 0, [(0, 0, 1)])\nprint(result)"
      }
    ]
  },
  {
    "title": "2D Array Padding with a Constant Value",
    "concepts": [
      "Padding a 2-dimensional array with a constant value.",
      "Specifying the padding size for each dimension.",
      "The padding configuration is a list of tuples, one for each dimension.",
      "The `lax.pad` function is used to apply the padding."
    ],
    "code_examples": [
      {
        "description": "Pads a 2D array with -1 as the padding value, with a pad size of 2 in each dimension.",
        "code": "from jax import lax\nimport jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\nresult = lax.pad(x, -1, [(2, 2, 0), (2, 2, 0)])\nprint(result)"
      },
      {
        "description": "Pads a 2D array with -1 as the padding value, with a pad size of 2 in each dimension.",
        "code": "from jax import lax\nimport jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n              [4, 5, 6]])\nresult = lax.pad(x, -1, [(2, 2, 0), (2, 2, 0)])\nprint(result)"
      }
    ]
  },
  {
    "title": "Platform-Specific Code with `platform_dependent`",
    "concepts": [
      "JAX determines the execution platform late in the process.",
      "Using Python conditionals for platform-dependent code is unsafe in JAX due to AOT lowering and serialization.",
      "The `platform_dependent` function allows staging out platform-specific code.",
      "All alternative branches in `platform_dependent` are traced and staged out to Jaxpr.",
      "The choice of execution branch is made during lowering or just before compilation.",
      "`platform_dependent` is similar to and implemented using `switch()`."
    ],
    "code_examples": [
      {
        "description": "Example usage of `platform_dependent` with CPU, TPU, and default branches.",
        "code": "def cpu_code(*args):\n  ...\n\ndef tpu_code(*args):\n  ...\n\ndef other_platforms_code(*args):\n  ...\n\nres = platform_dependent(*args, cpu=cpu_code, tpu=tpu_code, default=other_platforms_code)\n\ndef cpu_code(*args):\n  ...\n\ndef tpu_code(*args):\n  ...\n\ndef other_platforms_code(*args):\n  ...\n\nres = platform_dependent(*args, cpu=cpu_code, tpu=tpu_code, default=other_platforms_code)"
      }
    ]
  },
  {
    "title": "Elementwise Polygamma Function",
    "concepts": [
      "Defines the elementwise polygamma function denoted as \u03c8^(m)(x).",
      "The function takes two arguments: m and x.",
      "Both m and x can be an Array or ndarray, or a boolean, number, int, float or complex number."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Power Function",
    "concepts": [
      "Computes elementwise power (x^y).",
      "Lowers directly to stablehlo.pow operation.",
      "Handles dtype conversion using stablehlo.convert when necessary.",
      "Input 'x' must be a floating or complex type array.",
      "Input 'y' can be integer, floating, or complex type; it will be cast to x's dtype if needed.",
      "If 'x' and 'y' are not scalars, they must have the same number of dimensions and be broadcast-compatible.",
      "Returns an array of the same dtype as 'x' containing the elementwise power."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Derivative of Gamma Distribution Samples",
    "concepts": [
      "Calculates the elementwise derivative of samples from a Gamma distribution with a scale parameter of 1.",
      "The 'a' parameter represents the shape parameter of the Gamma distribution.",
      "The 'x' parameter represents the input samples."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Extract Real Part",
    "concepts": [
      "The function extracts the real part of a complex number.",
      "The function lowers directly to the stablehlo.real operation.",
      "The input array must have a complex dtype.",
      "The output array has the same shape as the input array.",
      "The output array has a float32 dtype if the input array has a complex64 dtype.",
      "The output array has a float64 dtype if the input array has a complex128 dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Reciprocal",
    "concepts": [
      "Calculates the reciprocal of each element in the input array.",
      "The reciprocal is defined as 1/x for each element x."
    ],
    "code_examples": []
  },
  {
    "title": "Bitwise AND Reduction",
    "concepts": [
      "Computes the bitwise AND of array elements over specified axes.",
      "The input array must have a boolean or integer dtype.",
      "The axes argument specifies the axes over which to reduce.",
      "The output is an array with the specified axes removed.",
      "Related to jax.numpy.bitwise_and.reduce() and jax.lax.reduce_and().",
      "Similar to other jax.lax reduction operators like reduce_sum, reduce_prod, etc."
    ],
    "code_examples": []
  },
  {
    "title": "Maximum Reduction over Array Axes",
    "concepts": [
      "Computes the maximum of elements over specified array axes.",
      "The `operand` is the array over which to compute the maximum.",
      "The `axes` is a sequence of integers specifying the axes to reduce over.",
      "Each axis must be within the valid range: 0 <= axis < operand.ndim.",
      "Returns an array with the same dtype as the operand.",
      "The shape of the result corresponds to the dimensions of operand.shape with axes removed."
    ],
    "code_examples": []
  },
  {
    "title": "Minimum Reduction of Array Elements",
    "concepts": [
      "Computes the minimum of elements over specified array axes.",
      "Input is an array-like object (operand).",
      "The 'axes' argument specifies the axes to reduce over.",
      "The output is an array with the specified axes removed from the shape.",
      "Output array has the same dtype as the input array (operand)."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.min() provides a more flexible NumPy-style min-reduction API.",
      "jax.numpy.min() is built around jax.lax.reduce_min().",
      "Other low-level jax.lax reduction operators: jax.lax.reduce_sum(), jax.lax.reduce_prod(), jax.lax.reduce_max(), jax.lax.reduce_and(), jax.lax.reduce_or(), jax.lax.reduce_xor()."
    ],
    "code_examples": []
  },
  {
    "title": "Bitwise OR Reduction",
    "concepts": [
      "Computes the bitwise OR of elements over specified array axes.",
      "The input array must have a boolean or integer dtype.",
      "The axes argument specifies the axes over which to reduce.",
      "The output array has the same dtype as the input array.",
      "The shape of the output array corresponds to the dimensions of the input array with the specified axes removed."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "operand: Input array for the reduction operation. Must have boolean or integer dtype.",
      "axes: Sequence of integers specifying the axes for reduction. Each axis must be within the bounds of the operand's dimensions."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "Returns an array with the bitwise OR result.",
      "The dtype is the same as the input array (operand).",
      "The shape is the same as the input array but with the reduced axes removed."
    ],
    "code_examples": []
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.numpy.bitwise_or.reduce(): Offers a more flexible NumPy-style logical reduction API.",
      "jax.lax.reduce_or(): Is the underlying primitive used by jax.numpy.bitwise_or.reduce().",
      "Other low-level jax.lax reduction operators: jax.lax.reduce_sum(), jax.lax.reduce_prod(), jax.lax.reduce_max(), jax.lax.reduce_min(), jax.lax.reduce_and(), jax.lax.reduce_xor()."
    ],
    "code_examples": []
  },
  {
    "title": "Reduce Precision Operator",
    "concepts": [
      "Wraps XLA's ReducePrecision operator.",
      "The operator takes a float or ArrayLike operand.",
      "It also takes exponent_bits and mantissa_bits as integer arguments.",
      "The return type is an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.lax.reduce_prod",
    "concepts": [
      "Computes the product of elements over specified array axes.",
      "Requires a numerical dtype for the input array (operand).",
      "The axes argument specifies the axes to reduce over.",
      "The result is an array with the specified axes removed from the shape.",
      "The output array has the same dtype as the input array.",
      "Unlike jax.numpy.prod(), jax.lax.reduce_prod() doesn't upcast narrow-width types, potentially leading to rounding errors."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.lax.reduce_sum",
    "concepts": [
      "Calculates the sum of array elements over specified axes.",
      "The operand must have a numerical data type.",
      "The axes argument specifies the axes to sum over; each axis must be a valid index for the operand.",
      "The result is an array with the same dtype as the operand, with the specified axes removed from the shape.",
      "It doesn't upcast narrow-width types, which may lead to rounding errors for 8-bit or 16-bit types."
    ],
    "code_examples": []
  },
  {
    "title": "Comparison with jax.numpy.sum",
    "concepts": [
      "jax.numpy.sum() provides a more flexible NumPy-style summation API.",
      "jax.numpy.sum() is built around jax.lax.reduce_sum()."
    ],
    "code_examples": []
  },
  {
    "title": "Related jax.lax Reduction Operators",
    "concepts": [
      "jax.lax.reduce_prod() calculates the product of elements over one or more array axes.",
      "jax.lax.reduce_max() calculates the maximum of elements over one or more array axes.",
      "jax.lax.reduce_min() calculates the minimum of elements over one or more array axes.",
      "jax.lax.reduce_and() calculates the logical AND of elements over one or more array axes.",
      "jax.lax.reduce_or() calculates the logical OR of elements over one or more array axes.",
      "jax.lax.reduce_xor() calculates the logical XOR of elements over one or more array axes."
    ],
    "code_examples": []
  },
  {
    "title": "ReduceWindowWithGeneralPadding Operator",
    "concepts": [
      "This section describes how to use the ReduceWindowWithGeneralPadding operator in XLA.",
      "The operator performs a reduction within sliding windows.",
      "It supports general padding configurations.",
      "The computation argument defines the reduction function.",
      "window_dimensions specifies the size of the window.",
      "window_strides determines the step size of the window.",
      "padding defines the padding applied to the input.",
      "base_dilation specifies the dilation factor for the base.",
      "window_dilation specifies the dilation factor for the window."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Remainder Operation",
    "concepts": [
      "Calculates the elementwise remainder of x divided by y.",
      "The sign of the result matches the dividend (x).",
      "The absolute value of the result is less than the divisor's (y) absolute value.",
      "Integer division overflow (remainder by zero or remainder of INT_SMIN with -1) results in implementation-defined behavior.",
      "x and y are ArrayLike objects."
    ],
    "code_examples": []
  },
  {
    "title": "Reshape Operator Overview",
    "concepts": [
      "The `reshape` function wraps XLA's Reshape operator.",
      "It is used to change the shape of an array.",
      "`lax.squeeze` and `lax.expand_dims` are preferred for inserting/removing dimensions of size 1.",
      "The size of the reshaped array must match the size of the input array."
    ],
    "code_examples": []
  },
  {
    "title": "Reshape Arguments",
    "concepts": [
      "`operand` is the array to be reshaped.",
      "`new_sizes` is a sequence of integers specifying the resulting shape.",
      "`dimensions` is an optional sequence specifying the permutation order of the input shape."
    ],
    "code_examples": []
  },
  {
    "title": "Simple Reshaping Example",
    "concepts": [
      "Reshaping a 1D array into a 2D array."
    ],
    "code_examples": [
      {
        "description": "Reshapes a 1D array of shape (6,) into a 2D array of shape (2, 3).",
        "code": "x = jnp.arange(6)\ny = reshape(x, (2, 3))\ny"
      },
      {
        "description": "Reshapes a 1D array of shape (6,) into a 2D array of shape (2, 3).",
        "code": "x = jnp.arange(6)\ny = reshape(x, (2, 3))\ny"
      }
    ]
  },
  {
    "title": "Reshaping Back to One Dimension",
    "concepts": [
      "Reshaping a 2D array back into a 1D array."
    ],
    "code_examples": [
      {
        "description": "Reshapes a 2D array `y` back to a 1D array of shape (6,).",
        "code": "reshape(y, (6,))"
      },
      {
        "description": "Reshapes a 2D array `y` back to a 1D array of shape (6,).",
        "code": "reshape(y, (6,))"
      }
    ]
  },
  {
    "title": "Reshaping with Dimension Permutation",
    "concepts": [
      "Reshaping and permuting dimensions of an array."
    ],
    "code_examples": [
      {
        "description": "Reshapes a 2D array `y` to a 1D array of shape (6,) with dimension permutation (1, 0).",
        "code": "reshape(y, (6,), (1, 0))"
      },
      {
        "description": "Reshapes a 2D array `y` to a 1D array of shape (6,) with dimension permutation (1, 0).",
        "code": "reshape(y, (6,), (1, 0))"
      }
    ]
  },
  {
    "title": "XLA Rev Operator",
    "concepts": [
      "Wraps XLA's Rev operator.",
      "The operator takes an operand (ArrayLike) and dimensions (Sequence[int]) as input.",
      "The operator returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Stateless PRNG Bit Generator",
    "concepts": [
      "This is an experimental feature and its use is discouraged.",
      "It returns uniformly distributed random bits with a specified shape and dtype.",
      "The dtype must be an integer type.",
      "It uses the platform-specific default algorithm or a specified one.",
      "It provides direct access to the RngBitGenerator primitive exposed by XLA.",
      "Most users should use jax.random for a stable API."
    ],
    "code_examples": []
  },
  {
    "title": "Stateful PRNG Generator Warning",
    "concepts": [
      "Stateful PRNG generator is experimental and discouraged.",
      "The generator returns uniformly distributed random numbers in the range [a, b).",
      "Result is undefined if b <= a.",
      "Use jax.random for most purposes.",
      "This API may be removed at any time."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Rounding with jax.lax.round",
    "concepts": [
      "The function rounds array values to the nearest integer.",
      "It lowers directly to the stablehlo.round operation.",
      "The input must be a floating-point array.",
      "The `rounding_method` parameter specifies how halfway values are rounded.",
      "The function returns an array of the same shape and dtype as the input.",
      "The default rounding method is AWAY_FROM_ZERO."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of `jax.lax.round` with the default `AWAY_FROM_ZERO` rounding method.",
        "code": "import jax.numpy as jnp\nfrom jax import lax\n\nx = jnp.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n\njax.lax.round(x)\n# defaults method is AWAY_FROM_ZERO"
      },
      {
        "description": "Demonstrates the use of `jax.lax.round` with the `TO_NEAREST_EVEN` rounding method.",
        "code": "import jax.numpy as jnp\nfrom jax import lax\n\nx = jnp.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n\njax.lax.round(x, rounding_method=jax.lax.RoundingMethod.TO_NEAREST_EVEN)"
      },
      {
        "description": "Demonstrates the use of `jax.lax.round` with the default `AWAY_FROM_ZERO` rounding method.",
        "code": "import jax.numpy as jnp\nfrom jax import lax\n\nx = jnp.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n\njax.lax.round(x)\n# defaults method is AWAY_FROM_ZERO"
      },
      {
        "description": "Demonstrates the use of `jax.lax.round` with the `TO_NEAREST_EVEN` rounding method.",
        "code": "import jax.numpy as jnp\nfrom jax import lax\n\nx = jnp.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n\njax.lax.round(x, rounding_method=jax.lax.RoundingMethod.TO_NEAREST_EVEN)"
      }
    ]
  },
  {
    "title": "Elementwise Reciprocal Square Root",
    "concepts": [
      "Calculates the elementwise reciprocal square root of an array (1 / sqrt(x)).",
      "The function lowers directly to the stablehlo.rsqrt operation.",
      "Input array must have a floating or complex dtype.",
      "Returns an array of the same shape and dtype as the input."
    ],
    "code_examples": []
  },
  {
    "title": "Scatter-Update Operator Overview",
    "concepts": [
      "The scatter operator replaces values in an operand array based on indices from another array.",
      "Multiple updates to the same index may be applied in any order.",
      "scatter() is a low-level operator.",
      "Using jax.numpy.ndarray.at() is the preferred approach for NumPy-style indexing.",
      "The function returns an array containing the sum of the operand and scattered updates.",
      "The operand is the array to be updated.",
      "scatter_indices specifies the indices in the operand to be updated.",
      "updates are the values to be scattered into the operand.",
      "dimension_numbers describes how dimensions of operand, scatter_indices, updates, and the output relate.",
      "indices_are_sorted indicates whether scatter_indices is sorted for potential performance improvements.",
      "unique_indices indicates whether updated elements in the operand do not overlap for potential performance improvements.",
      "mode specifies how to handle out-of-bounds indices ('clip', 'fill', 'drop', 'promise_in_bounds')."
    ],
    "code_examples": []
  },
  {
    "title": "Scatter-Update Examples",
    "concepts": [
      "jax.numpy.ndarray.at is the recommended way to perform scatter operations.",
      "jax.numpy.ndarray.at lowers to an XLA Scatter operation.",
      "scatter() can be used directly, but this is less common.",
      "The mode and indices_are_sorted arguments can also be passed to jax.numpy.ndarray.at."
    ],
    "code_examples": [
      {
        "description": "Updating entries in an array using jax.numpy.ndarray.at",
        "code": ">>> import jax.numpy as jnp\n>>> x = jnp.zeros(5)\n>>> indices = jnp.array([1, 2, 4])\n>>> values = jnp.array([2.0, 3.0, 4.0])\n>>> x.at[indices].set(values)\nArray([0., 2., 3., 0., 4.], dtype=float32)"
      },
      {
        "description": "Using indices_are_sorted and mode arguments with jax.numpy.ndarray.at",
        "code": ">>> import jax.numpy as jnp\n>>> x = jnp.zeros(5)\n>>> indices = jnp.array([1, 2, 4])\n>>> values = jnp.array([2.0, 3.0, 4.0])\n>>> x.at[indices].set(values, indices_are_sorted=True, mode='promise_in_bounds')\nArray([0., 2., 3., 0., 4.], dtype=float32)"
      },
      {
        "description": "Equivalent function call using lax.scatter directly",
        "code": ">>> import jax.numpy as jnp\n>>> import jax.lax as lax\n>>> x = jnp.zeros(5)\n>>> indices = jnp.array([1, 2, 4])\n>>> values = jnp.array([2.0, 3.0, 4.0])\n>>> lax.scatter(\n...     x,\n...     indices[:, None],\n...     values,\n...     dimension_numbers=lax.ScatterDimensionNumbers(\n...         update_window_dims=(),\n...         inserted_window_dims=(0,),\n...         scatter_dims_to_operand_dims=(0,)),\n...     indices_are_sorted=True,\n...     mode=lax.GatherScatterMode.PROMISE_IN_BOUNDS\n... )\nArray([0., 2., 3., 0., 4.], dtype=float32)"
      }
    ]
  },
  {
    "title": "Scatter-Add Operator Overview",
    "concepts": [
      "The scatter-add operator wraps XLA's Scatter operator, using addition to combine updates and operand values.",
      "The semantics of scatter are complex and may change.",
      "jax.numpy.ndarray.at with NumPy indexing syntax is preferred for most use cases."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Scatter-Add Operator",
    "concepts": [
      "operand: The array to which the scatter operation is applied.",
      "scatter_indices: The array providing indices in operand for applying updates.",
      "updates: The updates to be scattered onto operand.",
      "dimension_numbers: A lax.ScatterDimensionNumbers object describing dimension relationships.",
      "indices_are_sorted: A boolean indicating if scatter_indices is sorted for potential performance improvement.",
      "unique_indices: A boolean indicating if updated elements in operand don't overlap for potential performance improvement. Behavior is undefined if overlap exists and unique_indices is True.",
      "mode: Specifies how to handle out-of-bounds indices ('clip', 'fill', 'drop', or 'promise_in_bounds')."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "The function returns an array containing the sum of the operand and the scattered updates."
    ],
    "code_examples": []
  },
  {
    "title": "Scatter-Apply Operator Overview",
    "concepts": [
      "The scatter-apply operator wraps XLA's Scatter operator.",
      "It replaces values in an operand array based on the application of a function.",
      "Duplicate indices result in multiple applications of the function.",
      "It is recommended to use jax.numpy.ndarray.at for most use cases due to familiar NumPy indexing syntax.",
      "The current implementation is not compatible with automatic differentiation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of Scatter-Apply",
    "concepts": [
      "operand: The array to which the scatter should be applied.",
      "scatter_indices: An array providing the indices in the operand for updates.",
      "func: A unary function applied at each index.",
      "dimension_numbers: A ScatterDimensionNumbers object describing the relationship between dimensions.",
      "update_shape: The shape of the updates at the given indices.",
      "indices_are_sorted: Indicates if scatter_indices is sorted, potentially improving performance.",
      "unique_indices: Indicates if updated elements in the operand do not overlap, potentially improving performance. The behavior is undefined if overlap occurs when True.",
      "mode: Determines how out-of-bounds indices are handled ('clip', 'fill', 'drop', or 'promise_in_bounds')."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "Returns an array containing the result of applying the function to the operand at the given indices."
    ],
    "code_examples": []
  },
  {
    "title": "Scatter-Max Operator Overview",
    "concepts": [
      "The scatter-max operator wraps XLA's Scatter operator.",
      "It uses the max function to combine updates and values from the operand.",
      "The semantics of scatter are complex and may change in the future.",
      "Using jax.numpy.ndarray.at with NumPy indexing syntax is preferred for most use cases."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Scatter-Max Operator",
    "concepts": [
      "operand: The array to which the scatter operation is applied.",
      "scatter_indices: An array specifying the indices in the operand for applying updates.",
      "updates: The array containing the updates to be scattered onto the operand.",
      "dimension_numbers: A lax.ScatterDimensionNumbers object describing the relationship between operand, start_indices, updates, and output dimensions.",
      "indices_are_sorted: A boolean indicating whether scatter_indices is sorted (may improve performance).",
      "unique_indices: A boolean indicating whether updated elements in the operand are guaranteed to not overlap (may improve performance). Behavior is undefined if overlap exists when set to True.",
      "mode: Specifies how to handle out-of-bounds indices ('clip', 'fill', 'drop', or 'promise_in_bounds')."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "Returns an array containing the max of the operand and the scattered updates."
    ],
    "code_examples": []
  },
  {
    "title": "Scatter-min Operator Overview",
    "concepts": [
      "This operator wraps XLA's Scatter operator using the min function for combining updates.",
      "Scatter semantics are complex and subject to change.",
      "Using jax.numpy.ndarray.at with NumPy indexing is generally preferred.",
      "The operator scatters updates onto an operand array based on provided indices, using the min function to combine updates."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Scatter-min Operator",
    "concepts": [
      "operand: The array to be updated by the scatter operation.",
      "scatter_indices: The array containing indices in the operand where updates should be applied.",
      "updates: The array containing the updates to be scattered.",
      "dimension_numbers: A lax.ScatterDimensionNumbers object describing the relationship between dimensions of operand, scatter_indices, updates, and the output.",
      "indices_are_sorted: A boolean indicating if scatter_indices is sorted (for potential performance improvements).",
      "unique_indices: A boolean indicating if updated elements in operand are guaranteed to not overlap (for potential performance improvements). Behavior is undefined if overlaps occur when True.",
      "mode: Specifies how to handle out-of-bounds indices ('clip', 'fill', 'drop', or 'promise_in_bounds')."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "The operator returns an array containing the result of applying the scatter-min operation to the operand and updates."
    ],
    "code_examples": []
  },
  {
    "title": "Scatter-Multiply Operator Overview",
    "concepts": [
      "The scatter-multiply operator wraps XLA's Scatter operator.",
      "Multiplication combines updates and values from the operand.",
      "jax.numpy.ndarray.at property is generally preferred for most use cases.",
      "The scatter operator's API might change in the future."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of Scatter-Multiply Operator",
    "concepts": [
      "operand: The array to which the scatter should be applied.",
      "scatter_indices: The array that gives the indices in operand to which updates should be applied.",
      "updates: The updates that should be scattered onto operand.",
      "dimension_numbers: A lax.ScatterDimensionNumbers object describing the relationship between dimensions of operand, scatter_indices, updates, and the output.",
      "indices_are_sorted: A boolean indicating if scatter_indices is sorted.",
      "unique_indices: A boolean indicating if the elements to be updated in operand are guaranteed not to overlap.",
      "mode: How to handle out-of-bounds indices ('clip', 'fill', 'drop', 'promise_in_bounds')."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "The function returns an array containing the sum of the operand and scattered updates."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Left Shift",
    "concepts": [
      "Elementwise left shift operation is represented as x << y.",
      "This operation lowers directly to the stablehlo.shift_left operation.",
      "Input arrays x and y must have matching integer dtypes.",
      "If x and y are not scalars, they must have the same number of dimensions and be broadcast compatible.",
      "The function returns an array with the same dtype as x and y.",
      "The returned array contains the element-wise left shift of each pair of broadcasted entries.",
      "jax.numpy.left_shift() is a NumPy wrapper for this API, also accessible via the x << y operator on JAX arrays.",
      "jax.lax.shift_right_arithmetic() is the elementwise arithmetic right shift.",
      "jax.lax.shift_right_logical() is the elementwise logical right shift."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Arithmetic Right Shift",
    "concepts": [
      "Elementwise arithmetic right shift is performed using the `>>` operator.",
      "The function lowers directly to the stablehlo.shift_right_arithmetic operation.",
      "Inputs x and y must have matching integer dtypes.",
      "If neither input is a scalar, x and y must have the same number of dimensions and be broadcast compatible.",
      "The output array has the same dtype as x and y.",
      "The output contains the element-wise arithmetic right shift of each pair of broadcasted entries."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Sign Function",
    "concepts": [
      "The function lowers directly to the stablehlo.sign operation.",
      "The function returns an array of the same shape and dtype as the input array.",
      "For floating-point inputs, the sign function returns -1 for x < 0, -0 for x = -0, NaN for x = NaN, +0 for x = +0, and 1 for x > 0.",
      "For signed integer inputs, the sign function returns -1 for x < 0, 0 for x = 0, and 1 for x > 0.",
      "For complex inputs, the sign function returns the complex phase, i.e. sign(x) = x / |x|."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Sine Function",
    "concepts": [
      "The function calculates the element-wise sine of an input array.",
      "For floating-point inputs, it uses the stablehlo.sine operation.",
      "For complex inputs, it uses a sequence of HLO operations.",
      "The input array must have a floating-point or complex type.",
      "The output array has the same shape and dtype as the input array.",
      "Related functions include cosine, tangent, and arc sine."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Hyperbolic Sine",
    "concepts": [
      "The function calculates the elementwise hyperbolic sine of an input array.",
      "It lowers directly to the chlo.sinh operation.",
      "The input array must have a floating-point or complex type.",
      "The output is an array with the same shape and data type as the input array."
    ],
    "code_examples": []
  },
  {
    "title": "Description of lax.slice",
    "concepts": [
      "The function wraps XLA's Slice operator.",
      "It takes an array-like object as input.",
      "It requires start indices and limit indices.",
      "It accepts optional strides.",
      "It returns the sliced array."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of two-dimensional slices",
    "concepts": [
      "Demonstrates how to use lax.slice with a 2D array.",
      "Shows different combinations of start_indices, limit_indices, and strides.",
      "Illustrates the equivalence between lax.slice and Python slicing syntax."
    ],
    "code_examples": [
      {
        "description": "Creating a 2D array using jnp.arange and reshape.",
        "code": "x = jnp.arange(12).reshape(3, 4)\nx\n#Array([[ 0,  1,  2,  3],\n#       [ 4,  5,  6,  7],\n#       [ 8,  9, 10, 11]], dtype=int32)"
      },
      {
        "description": "Slicing the array using lax.slice with start indices (1, 0) and limit indices (3, 2).",
        "code": "lax.slice(x, (1, 0), (3, 2))\n#Array([[4, 5],\n#       [8, 9]], dtype=int32)"
      },
      {
        "description": "Slicing the array using lax.slice with start indices (0, 0), limit indices (3, 4), and strides (1, 2).",
        "code": "lax.slice(x, (0, 0), (3, 4), (1, 2))\n#Array([[ 0,  2],\n#       [ 4,  6],\n#       [ 8, 10]], dtype=int32)"
      },
      {
        "description": "Slicing using standard Python slicing syntax, equivalent to lax.slice(x, (1, 0), (3, 2)).",
        "code": "x[1:3, 0:2]\n#Array([[4, 5],\n#       [8, 9]], dtype=int32)"
      },
      {
        "description": "Slicing using standard Python slicing syntax with strides, equivalent to lax.slice(x, (0, 0), (3, 4), (1, 2)).",
        "code": "x[0:3, 0:4:2]\n#Array([[ 0,  2],\n#       [ 4,  6],\n#       [ 8, 10]], dtype=int32)"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "References to related functions such as jax.numpy.ndarray.at, jax.lax.slice_in_dim(), jax.lax.index_in_dim(), and jax.lax.dynamic_slice()."
    ],
    "code_examples": []
  },
  {
    "title": "Sorting Keys and Applying Permutations",
    "concepts": [
      "Sorts keys along a specified dimension.",
      "Applies the same permutation to corresponding values.",
      "Input 'keys' is an Array.",
      "Input 'values' is an ArrayLike.",
      "Input 'dimension' is an integer specifying the dimension to sort along.",
      "Input 'is_stable' is a boolean indicating whether the sort should be stable.",
      "Returns a tuple containing the sorted keys and the permuted values."
    ],
    "code_examples": []
  },
  {
    "title": "Array Splitting",
    "concepts": [
      "Splits an array along a specified axis.",
      "The input is an array-like object.",
      "The sizes of the split arrays are provided as a sequence of integers.",
      "The sum of the split sizes must equal the size of the axis dimension.",
      "The function returns a sequence of arrays with the specified sizes."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Square Root",
    "concepts": [
      "Calculates the elementwise square root of an array.",
      "Lowers directly to the stablehlo.sqrt operation.",
      "Input array must have floating or complex dtype.",
      "Returns an array of the same shape and dtype as the input containing the square root."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Square",
    "concepts": [
      "Elementwise square operation is denoted by \\(x^2\\).",
      "The input is an ArrayLike object represented by 'x'."
    ],
    "code_examples": []
  },
  {
    "title": "Array Dimension Squeezing",
    "concepts": [
      "Squeeze any number of size 1 dimensions from an array.",
      "array (ArrayLike) represents the input array.",
      "dimensions (Sequence[int]) represents the dimensions to be squeezed."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Subtraction",
    "concepts": [
      "Performs elementwise subtraction between two array-like objects.",
      "Input x is an ArrayLike object.",
      "Input y is an ArrayLike object.",
      "The result is an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Hyperbolic Tangent (tanh(x))",
    "concepts": [
      "Computes the elementwise hyperbolic tangent of an array.",
      "Lowers directly to the stablehlo.tanh operation.",
      "Input array must have floating-point or complex type.",
      "Returns an array of the same shape and dtype as the input, containing the element-wise hyperbolic tangent."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.lax.top_k",
    "concepts": [
      "The function returns the top k values and their indices along the last axis of an array.",
      "The input operand must be a N-dimensional array of non-complex type.",
      "The input k is an integer specifying the number of top entries to return.",
      "The function returns a tuple of two arrays: values and indices.",
      "values contains the top k values along the last axis.",
      "indices contains the indices corresponding to the top k values."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of jax.lax.top_k",
    "concepts": [
      "Find the largest three values and their indices within an array."
    ],
    "code_examples": [
      {
        "description": "This example demonstrates how to use jax.lax.top_k to find the 3 largest values and their indices in a JAX array.",
        "code": "x = jnp.array([9., 3., 6., 4., 10.])\nvalues, indices = jax.lax.top_k(x, 3)\nprint(values)\nprint(indices)"
      },
      {
        "description": "This example demonstrates how to use jax.lax.top_k to find the 3 largest values and their indices in a JAX array.",
        "code": "x = jnp.array([9., 3., 6., 4., 10.])\nvalues, indices = jax.lax.top_k(x, 3)\nprint(values)\nprint(indices)"
      }
    ]
  },
  {
    "title": "XLA Transpose Operator",
    "concepts": [
      "Wraps XLA\u2019s Transpose operator.",
      "The function takes an operand of type ArrayLike.",
      "The function takes a permutation of type Sequence[int] or np.ndarray."
    ],
    "code_examples": []
  },
  {
    "title": "ArrayLike",
    "concepts": [
      "x is an ArrayLike object."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Hurwitz zeta function",
    "concepts": [
      "The function is denoted as \u03b6(x, q).",
      "x can be an Array, ndarray, bool, number, int, float, or complex.",
      "q can be an Array, ndarray, bool, number, int, float, or complex."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of Associative Scan",
    "concepts": [
      "Associative scan performs a scan with an associative binary operation in parallel.",
      "An associative binary operation must satisfy the equation fn(a, fn(b, c)) == fn(fn(a, b), c)."
    ],
    "code_examples": []
  },
  {
    "title": "Associative Scan Parameters",
    "concepts": [
      "fn: A Python callable implementing an associative binary operation with signature r = fn(a, b).",
      "elems: A (possibly nested Python tree structure of) array(s), each with an axis dimension of size num_elems.",
      "reverse: A boolean stating if the scan should be reversed with respect to the axis dimension.",
      "axis: An integer identifying the axis over which the scan should occur."
    ],
    "code_examples": []
  },
  {
    "title": "Associative Scan Return Value",
    "concepts": [
      "Returns a (possibly nested Python tree structure of) array(s) of the same shape and structure as elems, in which the k\u2019th element of axis is the result of recursively applying fn to combine the first k elements of elems along axis."
    ],
    "code_examples": []
  },
  {
    "title": "Example 1: Partial Sums of an Array",
    "concepts": [
      "Demonstrates calculating partial sums of an array using associative_scan with jnp.add."
    ],
    "code_examples": [
      {
        "description": "Calculates the partial sums of the array [0, 1, 2, 3].",
        "code": "lax\n.\nassociative_scan\n(\njnp\n.\nadd\n,\njnp\n.\narange\n(\n0\n,\n4\n))\n"
      },
      {
        "description": "Calculates the partial sums of the array [0, 1, 2, 3].",
        "code": "lax\n.\nassociative_scan\n(\njnp\n.\nadd\n,\njnp\n.\narange\n(\n0\n,\n4\n))"
      }
    ]
  },
  {
    "title": "Example 2: Partial Products of an Array of Matrices",
    "concepts": [
      "Demonstrates calculating partial products of an array of matrices using associative_scan with jnp.matmul."
    ],
    "code_examples": [
      {
        "description": "Calculates the partial products of an array of matrices with shape (4, 2, 2).",
        "code": "mats\n=\njax\n.\nrandom\n.\nuniform\n(\njax\n.\nrandom\n.\nkey\n(\n0\n),\n(\n4\n,\n2\n,\n2\n))\n\npartial_prods\n=\nlax\n.\nassociative_scan\n(\njnp\n.\nmatmul\n,\nmats\n)\n"
      },
      {
        "description": "Calculates the partial products of an array of matrices with shape (4, 2, 2).",
        "code": "mats\n=\njax\n.\nrandom\n.\nuniform\n(\njax\n.\nrandom\n.\nkey\n(\n0\n),\n(\n4\n,\n2\n,\n2\n))\n\npartial_prods\n=\nlax\n.\nassociative_scan\n(\njnp\n.\nmatmul\n,\nmats\n)\n"
      }
    ]
  },
  {
    "title": "Example 3: Reversed Partial Sums of an Array",
    "concepts": [
      "Demonstrates calculating reversed partial sums of an array using associative_scan with jnp.add and reverse=True."
    ],
    "code_examples": [
      {
        "description": "Calculates the reversed partial sums of the array [0, 1, 2, 3].",
        "code": "lax\n.\nassociative_scan\n(\njnp\n.\nadd\n,\njnp\n.\narange\n(\n0\n,\n4\n),\nreverse\n=\nTrue\n)"
      },
      {
        "description": "Calculates the reversed partial sums of the array [0, 1, 2, 3].",
        "code": "lax\n.\nassociative_scan\n(\njnp\n.\nadd\n,\njnp\n.\narange\n(\n0\n,\n4\n),\nreverse\n=\nTrue\n)"
      }
    ]
  },
  {
    "title": "Introduction to fori_loop",
    "concepts": [
      "fori_loop is a looping construct similar to a for loop, but implemented using jax.lax primitives.",
      "It iterates from a lower bound (inclusive) to an upper bound (exclusive).",
      "The Haskell-like type signature describes the function's inputs and output.",
      "The loop-carried value must have a fixed shape and dtype across all iterations.",
      "Reverse-mode autodiff support depends on whether the trip count is static.",
      "Unrolling the loop is possible when the loop bounds are statically known using the `unroll` argument."
    ],
    "code_examples": [
      {
        "description": "Python implementation demonstrating the semantics of fori_loop",
        "code": "def fori_loop(lower, upper, body_fun, init_val):\n    val = init_val\n    for i in range(lower, upper):\n        val = body_fun(i, val)\n    return val"
      }
    ]
  },
  {
    "title": "Introduction to `lax.map`",
    "concepts": [
      "`lax.map` is similar to Python's `map` function but operates on stacked arrays.",
      "`lax.map` can be a memory-efficient alternative to `vmap` or a more performant version of `map`.",
      "`lax.map` is implemented using JAX primitives, offering advantages like compilation and support for nested pytrees.",
      "An optional `batch_size` argument is used to process the computation in parallel."
    ],
    "code_examples": [
      {
        "description": "Python implementation demonstrating the semantics of `map` when applied to an array.",
        "code": "def map(f, xs):\n    return np.stack([f(x) for x in xs])\ndef map(f, xs):\n    return np.stack([f(x) for x in xs])"
      }
    ]
  },
  {
    "title": "Usage of `lax.map` with `batch_size`",
    "concepts": [
      "The `batch_size` parameter allows for processing data in batches.",
      "If the array length is not divisible by `batch_size`, a separate `vmap` computation processes the remainder.",
      "Using `batch_size` can improve performance or reduce memory usage compared to directly mapping over the entire input."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use `lax.map` with a `batch_size`. It initializes a 3D array, defines a function `f` that prints the inner shape of the input and adds 1 to it, and then applies `lax.map` with a `batch_size` of 3.",
        "code": "import jax.numpy as jnp\nfrom jax import lax\n\nx = jnp.ones((10, 3, 4))\n\ndef f(x):\n  print('inner shape:', x.shape)\n  return x + 1\n\ny = lax.map(f, x, batch_size=3)\nprint(y.shape)"
      },
      {
        "description": "A second example with identical code showing redundant calls",
        "code": "import jax.numpy as jnp\nfrom jax import lax\n\nx = jnp.ones((10, 3, 4))\n\ndef f(x):\n  print('inner shape:', x.shape)\n  return x + 1\n\ny = lax.map(f, x, batch_size=3)\nprint(y.shape)"
      }
    ]
  },
  {
    "title": "Scan Function Overview",
    "concepts": [
      "The scan function applies a function over leading array axes while carrying state.",
      "The scan function's type signature is similar to Haskell's scan function.",
      "The input and output can be arrays or pytrees with arrays at the leaves.",
      "The scan function is a JAX primitive that is lowered to a WhileOp.",
      "The loop-carried value must have a fixed shape and dtype across iterations.",
      "The scan function compiles 'f', so combining it with 'jit()' is usually unnecessary."
    ],
    "code_examples": [
      {
        "description": "Python implementation of scan function when the type of xs is an array type or None, and the type of ys is an array type.",
        "code": "def scan(f, init, xs, length=None):\n    if xs is None:\n        xs = [None] * length\n    carry = init\n    ys = []\n    for x in xs:\n        carry, y = f(carry, x)\n        ys.append(y)\n    return carry, np.stack(ys)"
      }
    ]
  },
  {
    "title": "Scan Function Parameters and Return Value",
    "concepts": [
      "f: A function of type c -> a -> (c, b) that is scanned.",
      "init: An initial loop carry value of type c.",
      "xs: The value of type [a] over which to scan along the leading axis.",
      "length: Optional integer specifying the number of loop iterations.",
      "reverse: Optional boolean specifying whether to run the scan iteration forward or in reverse.",
      "unroll: Optional positive int or bool specifying how many scan iterations to unroll within a single iteration of a loop.",
      "_split_transpose: Experimental optional bool specifying whether to further split the transpose into a scan and a map.",
      "Return Value: A pair of type (c, [b]) where the first element represents the final loop carry value and the second element represents the stacked outputs of the second output of f when scanned over the leading axis of the inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Switch Operation",
    "concepts": [
      "Apply exactly one of the branches given by an index.",
      "Index is clamped to within bounds if out of bounds.",
      "It wraps XLA's Conditional operator internally.",
      "When transformed with vmap(), cond is converted to select().",
      "All branches must return the same output structure.",
      "The function returns the value of the selected branch applied to the operands."
    ],
    "code_examples": [
      {
        "description": "Python implementation of the switch function with clamping.",
        "code": "def switch(index, branches, *operands):\n    index = clamp(0, index, len(branches) - 1)\n    return branches[index](*operands)"
      },
      {
        "description": "Another Python implementation of the switch function with clamping.",
        "code": "def switch(index, branches, *operands):\n    index = clamp(0, index, len(branches) - 1)\n    return branches[index](*operands)"
      }
    ]
  },
  {
    "title": "Introduction to stop_gradient",
    "concepts": [
      "stop_gradient prevents gradient flow during automatic differentiation.",
      "stop_gradient acts as an identity function during normal evaluation.",
      "stop_gradient treats the input as a constant for gradient computation purposes.",
      "stop_gradient affects all levels of nested gradient computations."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Example: Squaring a Number",
    "concepts": [
      "Illustrates squaring a number and computing its gradient using JAX.",
      "Demonstrates the standard gradient calculation without stop_gradient."
    ],
    "code_examples": [
      {
        "description": "Define a function that squares its input and calculate the function value and its gradient at x=3.0",
        "code": "def f1(x):\n  return x**2\n\nx = jnp.float32(3.0)\nprint(f1(x))\nprint(jax.grad(f1)(x))"
      }
    ]
  },
  {
    "title": "Using stop_gradient to Block Gradients",
    "concepts": [
      "Demonstrates the use of stop_gradient to prevent gradient flow.",
      "Shows how stop_gradient causes the gradient to be zero, even though the function value remains the same.",
      "Illustrates that the function's output is identical with and without stop_gradient in standard evaluation."
    ],
    "code_examples": [
      {
        "description": "Define a function that squares its input after applying stop_gradient and calculate the function value and its gradient at x=3.0. The gradient is zero because stop_gradient blocks the gradient flow.",
        "code": "def f2(x):\n  return jax.lax.stop_gradient(x)**2\n\nx = jnp.float32(3.0)\nprint(f2(x))\nprint(jax.grad(f2)(x))"
      }
    ]
  },
  {
    "title": "Applications of stop_gradient",
    "concepts": [
      "stop_gradient is used for efficiency in JAX's internal implementations.",
      "jax.nn.softmax uses stop_gradient to normalize the input efficiently.",
      "stop_gradient is used to improve performance in specific neural network operations."
    ],
    "code_examples": []
  },
  {
    "title": "Matrix-Free Linear Solve with Implicitly Defined Gradients",
    "concepts": [
      "Performs a matrix-free linear solve.",
      "Defines gradients for the linear solve directly via implicit differentiation.",
      "Differentiating through the solve operation may be faster or more numerically stable.",
      "Differentiating through the solve operation may not be implemented.",
      "The function requires the invariant: x = solve(matvec, b) and matvec(x) == b.",
      "matvec is a differentiable linear function to invert.",
      "b is the constant right handle side of the equation.",
      "solve is a higher level function that solves for the linear equation.",
      "transpose_solve is a higher level function for solving the transpose linear equation.",
      "symmetric indicates if the linear map corresponds to a symmetric matrix.",
      "has_aux indicates whether the solve and transpose_solve functions return auxiliary data.",
      "Returns the result of solve(matvec, b) with gradients defined assuming that the solution x satisfies the linear equation matvec(x) == b."
    ],
    "code_examples": [
      {
        "description": "Solve the linear equation using the solve function and assert that the result satisfies the condition matvec(x) == b.",
        "code": "x = solve(matvec, b)\n# solve the linear equation\nassert matvec(x) == b\n# not checked"
      },
      {
        "description": "Solve the linear equation using the solve function and assert that the result satisfies the condition matvec(x) == b.",
        "code": "x = solve(matvec, b)\n# solve the linear equation\nassert matvec(x) == b\n# not checked"
      }
    ]
  },
  {
    "title": "Differentiable Root Finding with custom_root",
    "concepts": [
      "Differentiably solve for the roots of a function.",
      "This is a low-level routine intended for internal use in JAX.",
      "Gradients are defined via the implicit function theorem.",
      "The function `f` should accept a single argument and return a tree of arrays with the same structure as its input.",
      "`initial_guess` is the initial guess for a zero of `f`.",
      "`solve` is a function to solve for the roots of `f` given `f` and `initial_guess`.",
      "The solution from `solve` should satisfy `f(solution) = 0`.",
      "`tangent_solve` solves the tangent system and should take a linearized function `g` and a tree of arrays `y` as input.",
      "`tangent_solve` should return a solution `x` such that `g(x) = y`.",
      "`has_aux` indicates whether the `solve` function returns auxiliary data."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of tangent_solve",
    "concepts": [
      "Example for using tangent_solve with scalar `y`.",
      "Example for using tangent_solve with vector `y` and a linear solver with the Jacobian."
    ],
    "code_examples": [
      {
        "description": "Example of tangent_solve for scalar y.",
        "code": "lambda g, y: y / g(1.0)"
      },
      {
        "description": "Example of tangent_solve for vector y using a linear solve with the Jacobian.",
        "code": "lambda g, y: np.linalg.solve(jacobian(g)(y), y)"
      }
    ]
  },
  {
    "title": "Overview of jax.lax.all_gather",
    "concepts": [
      "Gathers values of x across all replicas.",
      "If x is a pytree then the result is equivalent to mapping this function to each leaf in the tree.",
      "Faster than all_to_all(broadcast(x)).",
      "axis_name is a hashable Python object used to name a pmapped axis.",
      "axis_index_groups is an optional list of lists containing axis indices.",
      "axis is a positional axis into which the chunks along axis_name will be concatenated.",
      "tiled determines whether the chunks will be stacked into a fresh positional axis or concatenated into an existing one."
    ],
    "code_examples": []
  },
  {
    "title": "Basic all_gather Example",
    "concepts": [
      "Demonstrates the basic usage of jax.lax.all_gather with jax.pmap.",
      "The input array is gathered across all devices along the specified axis."
    ],
    "code_examples": [
      {
        "description": "Illustrates gathering an array across all devices using pmap and all_gather.  Each device receives a copy of all the data from other devices.",
        "code": "x = np.arange(4)\ny = jax.pmap(lambda x: jax.lax.all_gather(x, 'i'), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "Illustrates gathering an array across all devices using pmap and all_gather.  Each device receives a copy of all the data from other devices.",
        "code": "x = np.arange(4)\ny = jax.pmap(lambda x: jax.lax.all_gather(x, 'i'), axis_name='i')(x)\nprint(y)"
      }
    ]
  },
  {
    "title": "all_gather with axis_index_groups",
    "concepts": [
      "Demonstrates using axis_index_groups to perform all_gather on subgroups of devices.",
      "Groups split by even & odd device ids.",
      "The input array is gathered within specified groups of devices."
    ],
    "code_examples": [
      {
        "description": "Illustrates using axis_index_groups with jax.lax.all_gather. Data is gathered only between specific groups of devices. In this example, devices 0 and 2 form one group, and devices 3 and 1 form another.",
        "code": "x = np.arange(16).reshape(4, 4)\nprint(x)\ndef f(x):\n  return jax.lax.all_gather(\n      x,\n      'i',\n      axis_index_groups=[[0, 2], [3, 1]])\ny = jax.pmap(f, axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "Illustrates using axis_index_groups with jax.lax.all_gather. Data is gathered only between specific groups of devices. In this example, devices 0 and 2 form one group, and devices 3 and 1 form another.",
        "code": "x = np.arange(16).reshape(4, 4)\nprint(x)\ndef f(x):\n  return jax.lax.all_gather(\n      x,\n      'i',\n      axis_index_groups=[[0, 2], [3, 1]])\ny = jax.pmap(f, axis_name='i')(x)\nprint(y)"
      }
    ]
  },
  {
    "title": "Materializing and Mapping Axes",
    "concepts": [
      "Materializes the mapped axis and maps a different axis.",
      "Equivalent to mapping the function to each leaf in the tree if x is a pytree.",
      "The input mapped axis axis_name is materialized at the logical axis position concat_axis.",
      "The input unmapped axis at position split_axis is mapped with the name axis_name.",
      "The group size of the mapped axis must be equal to the size of the unmapped axis.",
      "Must have lax.psum(1, axis_name, axis_index_groups=axis_index_groups) == x.shape[axis].",
      "Describes the expected shape change when tiled is False: np.insert(np.delete(x.shape, split_axis), concat_axis, axis_size).",
      "Describes the expected shape change when tiled is True: shape similar to the input shape, except with split_axis divided by axis size and concat_axis multiplied by axis size."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of psum",
    "concepts": [
      "psum computes an all-reduce sum over a pmapped axis.",
      "If the input is a pytree, the function is applied to each leaf.",
      "Boolean inputs are converted to integers before reduction.",
      "The axis_name is used to identify the pmapped axis.",
      "axis_index_groups can be used to perform psums over subgroups of replicas."
    ],
    "code_examples": []
  },
  {
    "title": "Basic psum Example",
    "concepts": [
      "Demonstrates psum with 4 XLA devices.",
      "The input array is summed across the 'i' axis using psum.",
      "The result is the same value on all devices.",
      "Shows how to divide by the psum to normalize the result."
    ],
    "code_examples": [
      {
        "description": "Basic psum calculation and normalization",
        "code": "import jax\nimport jax.numpy as np\n\nx = np.arange(4)\ny = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(x)\nprint(y)\ny = jax.pmap(lambda x: x / jax.lax.psum(x, 'i'), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "Basic psum calculation and normalization",
        "code": "import jax\nimport jax.numpy as np\n\nx = np.arange(4)\ny = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(x)\nprint(y)\ny = jax.pmap(lambda x: x / jax.lax.psum(x, 'i'), axis_name='i')(x)\nprint(y)"
      }
    ]
  },
  {
    "title": "psum with Axis Index Groups",
    "concepts": [
      "psum is performed within subgroups of devices.",
      "Example uses two groups: [0, 1] and [2, 3].",
      "Each group performs a psum independently."
    ],
    "code_examples": [
      {
        "description": "psum with axis index groups",
        "code": "import jax\nimport jax.numpy as np\n\nx = np.arange(4)\ny = jax.pmap(lambda x: jax.lax.psum(x, 'i', axis_index_groups=[[0, 1], [2, 3]]), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "psum with axis index groups",
        "code": "import jax\nimport jax.numpy as np\n\nx = np.arange(4)\ny = jax.pmap(lambda x: jax.lax.psum(x, 'i', axis_index_groups=[[0, 1], [2, 3]]), axis_name='i')(x)\nprint(y)"
      }
    ]
  },
  {
    "title": "psum with 2D Arrays",
    "concepts": [
      "Example demonstrates psum with a 2D array as input.",
      "Each row represents data from one device.",
      "The psum is performed across all devices or within subgroups."
    ],
    "code_examples": [
      {
        "description": "Reshaping the input array into a 2D structure",
        "code": "import jax\nimport jax.numpy as np\n\nx = np.arange(16).reshape(4, 4)\nprint(x)"
      },
      {
        "description": "Reshaping the input array into a 2D structure",
        "code": "import jax\nimport jax.numpy as np\n\nx = np.arange(16).reshape(4, 4)\nprint(x)"
      }
    ]
  },
  {
    "title": "Full psum and Grouped psum with 2D Arrays",
    "concepts": [
      "Shows full psum across all devices for a 2D array.",
      "Demonstrates psum within subgroups for a 2D array using axis_index_groups."
    ],
    "code_examples": [
      {
        "description": "Full psum across all devices for a 2D array",
        "code": "import jax\nimport jax.numpy as np\n\nx = np.arange(16).reshape(4, 4)\ny = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "Full psum across all devices for a 2D array",
        "code": "import jax\nimport jax.numpy as np\n\nx = np.arange(16).reshape(4, 4)\ny = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "psum within subgroups for a 2D array",
        "code": "import jax\nimport jax.numpy as np\n\nx = np.arange(16).reshape(4, 4)\ny = jax.pmap(lambda x: jax.lax.psum(x, 'i', axis_index_groups=[[0, 1], [2, 3]]), axis_name='i')(x)\nprint(y)"
      },
      {
        "description": "psum within subgroups for a 2D array",
        "code": "import jax\nimport jax.numpy as np\n\nx = np.arange(16).reshape(4, 4)\ny = jax.pmap(lambda x: jax.lax.psum(x, 'i', axis_index_groups=[[0, 1], [2, 3]]), axis_name='i')(x)\nprint(y)"
      }
    ]
  },
  {
    "title": "All-Reduce Max Computation",
    "concepts": [
      "Computes an all-reduce max operation on an array over a pmapped axis.",
      "The input `x` can be a pytree.",
      "The `axis_name` is a hashable Python object identifying the pmapped axis.",
      "The `axis_index_groups` allows for max reduction over subgroups of the axis."
    ],
    "code_examples": []
  },
  {
    "title": "All-Reduce Min Computation",
    "concepts": [
      "Computes an all-reduce min operation on an array over a pmapped axis.",
      "The axis over which the min is computed is identified by axis_name.",
      "If the input is a pytree, the operation is mapped to each leaf.",
      "Optional axis_index_groups allow for min reduction over subgroups of replicas.",
      "The function returns an array with the same shape as the input array."
    ],
    "code_examples": []
  },
  {
    "title": "pswapaxes Function Overview",
    "concepts": [
      "pswapaxes swaps a pmapped axis with an unmapped axis.",
      "If the input is a PyTree, the function is mapped to each leaf.",
      "The group size of the mapped axis must equal the size of the unmapped axis.",
      "By default, axis_index_groups encompasses all devices.",
      "pswapaxes is a special case of all_to_all."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of pswapaxes",
    "concepts": [
      "x: array(s) with a mapped axis named axis_name.",
      "axis_name: hashable Python object used to name a pmapped axis.",
      "axis: int indicating the unmapped axis to map with the name axis_name.",
      "axis_index_groups: optional list of lists containing axis indices for running pswapaxes over replicas."
    ],
    "code_examples": []
  },
  {
    "title": "Sharding Constraint Mechanism",
    "concepts": [
      "This mechanism constrains the sharding of an Array inside a jitted computation.",
      "It's a strict constraint for the GSPMD partitioner.",
      "with_sharding_constraint makes it possible to constrain intermediate values to an uneven sharding inside a jitted computation.",
      "Unevenly sharded values output by the jitted computation will come out as fully replicated."
    ],
    "code_examples": []
  },
  {
    "title": "Cholesky Decomposition Overview",
    "concepts": [
      "Cholesky decomposition computes A = L . L^H, where L is a lower triangular matrix.",
      "Input matrices A must be square, positive-definite, and either Hermitian (complex) or symmetric (real).",
      "The input x is an array representing a batch of square Hermitian/symmetric positive-definite matrices.",
      "symmetrize_input determines if the input matrix is symmetrized before decomposition.",
      "If symmetrize_input is True, the matrix is symmetrized by computing (x + x^H)/2.",
      "If symmetrize_input is False, only the lower triangle of x is used.",
      "The output is a matrix with the same dtype as x and shape [..., n, n] representing the Cholesky decomposition.",
      "If Cholesky decomposition fails, the function returns a matrix full of NaNs."
    ],
    "code_examples": []
  },
  {
    "title": "Eigendecomposition with MAGMA",
    "concepts": [
      "Nonsymmetric eigendecomposition is implemented on CPU and GPU.",
      "GPU implementation can use LAPACK or MAGMA.",
      "MAGMA may be faster for large matrices (>= 2048x2048).",
      "MAGMA requires separate installation.",
      "The jax_use_magma configuration variable controls MAGMA usage.",
      "The JAX_GPU_MAGMA_PATH environment variable can specify the MAGMA library path.",
      "If eigendecomposition fails, NaN arrays are returned."
    ],
    "code_examples": [
      {
        "description": "Enabling the MAGMA implementation using jax.config.update.",
        "code": "jax.config.update('jax_use_magma', 'on')\njax.config.update('jax_use_magma', 'on')"
      }
    ]
  },
  {
    "title": "Eigendecomposition Parameters and Return Values",
    "concepts": [
      "x is the input batch of square matrices.",
      "compute_left_eigenvectors determines if left eigenvectors are computed.",
      "compute_right_eigenvectors determines if right eigenvectors are computed.",
      "use_magma locally overrides the jax_use_magma flag on GPU.",
      "The function returns a tuple (w, vl, vr) containing eigenvalues (w), left eigenvectors (vl), and right eigenvectors (vr).",
      "vl and vr are only returned if requested."
    ],
    "code_examples": []
  },
  {
    "title": "Product of Elementary Householder Reflectors",
    "concepts": [
      "Describes the product of elementary Householder reflectors.",
      "Input 'a' is a matrix containing elementary Householder reflectors in its lower triangle.",
      "Input 'taus' is a vector containing scalar factors of the elementary Householder reflectors.",
      "The output is a batch of orthogonal (unitary) matrices."
    ],
    "code_examples": []
  },
  {
    "title": "LU Decomposition with Partial Pivoting",
    "concepts": [
      "Decomposes a matrix A into P, L, and U matrices.",
      "P is a permutation matrix representing row swaps.",
      "L is a lower-triangular matrix with unit-diagonal elements.",
      "U is an upper-triangular matrix.",
      "The input is a batch of matrices with shape [..., m, n].",
      "The output is a tuple containing the LU matrix, pivots, and permutation."
    ],
    "code_examples": []
  },
  {
    "title": "Output Tuple Structure",
    "concepts": [
      "The function returns a tuple containing lu, pivots, and permutation.",
      "lu contains the L and U matrices; L in the lower triangle and U in the upper triangle.",
      "The unit diagonal elements of L are not explicitly represented.",
      "pivots is an int32 array representing a sequence of row swaps with shape [..., min(m, n)].",
      "permutation represents the row swaps as a permutation with shape [..., m]."
    ],
    "code_examples": []
  },
  {
    "title": "QR Decomposition Overview",
    "concepts": [
      "QR decomposition computes A = Q * R.",
      "Q is a unitary (orthogonal) matrix.",
      "R is an upper-triangular matrix.",
      "Input 'x' represents a batch of matrices with shape [..., m, n]."
    ],
    "code_examples": []
  },
  {
    "title": "Pivoting and Full Matrices",
    "concepts": [
      "Pivoting allows rank-revealing QR decomposition (A[:, P] = Q @ R).",
      "P selects columns such that the diagonal of R is non-increasing.",
      "Pivoting is currently supported on CPU and GPU backends only.",
      "The 'full_matrices' parameter determines the shape of the returned matrices.",
      "If full_matrices=True, q has shape [..., m, m] and r has shape [..., m, n].",
      "If full_matrices=False, q has shape [..., m, min(m, n)] and r has shape [..., min(m, n), n]."
    ],
    "code_examples": []
  },
  {
    "title": "MAGMA Usage",
    "concepts": [
      "The use_magma parameter locally overrides the jax_use_magma flag.",
      "If use_magma is True, the pivoted QR factorization is computed using MAGMA (GPU only).",
      "If use_magma is False, the computation is done using LAPACK on the host CPU.",
      "If use_magma is None, the behavior is controlled by the jax_use_magma flag.",
      "MAGMA support is experimental and has limitations.",
      "If jax_use_magma is set to 'auto', MAGMA is used if available and the input matrix has at least 2048 columns."
    ],
    "code_examples": []
  },
  {
    "title": "Output",
    "concepts": [
      "The output is a pair of arrays (q, r) if pivoting=False, otherwise (q, r, p).",
      "p is an index vector with shape [..., n]."
    ],
    "code_examples": []
  },
  {
    "title": "Schur Decomposition",
    "concepts": [
      "Schur decomposition computes A = Q * U * Q^(-H) for a square matrix A.",
      "The implementation is CPU-only.",
      "Input 'x' is a batch of square matrices with shape [..., m, m].",
      "compute_schur_vectors controls whether Schur vectors (Q) are computed.",
      "The return value is a pair (U, Q) if compute_schur_vectors is True, otherwise only U is returned."
    ],
    "code_examples": []
  },
  {
    "title": "Singular Value Decomposition Overview",
    "concepts": [
      "Singular value decomposition (SVD) computes the decomposition of an input matrix.",
      "The input 'x' is a batch of matrices with shape [..., m, n].",
      "The 'full_matrices' parameter determines if full or reduced matrices are returned.",
      "The 'compute_uv' parameter determines if singular vectors are returned.",
      "If 'compute_uv' is True, left singular vectors, singular values, and adjoint of right singular vectors are returned.",
      "If 'compute_uv' is False, only singular values are returned.",
      "The 'subset_by_index' parameter can be used to return a subset of the singular values and vectors.",
      "The 'algorithm' parameter specifies the SVD algorithm to use."
    ],
    "code_examples": []
  },
  {
    "title": "Enum for SVD Algorithm",
    "concepts": [
      "Enumeration for different SVD (Singular Value Decomposition) algorithms.",
      "The enumeration includes the following algorithms: DEFAULT, QR, and JACOBI."
    ],
    "code_examples": []
  },
  {
    "title": "Symmetric Product Computation",
    "concepts": [
      "Computes the symmetric product: alpha * A * A^T + beta * C, where A is a rectangular matrix and C is a symmetric matrix.",
      "a_matrix represents a batch of matrices with shape [..., m, n].",
      "c_matrix represents a batch of matrices with shape [..., m, m].",
      "alpha is a scalar value.",
      "beta is a scalar value.",
      "symmetrize_output controls whether the upper triangle of the output is symmetrized.",
      "The output is a batch of matrices with shape [..., m, m], where the lower triangle is guaranteed to be correct.",
      "If symmetrize_output is True, the upper triangle is filled to make the entire matrix valid."
    ],
    "code_examples": []
  },
  {
    "title": "Triangular Solve Overview",
    "concepts": [
      "Solves the matrix equation op(A) . X = B or X . op(A) = B, where A is a triangular matrix.",
      "A must be a lower or upper triangular square matrix.",
      "op(A) may transpose A and/or take its complex conjugate.",
      "The function takes arguments to specify left or right side solve, lower or upper triangle, transpose, conjugate, and unit diagonal."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "a (ArrayLike): A batch of matrices with shape [..., m, m].",
      "b (ArrayLike): A batch of matrices with shape [..., m, n] if left_side is True or shape [..., n, m] otherwise.",
      "left_side (bool): Describes which of the two matrix equations to solve.",
      "lower (bool): Describes which triangle of a should be used.",
      "transpose_a (bool): If True, the value of a is transposed.",
      "conjugate_a (bool): If True, the complex conjugate of a is used in the solve.",
      "unit_diagonal (bool): If True, the diagonal of a is assumed to be unit (all 1s) and not accessed."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "Returns a batch of matrices the same shape and dtype as b."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Random Number Generation",
    "concepts": [
      "JAX provides deterministic pseudo-random number generation routines.",
      "JAX random functions require an explicit PRNG state passed as the first argument.",
      "The PRNG state is described by a key, usually generated by jax.random.key().",
      "Reusing the same key will lead to the same random number.",
      "jax.random.split() is used to generate new subkeys for different random numbers."
    ],
    "code_examples": [
      {
        "description": "Creating a PRNG key with jax.random.key().",
        "code": "from jax import random\nkey = random.key(0)\nprint(key)"
      },
      {
        "description": "Generating a uniform random number using a key.",
        "code": "from jax import random\nkey = random.key(0)\nrandom.uniform(key)"
      },
      {
        "description": "Splitting a key into subkeys and generating a uniform random number.",
        "code": "from jax import random\nkey = random.key(0)\nkey, subkey = random.split(key)\nrandom.uniform(subkey)"
      }
    ]
  },
  {
    "title": "Legacy Keys vs. Typed Keys",
    "concepts": [
      "JAX v0.4.16 introduced typed key arrays (e.g., key<fry>).",
      "Before v0.4.16, keys were represented as uint32 arrays.",
      "New-style typed key arrays are created with jax.random.key().",
      "Legacy uint32 key arrays are created with jax.random.PRNGKey().",
      "Conversion between the two can be done using jax.random.key_data() and jax.random.wrap_key_data().",
      "Typed keys are generally recommended.",
      "Legacy keys have an extra trailing dimension and a numeric dtype (uint32).",
      "Legacy keys do not carry information about the RNG implementation."
    ],
    "code_examples": []
  },
  {
    "title": "JAX PRNG Implementation",
    "concepts": [
      "JAX PRNG combines a Threefry counter PRNG with a functional array-oriented splitting model.",
      "The JAX PRNG aims to ensure reproducibility and parallelize well.",
      "JAX provides several PRNG implementations selectable via the impl keyword argument to jax.random.key.",
      "Available implementations include threefry2x32, rbg, and unsafe_rbg.",
      "The default implementation is threefry2x32.",
      "rbg and unsafe_rbg are experimental and have not been subject to thorough randomness testing.",
      "rbg and unsafe_rbg behave unusually under jax.vmap due to limited batching support in XLA RBG.",
      "Reasons to use an alternative RNG include potential slowness in compilation or execution on TPUs."
    ],
    "code_examples": []
  },
  {
    "title": "Automatic Partitioning",
    "concepts": [
      "Efficient auto-partitioning of sharded random number arrays requires extra flags.",
      "For threefry2x32 and rbg key derivation, set jax_threefry_partitionable=True.",
      "For unsafe_rbg and rbg random generation, set the XLA flag --xla_tpu_spmd_rng_bit_generator_unsafe=1.",
      "The XLA flag can be set using the XLA_FLAGS environment variable."
    ],
    "code_examples": []
  },
  {
    "title": "Summary of RNG Implementations",
    "concepts": [
      "Threefry is fast on TPU and efficiently shardable.",
      "rbg is fast on TPU and efficiently shardable.",
      "unsafe_rbg is fast on TPU and efficiently shardable.",
      "identical across shardings",
      "identical across CPU/GPU/TPU",
      "exact jax.vmap over keys"
    ],
    "code_examples": []
  },
  {
    "title": "Random Number Generation Functions",
    "concepts": [
      "key(seed, *[, impl]): Create a PRNG key given an integer seed.",
      "key_data(keys): Recover the bits of key data underlying a PRNG key array.",
      "wrap_key_data(key_bits_array, *[, impl]): Wrap an array of key data bits into a PRNG key array.",
      "fold_in(key, data): Folds in data to a PRNG key to form a new PRNG key.",
      "split(key[, num]): Splits a PRNG key into num new keys.",
      "clone(key): Clone a key for reuse",
      "PRNGKey(seed, *[, impl]): Create a legacy PRNG key given an integer seed.",
      "ball(key, d[, p, shape, dtype]): Sample uniformly from the unit Lp ball.",
      "bernoulli(key[, p, shape]): Sample Bernoulli random values.",
      "beta(key, a, b[, shape, dtype]): Sample Beta random values.",
      "binomial(key, n, p[, shape, dtype]): Sample Binomial random values.",
      "bits(key[, shape, dtype]): Sample uniform bits.",
      "categorical(key, logits[, axis, shape]): Sample from categorical distributions.",
      "cauchy(key[, shape, dtype]): Sample Cauchy random values.",
      "chisquare(key, df[, shape, dtype]): Sample Chisquare random values.",
      "choice(key, a[, shape, replace, p, axis]): Generate a random sample from an array.",
      "dirichlet(key, alpha[, shape, dtype]): Sample Dirichlet random values.",
      "double_sided_maxwell(key, loc, scale[, ...]): Sample from a double sided Maxwell distribution.",
      "exponential(key[, shape, dtype]): Sample Exponential random values.",
      "f(key, dfnum, dfden[, shape, dtype]): Sample F-distribution random values.",
      "gamma(key, a[, shape, dtype]): Sample Gamma random values.",
      "generalized_normal(key, p[, shape, dtype]): Sample from the generalized normal distribution.",
      "geometric(key, p[, shape, dtype]): Sample Geometric random values.",
      "gumbel(key[, shape, dtype, mode]): Sample Gumbel random values.",
      "laplace(key[, shape, dtype]): Sample Laplace random values.",
      "loggamma(key, a[, shape, dtype]): Sample log-gamma random values.",
      "logistic(key[, shape, dtype]): Sample logistic random values.",
      "lognormal(key[, sigma, shape, dtype]): Sample lognormal random values.",
      "maxwell(key[, shape, dtype]): Sample from a one sided Maxwell distribution.",
      "multivariate_normal(key, mean, cov[, shape, ...]): Sample multivariate normal random values.",
      "normal(key[, shape, dtype]): Sample standard normal random values.",
      "orthogonal(key, n[, shape, dtype, m]): Sample uniformly from the orthogonal group O(n).",
      "pareto(key, b[, shape, dtype]): Sample Pareto random values.",
      "permutation(key, x[, axis, independent]): Returns a randomly permuted array or range.",
      "poisson(key, lam[, shape, dtype]): Sample Poisson random values.",
      "rademacher(key[, shape, dtype]): Sample from a Rademacher distribution.",
      "randint(key, shape, minval, maxval[, dtype]): Sample uniform random integers.",
      "rayleigh(key, scale[, shape, dtype]): Sample Rayleigh random values.",
      "t(key, df[, shape, dtype]): Sample Student's t random values.",
      "triangular(key, left, mode, right[, shape, ...]): Sample Triangular random values.",
      "truncated_normal(key, lower, upper[, shape, ...]): Sample truncated normal random values.",
      "uniform(key[, shape, dtype, minval, maxval]): Sample uniform random values.",
      "wald(key, mean[, shape, dtype]): Sample Wald random values.",
      "weibull_min(key, scale, concentration[, ...]): Sample from a Weibull distribution."
    ],
    "code_examples": []
  },
  {
    "title": "PRNG Key Generation",
    "concepts": [
      "A pseudo-random number generator (PRNG) key is created from an integer seed.",
      "The result is a scalar array representing the key.",
      "The key's dtype indicates the default PRNG implementation.",
      "The PRNG implementation can be specified using the `impl` argument or the `jax_default_prng_impl` config flag.",
      "The seed must be a 64- or 32-bit integer.",
      "The resulting key can be used by random functions, split, and fold_in."
    ],
    "code_examples": []
  },
  {
    "title": "Key Recovery from PRNG Key Array",
    "concepts": [
      "Recovering underlying bits from a PRNG key array.",
      "The input is an ArrayLike object named 'keys'."
    ],
    "code_examples": []
  },
  {
    "title": "Wrapping Key Data into a PRNG Key Array",
    "concepts": [
      "The function wraps an array of key data bits into a PRNG key array.",
      "The `key_bits_array` parameter is a uint32 array.",
      "The trailing shape of `key_bits_array` corresponds to the key shape of the PRNG implementation.",
      "The `impl` parameter specifies a PRNG implementation.",
      "The function returns a PRNG key array with a dtype of `jax.dtypes.prng_key`.",
      "The shape of the returned array equals the leading shape of `key_bits_array.shape` up to the key bit dimensions."
    ],
    "code_examples": []
  },
  {
    "title": "Folding Data into a PRNG Key",
    "concepts": [
      "Folds in data to a PRNG key to form a new PRNG key.",
      "The `key` argument is a PRNG key (from `key`, `split`, `fold_in`).",
      "The `data` argument is a 32-bit integer representing data to be folded into the key.",
      "Returns a new PRNG key that is a deterministic function of the inputs.",
      "The returned key is statistically safe for producing a stream of new pseudo-random values."
    ],
    "code_examples": []
  },
  {
    "title": "PRNG Key Splitting",
    "concepts": [
      "Splitting a PRNG key into multiple new keys.",
      "Adding a leading axis to the key.",
      "The input key is an ArrayLike object.",
      "The number of keys to produce is specified by 'num', defaulting to 2.",
      "The output is an array-like object of new PRNG keys."
    ],
    "code_examples": []
  },
  {
    "title": "Key Cloning for Reuse",
    "concepts": [
      "Cloning a JAX random key allows for its reuse in multiple operations without side effects.",
      "Outside the key reuse checking context, cloning acts as an identity function.",
      "Cloned keys produce the same random numbers as the original key when used with JAX random number generation functions.",
      "jax.random.clone creates an independent copy of a PRNGKey."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating key cloning and its effect on random number generation using jax.random.uniform.",
        "code": "import jax\n\nkey = jax.random.key(0)\ndata = jax.random.uniform(key)\ncloned_key = jax.random.clone(key)\nsame_data = jax.random.uniform(cloned_key)\nassert data == same_data"
      },
      {
        "description": "Another Example demonstrating key cloning and its effect on random number generation using jax.random.uniform.",
        "code": "import jax\nkey = jax.random.key(0)\ndata = jax.random.uniform(key)\ncloned_key = jax.random.clone(key)\nsame_data = jax.random.uniform(cloned_key)\nassert data == same_data"
      }
    ]
  },
  {
    "title": "Sampling from Unit Lp Ball",
    "concepts": [
      "Sampling uniformly from the unit Lp ball.",
      "Uses a PRNG key for random number generation.",
      "Specifies the dimensionality of the ball.",
      "Defines the p parameter of the Lp norm.",
      "Allows for batch dimensions in the result.",
      "Specifies the data type of the returned values."
    ],
    "code_examples": []
  },
  {
    "title": "Bernoulli Random Values",
    "concepts": [
      "Generates Bernoulli distributed random values.",
      "The values are either 0 or 1.",
      "The probability mass function is f(k; p) = p^k(1 - p)^(1 - k).",
      "Requires a PRNG key.",
      "The mean 'p' determines the probability of success (value 1).",
      "The shape of the output array can be specified."
    ],
    "code_examples": []
  },
  {
    "title": "Binomial Random Values",
    "concepts": [
      "Generates binomial random values with a given shape and float dtype.",
      "The values are returned according to the probability mass function: f(k;n,p) = (n choose k) * p^k * (1-p)^(n-k).",
      "The domain is 0 < p < 1.",
      "n is a nonnegative integer representing the number of trials.",
      "p is a float representing the probability of success of an individual trial.",
      "key is a PRNG key used as the random key.",
      "n is a float or array of floats broadcast-compatible with shape representing the number of trials.",
      "p is a float or array of floats broadcast-compatible with shape representing the probability of success of an individual trial.",
      "shape is an optional tuple of nonnegative integers specifying the result shape. Must be broadcast-compatible with n and p.",
      "The default shape (None) produces a result shape equal to np.broadcast(n, p).shape.",
      "dtype is an optional float dtype for the returned values (default float64 if jax_enable_x64 is true, otherwise float32).",
      "Returns a random array with the specified dtype and with shape given by np.broadcast(n, p).shape."
    ],
    "code_examples": []
  },
  {
    "title": "Sampling Uniform Bits",
    "concepts": [
      "Generates uniform random bits as unsigned integers.",
      "Uses a PRNG key for random number generation.",
      "Allows specifying the shape of the output array.",
      "Provides option to set unsigned integer dtype for returned values."
    ],
    "code_examples": []
  },
  {
    "title": "Chisquare Random Value Generation",
    "concepts": [
      "Generates Chisquare distributed random values.",
      "The values follow the probability density function f(x; \u03bd) \u221d x^(\u03bd/2 - 1)e^(-x/2).",
      "The domain is 0 < x < \u221e.",
      "\u03bd represents the degrees of freedom, defined by the parameter df.",
      "The function takes a PRNG key, df (degrees of freedom), shape, and dtype as input.",
      "It returns a random array with the specified dtype and shape."
    ],
    "code_examples": []
  },
  {
    "title": "Random Sampling from an Array",
    "concepts": [
      "This function generates a random sample from a given array.",
      "If p has fewer non-zero elements than the requested number of samples, and replace=False, the output is ill-defined.",
      "The 'key' argument is a PRNG key used as the random key.",
      "The 'a' argument is the array or integer from which to sample.",
      "If 'a' is an integer, the sample is generated as if 'a' were arange(a).",
      "The 'shape' argument is the desired output shape.",
      "The 'replace' argument determines whether the sample is with or without replacement (default is True).",
      "The 'p' argument specifies the probabilities associated with each entry in 'a'.",
      "If 'p' is not given, a uniform distribution is assumed.",
      "The 'axis' argument specifies the axis along which the selection is performed (default is 0)."
    ],
    "code_examples": []
  },
  {
    "title": "Dirichlet Distribution",
    "concepts": [
      "Sampling Dirichlet random values with given shape and float dtype.",
      "The values are distributed according to a specific probability density function.",
      "The probability density function is defined as f({x_i}; {\u03b1_i}) \u221d \u220f_{i=1}^k x_i^{\u03b1_i - 1}.",
      "The sum of x_i from i=1 to k equals 1.",
      "0 <= x_i <= 1 for all x_i.",
      "key: PRNG key used as the random key.",
      "alpha: Concentration parameter of the random variables.",
      "shape: Batch shape of the result.",
      "dtype: Float dtype for the returned values."
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Random Values",
    "concepts": [
      "Generation of exponential random values.",
      "The probability density function is f(x) = e^{-x} on the domain 0 <= x < infinity.",
      "The function takes a PRNG key, shape, and dtype as input.",
      "Returns a random array with the specified shape and dtype."
    ],
    "code_examples": []
  },
  {
    "title": "F-Distribution Random Values",
    "concepts": [
      "Generates random values following the F-distribution.",
      "The F-distribution is defined by two degrees of freedom parameters: dfnum (numerator) and dfden (denominator).",
      "The probability density function of the F-distribution is given by a specific formula.",
      "The domain of the F-distribution is 0 < x < infinity.",
      "The function takes a PRNG key, numerator degrees of freedom (dfnum), denominator degrees of freedom (dfden), shape, and dtype as input.",
      "The shape of the output array can be specified, and it must be broadcast-compatible with dfnum and dfden.",
      "The default shape is determined by dfnum.shape and dfden.shape.",
      "The dtype of the output array is float64 by default if jax_enable_x64 is true, otherwise it is float32."
    ],
    "code_examples": []
  },
  {
    "title": "Gamma Distribution Sampling",
    "concepts": [
      "Gamma random values are sampled with a given shape and float dtype.",
      "The probability density function is f(x;a) \u221d x^(a - 1) e^(-x) for 0 <= x < \u221e, with a > 0.",
      "This is the standard gamma density with a unit scale/rate parameter.",
      "Dividing the sample output by the rate is equivalent to sampling from gamma(a, rate).",
      "Multiplying the sample output by the scale is equivalent to sampling from gamma(a, scale)."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters for Gamma Sampling",
    "concepts": [
      "key: A PRNG key used as the random key.",
      "a: A float or array of floats broadcast-compatible with shape, representing the parameter of the distribution.",
      "shape: An optional tuple of nonnegative integers specifying the result shape. Must be broadcast-compatible with a. The default (None) produces a result shape equal to a.shape.",
      "dtype: An optional float dtype for the returned values (default float64 if jax_enable_x64 is true, otherwise float32)."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value and Related Information",
    "concepts": [
      "The function returns a random array with the specified dtype and shape.",
      "The shape is determined by the shape parameter if provided, otherwise by a.shape.",
      "See also accuracy for small values of a."
    ],
    "code_examples": []
  },
  {
    "title": "Generalized Normal Distribution Sampling",
    "concepts": [
      "Samples are generated from a generalized normal distribution.",
      "The probability density function is proportional to e^(-|x|^p).",
      "The domain is -\u221e < x < \u221e.",
      "p > 0 is the shape parameter.",
      "key is a PRNG key.",
      "p is a float representing the shape parameter.",
      "shape defines the batch dimensions of the result.",
      "dtype specifies the data type of the returned values."
    ],
    "code_examples": []
  },
  {
    "title": "Geometric Random Values",
    "concepts": [
      "Generating Geometric random values with a specified shape and float dtype.",
      "The values are returned based on the probability mass function f(k;p) = p(1-p)^(k-1).",
      "The domain is 0 < p < 1.",
      "The 'key' parameter is a PRNG key.",
      "The 'p' parameter represents the probability of success.",
      "The 'shape' parameter defines the output shape.",
      "The 'dtype' parameter specifies the integer data type of the output."
    ],
    "code_examples": []
  },
  {
    "title": "Gumbel Random Value Generation",
    "concepts": [
      "Gumbel distribution probability density function is f(x) = e^{-(x + e^{-x})}",
      "A PRNG key is used as the random key.",
      "The shape parameter defines the output array dimensions.",
      "The dtype parameter specifies the data type of the output array.",
      "The mode parameter configures sampling bit usage."
    ],
    "code_examples": []
  },
  {
    "title": "Laplace Random Value Generation",
    "concepts": [
      "Generation of Laplace random values with a specified shape and float dtype.",
      "The values are distributed according to the probability density function f(x) = (1/2)e^{-|x|}.",
      "Requires a PRNG key for randomness.",
      "Allows specification of shape and dtype for the output array.",
      "Returns a random array with the specified shape and dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Log-Gamma Random Value Sampling",
    "concepts": [
      "Samples log-gamma random values with given shape and float dtype.",
      "Log-gamma sampling provides better precision for samples very close to zero (when a << 1).",
      "The function ensures that exp(loggamma(*args)) is close to gamma(*args) within a given tolerance."
    ],
    "code_examples": [
      {
        "description": "Assertion that loggamma and gamma functions are related by exponentiation within a tolerance.",
        "code": "np\n.\ntesting\n.\nassert_allclose\n(\njnp\n.\nexp\n(\nloggamma\n(\n*\nargs\n)),\ngamma\n(\n*\nargs\n),\nrtol\n=\nrtol\n)\nnp\n.\ntesting\n.\nassert_allclose\n(\njnp\n.\nexp\n(\nloggamma\n(\n*\nargs\n)),\ngamma\n(\n*\nargs\n),\nrtol\n=\nrtol\n)"
      }
    ]
  },
  {
    "title": "Parameters and Return Value",
    "concepts": [
      "key: a PRNG key used as the random key.",
      "a: a float or array of floats representing the parameter of the distribution, broadcast-compatible with shape.",
      "shape: optional tuple of nonnegative integers specifying the result shape, broadcast-compatible with a.",
      "dtype: optional float dtype for the returned values (default float64 if jax_enable_x64 is true, otherwise float32).",
      "Returns a random array with the specified dtype and shape (either given shape or a.shape)."
    ],
    "code_examples": []
  },
  {
    "title": "Logistic Random Values",
    "concepts": [
      "Generation of logistic random values.",
      "The values are distributed according to a specified probability density function.",
      "The function takes a PRNG key, shape, and dtype as input.",
      "The output is a random array with the specified shape and dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Lognormal Distribution",
    "concepts": [
      "Generates lognormally distributed random values.",
      "The distribution is defined by a probability density function.",
      "The function takes a PRNG key, standard deviation (sigma), shape, and dtype as input.",
      "Sigma represents the standard deviation of the underlying normal distribution.",
      "The shape parameter determines the shape of the output array.",
      "The dtype parameter specifies the data type of the output array."
    ],
    "code_examples": []
  },
  {
    "title": "Maxwell Distribution Sampling",
    "concepts": [
      "Samples are drawn from a one-sided Maxwell distribution.",
      "The probability density function is proportional to x^2 * e^(-x^2 / 2).",
      "The domain of the distribution is 0 <= x < infinity.",
      "The function takes a PRNG key as input.",
      "The function takes the desired shape of the returned samples as input.",
      "The function takes the data type for samples as input.",
      "The function returns a jnp.array of samples."
    ],
    "code_examples": []
  },
  {
    "title": "Multivariate Normal Distribution",
    "concepts": [
      "Generates random values from a multivariate normal distribution.",
      "The probability density function is defined by a formula.",
      "Requires a PRNG key, mean vector, and covariance matrix as input.",
      "Supports specifying the output shape and data type.",
      "Offers different methods for covariance factorization: 'svd', 'eigh', and 'cholesky'.",
      "The default method is 'cholesky'.",
      "Uses 'svd' or 'eigh' for singular covariance matrices.",
      "Returns a random array with the specified shape and dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Orthogonal/Unitary Matrix Sampling",
    "concepts": [
      "Samples uniformly from the orthogonal group O(n) or unitary group U(n).",
      "For complex dtype, samples from the unitary group.",
      "Generates semi-orthogonal matrices if rows and columns are unequal.",
      "Describes the orthonormal properties of the resulting matrix A and its conjugate transpose A^* depending on whether n <= m or m <= n.",
      "key is a PRNG key used as the random key.",
      "n represents the number of rows.",
      "m represents the number of columns, defaulting to n.",
      "shape represents the batch dimensions of the result.",
      "dtype is the float datatype for the returned values."
    ],
    "code_examples": []
  },
  {
    "title": "Poisson Distribution Sampling",
    "concepts": [
      "Poisson distribution probability mass function: f(k; \u03bb) = (\u03bb^k * e^-\u03bb) / k!",
      "k is a non-negative integer.",
      "\u03bb > 0 is the rate parameter (mean of the distribution).",
      "Function takes a PRNG key, rate parameter (lam), shape, and dtype as input.",
      "The shape argument is optional. If not provided, it defaults to the shape of lam.",
      "The dtype argument specifies the integer data type of the returned values; defaults to int64 if jax_enable_x64 is true, otherwise int32.",
      "Returns a random array with the specified dtype and shape."
    ],
    "code_examples": []
  },
  {
    "title": "Rademacher Distribution Overview",
    "concepts": [
      "The Rademacher distribution generates values of either -1 or 1.",
      "The probability mass function is f(k) = 1/2 * (delta(k - 1) + delta(k + 1)).",
      "The domain of the distribution is k \u2208 {-1, 1}."
    ],
    "code_examples": []
  },
  {
    "title": "Rademacher Distribution Parameters and Output",
    "concepts": [
      "key: A PRNG key is required for random number generation.",
      "shape: Specifies the shape of the output array; defaults to ().",
      "dtype: Determines the data type of the samples, expected to be an integer type.",
      "Output: Returns a JAX array of samples with the specified shape, where each element has an equal chance of being 1 or -1."
    ],
    "code_examples": []
  },
  {
    "title": "Uniform Random Value Generation",
    "concepts": [
      "Generating uniform random values within a specified range [minval, maxval).",
      "Using a PRNG key for random number generation.",
      "Specifying the shape of the output array.",
      "Defining the minimum (inclusive) and maximum (exclusive) values for the range.",
      "Selecting the data type (dtype) for the returned values.",
      "The default dtype is int64 if jax_enable_x64 is true, otherwise int32."
    ],
    "code_examples": []
  },
  {
    "title": "Rayleigh Distribution Sampling",
    "concepts": [
      "Generating Rayleigh-distributed random values.",
      "Rayleigh probability density function depends on scale parameter sigma.",
      "The function takes a PRNG key, scale, shape, and dtype as input.",
      "The shape must be broadcast-compatible with the scale parameter.",
      "The default shape is equal to scale.shape.",
      "The dtype must be a float type."
    ],
    "code_examples": []
  },
  {
    "title": "Student's t-distribution Sampling",
    "concepts": [
      "Generating random values from a Student's t-distribution.",
      "The distribution is defined by its degrees of freedom (df).",
      "The distribution's probability density function is defined as f(t; \u03bd) \u221d (1 + t^2/\u03bd)^(-(\u03bd + 1)/2).",
      "The function requires a PRNG key, degrees of freedom, desired shape, and data type.",
      "The shape of the output array is determined by the input shape or the shape of the degrees of freedom array.",
      "The data type of the output array defaults to float64 if jax_enable_x64 is true, otherwise float32."
    ],
    "code_examples": []
  },
  {
    "title": "Triangular Distribution Random Values",
    "concepts": [
      "Generates triangular distributed random numbers.",
      "The distribution is defined by a lower limit (left), peak value (mode), and upper limit (right).",
      "The output shape is determined by the shape parameter or the broadcasted shape of left, mode, and right.",
      "The output dtype is float64 or float32 based on jax_enable_x64 setting.",
      "The random key (key) is used for the random number generation."
    ],
    "code_examples": []
  },
  {
    "title": "Uniform Random Value Generation",
    "concepts": [
      "Generating uniform random values within a specified range [minval, maxval).",
      "Using a PRNG key for random number generation.",
      "Specifying the shape of the output array.",
      "Selecting the data type (dtype) of the output array.",
      "Broadcasting minval and maxval to the specified shape."
    ],
    "code_examples": []
  },
  {
    "title": "Weibull Distribution Sampling",
    "concepts": [
      "Sampling from a Weibull distribution.",
      "The Weibull distribution is defined by its probability density function.",
      "The probability density function depends on the scale and concentration parameters.",
      "The scale parameter (\u03c3) must be greater than 0.",
      "The concentration parameter (c) must be greater than 0.",
      "The domain of the Weibull distribution is 0 < x < \u221e."
    ],
    "code_examples": []
  },
  {
    "title": "Sharding Concepts and Attributes",
    "concepts": [
      "Describes how a jax.Array is laid out across devices.",
      "Sharding includes addressable devices indices map.",
      "Sharding spans a set of devices, which includes non-addressable devices in multi-controller JAX.",
      "Sharding provides a mapping from devices to the array slices each contains.",
      "Shardings are equivalent if they place the same logical array shards on the same devices.",
      "A sharding is fully addressable if the current process can address all devices named in the Sharding.",
      "A sharding is fully replicated if each device has a complete copy of the entire data.",
      "Sharding has a memory kind attribute.",
      "Sharding contains a number of devices.",
      "Sharding provides the shape of the data on each device.",
      "A new Sharding instance can be created with a specified memory kind."
    ],
    "code_examples": []
  },
  {
    "title": "SingleDeviceSharding",
    "concepts": [
      "SingleDeviceSharding places its data on a single device.",
      "It inherits from Sharding.",
      "It is fully addressable if the current process can address all devices named in the Sharding.",
      "It is fully replicated if each device has a complete copy of the entire data."
    ],
    "code_examples": [
      {
        "description": "Example of creating a SingleDeviceSharding instance.",
        "code": "single_device_sharding = jax.sharding.SingleDeviceSharding(jax.devices()[0])"
      },
      {
        "description": "Example of creating a SingleDeviceSharding instance.",
        "code": "single_device_sharding = jax.sharding.SingleDeviceSharding(jax.devices()[0])"
      }
    ]
  },
  {
    "title": "NamedSharding",
    "concepts": [
      "NamedSharding expresses sharding using named axes.",
      "It is a pair of a Mesh of devices and PartitionSpec.",
      "Mesh is a multidimensional NumPy array of JAX devices, where each axis has a name.",
      "PartitionSpec describes how an input dimension is partitioned across zero or more mesh dimensions.",
      "It inherits from Sharding.",
      "It is fully addressable if the current process can address all devices named in the Sharding.",
      "It is fully replicated if each device has a complete copy of the entire data."
    ],
    "code_examples": [
      {
        "description": "Example of creating a NamedSharding instance.",
        "code": "from jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\nmesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))\nspec = P('x', 'y')\nnamed_sharding = jax.sharding.NamedSharding(mesh, spec)"
      },
      {
        "description": "Example of creating a NamedSharding instance.",
        "code": "from jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\nmesh = Mesh(np.array(jax.devices()).reshape(2, 4), ('x', 'y'))\nspec = P('x', 'y')\nnamed_sharding = jax.sharding.NamedSharding(mesh, spec)"
      }
    ]
  },
  {
    "title": "PositionalSharding",
    "concepts": [
      "PositionalSharding describes a sharding used by jax.pmap().",
      "It inherits from Sharding.",
      "It is fully addressable if the current process can address all devices named in the Sharding.",
      "It is fully replicated if each device has a complete copy of the entire data."
    ],
    "code_examples": []
  },
  {
    "title": "GSPMDSharding",
    "concepts": [
      "Inherits from Sharding.",
      "Describes how to partition an array across a mesh of devices.",
      "Tuple describing how to partition an array across a mesh of devices.",
      "Each element is either None , a string, or a tuple of strings.",
      "It is fully addressable if the current process can address all devices named in the Sharding.",
      "It is fully replicated if each device has a complete copy of the entire data."
    ],
    "code_examples": []
  },
  {
    "title": "PartitionSpec",
    "concepts": [
      "Tuple describing how to partition an array across a mesh of devices.",
      "Each element is either None , a string, or a tuple of strings.",
      "This class exists so JAX\u2019s pytree utilities can distinguish a partition specifications from tuples that should be treated as pytrees."
    ],
    "code_examples": []
  },
  {
    "title": "Mesh",
    "concepts": [
      "Declares hardware resources available.",
      "All axis_names become valid resource names inside the managed block.",
      "Used with jax.experimental.pjit.pjit()."
    ],
    "code_examples": [
      {
        "description": "Example of using Mesh with pjit in a context manager.",
        "code": "from jax.experimental.pjit import pjit\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\ninp = np.arange(16).reshape((8, 2))\ndevices = np.array(jax.devices()).reshape(4, 2)\n\n# Declare a 2D mesh with axes `x` and `y`.\nglobal_mesh = Mesh(devices, ('x', 'y'))\n\n# Use the mesh object directly as a context manager.\nwith global_mesh:\n  out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)"
      },
      {
        "description": "Example of using Mesh with pjit in a context manager.",
        "code": "from jax.experimental.pjit import pjit\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\ninp = np.arange(16).reshape((8, 2))\ndevices = np.array(jax.devices()).reshape(4, 2)\n\n# Declare a 2D mesh with axes `x` and `y`.\nglobal_mesh = Mesh(devices, ('x', 'y'))\n\n# Use the mesh object directly as a context manager.\nwith global_mesh:\n  out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)"
      },
      {
        "description": "Example of using Mesh initialized as a context manager.",
        "code": "from jax.experimental.pjit import pjit\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\ninp = np.arange(16).reshape((8, 2))\ndevices = np.array(jax.devices()).reshape(4, 2)\n\n# Initialize the Mesh and use the mesh as the context manager.\nwith Mesh(devices, ('x', 'y')) as global_mesh:\n  out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)"
      },
      {
        "description": "Example of using Mesh initialized as a context manager.",
        "code": "from jax.experimental.pjit import pjit\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\ninp = np.arange(16).reshape((8, 2))\ndevices = np.array(jax.devices()).reshape(4, 2)\n\n# Initialize the Mesh and use the mesh as the context manager.\nwith Mesh(devices, ('x', 'y')) as global_mesh:\n  out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)"
      },
      {
        "description": "Example of using Mesh with `with ... as ...`.",
        "code": "from jax.experimental.pjit import pjit\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\ninp = np.arange(16).reshape((8, 2))\ndevices = np.array(jax.devices()).reshape(4, 2)\n\n# Also you can use it as `with ... as ...`.\nglobal_mesh = Mesh(devices, ('x', 'y'))\nwith global_mesh as m:\n  out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)"
      },
      {
        "description": "Example of using Mesh with `with ... as ...`.",
        "code": "from jax.experimental.pjit import pjit\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\ninp = np.arange(16).reshape((8, 2))\ndevices = np.array(jax.devices()).reshape(4, 2)\n\n# Also you can use it as `with ... as ...`.\nglobal_mesh = Mesh(devices, ('x', 'y'))\nwith global_mesh as m:\n  out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)"
      },
      {
        "description": "Example of using Mesh as `with Mesh(...)`.",
        "code": "from jax.experimental.pjit import pjit\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\ninp = np.arange(16).reshape((8, 2))\ndevices = np.array(jax.devices()).reshape(4, 2)\n\n# You can also use it as `with Mesh(...)`.\nwith Mesh(devices, ('x', 'y')):\n  out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)"
      },
      {
        "description": "Example of using Mesh as `with Mesh(...)`.",
        "code": "from jax.experimental.pjit import pjit\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\ninp = np.arange(16).reshape((8, 2))\ndevices = np.array(jax.devices()).reshape(4, 2)\n\n# You can also use it as `with Mesh(...)`.\nwith Mesh(devices, ('x', 'y')):\n  out = pjit(lambda x: x, in_shardings=None, out_shardings=None)(inp)"
      }
    ]
  },
  {
    "title": "Overview of JAX Runtime Value Debugging",
    "concepts": [
      "JAX provides runtime value debugging features.",
      "These features are useful for inspecting and visualizing array shardings inside (and outside) staged functions."
    ],
    "code_examples": []
  },
  {
    "title": "Debugging with Callbacks, Prints, and Breakpoints",
    "concepts": [
      "callback: Calls a stageable Python callback within JAX functions.",
      "print: Prints values within staged-out JAX functions.",
      "breakpoint: Enters a breakpoint at a specific point in a JAX program."
    ],
    "code_examples": []
  },
  {
    "title": "Inspecting and Visualizing Array Shardings",
    "concepts": [
      "inspect_array_sharding: Enables inspecting array sharding inside JIT-ted functions.",
      "visualize_array_sharding: Visualizes an array's sharding.",
      "visualize_sharding: Visualizes a Sharding using rich."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.debug.callback",
    "concepts": [
      "jax.debug.callback calls a stageable Python callback inside JAX programs.",
      "It follows JAX's pure operational semantics, meaning side-effects might be dropped, duplicated, or reordered.",
      "The design aims for innocuousness, minimizing changes to the JAX computation while revealing computation details.",
      "The purpose is to enable debugging and understanding of JAX programs.",
      "It is used with staged JAX programs."
    ],
    "code_examples": []
  },
  {
    "title": "jax.debug.callback Arguments",
    "concepts": [
      "callback: A Python callable returning None.",
      "*args: Positional arguments passed to the callback.",
      "ordered: Enforces ordering of callbacks with respect to others.",
      "**kwargs: Keyword arguments passed to the callback."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "The function returns None."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.experimental.io_callback() is designed for impure functions.",
      "jax.pure_callback() is designed for pure functions.",
      "jax.debug.print() is designed for printing."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jax.debug.print",
    "concepts": [
      "jax.debug.print prints values and works in staged out JAX functions.",
      "f-strings should not be used directly with jax.debug.print.",
      "Instead, use standard string formatting with named arguments.",
      "jax.debug.print is a convenience wrapper around jax.debug.callback.",
      "jax.debug.callback can be used directly for more complex debugging scenarios like logging."
    ],
    "code_examples": []
  },
  {
    "title": "Implementation of jax.debug.print",
    "concepts": [
      "jax.debug.print internally uses jax.debug.callback to print values.",
      "It formats the input string using the provided arguments."
    ],
    "code_examples": [
      {
        "description": "Implementation of jax.debug.print using jax.debug.callback",
        "code": "def debug_print(fmt: str, *args, **kwargs):\n    jax.debug.callback(\n        lambda *args, **kwargs: print(fmt.format(*args, **kwargs)),\n        *args,\n        **kwargs\n    )"
      },
      {
        "description": "Redundant definition of jax.debug.print implementation",
        "code": "def debug_print(fmt: str, *args, **kwargs):\n    jax.debug.callback(\n        lambda *args, **kwargs: print(fmt.format(*args, **kwargs)),\n        *args,\n        **kwargs\n    )"
      }
    ]
  },
  {
    "title": "Arguments for jax.debug.print",
    "concepts": [
      "fmt (str): A format string, e.g., \"hello {x}\".",
      "*args: Positional arguments to be formatted.",
      "ordered (bool): A keyword only argument used to indicate whether or not the staged out computation will enforce ordering of this jax.debug.print w.r.t. other ordered jax.debug.print calls.",
      "**kwargs: Keyword arguments to be formatted."
    ],
    "code_examples": []
  },
  {
    "title": "Breakpoint Function Description",
    "concepts": [
      "The function sets a breakpoint in a program for debugging.",
      "It allows specifying the debugger backend.",
      "It provides options to filter JAX-internal stack frames.",
      "It controls the number of stack frames available for inspection.",
      "It can enforce ordering with other debug calls using the 'ordered' argument.",
      "It can trigger based on the computation of a JAX array using the 'token' argument.",
      "The 'token' argument ensures the breakpoint is only hit after the token value is computed.",
      "If the 'token' return value is unused, the breakpoint is pruned."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of inspect_array_sharding",
    "concepts": [
      "inspect_array_sharding enables inspecting array sharding inside JIT-ted functions.",
      "It works in pjit-ted computations.",
      "The callback is called as early as possible when sharding information is available.",
      "The callback behavior depends on whether it's called without transformations, inside jax.jit, or inside pjit.",
      "The callback will be triggered by running the function, but not after the function is compiled and cached.",
      "This function is experimental and its behavior may change in the future."
    ],
    "code_examples": []
  },
  {
    "title": "inspect_array_sharding Usage Example",
    "concepts": [
      "The example demonstrates printing the sharding of an intermediate value in a pjit-ted computation.",
      "It uses jax, jax.numpy, jax.experimental.pjit, and jax.sharding libraries.",
      "It defines a function that applies sine and square operations, using inspect_array_sharding to print the sharding of the intermediate sine result.",
      "It uses pjit to shard the input and output across devices.",
      "The .lower().compile() AOT API triggers the callback at compile time."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using inspect_array_sharding to inspect the sharding of an intermediate value in a pjit-ted computation.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.experimental.pjit import pjit\nfrom jax.sharding import Mesh, PartitionSpec\n\nx = jnp.arange(8, dtype=jnp.float32)\n\ndef f_(x):\n    x = jnp.sin(x)\n    jax.debug.inspect_array_sharding(x, callback=print)\n    return jnp.square(x)\n\nf = pjit(\n    f_,\n    in_shardings=PartitionSpec('dev'),\n    out_shardings=PartitionSpec('dev')\n)\n\nwith Mesh(jax.devices(), ('dev',)):\n    f.lower(x).compile()\n"
      },
      {
        "description": "Demonstrates using inspect_array_sharding to inspect the sharding of an intermediate value in a pjit-ted computation.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.experimental.pjit import pjit\nfrom jax.sharding import Mesh, PartitionSpec\n\nx = jnp.arange(8, dtype=jnp.float32)\n\ndef f_(x):\n    x = jnp.sin(x)\n    jax.debug.inspect_array_sharding(x, callback=print)\n    return jnp.square(x)\n\nf = pjit(\n    f_,\n    in_shardings=PartitionSpec('dev'),\n    out_shardings=PartitionSpec('dev')\n)\n\nwith Mesh(jax.devices(), ('dev',)):\n    f.lower(x).compile()\n"
      }
    ]
  },
  {
    "title": "Array Sharding Visualization",
    "concepts": [
      "Visualization of array sharding is the main topic.",
      "The document's purpose is to show how arrays are sharded visually."
    ],
    "code_examples": []
  },
  {
    "title": "Sharding Visualization",
    "concepts": [
      "Visualizing sharding configurations.",
      "Using the rich library for visualization.",
      "Customizing the shape of the visualization.",
      "Adjusting scale, minimum width, and maximum width.",
      "Applying a colormap for visual differentiation."
    ],
    "code_examples": []
  },
  {
    "title": "DLPack Conversion Functions",
    "concepts": [
      "from_dlpack() creates an Array from a DLPack tensor.",
      "to_dlpack() returns a DLPack tensor representing an Array."
    ],
    "code_examples": [
      {
        "description": "Example of from_dlpack function signature",
        "code": "from_dlpack (external_array[,\u00a0device,\u00a0copy])"
      },
      {
        "description": "Example of to_dlpack function signature",
        "code": "to_dlpack (x[,\u00a0stream,\u00a0src_device,\u00a0...])"
      }
    ]
  },
  {
    "title": "DLPack Tensor Representation with JAX",
    "concepts": [
      "Returns an Array representation of a DLPack tensor.",
      "Returned Array shares memory with external_array if no device transfer or copy was requested.",
      "external_array is an array object with __dlpack__ and __dlpack_device__ methods.",
      "device specifies the device on which the returned array should be placed.",
      "If device is given, the result is committed to the device.",
      "If device is unspecified, the resulting array will be unpacked onto the same device it originated from.",
      "Setting device to a device different from the source of external_array will require a copy.",
      "copy controls whether a copy is performed.",
      "copy=True always performs a copy, even if unpacked onto the same device.",
      "copy=False never performs a copy and raises an error if necessary.",
      "copy=None may perform a copy if needed for a device transfer.",
      "JAX arrays are always immutable, but DLPack buffers cannot be marked as immutable.",
      "In-place mutation of DLPack buffers may lead to undefined behavior when using the associated JAX array."
    ],
    "code_examples": []
  },
  {
    "title": "DLPack Tensor Creation from Array",
    "concepts": [
      "The function returns a DLPack tensor that encapsulates a given array.",
      "The array can reside on either CPU or GPU.",
      "An optional stream parameter allows waiting for buffer readiness.",
      "The stream argument corresponds to the __dlpack__ documented in the DLPack specification.",
      "src_device specifies the CPU or GPU device.",
      "dl_device is a tuple of (dl_device_type, local_hardware_id) in DLPack format",
      "max_version specifies the maximum DLPack version supported by the consumer",
      "The function may not return a capsule of version max_version",
      "The 'copy' argument controls whether to copy the input array. copy=True always copies, copy=False never copies, copy=None avoids copies if possible.",
      "A DLPack PyCapsule object is returned.",
      "DLPackManagedTensor buffers cannot be marked immutable which can lead to undefined behavior if mutated externally.",
      "Future support for DLManagedTensorVersioned (DLPack 1.0) will enable specifying read-only buffers."
    ],
    "code_examples": []
  },
  {
    "title": "Initialization",
    "concepts": [
      "The `initialize` function initializes the JAX distributed system.",
      "The `initialize` function takes a list of coordinator addresses as an argument."
    ],
    "code_examples": []
  },
  {
    "title": "Shutdown",
    "concepts": [
      "The `shutdown` function shuts down the distributed system."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Distributed System Initialization",
    "concepts": [
      "The `initialize()` function prepares JAX for multi-host GPU and Cloud TPU execution.",
      "`initialize()` must be called before any JAX computations.",
      "The JAX distributed system enables process discovery, health checking, and distributed checkpointing.",
      "Arguments can be automatically detected when using TPU, Slurm, or Open MPI.",
      "The `cluster_detection_method` allows choosing a specific method for argument detection.",
      "For other MPI installations, using mpi4py can bootstrap arguments.",
      "If automatic detection fails, `coordinator_address`, `num_processes`, `process_id`, and `local_device_ids` must be provided.",
      "Proxy variables might need to be unset to avoid timeouts during initialization.",
      "The `coordinator_bind_address` allows specifying the address for the coordinator service."
    ],
    "code_examples": []
  },
  {
    "title": "Initialization Parameters",
    "concepts": [
      "coordinator_address: IP address and port for process 0 to launch a coordinator service.",
      "num_processes: Total number of processes in the distributed system.",
      "process_id: Unique ID of the current process (0 to num_processes - 1).",
      "local_device_ids: Restricts visible devices to the current process.",
      "cluster_detection_method: String for autodetecting the distributed run configuration.",
      "initialization_timeout: Time period (in seconds) for connection retries.",
      "coordinator_bind_address: Address and port for the coordinator service to bind to on process 0."
    ],
    "code_examples": []
  },
  {
    "title": "Example Initialization",
    "concepts": [
      "Illustrates how to initialize JAX in a distributed environment with two GPU processes.",
      "Demonstrates the required parameters on process 0 (the coordinator).",
      "Demonstrates the required parameters on process 1."
    ],
    "code_examples": [
      {
        "description": "Initialization on process 0",
        "code": "jax.distributed.initialize(\ncoordinator_address='10.0.0.1:1234',\nnum_processes=2,\nprocess_id=0\n)\njax.distributed.initialize(\ncoordinator_address='10.0.0.1:1234',\nnum_processes=2,\nprocess_id=0\n)"
      },
      {
        "description": "Initialization on process 1",
        "code": "jax.distributed.initialize(\ncoordinator_address='10.0.0.1:1234',\nnum_processes=2,\nprocess_id=1\n)\njax.distributed.initialize(\ncoordinator_address='10.0.0.1:1234',\nnum_processes=2,\nprocess_id=1\n)"
      }
    ]
  },
  {
    "title": "Shutdown Operation",
    "concepts": [
      "The operation shuts down the distributed system.",
      "If the system is not running, the operation does nothing."
    ],
    "code_examples": []
  },
  {
    "title": "bfloat16 Floating-Point Values",
    "concepts": [
      "bfloat16 is a floating-point data type.",
      "It represents bfloat16 floating-point values."
    ],
    "code_examples": []
  },
  {
    "title": "Canonicalizing Data Types",
    "concepts": [
      "canonicalize_dtype converts a dtype to a canonical dtype.",
      "The conversion is based on the x64_enabled configuration."
    ],
    "code_examples": []
  },
  {
    "title": "float0 Data Type",
    "concepts": [
      "float0 is a DType class.",
      "It corresponds to the scalar type and dtype of the same name."
    ],
    "code_examples": []
  },
  {
    "title": "Subtype Check",
    "concepts": [
      "issubdtype checks if a is a subtype of b.",
      "It returns True if a is lower/equal in the type hierarchy."
    ],
    "code_examples": []
  },
  {
    "title": "PRNG Key",
    "concepts": [
      "prng_key is a scalar class.",
      "It is used for PRNG Key dtypes."
    ],
    "code_examples": []
  },
  {
    "title": "Result Type Determination",
    "concepts": [
      "result_type is a function for JAX argument dtype promotion.",
      "It determines the resulting dtype from input arguments."
    ],
    "code_examples": []
  },
  {
    "title": "Scalar Type of a JAX Value",
    "concepts": [
      "scalar_type_of returns the scalar type associated with a JAX value."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of bfloat16 Scalar Methods",
    "concepts": [
      "bfloat16 is a floating-point data type.",
      "Scalar methods listed are identical to corresponding array attributes.",
      "Methods include mathematical operations, array manipulations, and data access."
    ],
    "code_examples": []
  },
  {
    "title": "bfloat16 Scalar Attributes",
    "concepts": [
      "Scalar attributes provide information about the bfloat16 value.",
      "Attributes include data pointers, data type information, and shape.",
      "Some attributes are identical to corresponding array attributes."
    ],
    "code_examples": []
  },
  {
    "title": "Dtype Conversion",
    "concepts": [
      "Converting a dtype to a canonical dtype.",
      "Conversion is based on the config.x64_enabled setting.",
      "The input is a dtype which can be of type Any.",
      "The parameter allow_extended_dtype is a boolean.",
      "The return type can be a DType or ExtendedDType."
    ],
    "code_examples": []
  },
  {
    "title": "DType Class Overview",
    "concepts": [
      "DType class corresponds to scalar type and dtype.",
      "Use numpy.dtype to create dtype instances.",
      "Refer to Data type objects (dtype) for more information."
    ],
    "code_examples": []
  },
  {
    "title": "Typecode Hierarchy Comparison",
    "concepts": [
      "The function checks if the first argument's typecode is lower or equal in the type hierarchy compared to the second argument.",
      "It functions similarly to numpy.issubdtype().",
      "It handles dtype extensions like jax.dtypes.bfloat16 and jax.dtypes.prng_key.",
      "The function accepts DTypeLike, ExtendedDType, or None as arguments.",
      "The function returns a boolean value."
    ],
    "code_examples": []
  },
  {
    "title": "Scalar Class for PRNG Key Dtypes",
    "concepts": [
      "This is an abstract class for PRNG Key dtypes.",
      "It should never be instantiated directly.",
      "It is used to check if a dtype is a PRNG key using jnp.issubdtype."
    ],
    "code_examples": [
      {
        "description": "Check if the dtype of a random key is a prng_key dtype.",
        "code": "from jax import random\nfrom jax import dtypes\nimport jax.numpy as jnp\n\nkey = random.key(0)\n\njnp.issubdtype(key.dtype, dtypes.prng_key)"
      },
      {
        "description": "Check if the dtype of a random key is a prng_key dtype. (duplicate example)",
        "code": "from jax import random\nfrom jax import dtypes\nimport jax.numpy as jnp\n\nkey = random.key(0)\n\njnp.issubdtype(key.dtype, dtypes.prng_key)"
      }
    ]
  },
  {
    "title": "Methods",
    "concepts": [
      "This section lists scalar methods that are identical to corresponding array attributes.",
      "Includes methods like all, any, argmax, argmin, astype, byteswap, choose, clip, compress, conj, conjugate, copy, cumprod, cumsum, diagonal, dump, dumps, fill, flatten, getfield, item, max, mean, min, nonzero, prod, put, ravel, repeat, reshape, resize, round, searchsorted, setfield, setflags, sort, squeeze, std, sum, swapaxes, take, to_device, tobytes, tofile, tolist, tostring, trace, transpose, var, and view."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "This section lists scalar attributes.",
      "T, base, data, device, dtype, flags, flat, imag, itemset, itemsize, nbytes, ndim, newbyteorder, ptp, real, shape, size, strides"
    ],
    "code_examples": []
  },
  {
    "title": "JAX Argument Type Promotion",
    "concepts": [
      "Applies JAX argument dtype promotion.",
      "The `return_weak_type_flag` parameter controls the return type.",
      "If `return_weak_type_flag` is True, a tuple of (dtype, weak_type) is returned.",
      "If `return_weak_type_flag` is False, only the dtype is returned."
    ],
    "code_examples": []
  },
  {
    "title": "Scalar Type of JAX Value",
    "concepts": [
      "The function determines the scalar type associated with a JAX value.",
      "The input 'x' can be of any type."
    ],
    "code_examples": []
  },
  {
    "title": "FFI Functions Overview",
    "concepts": [
      "The jax.extend.ffi module has been moved to jax.ffi.",
      "ffi_call(): Calls a foreign function interface (FFI) target.",
      "ffi_lowering(): Builds a lowering rule for an FFI target.",
      "pycapsule(): Wraps a ctypes function pointer in a PyCapsule.",
      "register_ffi_target(): Registers a foreign function target.",
      "register_ffi_type_id(): Registers a custom type ID for an FFI target."
    ],
    "code_examples": []
  },
  {
    "title": "Function Details (Repeated)",
    "concepts": [
      "ffi_call(): Calls a foreign function interface (FFI) target.",
      "ffi_lowering(): Builds a lowering rule for an FFI target.",
      "pycapsule(): Wraps a ctypes function pointer in a PyCapsule.",
      "register_ffi_target(): Registers a foreign function target."
    ],
    "code_examples": []
  },
  {
    "title": "FFI Target Call",
    "concepts": [
      "Calling a foreign function interface (FFI) target.",
      "The behavior of ffi_call under vmap() depends on the value of vmap_method.",
      "target_name: The name of the XLA FFI custom call target registered using register_ffi_target().",
      "result_shape_dtypes: Shape and dtype of the custom call output(s).",
      "has_side_effect: Specifies whether the custom call has side effects.",
      "vmap_method: Specifies how the FFI call transforms under vmap().",
      "input_layouts: Layouts for each input argument, specifying the memory layout expected by the FFI call target.",
      "output_layouts: Layouts for the output arrays.",
      "input_output_aliases: Dictionary mapping input indices to output indices, indicating aliased arrays.",
      "custom_call_api_version: The version number of the custom call API.",
      "legacy_backend_config: Attributes passed using an opaque string representation (for legacy targets).",
      "Keyword arguments are passed as named attributes to the FFI handler."
    ],
    "code_examples": []
  },
  {
    "title": "FFI Lowering Rule Overview",
    "concepts": [
      "Build a lowering rule for a foreign function interface (FFI) target.",
      "The rule can use input/output abstract values to compute types and shapes, assuming row-major layouts.",
      "Layouts passed as tuples should be in minor-to-major order (XLA).",
      "Keyword arguments passed to the lowering rule are treated as attributes and added to backend_config."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Lowering Rule",
    "concepts": [
      "call_target_name: The name of the custom call target (string).",
      "operand_layouts: Sequence of layouts (dimension orders) for each operand; defaults to row-major.",
      "result_layouts: Sequence of layouts (dimension orders) for each result; defaults to row-major.",
      "backend_config: Configuration data for the custom call; keyword arguments are added to this dictionary.",
      "lowering_args: Extra arguments to mlir.custom_call() that will be passed through."
    ],
    "code_examples": []
  },
  {
    "title": "Wrapping ctypes function pointer with PyCapsule",
    "concepts": [
      "Wraps a ctypes function pointer in a PyCapsule.",
      "Used to register function calls from external compiled libraries as XLA custom calls."
    ],
    "code_examples": [
      {
        "description": "Demonstrates wrapping a ctypes function pointer for XLA custom call registration.",
        "code": "import\nctypes\nimport\njax\nfrom\njax.lib\nimport\nxla_client\nlibfoo = ctypes.cdll.LoadLibrary('./foo.so')\nxla_client.register_custom_call_target(\n    name=\"bar\",\n    fn=jax.ffi.pycapsule(libfoo.bar),\n    platform=PLATFORM,\n    api_version=API_VERSION\n)"
      },
      {
        "description": "Demonstrates wrapping a ctypes function pointer for XLA custom call registration.",
        "code": "import\nctypes\nimport\njax\nfrom\njax.lib\nimport\nxla_client\nlibfoo = ctypes.cdll.LoadLibrary('./foo.so')\nxla_client.register_custom_call_target(\n    name=\"bar\",\n    fn=jax.ffi.pycapsule(libfoo.bar),\n    platform=PLATFORM,\n    api_version=API_VERSION\n)"
      }
    ]
  },
  {
    "title": "Registering Foreign Function Targets",
    "concepts": [
      "Registering a foreign function target allows extending XLA capabilities with custom code.",
      "The 'name' parameter specifies the target's name.",
      "The 'fn' parameter provides the function pointer as a PyCapsule or a dictionary of stage handlers.",
      "The 'platform' parameter identifies the target platform.",
      "The 'api_version' parameter selects the XLA custom call API version.",
      "The 'kwargs' parameter allows passing extra arguments to register_custom_call_target()."
    ],
    "code_examples": []
  },
  {
    "title": "Registering Custom Type IDs for FFI Targets",
    "concepts": [
      "Registers a custom type ID for a FFI target.",
      "The name of the type ID must be unique within the process.",
      "A PyCapsule object encapsulating a pointer to the type ID is used.",
      "Specifies the target platform."
    ],
    "code_examples": []
  },
  {
    "title": "FFI Lowering Rule Overview",
    "concepts": [
      "Build a lowering rule for a foreign function interface (FFI) target.",
      "Defaults to using input/output abstract values for type and shape computation, assuming row-major layouts.",
      "Layouts should be in minor-to-major order.",
      "Keyword arguments are treated as attributes and added to backend_config."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters for FFI Lowering Rule",
    "concepts": [
      "call_target_name: The name of the custom call target (string).",
      "operand_layouts: Sequence of layouts (dimension orders) for each operand; defaults to row-major if None.",
      "result_layouts: Sequence of layouts (dimension orders) for each result; defaults to row-major if None.",
      "backend_config: Configuration data for the custom call; keyword arguments are added to this dictionary.",
      "lowering_args: Any other arguments to mlir.custom_call()."
    ],
    "code_examples": []
  },
  {
    "title": "Wrapping ctypes Function Pointers in PyCapsules",
    "concepts": [
      "PyCapsules are used to wrap function calls from external compiled libraries.",
      "This allows the function calls to be registered as XLA custom calls.",
      "The function pycapsule wraps a ctypes function pointer in a PyCapsule object."
    ],
    "code_examples": [
      {
        "description": "Example of using pycapsule to register a function from a dynamic library as an XLA custom call target.",
        "code": "import ctypes\nimport jax\nfrom jax.lib import xla_client\n\nlibfoo = ctypes.cdll.LoadLibrary('./foo.so')\n\nxla_client.register_custom_call_target(\n    name=\"bar\",\n    fn=jax.ffi.pycapsule(libfoo.bar),\n    platform=PLATFORM,\n    api_version=API_VERSION\n)"
      },
      {
        "description": "Example of using pycapsule to register a function from a dynamic library as an XLA custom call target.",
        "code": "import ctypes\nimport jax\nfrom jax.lib import xla_client\nlibfoo = ctypes.cdll.LoadLibrary('./foo.so')\nxla_client.register_custom_call_target(\nname =\n\"bar\"\n,\nfn\n=\njax\n.\nffi\n.\npycapsule\n(\nlibfoo\n.\nbar\n),\nplatform\n=\nPLATFORM\n,\napi_version\n=\nAPI_VERSION\n)"
      }
    ]
  },
  {
    "title": "Registering a Foreign Function Target",
    "concepts": [
      "Registers a foreign function target with a given name.",
      "The function pointer can be passed as a PyCapsule object.",
      "The function can also be a dict of FFI stage names and PyCapsule objects.",
      "The platform of the target needs to be specified.",
      "The XLA custom call API version can be specified (1 or 0).",
      "Extra keyword arguments can be passed to register_custom_call_target()."
    ],
    "code_examples": []
  },
  {
    "title": "ravel_pytree Function",
    "concepts": [
      "The function flattens a PyTree (a tree-like data structure) into a 1D array.",
      "It takes a PyTree as input.",
      "The function is named ravel_pytree."
    ],
    "code_examples": []
  },
  {
    "title": "Raveling a PyTree",
    "concepts": [
      "Raveling flattens a pytree of arrays into a 1D array.",
      "The function returns a pair: a 1D array and a callable for unflattening.",
      "The 1D array contains the concatenated leaf values of the pytree.",
      "The dtype of the 1D array is determined by promoting the dtypes of the leaf values.",
      "The unflattening callable can reconstruct the original pytree structure from a 1D array.",
      "If the input pytree is empty, the function returns a 1D empty array of dtype float32."
    ],
    "code_examples": []
  },
  {
    "title": "Image Manipulation Functions",
    "concepts": [
      "Image manipulation functions are available.",
      "More image manipulation functions are available in libraries like PIX.",
      "The `resize` function resizes an image to a specified shape using a given method.",
      "The `scale_and_translate` function applies scaling and translation to an image."
    ],
    "code_examples": []
  },
  {
    "title": "Image Resize Methods",
    "concepts": [
      "Nearest-neighbor interpolation is a possible resize method.",
      "Linear interpolation is a possible resize method.",
      "Lanczos resampling with radius 3 is a possible resize method.",
      "Lanczos resampling with radius 5 is a possible resize method.",
      "Cubic interpolation with the Keys cubic kernel is a possible resize method."
    ],
    "code_examples": []
  },
  {
    "title": "Image Resize Overview",
    "concepts": [
      "The method argument expects one of the following resize methods: Nearest neighbor interpolation, Linear interpolation, Cubic interpolation, Lanczos resampling.",
      "The values of antialias and precision are ignored when Nearest neighbor interpolation is used.",
      "When using Linear interpolation, a triangular filter is used when downsampling if antialias is True.",
      "Cubic interpolation uses the Keys cubic kernel.",
      "Lanczos resampling is available with kernels of radius 3 and 5.",
      "The shape argument defines the output shape as a sequence of integers.",
      "The resize() function does not distinguish between spatial, batch, or channel dimensions.",
      "The method argument accepts either a ResizeMethod instance or a string representing the method.",
      "Available methods are: LINEAR, LANCZOS3, LANCZOS5, CUBIC.",
      "The antialias argument controls whether an antialiasing filter is used when downsampling, defaulting to True.",
      "Antialiasing has no effect when upsampling."
    ],
    "code_examples": []
  },
  {
    "title": "Image Scaling and Translation Overview",
    "concepts": [
      "Applies scaling and translation to an image.",
      "Generates a new image of a specified shape by resampling.",
      "Uses an inverse warp to determine sample locations in the input image.",
      "Assumes half-centered pixels.",
      "Sets output pixel values to zero if the corresponding input sample is outside the input boundaries."
    ],
    "code_examples": []
  },
  {
    "title": "Resizing Methods",
    "concepts": [
      "The method argument specifies the resizing method.",
      "Available methods include triangle (linear interpolation), cubic interpolation, and Lanczos resampling (radius 3 and 5)."
    ],
    "code_examples": []
  },
  {
    "title": "Arguments",
    "concepts": [
      "image: The input JAX array.",
      "shape: The output shape of the image.",
      "spatial_dims: Specifies the spatial dimensions where the scaling and translation are applied.",
      "scale: An array containing the scaling factor for each dimension.",
      "translation: An array containing the translation value for each dimension.",
      "method: Specifies the resizing method to be used.",
      "antialias: Determines whether an antialiasing filter should be used during downsampling."
    ],
    "code_examples": []
  },
  {
    "title": "Activation Functions",
    "concepts": [
      "Rectified Linear Unit (ReLU) activation function.",
      "Rectified Linear Unit 6 (ReLU6) activation function.",
      "Sigmoid activation function.",
      "Softplus activation function.",
      "Sparse plus function.",
      "Sparse sigmoid activation function.",
      "Soft-sign activation function.",
      "SiLU (aka swish) activation function.",
      "Log-sigmoid activation function.",
      "Leaky rectified linear unit activation function.",
      "Hard Sigmoid activation function.",
      "Hard SiLU (swish) activation function",
      "Hard tanh activation function.",
      "Exponential linear unit activation function.",
      "Continuously-differentiable exponential linear unit activation.",
      "Scaled exponential linear unit activation.",
      "Gaussian error linear unit activation function.",
      "Gated linear unit activation function.",
      "Squareplus activation function.",
      "Mish activation function."
    ],
    "code_examples": []
  },
  {
    "title": "Normalization and Encoding Functions",
    "concepts": [
      "Softmax function.",
      "Log-Softmax function.",
      "Log-sum-exp reduction.",
      "Normalizes an array by subtracting mean and dividing by the square root of variance.",
      "One-hot encodes the given indices."
    ],
    "code_examples": []
  },
  {
    "title": "Attention Function",
    "concepts": [
      "Scaled dot product attention function."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Initializers",
    "concepts": [
      "This module provides common neural network layer initializers, consistent with definitions used in Keras and Sonnet.",
      "An initializer is a function that takes a PRNG key, shape, and dtype as arguments.",
      "It returns an array with dimensions shape and data type dtype.",
      "The PRNG key is used to generate random numbers for initialization."
    ],
    "code_examples": []
  },
  {
    "title": "Constant Initializers",
    "concepts": [
      "constant(value[, dtype]): Builds an initializer that returns arrays full of a constant value.",
      "ones(key, shape[, dtype]): An initializer that returns a constant array full of ones.",
      "zeros(key, shape[, dtype]): An initializer that returns a constant array full of zeros."
    ],
    "code_examples": []
  },
  {
    "title": "Orthogonal and Delta Orthogonal Initializers",
    "concepts": [
      "orthogonal([scale, column_axis, dtype]): Builds an initializer that returns uniformly distributed orthogonal matrices.",
      "delta_orthogonal([scale, column_axis, dtype]): Builds an initializer for delta orthogonal kernels."
    ],
    "code_examples": []
  },
  {
    "title": "Glorot Initializers (Xavier)",
    "concepts": [
      "glorot_normal([in_axis, out_axis, ...]): Builds a Glorot normal initializer (aka Xavier normal initializer).",
      "glorot_uniform([in_axis, out_axis, ...]): Builds a Glorot uniform initializer (aka Xavier uniform initializer)."
    ],
    "code_examples": []
  },
  {
    "title": "He Initializers (Kaiming)",
    "concepts": [
      "he_normal([in_axis, out_axis, batch_axis, dtype]): Builds a He normal initializer (aka Kaiming normal initializer).",
      "he_uniform([in_axis, out_axis, batch_axis, ...]): Builds a He uniform initializer (aka Kaiming uniform initializer)."
    ],
    "code_examples": []
  },
  {
    "title": "Lecun Initializers",
    "concepts": [
      "lecun_normal([in_axis, out_axis, ...]): Builds a Lecun normal initializer.",
      "lecun_uniform([in_axis, out_axis, ...]): Builds a Lecun uniform initializer."
    ],
    "code_examples": []
  },
  {
    "title": "Normal and Truncated Normal Initializers",
    "concepts": [
      "normal([stddev, dtype]): Builds an initializer that returns real normally-distributed random arrays.",
      "truncated_normal([stddev, dtype, lower, upper]): Builds an initializer that returns truncated-normal random arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Uniform Initializer",
    "concepts": [
      "uniform([scale, dtype]): Builds an initializer that returns real uniformly-distributed random arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Variance Scaling Initializer",
    "concepts": [
      "variance_scaling(scale, mode, distribution): Initializer that adapts its scale to the shape of the weights tensor."
    ],
    "code_examples": []
  },
  {
    "title": "Constant Initializer",
    "concepts": [
      "Creates an initializer that returns arrays filled with a constant value.",
      "The `value` parameter specifies the constant value to use.",
      "The `dtype` parameter specifies the data type of the initializer (optional)."
    ],
    "code_examples": [
      {
        "description": "Example usage of the constant initializer with a negative value, demonstrating how to initialize an array with a constant value of -7.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.constant(-7)\ninitializer(jax.random.key(42), (2, 3), jnp.float32)\n"
      },
      {
        "description": "Redundant example usage of the constant initializer with a negative value, demonstrating how to initialize an array with a constant value of -7.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.constant(-7)\ninitializer(jax.random.key(42), (2, 3), jnp.float32)"
      }
    ]
  },
  {
    "title": "Delta Orthogonal Initializer Overview",
    "concepts": [
      "The function builds an initializer for delta orthogonal kernels.",
      "It uses a uniform distribution with an upper bound defined by the 'scale' parameter.",
      "The 'column_axis' parameter specifies the axis containing columns to be orthogonal.",
      "The 'dtype' parameter sets the default data type of the weights.",
      "The initializer works with 3D, 4D, or 5D shapes."
    ],
    "code_examples": []
  },
  {
    "title": "Delta Orthogonal Initializer Example",
    "concepts": [
      "The example demonstrates how to use the delta_orthogonal initializer.",
      "It initializes weights with a delta orthogonal distribution.",
      "It uses jax and jax.numpy for numerical operations."
    ],
    "code_examples": [
      {
        "description": "Example usage of delta_orthogonal initializer with jax.random.key(42) and shape (3, 3, 3)",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.delta_orthogonal()\ninitializer(jax.random.key(42), (3, 3, 3), jnp.float32)"
      },
      {
        "description": "Example usage of delta_orthogonal initializer with jax.random.key(42) and shape (3, 3, 3)",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.delta_orthogonal()\ninitializer(jax.random.key(42), (3, 3, 3), jnp.float32)"
      }
    ]
  },
  {
    "title": "Glorot Normal Initializer Overview",
    "concepts": [
      "Glorot normal initializer is a specialization of variance_scaling.",
      "It uses scale = 1.0, mode='fan_avg', and distribution='truncated_normal' in variance_scaling.",
      "The in_axis specifies the input dimension axes.",
      "The out_axis specifies the output dimension axes.",
      "The batch_axis specifies axes to ignore in the weight array.",
      "The dtype specifies the data type of the weights."
    ],
    "code_examples": []
  },
  {
    "title": "Glorot Normal Initializer Example",
    "concepts": [
      "Demonstrates how to use the glorot_normal initializer in JAX.",
      "The initializer is called with a random key, shape, and dtype to generate an array of initial weights."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the use of glorot_normal initializer to create a weight matrix.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.glorot_normal()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)"
      },
      {
        "description": "Example demonstrating the use of glorot_normal initializer to create a weight matrix.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.glorot_normal()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)"
      }
    ]
  },
  {
    "title": "He Normal Initializer",
    "concepts": [
      "He normal initializer is a specialization of variance_scaling initializer.",
      "The scale parameter is set to 2.0.",
      "The mode is set to fan_in.",
      "The distribution is set to truncated_normal.",
      "The initializer is used to initialize weights in neural networks.",
      "in_axis specifies the input dimension axes.",
      "out_axis specifies the output dimension axes.",
      "batch_axis specifies axes in the weight array to ignore.",
      "dtype specifies the data type of the weights."
    ],
    "code_examples": [
      {
        "description": "Example usage of the he_normal initializer with JAX.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.he_normal()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)\n"
      },
      {
        "description": "Example usage of the he_normal initializer with JAX.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.he_normal()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)\n"
      }
    ]
  },
  {
    "title": "He Uniform Initializer",
    "concepts": [
      "He uniform initializer is a specialization of variance_scaling.",
      "The scale parameter is set to 2.0.",
      "The mode parameter is set to fan_in.",
      "The distribution parameter is set to uniform."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of He Uniform Initializer",
    "concepts": [
      "in_axis specifies the input dimension axes in the weights array.",
      "out_axis specifies the output dimension axes in the weights array.",
      "batch_axis specifies axes in the weight array to be ignored.",
      "dtype specifies the data type of the weights."
    ],
    "code_examples": []
  },
  {
    "title": "He Uniform Initializer Example",
    "concepts": [
      "Demonstrates how to use the he_uniform initializer.",
      "The initializer is called with a random key, shape, and dtype."
    ],
    "code_examples": [
      {
        "description": "Example showcasing the usage of the he_uniform initializer from jax.nn.initializers with a random key, a shape of (2, 3), and a float32 dtype.",
        "code": "import jax, jax.numpy as jnp\n\ninitializer = jax.nn.initializers.he_uniform()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)\n"
      },
      {
        "description": "Example showcasing the usage of the he_uniform initializer from jax.nn.initializers with a random key, a shape of (2, 3), and a float32 dtype.",
        "code": "import jax, jax.numpy as jnp\n\ninitializer = jax.nn.initializers.he_uniform()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)"
      }
    ]
  },
  {
    "title": "Lecun Normal Initializer",
    "concepts": [
      "Lecun normal initializer is a specialization of variance scaling.",
      "It sets scale to 1.0, mode to fan_in, and distribution to truncated_normal.",
      "The `in_axis` parameter specifies the input dimension axes.",
      "The `out_axis` parameter specifies the output dimension axes.",
      "The `batch_axis` parameter specifies the axes to ignore.",
      "The `dtype` parameter specifies the data type of the weights."
    ],
    "code_examples": [
      {
        "description": "Example of using lecun_normal initializer with JAX.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.lecun_normal()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)\n"
      },
      {
        "description": "Example of using lecun_normal initializer with JAX.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.lecun_normal()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)\n"
      }
    ]
  },
  {
    "title": "Lecun Uniform Initializer",
    "concepts": [
      "Lecun uniform initializer is a specialization of variance scaling initializer.",
      "It uses scale = 1.0, mode='fan_in', and distribution='uniform'.",
      "The `in_axis` parameter specifies the input dimension axes in the weights array.",
      "The `out_axis` parameter specifies the output dimension axes in the weights array.",
      "The `batch_axis` parameter specifies axes in the weight array to ignore.",
      "The `dtype` parameter specifies the data type of the weights."
    ],
    "code_examples": [
      {
        "description": "Example usage of the lecun_uniform initializer.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.lecun_uniform()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)"
      },
      {
        "description": "Example usage of the lecun_uniform initializer.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.lecun_uniform()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)"
      }
    ]
  },
  {
    "title": "Normal Initializer",
    "concepts": [
      "Creates an initializer for real normally-distributed random arrays.",
      "The initializer returns arrays with values normally distributed with mean 0 and standard deviation stddev.",
      "stddev is the standard deviation of the distribution (optional).",
      "dtype is the initializer's default dtype (optional)."
    ],
    "code_examples": [
      {
        "description": "Example usage of the normal initializer with a standard deviation of 5.0, generating a (2,3) array of float32.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.normal(5.0)\ninitializer(jax.random.key(42), (2, 3), jnp.float32)\n"
      },
      {
        "description": "Example usage of the normal initializer with a standard deviation of 5.0, generating a (2,3) array of float32.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.normal(5.0)\ninitializer(jax.random.key(42), (2, 3), jnp.float32)"
      }
    ]
  },
  {
    "title": "Ones Initializer",
    "concepts": [
      "Returns a constant array filled with ones.",
      "The key argument is ignored.",
      "The function takes a key, shape, and dtype as input."
    ],
    "code_examples": [
      {
        "description": "Example usage of the ones initializer to create a (3, 2) array of ones with float32 data type.",
        "code": "import jax, jax.numpy as jnp\n\njax.nn.initializers.ones(jax.random.key(42), (3, 2), jnp.float32)"
      },
      {
        "description": "Another example usage of the ones initializer to create a (3, 2) array of ones with float32 data type.",
        "code": "import jax, jax.numpy as jnp\n\njax.nn.initializers.ones(jax.random.key(42), (3, 2), jnp.float32)"
      }
    ]
  },
  {
    "title": "Orthogonal Initializer Overview",
    "concepts": [
      "Creates an initializer for uniformly distributed orthogonal matrices.",
      "Handles non-square shapes by producing matrices with orthonormal rows or columns.",
      "The `scale` parameter defines the upper bound of the uniform distribution.",
      "The `column_axis` parameter specifies the axis containing the columns to be orthogonal.",
      "The `dtype` parameter determines the data type of the weights."
    ],
    "code_examples": []
  },
  {
    "title": "Orthogonal Initializer Example",
    "concepts": [
      "Demonstrates how to use the orthogonal initializer with JAX.",
      "Shows how to generate orthogonal matrices with a specified shape and data type."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the use of the orthogonal initializer.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.orthogonal()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)"
      },
      {
        "description": "Example demonstrating the use of the orthogonal initializer.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.orthogonal()\ninitializer(jax.random.key(42), (2, 3), jnp.float32)"
      }
    ]
  },
  {
    "title": "Truncated Normal Initializer",
    "concepts": [
      "The initializer returns arrays following a truncated normal distribution.",
      "The distribution has a mean of 0 and a standard deviation of stddev.",
      "The values are truncated to the range lower * stddev < x < upper * stddev.",
      "Users are expected to apply the standard deviation correction themselves via the stddev argument if they wish to employ it."
    ],
    "code_examples": [
      {
        "description": "Example of using the truncated_normal initializer with a specified standard deviation (5.0) to generate a (2, 3) array of float32 values.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.truncated_normal(5.0)\ninitializer(jax.random.key(42), (2, 3), jnp.float32)\n"
      },
      {
        "description": "Example of using the truncated_normal initializer with a specified standard deviation (5.0) to generate a (2, 3) array of float32 values. (Duplicated from original document)",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.truncated_normal(5.0)\ninitializer(jax.random.key(42), (2, 3), jnp.float32)\n"
      }
    ]
  },
  {
    "title": "Uniform Initializer",
    "concepts": [
      "Creates an initializer for real uniformly-distributed random arrays.",
      "The initializer returns arrays with values uniformly distributed in the range [0, scale).",
      "The `scale` parameter defines the upper bound of the random distribution.",
      "The `dtype` parameter defines the initializer's default data type."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use the uniform initializer to create a random array with values between 0 and 10.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.uniform(10.0)\ninitializer(jax.random.key(42), (2, 3), jnp.float32)\n"
      },
      {
        "description": "Demonstrates how to use the uniform initializer to create a random array with values between 0 and 10. Repeated example.",
        "code": "import jax, jax.numpy as jnp\ninitializer = jax.nn.initializers.uniform(10.0)\ninitializer(jax.random.key(42), (2, 3), jnp.float32)"
      }
    ]
  },
  {
    "title": "Zeros Initializer",
    "concepts": [
      "The zeros initializer returns a constant array filled with zeros.",
      "The key argument is ignored."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of the `zeros` initializer to create an array of zeros with shape (2, 3) and data type float32.",
        "code": "import jax, jax.numpy as jnp\n\njax.nn.initializers.zeros(jax.random.key(42), (2, 3), jnp.float32)"
      },
      {
        "description": "Demonstrates the use of the `zeros` initializer to create an array of zeros with shape (2, 3) and data type float32.",
        "code": "import jax, jax.numpy as jnp\n\njax.nn.initializers.zeros(jax.random.key(42), (2, 3), jnp.float32)"
      }
    ]
  },
  {
    "title": "ReLU Activation Function",
    "concepts": [
      "ReLU computes the element-wise maximum of the input and 0.",
      "During differentiation, the gradient of ReLU at 0 is defined as 0.",
      "The input 'x' is an array-like object.",
      "The output is an array."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the ReLU function with a JAX array containing negative, zero, and positive values.",
        "code": "jax.nn.relu(jax.numpy.array([-2., -1., -0.5, 0, 0.5, 1., 2.]))\nArray([0. , 0. , 0. , 0. , 0.5, 1. , 2. ], dtype=float32)"
      },
      {
        "description": "Another example showing ReLU function application on a JAX array.",
        "code": "jax.nn.relu(jax.numpy.array([-2., -1., -0.5, 0, 0.5, 1., 2.]))\nArray([0. , 0. , 0. , 0. , 0.5, 1. , 2. ], dtype=float32)"
      }
    ]
  },
  {
    "title": "ReLU6 Activation Function",
    "concepts": [
      "ReLU6 computes element-wise minimum of maximum of input and 0 and 6.",
      "relu6(x) = min(max(x, 0), 6)",
      "During differentiation, the gradient of ReLU6 at 0 and 6 is 0.",
      "Input is an array-like object."
    ],
    "code_examples": []
  },
  {
    "title": "Sigmoid Activation Function",
    "concepts": [
      "The sigmoid function computes the element-wise sigmoid of an input array.",
      "The sigmoid function is defined as 1 / (1 + e^(-x)).",
      "The input to the sigmoid function is an array."
    ],
    "code_examples": []
  },
  {
    "title": "Softplus Activation Function",
    "concepts": [
      "Softplus function computes log(1 + exp(x)) element-wise.",
      "The input is an array-like object (ArrayLike)."
    ],
    "code_examples": []
  },
  {
    "title": "Sparse Plus Function Definition",
    "concepts": [
      "The sparse_plus function is defined piecewise.",
      "The function returns 0 for x <= -1.",
      "The function returns (1/4)(x+1)^2 for -1 < x < 1.",
      "The function returns x for 1 <= x.",
      "It is a twin function of the softplus activation.",
      "It ensures a zero output for inputs less than -1 and a linear output for inputs greater than 1.",
      "The function is smooth, convex, and monotonic."
    ],
    "code_examples": []
  },
  {
    "title": "Soft-sign Activation Function",
    "concepts": [
      "The soft-sign function is computed element-wise.",
      "The soft-sign function is defined as x / (|x| + 1).",
      "The input is an array-like object."
    ],
    "code_examples": []
  },
  {
    "title": "SiLU (Swish) Activation Function",
    "concepts": [
      "SiLU (Swish) activation is computed element-wise.",
      "SiLU is equivalent to x * sigmoid(x).",
      "The formula for SiLU is x / (1 + e^(-x)).",
      "swish() and silu() are aliases for the same function."
    ],
    "code_examples": []
  },
  {
    "title": "SiLU/Swish Activation Function",
    "concepts": [
      "SiLU and Swish are aliases for the same activation function.",
      "The SiLU activation function is computed element-wise as x * sigmoid(x) or x / (1 + exp(-x)).",
      "The input to the function is an array."
    ],
    "code_examples": []
  },
  {
    "title": "Log-Sigmoid Activation Function",
    "concepts": [
      "The log-sigmoid function is defined as log(sigmoid(x)) or -log(1 + e^(-x)).",
      "The input is an array-like object.",
      "The output is an array."
    ],
    "code_examples": []
  },
  {
    "title": "Leaky ReLU Activation Function",
    "concepts": [
      "Leaky ReLU is an element-wise activation function.",
      "For positive inputs, the function returns the input directly (x).",
      "For negative inputs, the function returns the input multiplied by a negative slope (alpha * x).",
      "The negative slope (alpha) is a parameter that controls the slope for negative values, defaulting to 0.01."
    ],
    "code_examples": []
  },
  {
    "title": "Hard Sigmoid Activation Function",
    "concepts": [
      "Hard Sigmoid is computed element-wise.",
      "The formula for Hard Sigmoid is relu6(x + 3) / 6.",
      "The input is an array-like object.",
      "relu6() is related function."
    ],
    "code_examples": []
  },
  {
    "title": "Hard SiLU (swish) Activation Function",
    "concepts": [
      "The hard_silu function computes the element-wise function x * hard_sigmoid(x).",
      "hard_silu() and hard_swish() are aliases for the same function."
    ],
    "code_examples": []
  },
  {
    "title": "Hard SiLU (swish) activation function",
    "concepts": [
      "The hard_silu function computes element-wise x * hard_sigmoid(x).",
      "hard_silu() and hard_swish() are aliases for the same function."
    ],
    "code_examples": []
  },
  {
    "title": "Hard Tanh Activation Function",
    "concepts": [
      "Hard tanh computes element-wise tanh function.",
      "If x < -1, hard_tanh(x) = -1.",
      "If -1 <= x <= 1, hard_tanh(x) = x.",
      "If 1 < x, hard_tanh(x) = 1.",
      "Input is an array.",
      "Output is an array."
    ],
    "code_examples": []
  },
  {
    "title": "Exponential Linear Unit (ELU) Activation Function",
    "concepts": [
      "ELU computes element-wise activation based on input x.",
      "For x > 0, ELU returns x.",
      "For x <= 0, ELU returns alpha * (exp(x) - 1).",
      "alpha is a scalar or array of alpha values (default 1.0)."
    ],
    "code_examples": []
  },
  {
    "title": "CELU Activation Function",
    "concepts": [
      "CELU is a continuously-differentiable exponential linear unit activation function.",
      "It computes a different value based on whether the input x is greater than 0 or less than or equal to 0.",
      "The function involves a parameter alpha that affects the behavior for x <= 0.",
      "The CELU function is defined piecewise."
    ],
    "code_examples": []
  },
  {
    "title": "Mathematical Definition",
    "concepts": [
      "CELU(x) = x if x > 0",
      "CELU(x) = alpha * (exp(x/alpha) - 1) if x <= 0"
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "x is the input array.",
      "alpha is the array or scalar, defaulting to 1.0."
    ],
    "code_examples": []
  },
  {
    "title": "Gaussian Error Linear Unit (GELU) Activation Function",
    "concepts": [
      "GELU is an element-wise activation function.",
      "The exact GELU formulation involves the error function (erfc).",
      "An approximate GELU formulation uses the hyperbolic tangent function (tanh).",
      "The 'approximate' parameter determines which formulation is used."
    ],
    "code_examples": []
  },
  {
    "title": "Gated Linear Unit (GLU) Activation Function",
    "concepts": [
      "The GLU activation function splits the input array along a specified axis into two halves.",
      "It multiplies the first half of the array by the sigmoid of the second half.",
      "The size of the axis dimension must be divisible by two.",
      "The function is defined as  glu(x) = x[..., 0:n/2, ...] * sigmoid( x[..., n/2:n, ...])"
    ],
    "code_examples": []
  },
  {
    "title": "Squareplus Activation Function",
    "concepts": [
      "Squareplus activation function is defined as (x + sqrt(x^2 + b)) / 2.",
      "The function is described in https://arxiv.org/abs/2112.11687.",
      "The function takes an input array 'x'.",
      "The function takes a smoothness parameter 'b'."
    ],
    "code_examples": []
  },
  {
    "title": "Mish Activation Function",
    "concepts": [
      "Mish activation is computed element-wise.",
      "Mish is defined as x * tanh(softplus(x)).",
      "Mish is a self-regularized non-monotonic activation function."
    ],
    "code_examples": []
  },
  {
    "title": "Log-Softmax Function",
    "concepts": [
      "Log-Softmax rescales elements to the range (-\u221e, 0).",
      "The log_softmax function computes the logarithm of the softmax function.",
      "The formula for log_softmax is given as: log_softmax(x)_i = log(exp(x_i) / sum_j exp(x_j)).",
      "The 'axis' parameter specifies the axis or axes along which the computation should be performed.",
      "If any input values are +inf, the result will be all NaN."
    ],
    "code_examples": []
  },
  {
    "title": "Log-Sum-Exp Reduction Overview",
    "concepts": [
      "Log-sum-exp is a numerically stable way to compute the logarithm of a sum of exponentials.",
      "The formula for log-sum-exp is log(sum_j b * exp(a_ij)).",
      "The input array is denoted by 'a'.",
      "The 'axis' parameter specifies the axis along which the sum is computed.",
      "The 'b' parameter represents scaling factors for exp(a).",
      "The 'keepdims' parameter determines if reduced axes are kept with size 1.",
      "The 'return_sign' parameter controls whether the sign of the sum is returned.",
      "The 'where' parameter specifies elements to include in the reduction."
    ],
    "code_examples": []
  },
  {
    "title": "Array Normalization",
    "concepts": [
      "Normalizes an array.",
      "Subtracts the mean from the array elements.",
      "Divides the result by the square root of the variance.",
      "Input: x (ArrayLike)",
      "Axis: axis (int | tuple [ int , ... ] | None)",
      "Mean: mean (ArrayLike | None)",
      "Variance: variance (ArrayLike | None)",
      "Epsilon: epsilon (ArrayLike)",
      "Where: where (ArrayLike | None)"
    ],
    "code_examples": []
  },
  {
    "title": "One-Hot Encoding with JAX",
    "concepts": [
      "One-hot encoding converts indices into vectors.",
      "Each index is encoded as a vector of zeros with a one at the index position.",
      "Indices outside the range [0, num_classes) are encoded as zero vectors.",
      "The function takes an array of indices and the number of classes as input.",
      "The function returns a JAX array."
    ],
    "code_examples": [
      {
        "description": "Example of one-hot encoding an array of indices [0, 1, 2] with 3 classes.",
        "code": "jax.nn.one_hot(jnp.array([0, 1, 2]), 3)"
      },
      {
        "description": "Example of one-hot encoding an array of indices [-1, 3] with 3 classes. Indices outside the range [0, num_classes) are encoded as zeros.",
        "code": "jax.nn.one_hot(jnp.array([-1, 3]), 3)"
      }
    ]
  },
  {
    "title": "Scaled Dot Product Attention",
    "concepts": [
      "The scaled dot-product attention mechanism computes attention scores based on Query, Key, and Value tensors.",
      "The attention function is defined as Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V.",
      "Logits are the output of QK^T, and probabilities are the output of the softmax function.",
      "Different implementations such as multi-headed attention (MHA), grouped query attention (GQA), and multi-query attention (MQA) exist.",
      "Optional bias and mask arrays can be applied to the logits.",
      "Causal attention can be applied to mask out non-causal parts of the attention matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Array Shapes and Parameters",
    "concepts": [
      "B represents the batch size.",
      "S represents the length of the key/value (source).",
      "T represents the length of the query (target).",
      "N represents the number of attention heads.",
      "H represents the dimensions of each attention head.",
      "K represents the number of key/value heads.",
      "G represents the number of groups, which equals N // K.",
      "The 'query' parameter is an array with shape (BTNH|TNH).",
      "The 'key' parameter is an array with shape (BSKH|SKH).",
      "The 'value' parameter is an array with the same shape as the key array.",
      "The 'bias' parameter is an optional array to be added to logits, with shape broadcastable to (BNTS|NTS).",
      "The 'mask' parameter is an optional boolean array used to filter out logits, with shape broadcastable to (BNTS|NTS).",
      "The 'scale' parameter is a scale for the logits, defaulting to 1 / sqrt(H).",
      "The 'is_causal' parameter indicates whether causal attention is applied.",
      "The 'query_seq_lengths' parameter is an int32 array of sequence lengths for the query, with shape (B).",
      "The 'key_value_seq_lengths' parameter is an int32 array of sequence lengths for the key and value, with shape (B).",
      "The 'local_window_size' parameter specifies the (left_window_size, right_window_size) for local attention.",
      "The 'implementation' parameter controls which backend to use ('xla', 'cudnn')."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecation of jax.ops and Introduction of jax.numpy.ndarray.at",
    "concepts": [
      "jax.ops.index_update, jax.ops.index_add, etc. have been removed.",
      "Use jax.numpy.ndarray.at property on JAX arrays instead."
    ],
    "code_examples": []
  },
  {
    "title": "Segment Operations",
    "concepts": [
      "segment_max computes the maximum within segments of an array.",
      "segment_min computes the minimum within segments of an array.",
      "segment_prod computes the product within segments of an array.",
      "segment_sum computes the sum within segments of an array."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Segment Max",
    "concepts": [
      "Computes the maximum within segments of an array.",
      "Similar to TensorFlow\u2019s segment_max.",
      "The `data` array contains the values to be reduced.",
      "The `segment_ids` array indicates the segments of data.",
      "The `num_segments` parameter specifies the number of segments.",
      "`indices_are_sorted` indicates whether segment_ids is sorted.",
      "`unique_indices` indicates whether segment_ids is free of duplicates.",
      "`bucket_size` is size of bucket to group indices into.",
      "`mode` determines how out-of-bounds indices should be handled.",
      "Returns an array with segment maximums."
    ],
    "code_examples": []
  },
  {
    "title": "Simple 1D Segment Max Example",
    "concepts": [
      "Demonstrates a basic usage of segment_max with a 1D array."
    ],
    "code_examples": [
      {
        "description": "Calculates the segment max for a 1D array where segment_ids indicates which segment each element of the data array belongs to.",
        "code": "data = jnp.arange(6)\nsegment_ids = jnp.array([0, 0, 1, 1, 2, 2])\nsegment_max(data, segment_ids)"
      },
      {
        "description": "Calculates the segment max for a 1D array where segment_ids indicates which segment each element of the data array belongs to.",
        "code": "data = jnp.arange(6)\nsegment_ids = jnp.array([0, 0, 1, 1, 2, 2])\nsegment_max(data, segment_ids)"
      }
    ]
  },
  {
    "title": "Segment Max with JIT Compilation",
    "concepts": [
      "Illustrates how to use `segment_max` with JIT compilation.",
      "Using JIT requires static num_segments."
    ],
    "code_examples": [
      {
        "description": "Demonstrates JIT compilation with segment_max, where num_segments is specified as a static argument.",
        "code": "from jax import jit\n\njit(segment_max, static_argnums=2)(data, segment_ids, 3)"
      },
      {
        "description": "Demonstrates JIT compilation with segment_max, where num_segments is specified as a static argument.",
        "code": "from jax import jit\n\njit(segment_max, static_argnums=2)(data, segment_ids, 3)"
      }
    ]
  },
  {
    "title": "Segment Minimum Calculation",
    "concepts": [
      "Computes the minimum within segments of an array.",
      "Similar to TensorFlow\u2019s segment_min.",
      "The `data` input is the array to be reduced.",
      "The `segment_ids` input indicates the segments of data.",
      "The `num_segments` input specifies the number of segments.",
      "The `indices_are_sorted` input indicates whether segment_ids is known to be sorted.",
      "The `unique_indices` input indicates whether segment_ids is known to be free of duplicates.",
      "The `bucket_size` input represents size of bucket to group indices into.",
      "The `mode` input specifies how out-of-bounds indices should be handled."
    ],
    "code_examples": [
      {
        "description": "Simple 1D segment min example.",
        "code": "data = jnp.arange(6)\nsegment_ids = jnp.array([0, 0, 1, 1, 2, 2])\nsegment_min(data, segment_ids)"
      },
      {
        "description": "Simple 1D segment min example.",
        "code": "data = jnp.arange(6)\nsegment_ids = jnp.array([0, 0, 1, 1, 2, 2])\nsegment_min(data, segment_ids)"
      },
      {
        "description": "Using JIT requires static num_segments.",
        "code": "from jax import jit\n\njit(segment_min, static_argnums=2)(data, segment_ids, 3)"
      },
      {
        "description": "Using JIT requires static num_segments.",
        "code": "from jax import jit\n\njit(segment_min, static_argnums=2)(data, segment_ids, 3)"
      }
    ]
  },
  {
    "title": "Segment Product Overview",
    "concepts": [
      "Computes the product within segments of an array, similar to TensorFlow's segment_prod.",
      "The `data` array contains the values to be reduced.",
      "The `segment_ids` array indicates the segments of `data` to be reduced.",
      "Values in `segment_ids` can be repeated and need not be sorted.",
      "Values in `segment_ids` outside the range [0, num_segments) are dropped.",
      "`num_segments` is the number of segments.  A static value must be provided when using JIT.",
      "`indices_are_sorted` indicates whether segment_ids is known to be sorted.",
      "`unique_indices` indicates whether segment_ids is known to be free of duplicates.",
      "`bucket_size` indicates the size of bucket to group indices into.",
      "`mode` describes how out-of-bounds indices should be handled."
    ],
    "code_examples": []
  },
  {
    "title": "Simple 1D Segment Product Example",
    "concepts": [
      "Demonstrates basic usage of `segment_prod` with a 1D array and segment IDs."
    ],
    "code_examples": [
      {
        "description": "Computes the segment product for a 1D array.",
        "code": "data = jnp.arange(6)\nsegment_ids = jnp.array([0, 0, 1, 1, 2, 2])\nsegment_prod(data, segment_ids)"
      },
      {
        "description": "Computes the segment product for a 1D array.",
        "code": "data = jnp.arange(6)\nsegment_ids = jnp.array([0, 0, 1, 1, 2, 2])\nsegment_prod(data, segment_ids)"
      }
    ]
  },
  {
    "title": "Segment Product with JIT Compilation",
    "concepts": [
      "Shows how to use `segment_prod` with JIT compilation.",
      "Using JIT requires a static `num_segments` value, specified via `static_argnums`."
    ],
    "code_examples": [
      {
        "description": "Compiles segment_prod with a static num_segments argument for use with JIT.",
        "code": "from jax import jit\n\njit(segment_prod, static_argnums=2)(data, segment_ids, 3)"
      },
      {
        "description": "Compiles segment_prod with a static num_segments argument for use with JIT.",
        "code": "from jax import jit\n\njit(segment_prod, static_argnums=2)(data, segment_ids, 3)"
      }
    ]
  },
  {
    "title": "Segment Sum Definition and Parameters",
    "concepts": [
      "Computes the sum within segments of an array, similar to TensorFlow's segment_sum.",
      "The 'data' parameter is the array with values to be summed.",
      "The 'segment_ids' parameter indicates the segments of data to be summed.",
      "The 'num_segments' parameter specifies the number of segments; a static value is needed for JIT compilation.",
      "The 'indices_are_sorted' parameter indicates whether 'segment_ids' is sorted.",
      "The 'unique_indices' parameter indicates whether 'segment_ids' has duplicate values.",
      "The 'bucket_size' parameter controls bucketing for numerical stability.",
      "The 'mode' parameter specifies how out-of-bounds indices are handled.",
      "The output is an array representing the segment sums."
    ],
    "code_examples": []
  },
  {
    "title": "Simple 1D Segment Sum Example",
    "concepts": [
      "Demonstrates a basic example of using segment_sum with a 1D array.",
      "Illustrates how segment_ids maps data elements to their respective segments."
    ],
    "code_examples": [
      {
        "description": "Computes the segment sum for a 1D array with specified segment IDs.",
        "code": "data = jnp.arange(5)\nsegment_ids = jnp.array([0, 0, 1, 1, 2])\nsegment_sum(data, segment_ids)"
      },
      {
        "description": "Computes the segment sum for a 1D array with specified segment IDs.",
        "code": "data = jnp.arange(5)\nsegment_ids = jnp.array([0, 0, 1, 1, 2])\nsegment_sum(data, segment_ids)"
      }
    ]
  },
  {
    "title": "Segment Sum with JIT Compilation",
    "concepts": [
      "Illustrates the use of JIT compilation with segment_sum.",
      "Shows that static_argnums must be used to specify num_segments as a static argument for JIT compilation."
    ],
    "code_examples": [
      {
        "description": "Demonstrates JIT compilation of segment_sum, requiring num_segments to be a static argument.",
        "code": "from jax import jit\n\njit(segment_sum, static_argnums=2)(data, segment_ids, 3)"
      },
      {
        "description": "Demonstrates JIT compilation of segment_sum, requiring num_segments to be a static argument.",
        "code": "from jax import jit\n\njit(segment_sum, static_argnums=2)(data, segment_ids, 3)"
      }
    ]
  },
  {
    "title": "Introduction to Profiling in JAX",
    "concepts": [
      "Profiling computation involves using JAX's tracing and time profiling features.",
      "JAX provides tools to analyze the performance of JAX code."
    ],
    "code_examples": []
  },
  {
    "title": "Profiler Server Functions",
    "concepts": [
      "start_server(port) starts the profiler server on a specified port.",
      "The profiler server allows for monitoring and analyzing JAX's performance."
    ],
    "code_examples": []
  },
  {
    "title": "Profiler Trace Management",
    "concepts": [
      "start_trace(log_dir) starts a profiler trace, logging data to the specified directory.",
      "stop_trace() stops the currently running profiler trace.",
      "trace(log_dir) is a context manager for taking a profiler trace within a 'with' block."
    ],
    "code_examples": []
  },
  {
    "title": "Function Annotations",
    "concepts": [
      "annotate_function(func) is a decorator to generate trace events for function execution.",
      "TraceAnnotation is a context manager for generating trace events in the profiler.",
      "StepTraceAnnotation is a context manager for generating step trace events."
    ],
    "code_examples": []
  },
  {
    "title": "Device Memory Profiling",
    "concepts": [
      "Device memory profiling allows analyzing memory usage on JAX devices.",
      "device_memory_profile() captures a device memory profile as a pprof-format protocol buffer.",
      "save_device_memory_profile(filename) collects a device memory profile and saves it to a file."
    ],
    "code_examples": []
  },
  {
    "title": "Profiler Server Introduction",
    "concepts": [
      "Starts a profiler server on a specified port.",
      "Connects to the profiler server using TensorBoard 2.2 or newer.",
      "Allows sampling of execution traces showing CPU, GPU, and/or TPU device activity."
    ],
    "code_examples": []
  },
  {
    "title": "Profiler Server Implementation",
    "concepts": [
      "The port argument specifies the port number for the profiler server.",
      "The profiler server is part of the xla_client.profiler module."
    ],
    "code_examples": []
  },
  {
    "title": "Profiler Trace Initiation",
    "concepts": [
      "Starts a profiler trace to capture CPU, GPU, and/or TPU activity.",
      "Captures Python functions and JAX on-device operations.",
      "Use stop_trace() to end the trace and save the results to log_dir.",
      "The trace can be viewed with TensorBoard.",
      "Only one trace may be collected at a time.",
      "Raises a RuntimeError if start_trace() is called while another trace is running.",
      "log_dir specifies the directory to save the profiler trace.",
      "create_perfetto_link creates and prints a link to the Perfetto trace viewer UI.",
      "create_perfetto_trace dumps a Perfetto-compatible trace file."
    ],
    "code_examples": []
  },
  {
    "title": "Stopping the Profiler Trace",
    "concepts": [
      "Stops the currently running profiler trace.",
      "The trace is saved to the log_dir specified in start_trace().",
      "Raises RuntimeError if no trace has been started."
    ],
    "code_examples": []
  },
  {
    "title": "Profiler Trace Context Manager",
    "concepts": [
      "Context manager for capturing CPU, GPU, and/or TPU activity.",
      "Captures Python functions and JAX on-device operations.",
      "Trace can be viewed with TensorBoard.",
      "Only one trace can be collected at a time.",
      "Raises RuntimeError if another trace is running.",
      "Saves profiler trace to a specified directory.",
      "Option to create and print a link to the Perfetto trace viewer UI.",
      "Option to dump a Perfetto-compatible trace file."
    ],
    "code_examples": []
  },
  {
    "title": "Function Annotation with jax.profiler.annotate_function",
    "concepts": [
      "The `jax.profiler.annotate_function` decorator generates trace events for function executions.",
      "Events appear on the trace timeline when the function executes during tracing by TensorBoard.",
      "Arguments can be passed to the decorator using `functools.partial()`."
    ],
    "code_examples": [
      {
        "description": "Annotating a function with jax.profiler.annotate_function.  The jnp.dot function computes the dot product of a matrix with its transpose, and block_until_ready() ensures the computation is completed before proceeding.",
        "code": "@jax.profiler.annotate_function\ndef f(x):\n  return jnp.dot(x, x.T).block_until_ready()\n\nresult = f(jnp.ones((1000, 1000)))"
      },
      {
        "description": "Annotating a function with a custom event name using functools.partial.",
        "code": "from functools import partial\n\n@partial(jax.profiler.annotate_function, name=\"event_name\")\ndef f(x):\n  return jnp.dot(x, x.T).block_until_ready()\n\nresult = f(jnp.ones((1000, 1000)))"
      }
    ]
  },
  {
    "title": "TraceAnnotation Context Manager",
    "concepts": [
      "TraceAnnotation is a context manager for profiling code.",
      "It generates a trace event in the profiler.",
      "The trace event spans the duration of the code within the context.",
      "The event shows up on the trace timeline if the process is being traced."
    ],
    "code_examples": [
      {
        "description": "Example of using TraceAnnotation to profile a JAX dot product operation.",
        "code": "x = jnp.ones((1000, 1000))\nwith jax.profiler.TraceAnnotation(\"my_label\"):\n  result = jnp.dot(x, x.T).block_until_ready()"
      },
      {
        "description": "Another example of using TraceAnnotation to profile a JAX dot product operation.",
        "code": "x = jnp.ones((1000, 1000))\nwith jax.profiler.TraceAnnotation(\"my_label\"):\n  result = jnp.dot(x, x.T).block_until_ready()"
      }
    ]
  },
  {
    "title": "StepTraceAnnotation Context Manager",
    "concepts": [
      "StepTraceAnnotation generates a step trace event in the profiler.",
      "The event spans the duration of the code enclosed by the context.",
      "It can be used to mark training steps for performance analysis.",
      "The profiler provides performance analysis for each step trace event.",
      "Device trace timeline will also show the event if using accelerators.",
      "step_num can be used to pass the global step number to the profiler."
    ],
    "code_examples": [
      {
        "description": "Example usage of StepTraceAnnotation to mark training steps.  Note this is a simplified example as `NUM_STEPS` and `train_step()` are not defined here.",
        "code": "while global_step < NUM_STEPS:\n  ...\n  with jax.profiler.StepTraceAnnotation(\"train\", step_num=global_step):\n    ...\n    train_step()\n    ...\n  global_step += 1"
      },
      {
        "description": "Example usage of StepTraceAnnotation to mark training steps.  Note this is a simplified example as `NUM_STEPS` and `train_step()` are not defined here.",
        "code": "while global_step < NUM_STEPS:\n  ...\n  with jax.profiler.StepTraceAnnotation(\"train\", step_num=global_step):\n    ...\n    train_step()\n    ...\n  global_step += 1"
      }
    ]
  },
  {
    "title": "Methods and Attributes",
    "concepts": [
      "__init__(self, arg0, /, **kwargs) is a method.",
      "is_enabled is an attribute.",
      "set_metadata is an attribute."
    ],
    "code_examples": []
  },
  {
    "title": "Device Memory Profiling",
    "concepts": [
      "Collects a device memory profile.",
      "Writes the profile to a file.",
      "save_device_memory_profile() is a convenience wrapper around device_memory_profile().",
      "filename is the filename to which the profile should be written.",
      "backend is the name of the JAX backend for which the device memory profile should be collected."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Compilation Stages",
    "concepts": [
      "JAX transformations like jax.jit and jax.pmap support ahead-of-time (AOT) compilation.",
      "This module defines types that represent the stages of the compilation process.",
      "The AOT walkthrough provides further information."
    ],
    "code_examples": []
  },
  {
    "title": "Traced Function",
    "concepts": [
      "A Traced function is specialized to argument types and values.",
      "It's a traced computation ready for lowering.",
      "It includes the traced representation and information to lower, compile, and execute it later.",
      "The class contains the jaxpr, args_info and num_consts."
    ],
    "code_examples": []
  },
  {
    "title": "Lowered Function",
    "concepts": [
      "A Lowered function is specialized to argument types and values.",
      "A Lowering is a computation ready for compilation.",
      "The Lowered instance carries a lowering together with the remaining information needed to later compile and execute it.",
      "The class contains the lowering, args_info, out_tree and no_kwargs.",
      "It provides a common API for querying properties of lowered computations across JAX\u2019s various lowering paths (jit(), pmap(), etc.)."
    ],
    "code_examples": []
  },
  {
    "title": "Lowered Instance Properties and Methods",
    "concepts": [
      "The Lowered instance has a human-readable text representation for visualization and debugging.",
      "Use jax.export for reliable and portable serialization.",
      "The Lowered instance can be compiled, returning a Compiled instance.",
      "The Lowered instance provides an arbitrary object representation for debugging purposes.",
      "The Lowered instance provides a summary of execution cost estimates for visualization and debugging."
    ],
    "code_examples": []
  },
  {
    "title": "Compiled Function",
    "concepts": [
      "A Compiled function is specialized to types/values.",
      "A compiled computation is associated with an executable and the remaining information needed to execute it.",
      "It provides a common API for querying properties of compiled computations across JAX\u2019s various compilation paths and backends.",
      "The class contains the args_info and out_tree.",
      "A Compiled instance can be called as a function."
    ],
    "code_examples": []
  },
  {
    "title": "Compiled Instance Properties",
    "concepts": [
      "The Compiled instance provides a human-readable text representation of this executable for visualization and debugging.",
      "The Compiled instance provides a summary of execution cost estimates for visualization and debugging.",
      "The Compiled instance provides a summary of estimated memory requirements for visualization and debugging.",
      "The Compiled instance provides an arbitrary object representation of this executable for debugging purposes."
    ],
    "code_examples": []
  },
  {
    "title": "Gradient Checking Functions",
    "concepts": [
      "check_grads function checks gradients from automatic differentiation against finite differences.",
      "check_jvp function checks a JVP from automatic differentiation against finite differences.",
      "check_vjp function checks a VJP from automatic differentiation against finite differences."
    ],
    "code_examples": []
  },
  {
    "title": "JVP Verification with Finite Differences",
    "concepts": [
      "Checking Jacobian-vector product (JVP) from automatic differentiation against finite differences.",
      "Gradients are checked in a single, randomly chosen direction for efficiency.",
      "Function `f` is the function to check at `f(*args)`.",
      "Function `f_vjp` calculates `jax.jvp` applied to `f`.",
      "`args` is a tuple of argument values.",
      "`atol` is the absolute tolerance for gradient equality.",
      "`rtol` is the relative tolerance for gradient equality.",
      "`eps` is the step size used for finite differences.",
      "`err_msg` is an additional error message included if checks fail.",
      "AssertionError is raised if gradients do not match."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jax.tree Utilities",
    "concepts": [
      "The jax.tree namespace provides utilities for working with tree-like container data structures.",
      "jax.tree contains aliases of utilities from jax.tree_util."
    ],
    "code_examples": []
  },
  {
    "title": "Functions for Querying and Transforming PyTrees",
    "concepts": [
      "all(): Checks if all leaves of a tree satisfy a condition.",
      "flatten(): Flattens a pytree into a list of leaves.",
      "flatten_with_path(): Flattens a pytree and returns each leaf's key path.",
      "leaves(): Gets the leaves of a pytree.",
      "leaves_with_path(): Gets the leaves of a pytree and returns each leaf's key path.",
      "map(): Maps a multi-input function over pytree arguments to produce a new pytree.",
      "map_with_path(): Maps a multi-input function over pytree key path and arguments to produce a new pytree.",
      "reduce(): Applies reduce() over the leaves of a tree.",
      "structure(): Gets the treedef for a pytree.",
      "transpose(): Transform a tree having tree structure (outer, inner) into one having structure (inner, outer).",
      "unflatten(): Reconstructs a pytree from its treedef and leaves."
    ],
    "code_examples": []
  },
  {
    "title": "Using jax.tree.all()",
    "concepts": [
      "The function `jax.tree.all()` checks if all leaves of a pytree are True.",
      "It evaluates a pytree and returns True if all leaf values are True, otherwise False.",
      "The `tree` argument is the pytree to evaluate.",
      "The `is_leaf` argument is an optional function to determine if an object should be treated as a leaf."
    ],
    "code_examples": [
      {
        "description": "Example where all leaves are True, resulting in True.",
        "code": "import jax\n\njax.tree.all([True, {'a': True, 'b': (True, True)}])"
      },
      {
        "description": "Example with False leaves, resulting in False.",
        "code": "import jax\n\njax.tree.all([False, (True, False)])"
      },
      {
        "description": "Another example where all leaves are True, resulting in True.",
        "code": "import jax\n\njax.tree.all([True, {'a': True, 'b': (True, True)}])"
      },
      {
        "description": "Another example with False leaves, resulting in False.",
        "code": "import jax\n\njax.tree.all([False, (True, False)])"
      }
    ]
  },
  {
    "title": "Flattening a PyTree",
    "concepts": [
      "PyTrees are flattened into a list of leaves.",
      "The flattening order is deterministic (left-to-right, depth-first).",
      "An optional `is_leaf` function can customize which nodes are considered leaves."
    ],
    "code_examples": []
  },
  {
    "title": "Flatten Function Definition",
    "concepts": [
      "The `tree` argument is the PyTree to flatten.",
      "The `is_leaf` argument is an optional function to determine leaves.",
      "The function returns a tuple: a list of leaves and a PyTreeDef."
    ],
    "code_examples": []
  },
  {
    "title": "Flattening Examples",
    "concepts": [
      "Demonstrates flattening a nested list/tuple structure using `jax.tree.flatten`.",
      "Shows how the flattened values and the tree definition are returned."
    ],
    "code_examples": [
      {
        "description": "Flatten a nested list and tuple structure using jax.tree.flatten.",
        "code": "import jax\n\nvals, treedef = jax.tree.flatten([1, (2, 3), [4, 5]])\nprint(vals)\nprint(treedef)"
      },
      {
        "description": "Repeat the previous example to ensure consistency.",
        "code": "import jax\n\nvals, treedef = jax.tree.flatten([1, (2, 3), [4, 5]])\nprint(vals)\nprint(treedef)"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "References other related jax.tree functions: jax.tree.leaves(), jax.tree.structure(), and jax.tree.unflatten()."
    ],
    "code_examples": []
  },
  {
    "title": "Flattening PyTrees with Key Paths",
    "concepts": [
      "The function flattens a pytree and returns each leaf's key path.",
      "The function requires a pytree as input.",
      "Custom types within the pytree should be registered with `register_pytree_with_keys`.",
      "The function can take an optional `is_leaf` argument to specify custom leaf nodes.",
      "The function returns a list of key-leaf pairs and a treedef.",
      "Each key-leaf pair contains a leaf and its key path."
    ],
    "code_examples": [
      {
        "description": "Flattening a list with a dictionary and extracting key paths.",
        "code": "import jax\n\npath_vals, treedef = jax.tree.flatten_with_path([1, {'x': 3}])\nprint(path_vals)\nprint(treedef)"
      },
      {
        "description": "Flattening a list with a dictionary and extracting key paths (repeated example).",
        "code": "import jax\n\npath_vals, treedef = jax.tree.flatten_with_path([1, {'x': 3}])\nprint(path_vals)\nprint(treedef)"
      }
    ]
  },
  {
    "title": "Description of jax.tree.leaves",
    "concepts": [
      "The function `jax.tree.leaves` retrieves the leaves of a pytree.",
      "The `tree` argument is the pytree to process.",
      "The `is_leaf` argument is an optional function to determine if a subtree should be considered a leaf.",
      "The function returns a list of the leaves of the pytree."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the use of `jax.tree.leaves` to extract the leaves from a nested list and tuple structure.",
        "code": "import jax\n\njax.tree.leaves([1, (2, 3), [4, 5]])"
      },
      {
        "description": "Replicated example demonstrating the use of `jax.tree.leaves` to extract the leaves from a nested list and tuple structure.",
        "code": "import jax\n\njax.tree.leaves([1, (2, 3), [4, 5]])"
      }
    ]
  },
  {
    "title": "Description of `leaves_with_path`",
    "concepts": [
      "The function `leaves_with_path` retrieves the leaves of a PyTree.",
      "It returns a list of key-leaf pairs.",
      "Each key-leaf pair contains a leaf and its key path within the PyTree.",
      "Custom types within the PyTree should be registered using `register_pytree_with_keys`.",
      "The `is_leaf` argument allows specifying a custom function to determine if a value is a leaf.",
      "The return value is a list of tuples, where each tuple contains a key path and a leaf value."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use `jax.tree.leaves_with_path` to get the leaves and their corresponding key paths for a simple PyTree (list and dictionary).",
        "code": "import jax\n\njax.tree.leaves_with_path([1, {'x': 3}])"
      },
      {
        "description": "Demonstrates how to use `jax.tree.leaves_with_path` to get the leaves and their corresponding key paths for a simple PyTree (list and dictionary).",
        "code": "import jax\n\njax.tree.leaves_with_path([1, {'x': 3}])"
      }
    ]
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.tree.leaves()",
      "jax.tree.flatten_with_path()",
      "jax.tree_util.register_pytree_with_keys()"
    ],
    "code_examples": []
  },
  {
    "title": "Overview of `jax.tree.map_with_path`",
    "concepts": [
      "The function `jax.tree.map_with_path` maps a multi-input function over a pytree, providing the key path and arguments to the function.",
      "It's a more powerful alternative to `tree_map` because it provides the key path to each leaf.",
      "The function `f` takes the key path and the leaves of the pytrees as input.",
      "The `tree` argument is the pytree to be mapped over.",
      "The `*rest` argument is a tuple of pytrees with the same structure as `tree` or having `tree` as a prefix.",
      "The `is_leaf` argument is an optional function to determine if a value is a leaf."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of `jax.tree.map_with_path` usage",
    "concepts": [
      "This section provides examples of how to use the `jax.tree.map_with_path` function.",
      "The examples demonstrates how to add the index of the path to the value of each leaf in the tree."
    ],
    "code_examples": [
      {
        "description": "Example demonstrates how to add the index of the path to the value of each leaf in the tree using `jax.tree.map_with_path`.",
        "code": "import jax\n\njax.tree.map_with_path(lambda path, x: x + path[0].idx, [1, 2, 3])"
      },
      {
        "description": "Another example that demonstrates adding the index of the path to the value of each leaf in the tree using `jax.tree.map_with_path`.",
        "code": "import jax\n\njax.tree.map_with_path(lambda path, x: x + path[0].idx, [1, 2, 3])"
      }
    ]
  },
  {
    "title": "Related Functions",
    "concepts": [
      "This section lists related functions in the `jax.tree` module.",
      "`jax.tree.map()` is related to `jax.tree.map_with_path()` but does not provide the key path.",
      "`jax.tree.flatten_with_path()` flattens a pytree and returns the leaves along with their key paths.",
      "`jax.tree.leaves_with_path()` returns the leaves of a pytree along with their key paths.",
      "`jax.tree_util.register_pytree_with_keys()` registers a class as a pytree node, enabling key paths to be used."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.tree.reduce()",
    "concepts": [
      "The function `jax.tree.reduce()` applies a reduction function over the leaves of a pytree.",
      "It takes a reduction function, a pytree, an optional initializer, and an optional `is_leaf` function as input.",
      "The `is_leaf` function determines whether to traverse a subtree or treat it as a leaf.",
      "The function returns the reduced value."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.tree.reduce() Usage",
    "concepts": [
      "The example demonstrates how to use `jax.tree.reduce()` with `operator.add` to sum the leaves of a nested list.",
      "The `operator.add` function is used as the reduction function.",
      "The pytree is a nested list containing numbers.",
      "The `jax.tree.reduce()` function sums all the numbers in the nested list.",
      "The result of the reduction is 21."
    ],
    "code_examples": [
      {
        "description": "Sums the leaves of a pytree using operator.add.",
        "code": "import jax\nimport operator\n\njax.tree.reduce(operator.add, [1, (2, 3), [4, 5, 6]])"
      },
      {
        "description": "Sums the leaves of a pytree using operator.add (repeated example).",
        "code": "import jax\nimport operator\n\njax.tree.reduce(operator.add, [1, (2, 3), [4, 5, 6]])"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "Related functions are `jax.tree.leaves()` and `jax.tree.map()`."
    ],
    "code_examples": []
  },
  {
    "title": "Get the treedef for a pytree",
    "concepts": [
      "Retrieves the PyTreeDef representing the structure of a pytree.",
      "The `tree` argument is the pytree to analyze.",
      "The `is_leaf` argument is an optional function to determine if a subtree should be treated as a leaf."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to get the tree structure of a list containing numbers and tuples.",
        "code": "import jax\n\njax.tree.structure([1, (2, 3), [4, 5]])"
      },
      {
        "description": "Demonstrates how to get the tree structure of a list containing numbers and tuples (repeated example).",
        "code": "import jax\n\njax.tree.structure([1, (2, 3), [4, 5]])"
      }
    ]
  },
  {
    "title": "Transposing PyTrees",
    "concepts": [
      "Transforms a tree with (outer, inner) structure to (inner, outer) structure.",
      "Uses `outer_treedef` to represent the outer tree structure.",
      "Uses `inner_treedef` to represent the inner tree structure, which can be inferred.",
      "Takes `pytree_to_transpose` as input, which is the pytree to be transposed.",
      "Returns the transposed pytree."
    ],
    "code_examples": [
      {
        "description": "Demonstrates transposing a tree with a specified inner structure.",
        "code": "import jax\n\ntree = [(1, 2, 3), (4, 5, 6)]\ninner_structure = jax.tree.structure(('*', '*', '*'))\nouter_structure = jax.tree.structure(['*', '*'])\n\njax.tree.transpose(\n    outer_structure,\n    inner_structure,\n    tree\n)\n# Output: ([1, 4], [2, 5], [3, 6])"
      },
      {
        "description": "Demonstrates transposing a tree with inferred inner structure.",
        "code": "import jax\n\ntree = [(1, 2, 3), (4, 5, 6)]\ninner_structure = jax.tree.structure(('*', '*', '*'))\nouter_structure = jax.tree.structure(['*', '*'])\n\njax.tree.transpose(\n    outer_structure,\n    inner_structure,\n    tree\n)\n# Output: ([1, 4], [2, 5], [3, 6])"
      },
      {
        "description": "Demonstrates transposing a tree with inferred inner structure, repeated example.",
        "code": "import jax\n\ntree = [(1, 2, 3), (4, 5, 6)]\ninner_structure = jax.tree.structure(('*', '*', '*'))\nouter_structure = jax.tree.structure(['*', '*'])\n\njax.tree.transpose(\n    outer_structure,\n    inner_structure,\n    tree\n)\n# Output: ([1, 4], [2, 5], [3, 6])"
      }
    ]
  },
  {
    "title": "Reconstructing PyTrees with `jax.tree.unflatten`",
    "concepts": [
      "`jax.tree.unflatten` reconstructs a PyTree from a `PyTreeDef` and a list of leaves.",
      "It is the inverse operation of `jax.tree.flatten`.",
      "The order of leaves is important for reconstruction."
    ],
    "code_examples": [
      {
        "description": "Example showing how to use `jax.tree.unflatten` after using `jax.tree.flatten` to replace the leaves of a PyTree.",
        "code": "import jax\n\nvals, treedef = jax.tree.flatten([1, (2, 3), [4, 5]])\nnewvals = [100, 200, 300, 400, 500]\njax.tree.unflatten(treedef, newvals)"
      },
      {
        "description": "Another example demonstrating how to reconstruct a PyTree with new values using `jax.tree.unflatten`.",
        "code": "import jax\n\nvals, treedef = jax.tree.flatten([1, (2, 3), [4, 5]])\nnewvals = [100, 200, 300, 400, 500]\njax.tree.unflatten(treedef, newvals)"
      }
    ]
  },
  {
    "title": "Introduction to PyTrees",
    "concepts": [
      "Pytrees are tree-like container data structures such as nested tuples, lists, and dicts.",
      "Pytrees are defined recursively.",
      "The module provides utilities for working with pytrees.",
      "The primary purpose is to enable interoperability between user-defined data structures and JAX transformations.",
      "The set of Python types considered pytree nodes is extensible via a module-level registry."
    ],
    "code_examples": []
  },
  {
    "title": "Registering Custom Pytree Nodes",
    "concepts": [
      "Custom data structures can be registered as pytree nodes.",
      "Registration makes the type transparent to pytree utility functions.",
      "Several functions are provided for registering custom pytree nodes.",
      "register_dataclass registers dataclasses as pytree nodes.",
      "register_pytree_node registers a type with flatten/unflatten functions.",
      "register_pytree_node_class registers a class as a pytree node.",
      "register_pytree_with_keys allows registering a type with key information.",
      "register_static registers a class as a pytree with no leaves."
    ],
    "code_examples": []
  },
  {
    "title": "Pytree Manipulation Functions",
    "concepts": [
      "Functions are provided for flattening, unflattening, mapping, and reducing pytrees.",
      "tree_flatten flattens a pytree into a list of leaves and a treedef.",
      "tree_unflatten reconstructs a pytree from a treedef and a list of leaves.",
      "tree_map applies a function to each leaf of a pytree.",
      "tree_reduce reduces the leaves of a pytree to a single value.",
      "tree_structure returns the treedef of a pytree.",
      "Additional functions include: all_leaves, build_tree, treedef_children, treedef_is_leaf, treedef_tuple, tree_all, tree_leaves",
      "Functions that operate on pytrees with paths: tree_flatten_with_path, tree_leaves_with_path, tree_map_with_path",
      "Functions to support key-based operations: register_pytree_with_keys, register_pytree_with_keys_class, keystr"
    ],
    "code_examples": []
  },
  {
    "title": "Aliases to jax.tree",
    "concepts": [
      "Most functions in this module are now aliases to functions in jax.tree.",
      "This includes tree_flatten, tree_unflatten, tree_map, tree_reduce, tree_structure, tree_transpose, tree_leaves, tree_all, tree_flatten_with_path, tree_leaves_with_path, tree_map_with_path."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to `Partial`",
    "concepts": [
      "functools.partial is not compatible with JAX transformations.",
      "`Partial` provides a version of functools.partial that works in pytrees and is compatible with JAX.",
      "It allows partial function evaluation for use with JAX.",
      "It is important to explicitly opt-in to this behavior.",
      "Passing zero arguments to Partial effectively wraps the original function."
    ],
    "code_examples": [
      {
        "description": "Basic usage of Partial, similar to functools.partial.",
        "code": "import jax.numpy as jnp\nadd_one = Partial(jnp.add, 1)\nadd_one(2)"
      },
      {
        "description": "Using Partial within a jitted JAX function.",
        "code": "from jax import jit\n@jit\ndef call_func(f, *args):\n  return f(*args)\n\ncall_func(add_one, 2)"
      },
      {
        "description": "Wrapping a function with Partial using zero arguments.",
        "code": "call_func(Partial(jnp.add), 1, 2)"
      },
      {
        "description": "Demonstrates tracing behavior when Partial result is used in a traced context.",
        "code": "print_zero = Partial(print, 0)\nprint_zero()\ncall_func(print_zero)"
      }
    ]
  },
  {
    "title": "Methods and Attributes",
    "concepts": [
      "__init__() is the constructor.",
      "The `args` attribute holds the tuple of arguments to future partial calls.",
      "The `func` attribute holds the function object to use in future partial calls.",
      "The `keywords` attribute holds the dictionary of keyword arguments to future partial calls."
    ],
    "code_examples": []
  },
  {
    "title": "Description of `all_leaves` Function",
    "concepts": [
      "The function tests whether all elements in a given iterable are leaves.",
      "It is used in advanced cases, such as when a library allows map operations on a flat iterable of leaves.",
      "The function checks if the result is still a flat iterable of leaves.",
      "The function takes an iterable and an optional is_leaf callable as input.",
      "It returns a boolean indicating if all elements in the input are leaves."
    ],
    "code_examples": [
      {
        "description": "Example usage of `all_leaves` with jax.tree_util.tree_leaves. Demonstrates that a list of leaves returns True, while a list containing a tree returns False.",
        "code": "import jax\n\ntree = {\"a\": [1, 2, 3]}\n\nassert all_leaves(jax.tree_util.tree_leaves(tree))\nassert not all_leaves([tree])"
      },
      {
        "description": "Example usage of `all_leaves` with jax.tree_util.tree_leaves. Demonstrates that a list of leaves returns True, while a list containing a tree returns False. This is a repetition of the previous example.",
        "code": "import jax\n\ntree = {\"a\": [1, 2, 3]}\n\nassert all_leaves(jax.tree_util.tree_leaves(tree))\nassert not all_leaves([tree])"
      }
    ]
  },
  {
    "title": "Overview of register_dataclass",
    "concepts": [
      "Extends the set of types that are considered internal nodes in pytrees.",
      "Uses the optimized C++ dataclass builtin for registration.",
      "The decorated class is assumed to have dataclass semantics.",
      "Class attributes represent the whole of the object state.",
      "Attributes can be passed as keywords to the class constructor to create a copy of the object."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters: nodetype, meta_fields, data_fields, and drop_fields",
    "concepts": [
      "nodetype is a Python type to treat as an internal pytree node.",
      "meta_fields are attributes treated as static when the pytree is passed to jax.jit().",
      "meta_fields is optional if nodetype is a dataclass; fields can be marked static via dataclasses.field().",
      "Metadata fields must be static, hashable, immutable objects.",
      "data_fields are attributes treated as non-static when the pytree is passed to jax.jit().",
      "data_fields is optional if nodetype is a dataclass; fields are assumed data fields unless marked via dataclasses.field().",
      "Data fields must be JAX-compatible objects (arrays, scalars, pytrees with array/scalar leaves).",
      "None is a valid data field.",
      "drop_fields are fields to be dropped"
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage (JAX <= 0.4.35)",
    "concepts": [
      "Demonstrates how to register a dataclass as a PyTree using `register_dataclass` in older JAX versions.",
      "Explicitly defines `data_fields` and `meta_fields` when registering the dataclass."
    ],
    "code_examples": [
      {
        "description": "Registers a dataclass `MyStruct` as a PyTree, specifying data and metadata fields. This example is applicable to JAX versions older than or equal to v0.4.35.",
        "code": "import jax\nfrom dataclasses import dataclass\nfrom functools import partial\n\n@partial(\n    jax.tree_util.register_dataclass,\n    data_fields=['x', 'y'],\n    meta_fields=['op']\n)\n@dataclass\nclass MyStruct:\n    x: jax.Array\n    y: jax.Array\n    op: str\n\nm = MyStruct(x=jnp.ones(3), y=jnp.arange(3), op='add')\nm"
      }
    ]
  },
  {
    "title": "Example Usage (JAX >= 0.4.36)",
    "concepts": [
      "Shows how to register a dataclass as a PyTree in newer JAX versions.",
      "Uses `dataclasses.field` with `metadata={'static': True}` to mark fields as static.",
      "Data and metadata fields are automatically inferred if not explicitly specified."
    ],
    "code_examples": [
      {
        "description": "Registers a dataclass `MyStruct` as a PyTree, marking the `op` field as static using `dataclasses.field`. This example works for JAX versions starting from v0.4.36.",
        "code": "import jax\nfrom dataclasses import dataclass, field\n\n@jax.tree_util.register_dataclass\n@dataclass\nclass MyStruct:\n    x: jax.Array  # defaults to non-static data field\n    y: jax.Array  # defaults to non-static data field\n    op: str = field(metadata=dict(static=True))  # marked as static meta field.\n\nm = MyStruct(x=jnp.ones(3), y=jnp.arange(3), op='add')\nm"
      }
    ]
  },
  {
    "title": "Using the Registered Class",
    "concepts": [
      "Demonstrates how to use the registered dataclass with `jax.tree.flatten` and `jax.tree.unflatten`.",
      "Shows the treedef structure for the registered dataclass."
    ],
    "code_examples": [
      {
        "description": "Flattens and unflattens the registered dataclass to demonstrate its integration with JAX's tree utilities.",
        "code": "leaves, treedef = jax.tree.flatten(m)\nleaves\ntreedef\njax.tree.unflatten(treedef, leaves)"
      }
    ]
  },
  {
    "title": "Integration with jax.jit",
    "concepts": [
      "Demonstrates how the registered dataclass can be seamlessly used with `jax.jit`.",
      "Shows that data fields are treated as dynamic arguments and metadata fields are treated as static arguments during JIT compilation."
    ],
    "code_examples": [
      {
        "description": "Defines and JIT-compiles a function that uses the registered dataclass, showcasing the automatic handling of data and metadata fields.",
        "code": "@jax.jit\ndef compiled_func(m):\n    if m.op == 'add':\n        return m.x + m.y\n    else:\n        raise ValueError(f\"{m.op=}\")\n\ncompiled_func(m)"
      }
    ]
  },
  {
    "title": "Introduction to PyTree Registration",
    "concepts": [
      "PyTrees are tree-like data structures used by JAX for automatic differentiation and other transformations.",
      "By default, JAX only recognizes a limited set of types as PyTrees.",
      "The `register_pytree_node` function extends the set of types that JAX considers as internal nodes in PyTrees.",
      "It requires a node type, a flatten function, and an unflatten function as arguments."
    ],
    "code_examples": []
  },
  {
    "title": "Registering a Custom Class as a PyTree Node",
    "concepts": [
      "A custom class `MyContainer` is defined with attributes `x`, `y`, and `size`.",
      "Attempting to use `MyContainer` instances in JIT-compiled JAX functions results in a `TypeError` because JAX doesn't recognize the custom type.",
      "To resolve this, `MyContainer` is registered as a PyTree node using `jax.tree_util.register_pytree_node`.",
      "A `flatten_func` is defined to specify how to break down a `MyContainer` instance into its children (arrays `x` and `y`) and auxiliary data (size).",
      "An `unflatten_func` is defined to specify how to reconstruct a `MyContainer` instance from the auxiliary data and the unflattened children."
    ],
    "code_examples": [
      {
        "description": "Definition of a custom class MyContainer",
        "code": "class MyContainer:\n    def __init__(self, size):\n        self.x = jnp.zeros(size)\n        self.y = jnp.ones(size)\n        self.size = size"
      },
      {
        "description": "Example usage of MyContainer in a JIT compiled function before registering it as a pytree",
        "code": "m = MyContainer(size=5)\n\ndef f(m):\n    return m.x + m.y + jnp.arange(m.size)\n\njax.jit(f)(m)"
      },
      {
        "description": "Definition of flatten_func for MyContainer",
        "code": "def flatten_func(obj):\n    children = (obj.x, obj.y)  # children must contain arrays & pytrees\n    aux_data = (obj.size,)  # aux_data must contain static, hashable data.\n    return (children, aux_data)"
      },
      {
        "description": "Definition of unflatten_func for MyContainer",
        "code": "def unflatten_func(aux_data, children):\n    # Here we avoid `__init__` because it has extra logic we don't require:\n    obj = object.__new__(MyContainer)\n    obj.x, obj.y = children\n    obj.size, = aux_data\n    return obj"
      },
      {
        "description": "Registering MyContainer as a PyTree node",
        "code": "jax.tree_util.register_pytree_node(\n    MyContainer,\n    flatten_func,\n    unflatten_func\n)"
      }
    ]
  },
  {
    "title": "Using the Registered Class in JIT-compiled Functions",
    "concepts": [
      "After registering `MyContainer` as a PyTree node, instances of this type can be used in JIT-compiled functions without causing a `TypeError`.",
      "The JIT-compiled function `f` now correctly processes the `MyContainer` instance."
    ],
    "code_examples": [
      {
        "description": "Calling the JIT compiled function with the registered MyContainer",
        "code": "jax.jit(f)(m)"
      }
    ]
  },
  {
    "title": "Introduction to `register_pytree_node_class`",
    "concepts": [
      "Extends the set of types considered internal nodes in pytrees.",
      "It's a class-oriented interface for register_pytree_node.",
      "The input class is returned unchanged after registration, enabling its use as a decorator.",
      "The function registers a class as a JAX pytree."
    ],
    "code_examples": []
  },
  {
    "title": "Registering a Custom Container with `register_pytree_node_class`",
    "concepts": [
      "Demonstrates defining a custom container compatible with JAX transformations like `jax.jit()`.",
      "The custom container must define `tree_flatten` and `tree_unflatten` methods.",
      "`tree_flatten` specifies how to break the container into children and auxiliary data.",
      "`tree_unflatten` specifies how to reconstruct the container from children and auxiliary data."
    ],
    "code_examples": [
      {
        "description": "Defines a custom container `MyContainer` and registers it as a pytree node class.  It then defines a function `f` that operates on the container and uses `jax.jit` to compile it.",
        "code": "import jax\n\n@jax.tree_util.register_pytree_node_class\nclass MyContainer:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def tree_flatten(self):\n        return ((self.x, self.y), None)\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n\n\nm = MyContainer(jnp.zeros(4), jnp.arange(4))\n\ndef f(m):\n    return m.x + 2 * m.y\n\njax.jit(f)(m)"
      },
      {
        "description": "Defines a custom container `MyContainer` and registers it as a pytree node class.  It then defines a function `f` that operates on the container and uses `jax.jit` to compile it.",
        "code": "import jax\n\n@jax.tree_util.register_pytree_node_class\nclass MyContainer:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def tree_flatten(self):\n        return ((self.x, self.y), None)\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n\n\nm = MyContainer(jnp.zeros(4), jnp.arange(4))\n\ndef f(m):\n    return m.x + 2 * m.y\n\njax.jit(f)(m)"
      }
    ]
  },
  {
    "title": "Overview of `register_pytree_with_keys`",
    "concepts": [
      "Extends the set of types considered internal nodes in pytrees.",
      "It is a more powerful alternative to `register_pytree_node`.",
      "Allows access to each pytree leaf's key path when flattening and tree-mapping.",
      "`nodetype` is a Python type to treat as an internal pytree node.",
      "`flatten_with_keys` is a function used during flattening, taking a `nodetype` and returning an iterable of key-child pairs and auxiliary data.",
      "`unflatten_func` takes auxiliary data and unflattened children, returning an instance of `nodetype`.",
      "`flatten_func` is an optional function similar to `flatten_with_keys`, but returns only children and auxiliary data for faster traversal."
    ],
    "code_examples": []
  },
  {
    "title": "Defining a Custom Type: `MyContainer`",
    "concepts": [
      "Defines a custom class `MyContainer` with `x`, `y`, and `size` attributes.",
      "The `__init__` method initializes the attributes with JAX arrays."
    ],
    "code_examples": [
      {
        "description": "Defines the `MyContainer` class with an initializer that takes `size` as an argument and initializes `x` as a JAX array of zeros and `y` as a JAX array of ones, both of size `size`.",
        "code": "class MyContainer:\n    def __init__(self, size):\n        self.x = jnp.zeros(size)\n        self.y = jnp.ones(size)\n        self.size = size"
      }
    ]
  },
  {
    "title": "Registering `MyContainer` as a PyTree Node",
    "concepts": [
      "Registers the `MyContainer` class as a PyTree node using `register_pytree_node`.",
      "Defines a `flatten_with_keys` function that returns the children (x and y) along with their corresponding `GetAttrKey` and auxiliary data (size).",
      "Defines an `unflatten` function that takes the auxiliary data and children and reconstructs a `MyContainer` object."
    ],
    "code_examples": [
      {
        "description": "Imports necessary modules from `jax.tree_util`, including `register_pytree_with_keys_class` and `GetAttrKey`.",
        "code": "from jax.tree_util import register_pytree_with_keys_class, GetAttrKey"
      },
      {
        "description": "Defines the `flatten_with_keys` function, which takes a `MyContainer` object, extracts its `x` and `y` attributes along with their `GetAttrKey` and its size and returns them as children and auxiliary data respectively.",
        "code": "def flatten_with_keys(obj):\n    children = [(GetAttrKey('x'), obj.x),\n                (GetAttrKey('y'), obj.y)]\n    # children must contain arrays & pytrees\n    aux_data = (obj.size,)\n    # aux_data must contain static, hashable data.\n    return children, aux_data"
      },
      {
        "description": "Defines the `unflatten` function, which takes the auxiliary data (size) and children (x, y) and reconstructs a `MyContainer` object.  It avoids calling `__init__` to skip extra logic.",
        "code": "def unflatten(aux_data, children):\n    # Here we avoid `__init__` because it has extra logic we don't require:\n    obj = object.__new__(MyContainer)\n    obj.x, obj.y = children\n    obj.size, = aux_data\n    return obj"
      },
      {
        "description": "Registers the `MyContainer` class as a PyTree node, associating it with the defined `flatten_with_keys` and `unflatten` functions.",
        "code": "jax.tree_util.register_pytree_node(\n    MyContainer,\n    flatten_with_keys,\n    unflatten\n)"
      }
    ]
  },
  {
    "title": "Using `tree_flatten_with_path` with `MyContainer`",
    "concepts": [
      "Creates an instance of `MyContainer`.",
      "Uses `tree_flatten_with_path` to flatten the `MyContainer` instance and obtain the leaves and treedef."
    ],
    "code_examples": [
      {
        "description": "Creates an instance of the `MyContainer` class with `size` set to 4.",
        "code": "m = MyContainer(4)"
      },
      {
        "description": "Flattens the `MyContainer` instance `m` using `tree_flatten_with_path` to retrieve the leaves and the treedef.",
        "code": "leaves, treedef = jax.tree_util.tree_flatten_with_path(m)"
      }
    ]
  },
  {
    "title": "Introduction to `register_pytree_with_keys_class`",
    "concepts": [
      "Extends the set of types that are considered internal nodes in pytrees.",
      "Requires a class that defines how it can be flattened with keys.",
      "It is a thin wrapper around register_pytree_with_keys.",
      "Provides a class-oriented interface.",
      "The input class is returned unchanged, allowing usage as a decorator."
    ],
    "code_examples": []
  },
  {
    "title": "Registering a Class as a PyTree Node with Keys",
    "concepts": [
      "Demonstrates how to register a class as a PyTree node using `register_pytree_with_keys_class`.",
      "Shows how to define `tree_flatten_with_keys` to specify how the class's attributes are flattened with keys.",
      "Shows how to define `tree_unflatten` to reconstruct the class from flattened children."
    ],
    "code_examples": [
      {
        "description": "Example showcasing the registration of the `Special` class as a pytree node with keys. It defines how to flatten and unflatten the class, using `GetAttrKey` to associate keys with attributes.",
        "code": "from jax.tree_util import register_pytree_with_keys_class, GetAttrKey\n\n@register_pytree_with_keys_class\nclass Special:\n  def __init__(self, x, y):\n    self.x = x\n    self.y = y\n\n  def tree_flatten_with_keys(self):\n    return (((GetAttrKey('x'), self.x), (GetAttrKey('y'), self.y)), None)\n\n  @classmethod\n  def tree_unflatten(cls, aux_data, children):\n    return cls(*children)"
      },
      {
        "description": "Example showcasing the registration of the `Special` class as a pytree node with keys. It defines how to flatten and unflatten the class, using `GetAttrKey` to associate keys with attributes.",
        "code": "from jax.tree_util import register_pytree_with_keys_class, GetAttrKey\n\n@register_pytree_with_keys_class\nclass Special:\n  def __init__(self, x, y):\n    self.x = x\n    self.y = y\n\n  def tree_flatten_with_keys(self):\n    return (((GetAttrKey('x'), self.x), (GetAttrKey('y'), self.y)), None)\n\n  @classmethod\n  def tree_unflatten(cls, aux_data, children):\n    return cls(*children)"
      }
    ]
  },
  {
    "title": "Registering a Class as a Static PyTree",
    "concepts": [
      "The `register_static` function registers a class as a pytree with no leaves.",
      "Instances of the registered class are treated as static by JAX transformations like `jax.jit` and `jax.pmap`.",
      "This provides an alternative to specifying static arguments using `static_argnums` or `static_argnames` in JAX transformations.",
      "The class must be hashable.",
      "The input class is returned unchanged, allowing the function to be used as a decorator."
    ],
    "code_examples": [
      {
        "description": "Registering a class `StaticStr` as static using `jax.tree_util.register_static`.",
        "code": "import jax\n\n@jax.tree_util.register_static\nclass StaticStr(str):\n  pass"
      },
      {
        "description": "Registering a class `StaticStr` as static using `jax.tree_util.register_static`.",
        "code": "import jax\n\n@jax.tree_util.register_static\nclass StaticStr(str):\n  pass"
      },
      {
        "description": "Using the static string in a `jax.jit`-compiled function without needing to specify it as a static argument.",
        "code": "@jax.jit\ndef f(x, y, s):\n  return x + y if s == 'add' else x - y\n\nf(1, 2, StaticStr('add'))"
      },
      {
        "description": "Using the static string in a `jax.jit`-compiled function without needing to specify it as a static argument.",
        "code": "@jax.jit\ndef f(x, y, s):\n  return x + y if s == 'add' else x - y\n\nf(1, 2, StaticStr('add'))"
      }
    ]
  },
  {
    "title": "Alias and Parameters",
    "concepts": [
      "This documents is an alias of jax.tree.flatten_with_path().",
      "The function takes 'tree' as an argument of type Any.",
      "The function takes 'is_leaf' as an argument which is either a Callable[[Any], bool] or None."
    ],
    "code_examples": []
  },
  {
    "title": "Return Type",
    "concepts": [
      "The function returns a tuple containing a list of tuples and a PyTreeDef.",
      "The list of tuples consists of KeyPath and Any types."
    ],
    "code_examples": []
  },
  {
    "title": "Alias of jax.tree.leaves_with_path()",
    "concepts": [
      "This section describes an alias of the jax.tree.leaves_with_path() function.",
      "The function takes a tree as input.",
      "The function takes an optional is_leaf argument, which is a callable that determines if a node is a leaf."
    ],
    "code_examples": []
  },
  {
    "title": "Alias of jax.tree.map_with_path()",
    "concepts": [
      "This section describes an alias of the jax.tree.map_with_path() function.",
      "The function `f` is a callable that is applied to each element of the tree.",
      "The `tree` argument represents the input tree structure.",
      "The `rest` argument represents any additional arguments passed to the function.",
      "The `is_leaf` argument is an optional callable to determine if a value is a leaf."
    ],
    "code_examples": []
  },
  {
    "title": "treedef_children Function",
    "concepts": [
      "Returns a list of treedefs for immediate children.",
      "Takes a single PyTreeDef as input.",
      "Returns a list of PyTreeDefs representing the children of the input treedef."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of `treedef_children` with a nested data structure.",
        "code": "import jax\n\nx = [(1, 2), 3, {'a': 4}]\ntreedef = jax.tree.structure(x)\n\njax.tree_util.treedef_children(treedef)\n# Expected Output: [PyTreeDef((*, *)), PyTreeDef(*), PyTreeDef({'a': *})]"
      },
      {
        "description": "Verify that the result of `treedef_children` is equivalent to applying `jax.tree.structure` to each value in the original data structure.",
        "code": "import jax\n\nx = [(1, 2), 3, {'a': 4}]\ntreedef = jax.tree.structure(x)\n\njax.tree_util.treedef_children(treedef)\n\n_ == [\n    jax.tree.structure(vals)\n    for vals in x\n]\n# Expected Output: True"
      },
      {
        "description": "Demonstrates the usage of `treedef_children` with a nested data structure (repeated example).",
        "code": "import jax\n\nx = [(1, 2), 3, {'a': 4}]\ntreedef = jax.tree.structure(x)\n\njax.tree_util.treedef_children(treedef)\n# Expected Output: [PyTreeDef((*, *)), PyTreeDef(*), PyTreeDef({'a': *})]"
      },
      {
        "description": "Verify that the result of `treedef_children` is equivalent to applying `jax.tree.structure` to each value in the original data structure (repeated example).",
        "code": "import jax\n\nx = [(1, 2), 3, {'a': 4}]\ntreedef = jax.tree.structure(x)\n\njax.tree_util.treedef_children(treedef)\n\n_ == [\n    jax.tree.structure(vals)\n    for vals in x\n]\n# Expected Output: True"
      }
    ]
  },
  {
    "title": "Checking if a PyTreeDef is a Leaf",
    "concepts": [
      "The function `treedef_is_leaf` checks if a given PyTreeDef represents a leaf node.",
      "A leaf node is defined as a tree with a single node.",
      "The function returns True if the PyTreeDef is a leaf, and False otherwise."
    ],
    "code_examples": [
      {
        "description": "Example 1: Checking if a PyTreeDef representing a single integer is a leaf.",
        "code": "import jax\n\ntree1 = jax.tree.structure(1)\n\njax.tree_util.treedef_is_leaf(tree1)"
      },
      {
        "description": "Example 2: Checking if a PyTreeDef representing a list of integers is a leaf.",
        "code": "import jax\n\ntree2 = jax.tree.structure([1, 2])\n\njax.tree_util.treedef_is_leaf(tree2)"
      },
      {
        "description": "Example 3: Checking if a PyTreeDef representing a single integer is a leaf.",
        "code": "import jax\n\ntree1 = jax.tree.structure(1)\n\njax.tree_util.treedef_is_leaf(tree1)"
      },
      {
        "description": "Example 4: Checking if a PyTreeDef representing a list of integers is a leaf.",
        "code": "import jax\n\ntree2 = jax.tree.structure([1, 2])\n\njax.tree_util.treedef_is_leaf(tree2)"
      }
    ]
  },
  {
    "title": "treedef_tuple Function",
    "concepts": [
      "The function makes a tuple treedef from an iterable of child treedefs.",
      "It takes an iterable of PyTreeDef objects as input.",
      "It returns a single treedef representing a tuple of the structures."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to create a treedef_tuple from two PyTreeDef objects representing a list and a dictionary, and verifies that it matches the treedef of a tuple containing the list and dictionary.",
        "code": "import jax\n\nx = [1, 2, 3]\ny = {'a': 4, 'b': 5}\nx_tree = jax.tree.structure(x)\ny_tree = jax.tree.structure(y)\nxy_tree = jax.tree_util.treedef_tuple([x_tree, y_tree])\n\nxy_tree == jax.tree.structure((x, y))"
      },
      {
        "description": "Demonstrates how to create a treedef_tuple from two PyTreeDef objects representing a list and a dictionary, and verifies that it matches the treedef of a tuple containing the list and dictionary.",
        "code": "import jax\n\nx = [1, 2, 3]\ny = {'a': 4, 'b': 5}\nx_tree = jax.tree.structure(x)\ny_tree = jax.tree.structure(y)\nxy_tree = jax.tree_util.treedef_tuple([x_tree, y_tree])\n\nxy_tree == jax.tree.structure((x, y))"
      }
    ]
  },
  {
    "title": "Type Variable Definition and Usage",
    "concepts": [
      "TypeVar is used to define type variables for generic types and functions.",
      "Type variables can be unconstrained or constrained to specific types.",
      "Unconstrained type variables can be any type.",
      "Constrained type variables must be one of the specified types.",
      "Type variables primarily benefit static type checkers.",
      "Generic functions work via overloading.",
      "Type variables cannot be used with isinstance() or issubclass() at runtime."
    ],
    "code_examples": [
      {
        "description": "Define an unconstrained type variable T.",
        "code": "T = TypeVar('T')"
      },
      {
        "description": "Define a constrained type variable A, which can be either str or bytes.",
        "code": "A = TypeVar('A', str, bytes)"
      },
      {
        "description": "Example usage of a generic function with type variable T.",
        "code": "def repeat(x: T, n: int) -> list[T]:\n    '''Return a list containing n references to x.'''\n    return [x]*n"
      },
      {
        "description": "Example usage of a generic function with type variable T.",
        "code": "def longest(x: T, y: T) -> T:\n    '''Return the longest of two strings.'''\n    return x if len(x) >= len(y) else y"
      }
    ]
  },
  {
    "title": "Covariance, Contravariance, and Introspection",
    "concepts": [
      "Type variables can be declared as covariant or contravariant.",
      "Generic types are invariant by default.",
      "Type variables can be introspected to access their properties.",
      "Type variables defined in global scope can be pickled."
    ],
    "code_examples": [
      {
        "description": "Example of introspecting type variable properties.",
        "code": "T.__name__ == \u2018T\u2019\nT.__constraints__ == ()\nT.__covariant__ == False\nT.__contravariant__ = False\nA.__constraints__ == (str, bytes)"
      }
    ]
  },
  {
    "title": "Alias Definition",
    "concepts": [
      "The document defines an alias.",
      "The alias is named 'tuple'.",
      "The tuple consists of 'KeyEntry' elements."
    ],
    "code_examples": []
  },
  {
    "title": "keystr Function Overview",
    "concepts": [
      "The `keystr` function pretty-prints a tuple of keys.",
      "The `keys` argument is a tuple of KeyEntry or any class that can be converted to a string.",
      "The `simple` argument controls whether a simplified string representation is used.",
      "The `separator` argument specifies the separator used to join string representations of keys.",
      "The function returns a string representation of the key path."
    ],
    "code_examples": [
      {
        "description": "Example of using `keystr` with default settings to print key paths in a nested dictionary.",
        "code": "import jax\n\nparams = {\n    'foo': {\n        'bar': {\n            'baz': 1,\n            'bat': [2, 3]\n        }\n    }\n}\n\nfor path, _ in jax.tree_util.tree_leaves_with_path(params):\n    print(jax.tree_util.keystr(path))"
      },
      {
        "description": "Example of using `keystr` with `simple=True` and a custom separator to print key paths in a nested dictionary.",
        "code": "import jax\n\nparams = {\n    'foo': {\n        'bar': {\n            'baz': 1,\n            'bat': [2, 3]\n        }\n    }\n}\n\nfor path, _ in jax.tree_util.tree_leaves_with_path(params):\n    print(jax.tree_util.keystr(path, simple=True, separator='/'))"
      },
      {
        "description": "Another example of using `keystr` with default settings to print key paths in a nested dictionary. This example is duplicated in the original text.",
        "code": "import jax\n\nparams = {\n    'foo': {\n        'bar': {\n            'baz': 1,\n            'bat': [2, 3]\n        }\n    }\n}\n\nfor path, _ in jax.tree_util.tree_leaves_with_path(params):\n    print(jax.tree_util.keystr(path))"
      },
      {
        "description": "Another example of using `keystr` with `simple=True` and a custom separator to print key paths in a nested dictionary. This example is duplicated in the original text.",
        "code": "import jax\n\nparams = {\n    'foo': {\n        'bar': {\n            'baz': 1,\n            'bat': [2, 3]\n        }\n    }\n}\n\nfor path, _ in jax.tree_util.tree_leaves_with_path(params):\n    print(jax.tree_util.keystr(path, simple=True, separator='/'))"
      }
    ]
  },
  {
    "title": "Alias of jax.tree.all()",
    "concepts": [
      "This section describes an alias for the jax.tree.all() function.",
      "The function takes a tree-like data structure as input.",
      "It also takes an optional is_leaf function to specify custom leaf nodes.",
      "The function returns a boolean value."
    ],
    "code_examples": []
  },
  {
    "title": "Alias of jax.tree.flatten()",
    "concepts": [
      "jax.tree.flatten() is aliased.",
      "It takes a tree as input.",
      "It takes an optional is_leaf callable.",
      "It returns a tuple containing a list of leaves and a PyTreeDef."
    ],
    "code_examples": []
  },
  {
    "title": "Alias of jax.tree.leaves()",
    "concepts": [
      "This section describes the alias of the jax.tree.leaves() function.",
      "The function takes a tree-like data structure as input.",
      "The function takes an optional `is_leaf` argument to specify how to identify leaves."
    ],
    "code_examples": []
  },
  {
    "title": "Alias of jax.tree.map()",
    "concepts": [
      "This section describes an alias of the jax.tree.map() function.",
      "The function f is a callable that will be applied to each element of the tree.",
      "The argument tree is the tree structure to be mapped over.",
      "The argument rest represents additional arguments to the function f.",
      "The argument is_leaf is a callable that determines whether a node is a leaf."
    ],
    "code_examples": []
  },
  {
    "title": "jax.tree.reduce() Alias",
    "concepts": [
      "jax.tree.reduce() is an alias.",
      "It reduces a pytree to a single value using a function.",
      "The function takes an accumulator and a tree element as input.",
      "An initializer can be provided for the reduction.",
      "A custom `is_leaf` function can be used to define tree structure."
    ],
    "code_examples": []
  },
  {
    "title": "Alias and Tree Structure",
    "concepts": [
      "jax.tree.structure() is aliased.",
      "The function `tree` takes any object as input.",
      "The function `is_leaf` can be None or a callable that takes any object and returns a boolean.",
      "PyTreeDef is mentioned, likely referring to the definition of a PyTree structure."
    ],
    "code_examples": []
  },
  {
    "title": "Alias of jax.tree.transpose()",
    "concepts": [
      "This section refers to the alias of jax.tree.transpose().",
      "outer_treedef is a PyTreeDef.",
      "inner_treedef is a PyTreeDef or None.",
      "pytree_to_transpose is of Any type."
    ],
    "code_examples": []
  },
  {
    "title": "jax.tree.unflatten Alias",
    "concepts": [
      "Alias of jax.tree.unflatten().",
      "The function takes a PyTreeDef and an iterable of leaves as input.",
      "The function reconstructs a pytree from the treedef and leaves."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Typing",
    "concepts": [
      "The jax.typing module provides JAX-specific static type annotations.",
      "jax.Array is an annotation for JAX arrays or tracers.",
      "jax.typing.ArrayLike is an annotation for values that can be implicitly cast to a JAX array.",
      "jax.typing.DTypeLike is an annotation for values that can be cast to a JAX-compatible dtype.",
      "ArrayLike is recommended for array inputs in public API functions, and Array for array outputs."
    ],
    "code_examples": []
  },
  {
    "title": "Example of Using ArrayLike and Array",
    "concepts": [
      "Using ArrayLike for input type hints and Array for output type hints.",
      "Performing runtime type validation for ArrayLike.",
      "Converting ArrayLike inputs to jax.Array using jnp.asarray().",
      "JAX functions return Array types.",
      "JAX functions should not accept sequences such as list or tuple in place of arrays."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of ArrayLike for input and Array for output type hints in a JAX function, including runtime type validation.",
        "code": "import numpy as np\nimport jax.numpy as jnp\nfrom jax import Array\nfrom jax.typing import ArrayLike\n\ndef my_function(x: ArrayLike) -> Array:\n    # Runtime type validation, Python 3.10 or newer:\n    if not isinstance(x, ArrayLike):\n        raise TypeError(f\"Expected arraylike input; got {x}\")\n\n    # Runtime type validation, any Python version:\n    if not (isinstance(x, (np.ndarray, Array)) or np.isscalar(x)):\n        raise TypeError(f\"Expected arraylike input; got {x}\")\n\n    # Convert input to jax.Array:\n    x_arr = jnp.asarray(x)\n    # ... do some computation; JAX functions will return Array types:\n    result = x_arr.sum(0) / x_arr.shape[0]\n    # return an Array\n    return result"
      },
      {
        "description": "Demonstrates the use of ArrayLike for input and Array for output type hints in a JAX function, including runtime type validation.",
        "code": "import numpy as np\nimport jax.numpy as jnp\nfrom jax import Array\nfrom jax.typing import ArrayLike\n\ndef my_function(x: ArrayLike) -> Array:\n    # Runtime type validation, Python 3.10 or newer:\n    if not isinstance(x, ArrayLike):\n        raise TypeError(f\"Expected arraylike input; got {x}\")\n\n    # Runtime type validation, any Python version:\n    if not (isinstance(x, (np.ndarray, Array)) or np.isscalar(x)):\n        raise TypeError(f\"Expected arraylike input; got {x}\")\n\n    # Convert input to jax.Array:\n    x_arr = jnp.asarray(x)\n    # ... do some computation; JAX functions will return Array types:\n    result = x_arr.sum(0) / x_arr.shape[0]\n    # return an Array\n    return result"
      }
    ]
  },
  {
    "title": "JAX Typing Definitions",
    "concepts": [
      "ArrayLike: Type annotation for JAX array-like objects.",
      "DTypeLike: Alias for acceptable types of JAX dtypes (str | type [ Any ] | dtype | SupportsDType)."
    ],
    "code_examples": []
  },
  {
    "title": "Type Annotation for JAX Array-Like Objects",
    "concepts": [
      "Type annotation for JAX array-like objects.",
      "The type is an alias of Array, ndarray, bool, number, int, float, or complex."
    ],
    "code_examples": []
  },
  {
    "title": "Type Aliases",
    "concepts": [
      "The document defines type aliases.",
      "The alias can be a string.",
      "The alias can be a list of Any type.",
      "The alias can be a data type (dtype).",
      "The alias can be SupportsDType."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Export Library Overview",
    "concepts": [
      "jax.export is used for exporting and serializing JAX functions.",
      "Exported functions can be archived persistently.",
      "The library supports exporting to StableHLO.",
      "It enables cross-platform and multi-platform export.",
      "Shape polymorphism is supported through dimension variables.",
      "It has functionalities for reverse-mode AD.",
      "Provides APIs for calling exported functions from JAX programs.",
      "It uses calling convention versions for compatibility.",
      "Users can add platforms to JAX.",
      "Serialization can be customized by registering PyTree nodes.",
      "Symbolic shapes can be constructed from string representations.",
      "Provides tools for working with symbolic dimensions and scopes."
    ],
    "code_examples": []
  },
  {
    "title": "Exported Function Parameters",
    "concepts": [
      "fun_name is the name of the exported function.",
      "in_tree describes the input (args, kwargs) pytree structure.",
      "in_avals is a flat tuple of input abstract values, potentially with dimension expressions.",
      "out_tree describes the output pytree structure.",
      "out_avals is a flat tuple of output abstract values, potentially with dimension expressions.",
      "in_shardings_hlo is a flattened sequence of input HloShardings, where None means unspecified.",
      "out_shardings_hlo is a flattened sequence of output HloShardings, where None means unspecified.",
      "nr_devices is the number of devices the module was lowered for.",
      "platforms is a tuple of platforms for which the function is exported.",
      "ordered_effects are the ordered effects present in the serialized module.",
      "unordered_effects are the unordered effects present in the serialized module.",
      "mlir_module_serialized is the serialized lowered VHLO module (bytes).",
      "calling_convention_version is a version number for the calling convention.",
      "module_kept_var_idx is the sorted indices of the arguments among in_avals that must be passed to the module",
      "uses_global_constants indicates whether the mlir_module_serialized uses shape polymorphism or multi-platform export.",
      "disabled_safety_checks is a list of descriptors of safety checks that have been disabled at export time.",
      "_get_vjp is an optional function to obtain the exported VJP function."
    ],
    "code_examples": []
  },
  {
    "title": "Calling Exported Functions",
    "concepts": [
      "Exported functions can be called from JAX programs.",
      "Arguments should have the same pytree structure as when the function was exported.",
      "Supports reverse-mode AD, shape polymorphism, multi-platform, and device polymorphism."
    ],
    "code_examples": []
  },
  {
    "title": "Sharding and Mesh Integration",
    "concepts": [
      "The `in_shardings_jax` and `out_shardings_jax` methods create Shardings corresponding to `in_shardings_hlo` and `out_shardings_hlo`, respectively.",
      "The `Exported` object stores shardings as `HloShardings`, independent of a mesh.",
      "These methods construct `Sharding` objects usable with JAX APIs like `jax.jit` and `jax.device_put`."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating how to create shardings corresponding to self.in_shardings_hlo and use them with jax.device_put",
        "code": ">>> from jax import export\n>>> import jax\n>>> from jax import numpy as np\n>>> from jax import sharding\n\n>>> # Prepare the exported object:\n>>> exp_mesh = sharding.Mesh(jax.devices(), (\"a\",))\n>>> exp = export.export(\n...     jax.jit(\n...         lambda x: jax.numpy.add(x, x),\n...         in_shardings=sharding.NamedSharding(exp_mesh, sharding.PartitionSpec(\"a\"))\n...     )\n... )(\n...     np.arange(jax.device_count())\n... )\n>>> exp.in_shardings_hlo  # doctest: +ELLIPSIS\n({devices=[8]<=[8]},)\n>>> # Create a mesh for running the exported object\n>>> run_mesh = sharding.Mesh(jax.devices()[::-1], (\"b\",))\n>>> # Put the args and kwargs on the appropriate devices\n>>> run_arg = jax.device_put(\n...     np.arange(jax.device_count()),\n...     exp.in_shardings_jax(run_mesh)[0]\n... )\n>>> res = exp.call(run_arg)\n>>> res.addressable_shards  # doctest: +ELLIPSIS\n[Shard(device=CpuDevice(id=7), index=(slice(0, 1, None),), replica_id=0, data=[0]),\n Shard(device=CpuDevice(id=6), index=(slice(1, 2, None),), replica_id=0, data=[2]),\n Shard(device=CpuDevice(id=5), index=(slice(2, 3, None),), replica_id=0, data=[4]),\n Shard(device=CpuDevice(id=4), index=(slice(3, 4, None),), replica_id=0, data=[6]),\n Shard(device=CpuDevice(id=3), index=(slice(4, 5, None),), replica_id=0, data=[8]),\n Shard(device=CpuDevice(id=2), index=(slice(5, 6, None),), replica_id=0, data=[10]),\n Shard(device=CpuDevice(id=1), index=(slice(6, 7, None),), replica_id=0, data=[12]),\n Shard(device=CpuDevice(id=0), index=(slice(7, 8, None),), replica_id=0, data=[14])]\n"
      },
      {
        "description": "An identical example to the one above, included twice in the original documentation",
        "code": ">>> from jax import export\n>>> import jax\n>>> from jax import numpy as np\n>>> from jax import sharding\n\n>>> # Prepare the exported object:\n>>> exp_mesh = sharding.Mesh(jax.devices(), (\"a\",))\n>>> exp = export.export(\n...     jax.jit(\n...         lambda x: jax.numpy.add(x, x),\n...         in_shardings=sharding.NamedSharding(exp_mesh, sharding.PartitionSpec(\"a\"))\n...     )\n... )(\n...     np.arange(jax.device_count())\n... )\n>>> exp.in_shardings_hlo  # doctest: +ELLIPSIS\n({devices=[8]<=[8]},)\n>>> # Create a mesh for running the exported object\n>>> run_mesh = sharding.Mesh(jax.devices()[::-1], (\"b\",))\n>>> # Put the args and kwargs on the appropriate devices\n>>> run_arg = jax.device_put(\n...     np.arange(jax.device_count()),\n...     exp.in_shardings_jax(run_mesh)[0]\n... )\n>>> res = exp.call(run_arg)\n>>> res.addressable_shards  # doctest: +ELLIPSIS\n[Shard(device=CpuDevice(id=7), index=(slice(0, 1, None),), replica_id=0, data=[0]),\n Shard(device=CpuDevice(id=6), index=(slice(1, 2, None),), replica_id=0, data=[2]),\n Shard(device=CpuDevice(id=5), index=(slice(2, 3, None),), replica_id=0, data=[4]),\n Shard(device=CpuDevice(id=4), index=(slice(3, 4, None),), replica_id=0, data=[6]),\n Shard(device=CpuDevice(id=3), index=(slice(4, 5, None),), replica_id=0, data=[8]),\n Shard(device=CpuDevice(id=2), index=(slice(5, 6, None),), replica_id=0, data=[10]),\n Shard(device=CpuDevice(id=1), index=(slice(6, 7, None),), replica_id=0, data=[12]),\n Shard(device=CpuDevice(id=0), index=(slice(7, 8, None),), replica_id=0, data=[14])]\n"
      }
    ]
  },
  {
    "title": "Serialization and Deserialization",
    "concepts": [
      "Exported objects can be serialized using `serialize`.",
      "Serialization allows specifying the maximum VJP order to include.",
      "Exported objects can be deserialized using `deserialize`.",
      "Serialization and deserialization have minimum and maximum supported calling convention versions.",
      "Safety checks can be disabled during (de)serialization.",
      "Custom PyTree nodes and namedtuples can be registered for serialization.",
      "Custom call targets can be allowed for serialization."
    ],
    "code_examples": []
  },
  {
    "title": "Safety Checks",
    "concepts": [
      "Safety checks can be disabled during serialization and deserialization.",
      "Disabled checks can be specified by their string identifier.",
      "The compilation platform can be allowed to differ from the export platform during deserialization."
    ],
    "code_examples": []
  },
  {
    "title": "Export Function",
    "concepts": [
      "The `export` function is used to export a JAX function for persistent serialization.",
      "It takes a JAX function as input.",
      "Optional arguments include specifying the target platforms and disabled safety checks."
    ],
    "code_examples": []
  },
  {
    "title": "Helper Functions",
    "concepts": [
      "default_export_platform() retrieves the default export platform.",
      "is_symbolic_dim(p) Checks if a dimension is symbolic.",
      "SymbolicScope([constraints_str]) Identifies a scope for symbolic expressions."
    ],
    "code_examples": []
  },
  {
    "title": "Exporting JAX Functions",
    "concepts": [
      "The `jax.export` function allows for persistent serialization of JAX functions.",
      "The function to export should be the result of `jax.jit`.",
      "The exported code can be configured to run on multiple platforms (TPU, CPU, CUDA, ROCM).",
      "Safety checks can be disabled during the export process.",
      "The function takes args and kwargs pytrees of jax.ShapeDtypeStruct, or values with .shape and .dtype attributes, and returns an Exported object."
    ],
    "code_examples": [
      {
        "description": "Exporting a JAX function, serializing it, deserializing it, and calling the rehydrated function.",
        "code": "from jax import export\n\nexported: export.Exported = export.export(jnp.sin)(np.arange(4, dtype=np.float32))\n\n# You can inspect the Exported object\nexported.in_avals\n\nblob: bytearray = exported.serialize()\n\n# The serialized bytes are safe to use in a separate process\nrehydrated: export.Exported = export.deserialize(blob)\nrehydrated.fun_name\n\nrehydrated.call(np.array([.1, .2, .3, .4], dtype=np.float32))"
      },
      {
        "description": "Exporting a JAX function, serializing it, deserializing it, and calling the rehydrated function.",
        "code": "from jax import export\n\nexported: export.Exported = export.export(jnp.sin)(np.arange(4, dtype=np.float32))\n\n# You can inspect the Exported object\nexported.in_avals\n\nblob: bytearray = exported.serialize()\n\n# The serialized bytes are safe to use in a separate process\nrehydrated: export.Exported = export.deserialize(blob)\nrehydrated.fun_name\n\nrehydrated.call(np.array([.1, .2, .3, .4], dtype=np.float32))"
      }
    ]
  },
  {
    "title": "Deserialization of Exported Data",
    "concepts": [
      "Deserialization converts a bytearray into an object.",
      "The bytearray originates from the Exported.serialize method."
    ],
    "code_examples": []
  },
  {
    "title": "Integer Conversion",
    "concepts": [
      "The `int()` function converts a number or string to an integer.",
      "If no argument is given, `int()` returns 0.",
      "For floating-point numbers, `int()` truncates towards zero.",
      "If a base is provided, the input must be a string representing an integer literal in that base.",
      "The string can be preceded by '+' or '-' and be surrounded by whitespace.",
      "Valid bases are 0 and 2-36.",
      "Base 0 means to interpret the base from the string as an integer literal."
    ],
    "code_examples": [
      {
        "description": "Example of converting a binary string to an integer using base 0.",
        "code": "int('0b100', base=0)"
      }
    ]
  },
  {
    "title": "Integer Conversion",
    "concepts": [
      "The `int()` function converts a number or string to an integer.",
      "If no arguments are given, `int()` returns 0.",
      "For floating-point numbers, `int()` truncates towards zero.",
      "If a base is given, the input must be a string representing an integer literal.",
      "The string can be preceded by '+' or '-' and surrounded by whitespace.",
      "Valid bases are 0 and 2-36.",
      "Base 0 means to interpret the base from the string as an integer literal."
    ],
    "code_examples": [
      {
        "description": "Example of converting a binary string to an integer using base 0 to auto-detect the base.",
        "code": "int('0b100', base=0)"
      }
    ]
  },
  {
    "title": "Default Export Platform",
    "concepts": [
      "Retrieves the default export platform.",
      "Possible platforms are tpu, cpu, cuda, and rocm.",
      "The platform is represented as a string."
    ],
    "code_examples": []
  },
  {
    "title": "Registering namedtuple for Serialization",
    "concepts": [
      "JAX natively supports PyTree for collections.namedtuple.",
      "Registration is required for serializing functions with namedtuple inputs/outputs.",
      "nodetype is the type to serialize as a PyTree node.",
      "Registering multiple serializations for the same nodetype is an error.",
      "During deserialization, the type must have the same keys as during serialization.",
      "serialized_name is a string used for lookup during deserialization.",
      "Registering multiple serializations for the same serialized_name is an error.",
      "The function can be used as a class decorator."
    ],
    "code_examples": []
  },
  {
    "title": "Symbolic Shape Construction",
    "concepts": [
      "Constructs a symbolic shape from a string representation.",
      "Shape specification is a string representation of a tuple with comma-separated dimension expressions.",
      "Dimension expressions can be integer constants, dimension variables, or arithmetic expressions.",
      "Constraints are specified as a sequence of strings.",
      "SymbolicScope can be used to parse expressions in a specific scope.",
      "The `like` parameter fills placeholders in shape_spec with dimensions from a provided shape.",
      "Returns a tuple with integers or symbolic dimension expressions."
    ],
    "code_examples": []
  },
  {
    "title": "Constructing Pytrees of jax.ShapeDtypeSpec",
    "concepts": [
      "Creates a pytree of jax.ShapeDtypeSpec for export.",
      "Utilizes jax.export.symbolic_shape() for shape polymorphism.",
      "Arguments can be jax.Array or jax.ShapeDtypeSpec.",
      "Uses arguments to learn the pytree structure and dtypes.",
      "Fills in shapes from arguments where shape_specs contains placeholders.",
      "Only shape dimensions from args are used if shapes_specs is a placeholder.",
      "shapes_specs can be None, a single string, or a pytree matching a prefix of args.",
      "Constraints are used as in jax.export.symbolic_shape().",
      "Scope is used as in jax.export.symbolic_shape()."
    ],
    "code_examples": []
  },
  {
    "title": "Symbolic Dimension Check",
    "concepts": [
      "Checks if a dimension is symbolic.",
      "The input is a DimSize.",
      "The output is a boolean value."
    ],
    "code_examples": []
  },
  {
    "title": "Symbolic Expression Scope",
    "concepts": [
      "Defines a scope for symbolic expressions.",
      "Symbolic expressions that interact must be from the same scope.",
      "Symbolic expressions must share the same SymbolicScope object.",
      "Holds constraints on symbolic expressions.",
      "Constraints can be specified as a sequence of strings."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Mini-Libraries",
    "concepts": [
      "JAX provides small, experimental libraries for machine learning.",
      "These libraries serve as tools and examples for building libraries using JAX.",
      "Each library is small (<300 lines of code) and adaptable.",
      "Mini-libraries are meant to be an inspiration, not a prescription.",
      "Code samples are kept minimal to encourage adaptation and innovation.",
      "Feature requests should be directed to more fully-featured libraries like Haiku or Flax."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Optimizers",
    "concepts": [
      "This module provides example optimizer definitions for JAX.",
      "It is not intended for production use; Optax is recommended for a full-featured optimizer library.",
      "Optimizers are defined as a triple of functions: init_fun, update_fun, and get_params."
    ],
    "code_examples": []
  },
  {
    "title": "Optimizer Functions",
    "concepts": [
      "init_fun initializes the optimizer state from the parameters.",
      "update_fun updates the optimizer state based on the step, gradients, and current state.",
      "get_params extracts the parameters from the optimizer state.",
      "The optimizer state can be any pytree of JaxTypes.",
      "The optimizer state must be consumable by update_fun and get_params."
    ],
    "code_examples": []
  },
  {
    "title": "Optimizer Function Signatures",
    "concepts": [
      "init_fun(params) -> opt_state: Initializes optimizer state.",
      "update_fun(step, grads, opt_state) -> opt_state: Updates optimizer state.",
      "get_params(opt_state) -> params: Extracts parameters from optimizer state.",
      "params == get_params(init_fun(params)) is an invariant."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage",
    "concepts": [
      "This section shows example usage of the optimizers.",
      "The optimizers are called with the loss function and the parameters."
    ],
    "code_examples": [
      {
        "description": "Example usage of an optimizer. This code shows how to initialize the optimizer, define a step function, and use it in a loop.",
        "code": "opt_init\n,\nopt_update\n,\nget_params\n=\noptimizers\n.\nsgd\n(\nlearning_rate\n)\nopt_state\n=\nopt_init\n(\nparams\n)\ndef\nstep\n(\nstep\n,\nopt_state\n):\nvalue\n,\ngrads\n=\njax\n.\nvalue_and_grad\n(\nloss_fn\n)(\nget_params\n(\nopt_state\n))\nopt_state\n=\nopt_update\n(\nstep\n,\ngrads\n,\nopt_state\n)\nreturn\nvalue\n,\nopt_state\nfor\ni\nin\nrange\n(\nnum_steps\n):\nvalue\n,\nopt_state\n=\nstep\n(\ni\n,\nopt_state\n)"
      },
      {
        "description": "Example usage of an optimizer. This code shows how to initialize the optimizer, define a step function, and use it in a loop.",
        "code": "opt_init\n,\nopt_update\n,\nget_params\n=\noptimizers\n.\nsgd\n(\nlearning_rate\n)\nopt_state\n=\nopt_init\n(\nparams\n)\ndef\nstep\n(\nstep\n,\nopt_state\n):\nvalue\n,\ngrads\n=\njax\n.\nvalue_and_grad\n(\nloss_fn\n)(\nget_params\n(\nopt_state\n))\nopt_state\n=\nopt_update\n(\nstep\n,\ngrads\n,\nopt_state\n)\nreturn\nvalue\n,\nopt_state\nfor\ni\nin\nrange\n(\nnum_steps\n):\nvalue\n,\nopt_state\n=\nstep\n(\ni\n,\nopt_state\n)"
      }
    ]
  },
  {
    "title": "Boundary Class",
    "concepts": [
      "Marks the boundary between two joined (nested) pytrees."
    ],
    "code_examples": []
  },
  {
    "title": "Optimizer NamedTuple",
    "concepts": [
      "NamedTuple that holds init_fn, update_fn, and params_fn",
      "Represents an optimizer triple."
    ],
    "code_examples": []
  },
  {
    "title": "Adagrad Optimizer",
    "concepts": [
      "Constructs an optimizer triple for Adagrad.",
      "Implements Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.",
      "Uses step_size and optional momentum."
    ],
    "code_examples": []
  },
  {
    "title": "Adam Optimizer",
    "concepts": [
      "Constructs an optimizer triple for Adam.",
      "Uses step_size, b1 (beta_1), b2 (beta_2), and eps (epsilon) parameters."
    ],
    "code_examples": []
  },
  {
    "title": "AdaMax Optimizer",
    "concepts": [
      "Constructs an optimizer triple for AdaMax (a variant of Adam based on infinity norm).",
      "Uses step_size, b1 (beta_1), b2 (beta_2), and eps (epsilon) parameters."
    ],
    "code_examples": []
  },
  {
    "title": "Gradient Clipping",
    "concepts": [
      "Clips gradients stored as a pytree of arrays to maximum norm max_norm."
    ],
    "code_examples": []
  },
  {
    "title": "L2 Norm Computation",
    "concepts": [
      "Computes the l2 norm of a pytree of arrays, useful for weight decay."
    ],
    "code_examples": []
  },
  {
    "title": "SGD with Momentum Optimizer",
    "concepts": [
      "Constructs an optimizer triple for SGD with momentum.",
      "Uses step_size and mass (momentum coefficient) parameters."
    ],
    "code_examples": []
  },
  {
    "title": "SGD with Nesterov Momentum Optimizer",
    "concepts": [
      "Constructs an optimizer triple for SGD with Nesterov momentum.",
      "Uses step_size and mass (momentum coefficient) parameters."
    ],
    "code_examples": []
  },
  {
    "title": "Generalization Decorator",
    "concepts": [
      "Decorator to make an optimizer defined for arrays generalize to containers.",
      "Allows writing init, update, and get_params functions that operate on single arrays and converts them to corresponding functions that operate on pytrees of parameters."
    ],
    "code_examples": []
  },
  {
    "title": "RMSProp Optimizer",
    "concepts": [
      "Constructs an optimizer triple for RMSProp.",
      "Uses step_size, gamma (decay), and eps (epsilon) parameters."
    ],
    "code_examples": []
  },
  {
    "title": "RMSProp with Momentum Optimizer",
    "concepts": [
      "Constructs an optimizer triple for RMSProp with momentum.",
      "Uses step_size, gamma (decay), eps (epsilon), and momentum parameters."
    ],
    "code_examples": []
  },
  {
    "title": "SGD Optimizer",
    "concepts": [
      "Constructs an optimizer triple for stochastic gradient descent.",
      "Uses step_size parameter."
    ],
    "code_examples": []
  },
  {
    "title": "SM3 Optimizer",
    "concepts": [
      "Constructs an optimizer triple for SM3.",
      "Memory-Efficient Adaptive Optimization for Large-Scale Learning.",
      "Uses step_size and optional momentum."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Stax",
    "concepts": [
      "Stax is a small, flexible neural network specification library built from scratch.",
      "Stax is intended as an example library only.",
      "Flax and Haiku are more fully-featured neural network libraries for JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Layer Construction Functions",
    "concepts": [
      "Layer construction function for a pooling layer.",
      "Layer construction function for a batch normalization layer.",
      "Layer construction function for a general convolution layer.",
      "Layer construction function for a general transposed-convolution layer.",
      "Layer constructor function for a dense (fully-connected) layer.",
      "Layer construction function for a dropout layer with given rate.",
      "Layer construction function for a fan-in concatenation layer.",
      "Layer construction function for a fan-out layer.",
      "Layer construction function for a layer that applies a scalar function elementwise on its inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Parallel Layer Combinator (Serial)",
    "concepts": [
      "Combinator for composing layers in parallel.",
      "Resulting layer is often used with FanOut and FanInSum layers.",
      "Takes a sequence of layers as input.",
      "Returns a layer that applies the layers in parallel to a sequence of inputs.",
      "The returned layer takes a sequence of inputs and returns a sequence of outputs with the same length as the argument layers."
    ],
    "code_examples": []
  },
  {
    "title": "Serial Layer Combinator (Serial)",
    "concepts": [
      "Combinator for composing layers in serial.",
      "Takes a sequence of layers as input.",
      "Returns a layer that applies the layers in sequence."
    ],
    "code_examples": []
  },
  {
    "title": "Delayed Layer Constructor",
    "concepts": [
      "Combinator to delay layer constructor pair until input shapes are known.",
      "Takes a one-argument function `make_layer` that accepts an input shape and returns an (init_fun, apply_fun) pair.",
      "Returns a layer with construction delayed until input shapes are known."
    ],
    "code_examples": []
  },
  {
    "title": "Package and Module Moves",
    "concepts": [
      "jax.experimental.optix has been moved to the optax package.",
      "jax.experimental.ann has been moved to jax.lax."
    ],
    "code_examples": []
  },
  {
    "title": "X64 Mode Context Managers",
    "concepts": [
      "enable_x64 is an experimental context manager to enable X64 mode temporarily.",
      "disable_x64 is an experimental context manager to disable X64 mode temporarily."
    ],
    "code_examples": []
  },
  {
    "title": "Checkify Function",
    "concepts": [
      "The `checkify` function functionalizes check calls in a given function.",
      "It optionally adds runtime error checks."
    ],
    "code_examples": []
  },
  {
    "title": "Check Function",
    "concepts": [
      "The `check` function evaluates a predicate.",
      "If the predicate is False, it adds an error message.",
      "It allows for formatted error messages via `fmt_args`.",
      "It supports debugging."
    ],
    "code_examples": []
  },
  {
    "title": "Check Error Function",
    "concepts": [
      "The `check_error` function raises an Exception if a given error represents a failure."
    ],
    "code_examples": []
  },
  {
    "title": "Error Class",
    "concepts": [
      "The `Error` class represents an error with a predicate, code, metadata, and payload."
    ],
    "code_examples": []
  },
  {
    "title": "JaxRuntimeError",
    "concepts": [
      "JaxRuntimeError is likely a type of exception related to JAX runtime failures."
    ],
    "code_examples": []
  },
  {
    "title": "Check Sets",
    "concepts": [
      "Various frozensets containing different types of checks are defined.",
      "`user_checks`: Likely contains user-defined checks.",
      "`nan_checks`: Likely contains checks for NaN (Not a Number) values.",
      "`index_checks`: Likely contains checks related to array indices.",
      "`div_checks`: Likely contains checks for division by zero.",
      "`float_checks`: Likely contains checks for floating-point numbers.",
      "`automatic_checks`: Likely contains automatically generated checks.",
      "`all_checks`: Likely contains a combination of all available checks."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of checkify.check",
    "concepts": [
      "The check function validates a predicate and adds an error if it's false.",
      "It is an effectful operation and cannot be staged directly.",
      "Functions with checks should be preprocessed with checkify() before staging.",
      "The `debug` argument controls whether checks are removed during execution or functionalized using checkify.checkify.",
      "The `fmt_args` and `fmt_kwargs` arguments allow format strings with run-time values in error messages, potentially increasing memory usage."
    ],
    "code_examples": [
      {
        "description": "Example usage of checkify.check to ensure a value is positive, including error handling.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.experimental import checkify\n\ndef f(x):\n    checkify.check(x > 0, \"{x} needs to be positive!\", x=x)\n    return 1 / x\n\nchecked_f = checkify.checkify(f)\nerr, out = jax.jit(checked_f)(-3.)\nerr.throw()"
      },
      {
        "description": "Example usage of checkify.check to ensure a value is positive, including error handling.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.experimental import checkify\n\ndef f(x):\n    checkify.check(x > 0, \"{x} needs to be positive!\", x=x)\n    return 1 / x\n\nchecked_f = checkify.checkify(f)\nerr, out = jax.jit(checked_f)(-3.)\nerr.throw()"
      }
    ]
  },
  {
    "title": "Introduction to check_error",
    "concepts": [
      "check_error raises an exception if an error represents a failure.",
      "check_error is the inverse operation of checkify.",
      "check_error can turn Error values back into Python Exceptions.",
      "check_error takes an Error value as input and raises an exception.",
      "check_error is useful to re-inject error value outside jit()."
    ],
    "code_examples": [
      {
        "description": "Illustrates the basic semantics of check_error, showing that it throws an exception based on the input error.",
        "code": "def check_error(err: Error) -> None:\n  ...  \n  err.throw()\n  # can raise ValueError"
      }
    ]
  },
  {
    "title": "Usage Example with JIT and checkify",
    "concepts": [
      "This example demonstrates how check_error is used in conjunction with checkify and JIT compilation.",
      "checkify is used to functionalize checks and produce Error values.",
      "jax.jit is used to stage out the functionalized code.",
      "check_error is used to re-inject the error value outside the JIT-compiled region and raise a Python Exception if necessary."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use `check_error` after staging out code with `jax.jit` and functionalizing checks with `checkify`.",
        "code": "import jax\nfrom jax.experimental import checkify\n\ndef f(x):\n  checkify.check(x > 0, \"must be positive!\")\n  return x\n\ndef with_inner_jit(x):\n  checked_f = checkify.checkify(f)\n  # a checkified function can be jitted\n  error, out = jax.jit(checked_f)(x)\n  checkify.check_error(error)\n  return out\n\n_ = with_inner_jit(1) # no failed check\n\nwith_inner_jit(-1)\n#Traceback (most recent call last):\n#... jax._src.JaxRuntimeError: must be positive!\n\n# can re-checkify\nerror, _ = checkify.checkify(with_inner_jit)(-1)"
      },
      {
        "description": "Duplicate version of the previous example to fix some formatting issues, but otherwise contains identical code and functionality.",
        "code": "import jax\nfrom jax.experimental import checkify\n\ndef f(x):\n  checkify.check(x > 0, \"must be positive!\")\n  return x\n\ndef with_inner_jit(x):\n  checked_f = checkify.checkify(f)\n  # a checkified function can be jitted\n  error, out = jax.jit(checked_f)(x)\n  checkify.check_error(error)\n  return out\n\n_ = with_inner_jit(1) # no failed check\n\nwith_inner_jit(-1)\n#Traceback (most recent call last):\n#... jax._src.JaxRuntimeError: must be positive!\n\n# can re-checkify\nerror, _ = checkify.checkify(with_inner_jit)(-1)"
      }
    ]
  },
  {
    "title": "Attributes",
    "concepts": [
      "The object has a boolean predicate for each ErrorEffect.",
      "The object has an integer code for each ErrorEffect.",
      "The object has metadata for each integer.",
      "The object has a payload for each ErrorEffect."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "The object has an __init__ method to initialize its attributes.",
      "The object has a get() method to return an error message or None.",
      "The object has a get_exception() method to return a Python exception or None.",
      "The object has a throw() method.",
      "The object has a tree_flatten() method.",
      "The object has a tree_unflatten() method."
    ],
    "code_examples": []
  },
  {
    "title": "Frozenset Definition and Usage",
    "concepts": [
      "frozenset() creates an empty frozenset object.",
      "frozenset(iterable) creates a frozenset object from an iterable.",
      "frozenset is an immutable unordered collection of unique elements."
    ],
    "code_examples": []
  },
  {
    "title": "Frozenset Definition",
    "concepts": [
      "frozenset() creates an empty frozenset object.",
      "frozenset(iterable) creates a frozenset object from an iterable.",
      "Frozensets are immutable, unordered collections of unique elements."
    ],
    "code_examples": []
  },
  {
    "title": "Frozenset Definition and Usage",
    "concepts": [
      "frozenset() creates an empty frozenset object.",
      "frozenset(iterable) creates a frozenset object from an iterable.",
      "Frozensets are immutable unordered collections of unique elements."
    ],
    "code_examples": []
  },
  {
    "title": "Frozenset Definition",
    "concepts": [
      "Creates an empty frozenset object when called without arguments.",
      "Creates a frozenset object from an iterable.",
      "Frozensets are immutable unordered collections of unique elements."
    ],
    "code_examples": []
  },
  {
    "title": "Frozenset Definition and Usage",
    "concepts": [
      "frozenset() creates an empty frozenset object.",
      "frozenset(iterable) creates a frozenset object from an iterable.",
      "Frozensets are immutable unordered collections of unique elements."
    ],
    "code_examples": []
  },
  {
    "title": "Frozenset Definition",
    "concepts": [
      "frozenset() creates an empty frozenset object.",
      "frozenset(iterable) creates a frozenset object from an iterable.",
      "A frozenset is an immutable unordered collection of unique elements."
    ],
    "code_examples": []
  },
  {
    "title": "Frozenset Definition and Usage",
    "concepts": [
      "frozenset() creates an empty frozenset object.",
      "frozenset(iterable) creates a frozenset object from an iterable.",
      "A frozenset is an immutable unordered collection of unique elements."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Disk Compilation Cache Status",
    "concepts": [
      "This API is deprecated.",
      "Check if the cache is enabled.",
      "Initialization can be deferred, so initialized status is not checked.",
      "The name is retained for backwards compatibility."
    ],
    "code_examples": []
  },
  {
    "title": "Setting the Cache Path (Deprecated)",
    "concepts": [
      "This API is deprecated; use set_cache_dir instead.",
      "Set the path for the compilation cache.",
      "The cache path must be set before calling get_executable_and_time() or put_executable_and_time()."
    ],
    "code_examples": []
  },
  {
    "title": "Setting the Persistent Compilation Cache Directory",
    "concepts": [
      "Sets the persistent compilation cache directory.",
      "Jit-compiled functions are saved to the specified path.",
      "Cached functions do not need to be recompiled on restart.",
      "Jax also looks for compiled functions in this directory before compiling."
    ],
    "code_examples": []
  },
  {
    "title": "Resetting to Uninitialized State",
    "concepts": [
      "Get back to pristine, uninitialized state."
    ],
    "code_examples": []
  },
  {
    "title": "Custom DCE",
    "concepts": [
      "Customize the DCE (Dead Code Elimination) behavior of a JAX-transformable function.",
      "The function takes the function to be customized, and optionally static argument numbers as input."
    ],
    "code_examples": []
  },
  {
    "title": "Custom DCE Rule Definition",
    "concepts": [
      "Define a custom DCE rule for a function customized with custom_dce.",
      "It uses custom_dce.def_dce to associate a specific DCE rule (dce_rule) with the custom DCE configuration."
    ],
    "code_examples": []
  },
  {
    "title": "Custom DCE Rule Definition",
    "concepts": [
      "A custom Dead Code Elimination (DCE) rule can be defined for a function.",
      "The `dce_rule` is a callable function.",
      "The `dce_rule` takes static arguments, a Pytree of boolean values indicating used outputs, and the remaining non-static arguments.",
      "The `dce_rule` should return a Pytree with the same structure as the original function's output.",
      "Unused outputs in the Pytree, as indicated by `used_outs`, should be replaced with `None`."
    ],
    "code_examples": []
  },
  {
    "title": "Defining Custom Partitioning Rules",
    "concepts": [
      "The `@custom_partitioning` decorator allows inserting a CustomCallOp into the XLA graph with custom SPMD lowering rules.",
      "The `propagate_user_sharding` function allows updating the sharding of the op from a user's shape.sharding.",
      "The `partition` function returns the mesh, a per-shard lowering function, and the final input and output sharding specs.",
      "The `infer_sharding_from_operands` function computes an output NamedSharding from the NamedSharding chosen for each argument.",
      "The `def_partition` function is used to define the partitioning rules for the custom partitioned function.",
      "The `sharding_rule` argument in `def_partition` allows specifying the sharding rule using an Einsum-like notation string or an SdyShardingRule object.",
      "When `config.use_shardy_partitioner.value` is True, `sharding_rule` is used; otherwise, `propagate_user_sharding` and `infer_sharding_from_operands` are used."
    ],
    "code_examples": [
      {
        "description": "Defines a function f with custom partitioning rules.",
        "code": "@custom_partitioning\ndef f(*args):\n    return ...\n\ndef propagate_user_sharding(mesh, user_shape):\n    '''Update the sharding of the op from a user's shape.sharding.'''\n    user_sharding = jax.tree.map(lambda x: x.sharding, user_shape)\n\ndef partition(mesh, arg_shapes, result_shape):\n    def lower_fn(*args):\n        ... builds computation on per-device shapes ...\n\n    result_shardings = jax.tree.map(lambda x: x.sharding, result_shape)\n    arg_shardings = jax.tree.map(lambda x: x.sharding, arg_shapes)\n    # result_sharding and arg_shardings may optionally be modified and the\n    # partitioner will insert collectives to reshape.\n    return mesh, lower_fn, result_sharding, arg_shardings\n\ndef infer_sharding_from_operands(mesh, arg_shapes, shape):\n    '''Compute the result sharding from the sharding of the operands.'''\n    arg_shardings = jax.tree.map(lambda x: x.sharding, arg_shapes)\n\nf.def_partition(\n    partition,\n    propagate_user_sharding,\n    infer_sharding_from_operands=infer_sharding_from_operands,\n    sharding_rule='i j -> i j'\n)"
      }
    ]
  },
  {
    "title": "Example: Custom Partitioning for jax.numpy.fft.fft",
    "concepts": [
      "This section demonstrates how to enhance `jax.numpy.fft.fft` using custom partitioning.",
      "The goal is to avoid unnecessary gathering of the input on all devices by preserving the sharding along the first N-1 dimensions for an N-D input.",
      "The `supported_sharding` function creates a NamedSharding that keeps sharding along the first N-1 dimensions and replicates along the last dimension.",
      "The `partition` function uses `supported_sharding` to specify the input and output shardings for the FFT operation.",
      "The `infer_sharding_from_operands` function infers the result sharding from the operand sharding.",
      "The `my_fft` function is defined using `@custom_partitioning` and its partitioning rules are defined using `def_partition`."
    ],
    "code_examples": [
      {
        "description": "Defines a custom partitioned version of jax.numpy.fft.fft that preserves sharding along the first N-1 dimensions.",
        "code": "import jax\nfrom jax.sharding import NamedSharding\nfrom jax.experimental.custom_partitioning import custom_partitioning\nfrom jax.experimental.pjit import pjit\nfrom jax.sharding import PartitionSpec as P\nfrom jax.sharding import Mesh\nfrom jax.numpy.fft import fft\nimport regex as re\nimport numpy as np\n\n# Pattern to detect all-gather or dynamic-slice in the generated HLO\n_PATTERN = '(dynamic-slice|all-gather)'\n\n# For an N-D input, keeps sharding along the first N-1 dimensions\n# but replicate along the last dimension\ndef supported_sharding(sharding, shape):\n    rank = len(shape.shape)\n    max_shared_dims = min(len(sharding.spec), rank - 1)\n    names = tuple(sharding.spec[:max_shared_dims]) + tuple(None for _ in range(rank - max_shared_dims))\n    return NamedSharding(sharding.mesh, P(*names))\n\ndef partition(mesh, arg_shapes, result_shape):\n    result_shardings = jax.tree.map(lambda x: x.sharding, result_shape)\n    arg_shardings = jax.tree.map(lambda x: x.sharding, arg_shapes)\n    return mesh, fft, supported_sharding(arg_shardings[0], arg_shapes[0]), (supported_sharding(arg_shardings[0], arg_shapes[0]),)\n\ndef infer_sharding_from_operands(mesh, arg_shapes, result_shape):\n    arg_shardings = jax.tree.map(lambda x: x.sharding, arg_shapes)\n    return supported_sharding(arg_shardings[0], arg_shapes[0])\n\n@custom_partitioning\ndef my_fft(x):\n    return fft(x)\n\n# Use Einsum-like notation to specify the sharding rule.\nmy_fft.def_partition(\n    infer_sharding_from_operands=infer_sharding_from_operands,\n    partition=partition,\n    sharding_rule='...i -> ...i'\n)\n\n# Use SdyShardingRule object to specify the sharding rule.\nmy_fft.def_partition(\n    infer_sharding_from_operands=infer_sharding_from_operands,\n    partition=partition,\n    sharding_rule=SdyShardingRule(\n        operand_mappings=((SDY_BATCHING, 'i'),),\n        result_mappings=((SDY_BATCHING, 'i'),)))\n"
      },
      {
        "description": "Example of using my_fft with pjit and comparing the generated HLO with jax.numpy.fft.fft for a 2D array.",
        "code": "with Mesh(np.array(jax.devices()), ('x',)):\n    x = np.asarray(np.random.randn(32 * 1024, 1024), dtype=np.complex64)\n    y = pjit(lambda x: x, in_shardings=None, out_shardings=P('x'))(x)\n    pjit_my_fft = pjit(my_fft, in_shardings=P('x'), out_shardings=P('x'))\n    pjit_fft = pjit(fft, in_shardings=P('x'), out_shardings=P('x'))\n\n    print(pjit_my_fft(y))\n    print(pjit_fft(y))\n\n    # dynamic-slice or all-gather are not present in the HLO for my_fft, because x is a 2D array\n    assert (re.search(_PATTERN, pjit_my_fft.lower(x).compile().runtime_executable().hlo_modules()[0].to_string()) is None)\n    # dynamic-slice or all-gather are present in the HLO for fft\n    assert (re.search(_PATTERN, pjit_fft.lower(x).compile().runtime_executable().hlo_modules()[0].to_string()) is not None)"
      },
      {
        "description": "Example of using my_fft with pjit and comparing the generated HLO with jax.numpy.fft.fft for a 1D array.",
        "code": "with Mesh(np.array(jax.devices()), ('x',)):\n    x = np.asarray(np.random.randn(32 * 1024 * 1024), dtype=np.complex64)\n    y = pjit(lambda x: x, in_shardings=None, out_shardings=P('x'))(x)\n    pjit_my_fft = pjit(my_fft, in_shardings=P('x'), out_shardings=P('x'))\n    pjit_fft = pjit(fft, in_shardings=P('x'), out_shardings=P('x'))\n\n    print(pjit_my_fft(y))\n    print(pjit_fft(y))\n\n    # dynamic-slice or all-gather are present in the HLO for my_fft, because x is a 1D array\n    assert (re.search(_PATTERN, pjit_my_fft.lower(x).compile().runtime_executable().hlo_modules()[0].to_string()) is None)\n    # dynamic-slice or all-gather are present in the HLO for fft\n    assert (re.search(_PATTERN, pjit_fft.lower(x).compile().runtime_executable().hlo_modules()[0].to_string()) is not None)"
      }
    ]
  },
  {
    "title": "Introduction to Jet and Higher-Order Automatic Differentiation",
    "concepts": [
      "Jet is an experimental module for higher-order automatic differentiation.",
      "It propagates truncated Taylor polynomials instead of relying on repeated first-order automatic differentiation.",
      "First-order automatic differentiation computes (f(x), \u2202f(x)[v]) from (h(x), \u2202h(x)[v]).",
      "Jet computes (f_0, ..., f_K) from (h_0, ..., h_K), where these represent K-th order Taylor approximations.",
      "jet() computes f_0, (f_1, . . . , f_K) = jet(f, h_0, (h_1, . . . , h_K)) for high-order automatic differentiation."
    ],
    "code_examples": []
  },
  {
    "title": "Jet Function Definition and Usage",
    "concepts": [
      "The jet function performs Taylor-mode higher-order automatic differentiation.",
      "fun is the function to be differentiated.",
      "primals are the primal values at which the Taylor approximation is evaluated.",
      "series are the higher order Taylor-series-coefficients.",
      "primals and series together form a truncated Taylor polynomial.",
      "The function returns a (primals_out, series_out) pair representing the truncated Taylor polynomial of f(h(.)).",
      "primals_out has the same Python tree structure as primals and series_out has the same structure as series."
    ],
    "code_examples": [
      {
        "description": "Illustrates how to use the jet function with a simple example involving sine and cubing functions.",
        "code": "import jax\nimport jax.numpy as np\nimport jax\nimport jax.numpy as np\n\nh0, h1, h2 = 0.5**3., 3. * 0.5**2., 6. * 0.5\nf, df, ddf = np.sin, np.cos, lambda *args: -np.sin(*args)\n\nf0, (f1, f2) = jet(f, (h0,), ((h1, h2),))\nprint(f0, f(h0))\n\nf0, (f1, f2) = jet(f, (h0,), ((h1, h2),))\nprint(f0, f(h0))\n\nprint(f1, df(h0) * h1)\n\nprint(f1, df(h0) * h1)\n\nprint(f2, ddf(h0) * h1**2 + df(h0) * h2)\n\nprint(f2, ddf(h0) * h1**2 + df(h0) * h2)"
      }
    ]
  },
  {
    "title": "Introduction to Key Reuse Detection in JAX",
    "concepts": [
      "This module provides experimental functionality for detecting reuse of random keys in JAX programs.",
      "It is under active development and the APIs are subject to change.",
      "Requires JAX version 0.4.26 or newer."
    ],
    "code_examples": []
  },
  {
    "title": "Enabling Key Reuse Checking",
    "concepts": [
      "Key reuse checking can be enabled using the `jax_debug_key_reuse` configuration.",
      "It can be enabled globally using `jax.config.update('jax_debug_key_reuse', True)`.",
      "It can be enabled locally using the `jax.debug_key_reuse()` context manager."
    ],
    "code_examples": [
      {
        "description": "Enabling key reuse checking globally.",
        "code": "jax.config.update('jax_debug_key_reuse', True)"
      },
      {
        "description": "Enabling key reuse checking locally using a context manager.",
        "code": "import jax\n\nwith jax.debug_key_reuse(True):\n  pass"
      }
    ]
  },
  {
    "title": "KeyReuseError Demonstration",
    "concepts": [
      "When key reuse checking is enabled, using the same key twice will result in a `KeyReuseError`."
    ],
    "code_examples": [
      {
        "description": "Demonstrates a KeyReuseError when the same key is used twice within a debug_key_reuse context.",
        "code": "import jax\n\nwith jax.debug_key_reuse(True):\n  key = jax.random.key(0)\n  val1 = jax.random.normal(key)\n  val2 = jax.random.normal(key)"
      },
      {
        "description": "Demonstrates a KeyReuseError when the same key is used twice within a debug_key_reuse context.",
        "code": "import jax\n\nwith jax.debug_key_reuse(True):\n  key = jax.random.key(0)\n  val1 = jax.random.normal(key)\n  val2 = jax.random.normal(key)"
      }
    ]
  },
  {
    "title": "Future of Key Reuse Checking",
    "concepts": [
      "The key reuse checker is currently experimental.",
      "It is likely to be enabled by default in the future."
    ],
    "code_examples": []
  },
  {
    "title": "Device Mesh Utilities",
    "concepts": [
      "Utilities for building device meshes are provided.",
      "The `create_device_mesh` function creates a performant device mesh for jax.sharding.Mesh.",
      "The `create_hybrid_device_mesh` function creates a device mesh for hybrid parallelism (e.g., ICI and DCN)."
    ],
    "code_examples": []
  },
  {
    "title": "Device Mesh Creation",
    "concepts": [
      "Creates a performant device mesh for jax.sharding.Mesh.",
      "mesh_shape defines the shape of the logical mesh, ordered by increasing network-intensity.",
      "devices specifies the devices to construct the mesh for, defaulting to jax.devices().",
      "contiguous_submeshes attempts to create a mesh where each process\u2019s local devices form a contiguous submesh.",
      "allow_split_physical_axes splits physical axes if necessary to produce the desired device mesh.",
      "ValueError is raised if the number of devices doesn\u2019t equal the product of mesh_shape."
    ],
    "code_examples": []
  },
  {
    "title": "Device Mesh Creation for Hybrid Parallelism",
    "concepts": [
      "Creates a device mesh for hybrid parallelism (e.g., ICI and DCN).",
      "The `mesh_shape` parameter defines the shape of the logical mesh for the faster/inner network.",
      "The `dcn_mesh_shape` parameter defines the shape of the logical mesh for the slower/outer network.",
      "The order of dimensions in `mesh_shape` and `dcn_mesh_shape` is based on increasing network intensity.",
      "The `devices` parameter allows specifying the devices to construct the mesh for; defaults to `jax.devices()`.",
      "The `process_is_granule` parameter determines whether processes or device slices are used as units of the slower/outer network.",
      "Enabling `process_is_granule` is a fallback for platforms lacking `slice_index`.",
      "The `should_sort_granules_by_key` parameter controls sorting of device granules by their key (slice or process index).",
      "The `allow_split_physical_axes` parameter controls the splitting of physical axes to match the desired device mesh.",
      "A ValueError is raised if the number of slices or devices per slice does not match the product of the corresponding mesh shapes.",
      "Returns a NumPy array of JAX devices with the combined shape `mesh_shape * dcn_mesh_shape` suitable for `jax.sharding.Mesh`."
    ],
    "code_examples": []
  },
  {
    "title": "Synchronization and Communication Utilities",
    "concepts": [
      "Utilities for synchronizing and communicating across multiple hosts are provided.",
      "Data can be broadcast from a source host to all other hosts.",
      "A barrier can be created across all hosts/devices.",
      "Data can be gathered from across processes.",
      "Verification that all hosts have the same tree of values is possible.",
      "Host local values can be converted to globally sharded jax.Arrays.",
      "Global jax.Arrays can be converted to host local jax.Arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Function Summaries",
    "concepts": [
      "broadcast_one_to_all broadcasts data from a source host to all other hosts.",
      "sync_global_devices creates a barrier across all hosts/devices.",
      "process_allgather gathers data from across processes.",
      "assert_equal verifies that all the hosts have the same tree of values.",
      "host_local_array_to_global_array converts a host local value to a globally sharded jax.Array.",
      "global_array_to_host_local_array converts a global jax.Array to a host local jax.Array."
    ],
    "code_examples": []
  },
  {
    "title": "Broadcast Data",
    "concepts": [
      "Broadcast data from a source host to all other hosts.",
      "The input data must be a pytree of arrays with the same shape across all hosts.",
      "Optionally, a boolean can denote the source host; otherwise, host 0 is used as the default.",
      "The output is a pytree where leaves contain data from the source host."
    ],
    "code_examples": []
  },
  {
    "title": "Barrier Operation",
    "concepts": [
      "Creates a barrier across all hosts/devices.",
      "The 'name' parameter is a string."
    ],
    "code_examples": []
  },
  {
    "title": "Host Tree Verification",
    "concepts": [
      "Verification that all hosts share the same tree of values.",
      "The fail_message argument is a string."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to host_local_array_to_global_array",
    "concepts": [
      "The function converts a host-local value to a globally sharded jax.Array.",
      "It populates a global array with data, where each device gets the appropriate slice according to the sharding defined by the global_mesh and pspecs.",
      "It is an undefined behavior to have not identical local_inputs with pspec indicating data replication.",
      "The function can be used to transition to jax.Array and is useful when passing host local values to pjit."
    ],
    "code_examples": []
  },
  {
    "title": "Example with Partitioning Along 'x' axis",
    "concepts": [
      "Demonstrates creating a global array sharded along the 'x' axis.",
      "Each host's devices receive a slice of the data based on the host ID.",
      "The resulting array has a shape determined by the number of processes and local devices."
    ],
    "code_examples": [
      {
        "description": "Illustrates converting a host-local array to a global array sharded along the 'x' axis.",
        "code": "global_mesh = jax.sharding.Mesh(jax.devices(), 'x')\npspecs = jax.sharding.PartitionSpec('x')\nhost_id = jax.process_index()\narr = host_local_array_to_global_array(np.arange(4) * host_id, mesh, pspecs)\n# NB: assumes jax.local_device_count() divides 4."
      },
      {
        "description": "Illustrates converting a host-local array to a global array sharded along the 'x' axis.",
        "code": "global_mesh = jax.sharding.Mesh(jax.devices(), 'x')\npspecs = jax.sharding.PartitionSpec('x')\nhost_id = jax.process_index()\narr = host_local_array_to_global_array(np.arange(4) * host_id, mesh, pspecs)\n# NB: assumes jax.local_device_count() divides 4."
      }
    ]
  },
  {
    "title": "Example with Partitioning Along 'host' axis",
    "concepts": [
      "Demonstrates partitioning data across hosts.",
      "Each slice is replicated across corresponding host devices.",
      "Illustrates using a mesh that considers both 'host' and 'dev' axes."
    ],
    "code_examples": [
      {
        "description": "Shows how to convert a host-local array to a global array partitioned along the 'host' axis, where each host's devices receive a replicated slice of the data.",
        "code": "mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(jax.process_count(), jax.local_device_count()), ['host', 'dev'])\npspecs = jax.sharding.PartitionSpec('host')\nhost_id = jax.process_index()\narr = host_local_array_to_global_array(np.arange(4) * host_id, mesh, pspecs)"
      },
      {
        "description": "Shows how to convert a host-local array to a global array partitioned along the 'host' axis, where each host's devices receive a replicated slice of the data.",
        "code": "mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(jax.process_count(), jax.local_device_count()), ['host', 'dev'])\npspecs = jax.sharding.PartitionSpec('host')\nhost_id = jax.process_index()\narr = host_local_array_to_global_array(np.arange(4) * host_id, mesh, pspecs)"
      }
    ]
  },
  {
    "title": "Example with Replication",
    "concepts": [
      "Illustrates replicating data across all hosts and devices.",
      "The resulting array has the same shape as the input and is replicated everywhere."
    ],
    "code_examples": [
      {
        "description": "Demonstrates replicating a host-local array across all hosts and devices.",
        "code": "pspecs = jax.sharding.PartitionSpec()\narr = host_local_array_to_global_array(np.arange(4), mesh, pspecs)"
      },
      {
        "description": "Demonstrates replicating a host-local array across all hosts and devices.",
        "code": "pspecs = jax.sharding.PartitionSpec()\narr = host_local_array_to_global_array(np.arange(4), mesh, pspecs)"
      }
    ]
  },
  {
    "title": "Usage with pjit",
    "concepts": [
      "Shows how to use host_local_array_to_global_array to prepare inputs for pjit.",
      "jax.Array inputs to pjit should be globally shaped.",
      "Demonstrates the transition from host-local values to global Arrays for use with pjit."
    ],
    "code_examples": [
      {
        "description": "Example of how to use host_local_array_to_global_array to convert host-local inputs into global inputs for pjit and how to retrieve host-local outputs from the global output of pjit.",
        "code": "from jax.experimental import multihost_utils\n\nglobal_inputs = multihost_utils.host_local_array_to_global_array(host_local_inputs, global_mesh, in_pspecs)\n\nwith mesh:\n  global_out = pjitted_fun(global_inputs)\n\nhost_local_output = multihost_utils.global_array_to_host_local_array(global_out, mesh, out_pspecs)"
      },
      {
        "description": "Example of how to use host_local_array_to_global_array to convert host-local inputs into global inputs for pjit and how to retrieve host-local outputs from the global output of pjit.",
        "code": "from jax.experimental import multihost_utils\n\nglobal_inputs = multihost_utils.host_local_array_to_global_array(host_local_inputs, global_mesh, in_pspecs)\n\nwith mesh:\n  global_out = pjitted_fun(global_inputs)\n\nhost_local_output = multihost_utils.global_array_to_host_local_array(global_out, mesh, out_pspecs)"
      }
    ]
  },
  {
    "title": "Mesh Requirements and Alternatives",
    "concepts": [
      "The function requires the global mesh to be a continuous mesh.",
      "Devices belonging to each host should form a subcube in the mesh.",
      "For non-continuous meshes, use jax.make_array_from_callback or jax.make_array_from_single_device_arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "local_inputs: A Pytree of host local values.",
      "global_mesh: A jax.sharding.Mesh object representing the global mesh.",
      "pspecs: A Pytree of jax.sharding.PartitionSpec's."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Global and Host-Local jax.Arrays",
    "concepts": [
      "jax.Array can be global or host-local.",
      "Using jax.Array with pjit has the same semantics of using GDA with pjit, i.e., all jax.Array inputs to pjit should be globally shaped.",
      "The output from pjit will also be globally shaped jax.Arrays.",
      "You can convert the globally shaped jax.Array output from pjit to host local values."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage with multihost_utils",
    "concepts": [
      "Use multihost_utils to convert between host-local and global arrays.",
      "The function host_local_array_to_global_array converts host-local arrays to global arrays.",
      "The function global_array_to_host_local_array converts global arrays to host-local arrays.",
      "These conversions are useful when transitioning to jax.Array and using pjit."
    ],
    "code_examples": [
      {
        "description": "Demonstrates converting host-local arrays to global arrays, using pjit with a mesh, and converting the output back to host-local arrays.",
        "code": "from jax.experimental import multihost_utils\n\n\nglobal_inputs = multihost_utils.host_local_array_to_global_array(host_local_inputs, global_mesh, in_pspecs)\n\n\nwith mesh:\n  global_out = pjitted_fun(global_inputs)\n\n\nhost_local_output = multihost_utils.global_array_to_host_local_array(global_out, mesh, out_pspecs)"
      },
      {
        "description": "Demonstrates converting host-local arrays to global arrays, using pjit with a mesh, and converting the output back to host-local arrays.",
        "code": "from jax.experimental import multihost_utils\n\n\nglobal_inputs = multihost_utils.host_local_array_to_global_array(host_local_inputs, global_mesh, in_pspecs)\n\n\nwith mesh:\n  global_out = pjitted_fun(global_inputs)\n\n\nhost_local_output = multihost_utils.global_array_to_host_local_array(global_out, mesh, out_pspecs)"
      }
    ]
  },
  {
    "title": "Parameter Definitions",
    "concepts": [
      "global_inputs: A Pytree of global jax.Array\u2019s.",
      "global_mesh: A jax.sharding.Mesh object, which must be contiguous.",
      "pspecs: A Pytree of jax.sharding.PartitionSpec objects.",
      "host_local_inputs: A Pytree of host-local arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Pallas Module Overview",
    "concepts": [
      "Pallas is a JAX extension for custom kernels.",
      "It allows specifying how arrays are sliced for kernel invocations.",
      "It provides a way to define grid parameters for kernel execution."
    ],
    "code_examples": []
  },
  {
    "title": "Core Data Structures",
    "concepts": [
      "BlockSpec specifies how an array should be sliced for each kernel invocation.",
      "GridSpec encodes grid parameters for `jax.experimental.pallas.pallas_call()`.",
      "Slice represents a slice with a start index and a size.",
      "MemoryRef is similar to jax.ShapeDtypeStruct but includes memory spaces."
    ],
    "code_examples": []
  },
  {
    "title": "Kernel Execution Functions",
    "concepts": [
      "pallas_call invokes a Pallas kernel on some inputs.",
      "program_id returns the kernel execution position along the grid axis.",
      "num_programs returns the size of the grid along the given axis."
    ],
    "code_examples": []
  },
  {
    "title": "Memory Access Operations",
    "concepts": [
      "load returns an array loaded from the given index.",
      "store stores a value at the given index.",
      "swap swaps the value at the given index and returns the old value."
    ],
    "code_examples": []
  },
  {
    "title": "Atomic Operations",
    "concepts": [
      "atomic_and atomically computes x_ref_or_view[idx] &= val.",
      "atomic_add atomically computes x_ref_or_view[idx] += val.",
      "atomic_cas performs an atomic compare-and-swap of the value in the ref with the given value.",
      "atomic_max atomically computes x_ref_or_view[idx] = max(x_ref_or_view[idx], val).",
      "atomic_min atomically computes x_ref_or_view[idx] = min(x_ref_or_view[idx], val).",
      "atomic_or atomically computes x_ref_or_view[idx] |= val.",
      "atomic_xchg atomically exchanges the given value with the value at the given index.",
      "atomic_xor atomically computes x_ref_or_view[idx] ^= val."
    ],
    "code_examples": []
  },
  {
    "title": "Other Utility Functions",
    "concepts": [
      "broadcast_to broadcasts an array to a specified shape.",
      "debug_print prints values from inside a Pallas kernel.",
      "dot performs a dot product of two arrays.",
      "max_contiguous computes the maximum contiguous values.",
      "multiple_of checks if values are multiples of x.",
      "run_scoped calls a function with allocated references.",
      "when executes a block of code conditionally."
    ],
    "code_examples": []
  },
  {
    "title": "Overview",
    "concepts": [
      "Experimental GPU backend for Pallas targeting H100.",
      "APIs are highly unstable and can change weekly.",
      "Use at your own risk."
    ],
    "code_examples": []
  },
  {
    "title": "Classes and Objects",
    "concepts": [
      "Barrier: Represents a barrier with a specified number of arrivals and barriers.",
      "GPUBlockSpec: Specifies a block shape and index map for GPU operations.",
      "GPUCompilerParams: Parameters for the Mosaic GPU compiler, including approximate math.",
      "GPUMemorySpace: Enumeration of GPU memory spaces.",
      "Layout: Enumeration defining the layout of data.",
      "SwizzleTransform: Transformation that applies swizzling to memory access.",
      "TilingTransform: Represents a tiling transformation for memory refs.",
      "TransposeTransform: Transposes a tiled memref.",
      "WGMMAAccumulatorRef: Represents an accumulator for warp group matrix multiply-accumulate operations."
    ],
    "code_examples": []
  },
  {
    "title": "Functions",
    "concepts": [
      "barrier_arrive: Arrives at the given barrier.",
      "barrier_wait: Waits on the given barrier.",
      "commit_smem: Commits all writes to SMEM, making them visible to loads, TMA and WGMMA.",
      "copy_gmem_to_smem: Asynchronously copies a GMEM reference to a SMEM reference.",
      "copy_smem_to_gmem: Asynchronously copies a SMEM reference to a GMEM reference.",
      "emit_pipeline: Creates a function to emit a manual pipeline within a Pallas kernel.",
      "layout_cast: Casts the layout of the given array.",
      "set_max_registers: Sets the maximum number of registers owned by a warp.",
      "wait_smem_to_gmem: Waits until there are no more than n SMEM->GMEM copies in flight.",
      "wgmma: Performs an asynchronous warp group matmul-accumulate on the given references.",
      "wgmma_wait: Waits until there is no more than n WGMMA operations in flight."
    ],
    "code_examples": []
  },
  {
    "title": "Aliases",
    "concepts": [
      "ACC: Alias of WGMMAAccumulatorRef",
      "GMEM: Alias of jax.experimental.pallas.mosaic_gpu.GPUMemorySpace.GMEM.",
      "SMEM: Alias of jax.experimental.pallas.mosaic_gpu.GPUMemorySpace.SMEM."
    ],
    "code_examples": []
  },
  {
    "title": "Class Attributes and Methods",
    "concepts": [
      "The class has attributes num_arrivals and num_barriers.",
      "The class has methods __init__ and get_ref_aval."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of Attributes and Methods",
    "concepts": [
      "The document describes attributes and methods related to a data structure.",
      "Attributes include block_shape, index_map, pipeline_mode, transforms, memory_space, and indexing_mode.",
      "Methods include __init__, replace, and to_block_mapping."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "__init__ is the constructor method, taking optional arguments like block_shape and index_map.",
      "replace creates a new object with specified fields updated.",
      "to_block_mapping converts data to a block mapping based on origin and array_aval."
    ],
    "code_examples": []
  },
  {
    "title": "Mosaic GPU Compiler Parameters",
    "concepts": [
      "approx_math: Specifies whether approximate math implementations are allowed.",
      "dimension_semantics: Defines the execution order for each grid dimension (parallel or sequential).",
      "max_concurrent_steps: Maximum number of concurrent sequential stages.",
      "delay_release: Number of steps to wait before reusing input/output references.",
      "profile_space: Number of profiler events that can be collected in a single invocation.",
      "profile_dir: Directory where profiling traces are written.",
      "thread_semantics: Defines the thread semantics."
    ],
    "code_examples": []
  },
  {
    "title": "Parameter Details",
    "concepts": [
      "approx_math is a boolean value defaulting to False.",
      "dimension_semantics is a sequence of DimensionSemantics or None.",
      "max_concurrent_steps is an integer defaulting to 1.",
      "delay_release is an integer defaulting to 0 and must be less than max_concurrent_steps.",
      "profile_space is an integer.",
      "profile_dir is a string."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes and Methods",
    "concepts": [
      "The class has attributes such as PLATFORM, approx_math, delay_release, dimension_semantics, max_concurrent_steps, profile_dir, profile_space, and thread_semantics.",
      "The class has an __init__ method for initialization."
    ],
    "code_examples": []
  },
  {
    "title": "Enumeration Description",
    "concepts": [
      "The document describes an enumeration with attributes.",
      "GMEM represents global memory.",
      "SMEM represents shared memory.",
      "REGS represents registers."
    ],
    "code_examples": []
  },
  {
    "title": "Enumeration Overview",
    "concepts": [
      "This section describes an enumeration type.",
      "The enumeration has attributes representing different matrix structures."
    ],
    "code_examples": []
  },
  {
    "title": "WGMMA Attribute",
    "concepts": [
      "Represents a matrix of size [m, n].",
      "m is a multiple of 64.",
      "n is a multiple of 8."
    ],
    "code_examples": []
  },
  {
    "title": "WGMMA_ROW Attribute",
    "concepts": [
      "Represents a matrix of size [m].",
      "m is a multiple of 64."
    ],
    "code_examples": []
  },
  {
    "title": "WG_SPLAT Attribute",
    "concepts": [
      "This section describes the WG_SPLAT attribute.",
      "No further information is given about this attribute."
    ],
    "code_examples": []
  },
  {
    "title": "WG_STRIDED Attribute",
    "concepts": [
      "This section describes the WG_STRIDED attribute.",
      "No further information is given about this attribute."
    ],
    "code_examples": []
  },
  {
    "title": "Class Constructor: __init__",
    "concepts": [
      "The __init__ method initializes a `swizzle` object.",
      "The constructor takes an integer `swizzle` as input."
    ],
    "code_examples": []
  },
  {
    "title": "batch",
    "concepts": [
      "The batch method returns a transform.",
      "The transform accepts a ref with extra leading_rank dimensions.",
      "leading_rank determines the number of extra leading dimensions."
    ],
    "code_examples": []
  },
  {
    "title": "to_gpu_transform",
    "concepts": [
      "The to_gpu_transform method transforms something to GPU format."
    ],
    "code_examples": []
  },
  {
    "title": "undo",
    "concepts": [
      "The undo method performs an undo operation on a ref."
    ],
    "code_examples": []
  },
  {
    "title": "undo_to_gpu_transform",
    "concepts": [
      "The undo_to_gpu_transform method performs an undo operation on a GPU transform."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "The class has an attribute named `swizzle`."
    ],
    "code_examples": []
  },
  {
    "title": "Tiling Transformation",
    "concepts": [
      "Represents a tiling transformation for memory refs.",
      "Tiling of (X, Y) on an array of shape (M, N) results in a transformed shape of (M // X, N // Y, X, Y).",
      "Example: A (256, 256) block tiled with a tiling of (64, 32) will be tiled as (4, 8, 64, 32)."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "__init__(tiling): Initializes the tiling transformation with the given tiling tuple.",
      "batch(leading_rank): Returns a transform that accepts a ref with the extra leading_rank dims.",
      "to_gpu_transform(): Converts the transform to a GPU compatible transform.",
      "undo(ref): Undoes the tiling transformation on the given ref."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "tiling: Stores the tiling tuple."
    ],
    "code_examples": []
  },
  {
    "title": "Transpose Tiled Memref",
    "concepts": [
      "Transpose a tiled memref.",
      "The permutation attribute defines the transposition."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "__init__(permutation): Initializes the transpose with a given permutation.",
      "batch(leading_rank): Returns a transform that accepts a ref with extra leading dimensions.",
      "to_gpu_transform(): Returns a GPU-specific transform.",
      "undo(ref): Reverses the transformation."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "permutation: Stores the permutation for the transpose operation."
    ],
    "code_examples": []
  },
  {
    "title": "Class Constructor: __init__",
    "concepts": [
      "The constructor initializes the class.",
      "It takes shape, dtype, and _init as arguments.",
      "The 'shape' parameter is a tuple of integers.",
      "The 'dtype' parameter specifies the data type.",
      "The '_init' parameter takes any type of value."
    ],
    "code_examples": [
      {
        "description": "Class constructor with shape, dtype, and _init parameters.",
        "code": "__init__ (shape[,\u00a0dtype,\u00a0_init])"
      }
    ]
  },
  {
    "title": "Methods",
    "concepts": [
      "The class defines methods for interacting with its instances.",
      "get_ref_aval() is a method.",
      "init(array) is a method that likely initializes the object with an array."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "The class has attributes that describe its properties.",
      "shape is an attribute."
    ],
    "code_examples": []
  },
  {
    "title": "Barrier Arrival",
    "concepts": [
      "The function `barrier` synchronizes threads at a specified memory location.",
      "It utilizes `pallas_core.AbstractMemoryRef` to define the barrier's memory location.",
      "The return type of the function is None, indicating its side-effect nature (synchronization)."
    ],
    "code_examples": []
  },
  {
    "title": "Barrier Synchronization",
    "concepts": [
      "Waits on the given barrier.",
      "The barrier is represented by `pallas_core.AbstractMemoryRef`."
    ],
    "code_examples": []
  },
  {
    "title": "Committing Writes to SMEM",
    "concepts": [
      "Writes to SMEM are committed.",
      "Committed writes become visible to loads.",
      "Committed writes become visible to TMA.",
      "Committed writes become visible to WGMMA."
    ],
    "code_examples": []
  },
  {
    "title": "Asynchronous GMEM to SMEM Copy",
    "concepts": [
      "Asynchronously copies a GMEM reference to a SMEM reference.",
      "Related to jax.experimental.mosaic.gpu.barrier_arrive()",
      "Related to jax.experimental.mosaic.gpu.barrier_wait()",
      "src represents the source _Ref.",
      "dst represents the destination _Ref.",
      "barrier represents the _Ref for synchronization."
    ],
    "code_examples": []
  },
  {
    "title": "Asynchronous SMEM to GMEM Copy",
    "concepts": [
      "Copies an SMEM reference to a GMEM reference asynchronously.",
      "The `src` parameter is the SMEM reference to copy from.",
      "The `dst` parameter is the GMEM reference to copy to.",
      "The `predicate` parameter is a boolean indicating whether the copy should be performed.",
      "If `predicate` is None, the copy is always performed.",
      "The `commit_group` parameter indicates whether to commit the copy to a group for joint awaiting.",
      "Related functions: `jax.experimental.mosaic.gpu.wait_smem_to_gmem()`, `jax.experimental.mosaic.gpu.commit_smem()`"
    ],
    "code_examples": []
  },
  {
    "title": "Casting Array Layouts",
    "concepts": [
      "The function casts the layout of a given array.",
      "The function takes the array 'x' as input.",
      "The function takes the new layout 'new_layout' (Layout or ParameterizedLayout) as input."
    ],
    "code_examples": []
  },
  {
    "title": "Warp Register Limits",
    "concepts": [
      "Setting the maximum number of registers owned by a warp.",
      "The 'n' parameter specifies the register limit as an integer.",
      "The 'action' parameter specifies whether to 'increase' or 'decrease' the register limit."
    ],
    "code_examples": []
  },
  {
    "title": "Waiting for SMEM->GMEM Copies",
    "concepts": [
      "Function waits until the number of SMEM->GMEM copies in flight is no more than a specified maximum.",
      "The maximum number of copies in flight is determined by the 'n' parameter.",
      "The 'wait_read_only' parameter determines whether to wait for SMEM reads to complete.",
      "The function returns None."
    ],
    "code_examples": []
  },
  {
    "title": "Asynchronous Warp Group Matmul-Accumulate",
    "concepts": [
      "Performs an asynchronous warp group matmul-accumulate operation.",
      "Conceptually equivalent to acc[...] += a[...] @ b[...] but asynchronously.",
      "acc is a gpu_core.WGMMAAbstractAccumulatorRef allocated via jax.experimental.pallas.run_scoped() with a jax.experimental.pallas.mosaic_gpu.WGMMAAccumulatorRef().",
      "a is the left-hand side operand reference.",
      "b is the right-hand side operand reference (pallas_core.TransformedRef)."
    ],
    "code_examples": []
  },
  {
    "title": "WGMMA Operation Wait",
    "concepts": [
      "Waits until there are no more than n WGMMA operations in flight.",
      "The parameter n represents the maximum number of allowed in-flight WGMMA operations."
    ],
    "code_examples": []
  },
  {
    "title": "WGMMAAccumulatorRef Alias",
    "concepts": [
      "WGMMAAccumulatorRef is an alias.",
      "The document defines the alias of WGMMAAccumulatorRef."
    ],
    "code_examples": []
  },
  {
    "title": "GPUMemorySpace.GMEM Alias",
    "concepts": [
      "GMEM is an alias of jax.experimental.pallas.mosaic_gpu.GPUMemorySpace.GMEM.",
      "shape: Represents the shape of the memory space (tuple of integers).",
      "dtype: Represents the data type of the memory space (jnp.dtype).",
      "transforms: A sequence of MemoryRefTransform objects."
    ],
    "code_examples": []
  },
  {
    "title": "GPUMemorySpace.SMEM Alias",
    "concepts": [
      "Alias for jax.experimental.pallas.mosaic_gpu.GPUMemorySpace.SMEM.",
      "Represents shared memory space on the GPU.",
      "Contains attributes: shape, dtype, and transforms.",
      "Inherits from pallas_core.MemoryRef."
    ],
    "code_examples": []
  },
  {
    "title": "Triton Pallas APIs",
    "concepts": [
      "Provides Triton-specific Pallas APIs.",
      "Includes compiler parameters for Triton.",
      "Offers an elementwise approximate hyperbolic tangent function.",
      "Allows synchronization of all kernel executions in the grid.",
      "Supports inline assembly with elementwise operations."
    ],
    "code_examples": []
  },
  {
    "title": "TritonCompilerParams",
    "concepts": [
      "Represents compiler parameters for Triton.",
      "Includes parameters like num_warps."
    ],
    "code_examples": []
  },
  {
    "title": "approx_tanh",
    "concepts": [
      "Computes the elementwise approximate hyperbolic tangent of a value.",
      "The function is defined as tanh(x)."
    ],
    "code_examples": []
  },
  {
    "title": "debug_barrier",
    "concepts": [
      "Synchronizes all kernel executions within a grid.",
      "Used for debugging purposes."
    ],
    "code_examples": []
  },
  {
    "title": "elementwise_inline_asm",
    "concepts": [
      "Enables the use of inline assembly for elementwise operations.",
      "Takes assembly code and arguments as input."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Approximate Hyperbolic Tangent",
    "concepts": [
      "The function calculates the hyperbolic tangent of an array elementwise.",
      "It refers to the NVIDIA CUDA documentation for details on the underlying implementation.",
      "The input x is a jax.Array.",
      "The output is also a jax.Array."
    ],
    "code_examples": []
  },
  {
    "title": "Synchronization",
    "concepts": [
      "Synchronizes all kernel executions in the grid."
    ],
    "code_examples": []
  },
  {
    "title": "Inline Assembly Elementwise Operation",
    "concepts": [
      "Inline assembly allows integrating low-level code within JAX.",
      "The `asm` argument specifies the assembly code to be executed.",
      "The `args` argument provides the input JAX arrays to the assembly code.",
      "The `constraints` argument defines LLVM inline assembly constraints.",
      "The `pack` argument determines how many elements from each argument are processed by a single assembly instruction.",
      "The `result_shape_dtypes` argument specifies the shapes and data types of the results produced by the assembly code.",
      "The function returns a sequence of JAX arrays representing the results."
    ],
    "code_examples": []
  },
  {
    "title": "Mosaic-Specific Pallas APIs",
    "concepts": [
      "The document introduces Mosaic-specific Pallas APIs."
    ],
    "code_examples": []
  },
  {
    "title": "Slice Definition",
    "concepts": [
      "A slice has a start index and a size.",
      "Start index and size can be static or dynamic.",
      "Slices also have a stride."
    ],
    "code_examples": []
  },
  {
    "title": "Slice Attributes",
    "concepts": [
      "is_dynamic_size indicates if the slice size is dynamic.",
      "is_dynamic_start indicates if the slice start is dynamic.",
      "stride is the step size of the slice.",
      "start is the starting index of the slice.",
      "size is the size of the slice."
    ],
    "code_examples": []
  },
  {
    "title": "Slice Methods",
    "concepts": [
      "__init__(start, size[, stride]) initializes a slice object.",
      "from_slice(slc, size) creates a slice from an existing slice object.",
      "tree_flatten() flattens the slice object for tree traversal.",
      "tree_unflatten(aux_data, children) reconstructs a slice object from flattened data."
    ],
    "code_examples": []
  },
  {
    "title": "Description of ShapeDtypeStruct",
    "concepts": [
      "Similar to jax.ShapeDtypeStruct but includes memory spaces.",
      "Represents the shape of an array as a tuple of integers.",
      "Represents the data type of an array (jnp.dtype).",
      "Represents the memory space where the array is stored (Any)."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "shape: Represents the shape of an array as a tuple of integers.",
      "dtype: Represents the data type of an array (jnp.dtype).",
      "memory_space: Represents the memory space where the array is stored (Any)."
    ],
    "code_examples": []
  },
  {
    "title": "Kernel Execution Position",
    "concepts": [
      "Returns the kernel execution position along the given axis of the grid.",
      "The returned value is an array of shape () and dtype int32."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage",
    "concepts": [
      "For a 2D grid, program_id(axis=0) returns the x-coordinate and program_id(axis=1) returns the y-coordinate."
    ],
    "code_examples": []
  },
  {
    "title": "Axis Parameter",
    "concepts": [
      "axis (int): The axis of the grid along which to count the program."
    ],
    "code_examples": []
  },
  {
    "title": "Return Type",
    "concepts": [
      "The return type is a jax.Array."
    ],
    "code_examples": []
  },
  {
    "title": "Grid Size Retrieval",
    "concepts": [
      "The function returns the size of a grid.",
      "The size is determined along a specific axis.",
      "The axis is specified as an integer.",
      "The return type can be either an integer or a JAX array."
    ],
    "code_examples": []
  },
  {
    "title": "Array Loading with Indexing",
    "concepts": [
      "Loads an array from a given index.",
      "Behavior is similar to x_ref_or_view[idx] in JAX if mask and other are not specified.",
      "Uses x_ref_or_view as the ref to load from.",
      "Uses idx as the indexer.",
      "Accepts an optional boolean mask.",
      "Accepts an optional value for indices where mask is False.",
      "cache_modifier, eviction_policy, and volatile are parameters that needs to be documented."
    ],
    "code_examples": []
  },
  {
    "title": "Value Storage",
    "concepts": [
      "Stores a value at a specified index.",
      "The arguments' meaning can be found in the load() function documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Value Swapping",
    "concepts": [
      "Swaps the value at a given index.",
      "Returns the old value that was stored at the index prior to the swap.",
      "Uses load() for argument handling.",
      "Operates on jax.Array objects."
    ],
    "code_examples": []
  },
  {
    "title": "Atomic Bitwise AND Operation",
    "concepts": [
      "Atomically computes a bitwise AND operation between a value and an element of a reference or view.",
      "x_ref_or_view is the reference being operated on.",
      "idx specifies the index of the element to be modified.",
      "val is the value to perform the bitwise AND with.",
      "mask is a parameter that requires further documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Atomic Addition Operation",
    "concepts": [
      "Atomically computes x_ref_or_view[idx] += val.",
      "x_ref_or_view is the reference to operate on.",
      "idx is the indexer used to access the element.",
      "mask is a parameter that is not yet documented.",
      "The return value is the value at the given index prior to the atomic operation."
    ],
    "code_examples": []
  },
  {
    "title": "Atomic Compare-and-Swap Operation",
    "concepts": [
      "Performs an atomic compare-and-swap.",
      "The operation involves a reference (ref), a comparison value (cmp), and a new value (val).",
      "It compares the value at the reference with 'cmp', and if they match, it swaps the value at the reference with 'val'.",
      "The function returns the value at the given index prior to the atomic operation."
    ],
    "code_examples": []
  },
  {
    "title": "Atomic Maximum Operation",
    "concepts": [
      "Atomically computes the maximum value between an existing value and a new value at a specific index in a reference or view.",
      "x_ref_or_view is the reference being operated upon.",
      "idx is the index used to access the element.",
      "mask's purpose is not documented."
    ],
    "code_examples": []
  },
  {
    "title": "Atomic Minimum Computation",
    "concepts": [
      "Atomically computes the minimum value between an existing value and a new value at a specified index.",
      "The operation is performed on a reference or view (x_ref_or_view).",
      "An indexer (idx) is used to specify the location within the reference or view.",
      "A mask can be used (purpose needs to be documented)."
    ],
    "code_examples": []
  },
  {
    "title": "Atomic OR Operation",
    "concepts": [
      "Atomically computes a bitwise OR operation.",
      "The operation targets a specific element within a reference or view.",
      "It takes a reference or view, an index, and a value as input.",
      "Returns the value at the given index prior to the atomic operation."
    ],
    "code_examples": []
  },
  {
    "title": "Atomic Exchange Operation",
    "concepts": [
      "Atomically exchanges a given value with the value at a specified index.",
      "Requires a reference or view object.",
      "Uses an indexer to locate the position for the exchange.",
      "A mask may be used (purpose not fully documented)."
    ],
    "code_examples": []
  },
  {
    "title": "Atomic XOR Operation",
    "concepts": [
      "Atomic XOR operation is performed on a reference or view.",
      "The operation is `x_ref_or_view[idx] ^= val`",
      "`x_ref_or_view` represents the reference to be operated on.",
      "`idx` specifies the indexer to use.",
      "`mask`'s purpose is not documented."
    ],
    "code_examples": []
  },
  {
    "title": "Array Definition",
    "concepts": [
      "Arrays are represented as a type 'Array'.",
      "Arrays have a shape which is a tuple of integers."
    ],
    "code_examples": []
  },
  {
    "title": "Pallas Kernel Value Printing",
    "concepts": [
      "The function prints values from inside a Pallas kernel.",
      "The fmt argument is a format string that can be used to control the output.",
      "The restrictions on the format string depend on the backend (GPU with Triton, GPU with Mosaic, TPU).",
      "On GPU with Triton, the format string must not contain any placeholders.",
      "On GPU with Mosaic, the format string must contain a placeholder for each value.",
      "On TPU, if all inputs are scalars and the format string contains placeholders, all values must be 32-bit integers.",
      "On TPU, if all inputs are scalars and there are no placeholders in the format string, the values are printed after the format string.",
      "On TPU, if the input is a single vector, the vector is printed after the format string and the format string must end with a single placeholder {}.",
      "*args are the values to be printed."
    ],
    "code_examples": []
  },
  {
    "title": "Boolean Parameters: trans_a, trans_b, allow_tf32",
    "concepts": [
      "trans_a is a boolean parameter.",
      "trans_b is a boolean parameter.",
      "allow_tf32 is a boolean or None parameter."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to the Document",
    "concepts": [
      "This is an example document.",
      "It serves as input for the JSON extraction task.",
      "The aim is to organize the document content into sections, concepts and code examples.",
      "This section provides the context for the overall task."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Python Example",
    "concepts": [
      "This section showcases a simple Python code snippet.",
      "It demonstrates the basic syntax and functionality of Python.",
      "The code prints 'Hello, world!' to the console."
    ],
    "code_examples": [
      {
        "description": "Prints 'Hello, world!' to the console.",
        "code": "print(\"Hello, world!\")"
      }
    ]
  },
  {
    "title": "Working with Variables",
    "concepts": [
      "This section demonstrates how to define and use variables in Python.",
      "It shows how to assign values to variables and print their values.",
      "Integer and String variables are demonstrated."
    ],
    "code_examples": [
      {
        "description": "Defines and prints variables of different types.",
        "code": "x = 5\ny = \"John\"\nprint(x)\nprint(y)"
      }
    ]
  },
  {
    "title": "Conditional Statements",
    "concepts": [
      "This section illustrates the use of conditional statements (if, else) in Python.",
      "It demonstrates how to execute different code blocks based on certain conditions.",
      "The code checks if a number is positive, negative, or zero."
    ],
    "code_examples": [
      {
        "description": "Checks if a number is positive, negative, or zero.",
        "code": "num = -1\nif num > 0:\n    print(\"Positive number\")\nelif num == 0:\n    print(\"Zero\")\nelse:\n    print(\"Negative number\")"
      }
    ]
  },
  {
    "title": "Loops",
    "concepts": [
      "This section explains the use of loops in Python.",
      "It demonstrates 'for' loop used for iterating over a sequence.",
      "It demonstrates 'while' loop which executes as long as a condition is true."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the use of a 'for' loop to iterate over a list.",
        "code": "fruits = [\"apple\", \"banana\", \"cherry\"]\nfor x in fruits:\n  print(x)"
      },
      {
        "description": "Demonstrates the use of a 'while' loop.",
        "code": "i = 1\nwhile i < 6:\n  print(i)\n  i += 1"
      }
    ]
  },
  {
    "title": "JAX Array and Values",
    "concepts": [
      "x is a JAX array.",
      "values can be a list of integers or a single integer.",
      "values is a JAX array."
    ],
    "code_examples": []
  },
  {
    "title": "Function Calling with Allocated References",
    "concepts": [
      "The function is called with pre-allocated memory references.",
      "Positional and keyword arguments specify the reference types.",
      "Backends can have their own reference types, in addition to `jax.experimental.pallas.MemoryRef`."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to the Document",
    "concepts": [
      "This document likely introduces the context, purpose, or scope of the topic it covers.",
      "There is no specific technical content in this section."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to pjit",
    "concepts": [
      "pjit is now equivalent to jax.jit.",
      "pjit compiles a function and automatically partitions it across multiple devices.",
      "The function runs as an XLA computation across multiple devices.",
      "Partitioning is based on in_shardings and out_shardings.",
      "in_shardings and out_shardings must refer to mesh axes.",
      "Inputs are automatically partitioned based on in_shardings.",
      "Pre-partitioning inputs can increase performance.",
      "The mesh definition at pjit application time is ignored; the function uses the mesh at the call site.",
      "pjit can be used to run computations across all available devices across processes on multi-process platforms (e.g. TPU pods).",
      "pjit is designed for SPMD Python programs.",
      "All input arguments must be globally shaped in SPMD programs."
    ],
    "code_examples": []
  },
  {
    "title": "Function Arguments and Sharding Specifications",
    "concepts": [
      "fun is the function to be compiled, which should be a pure function.",
      "fun arguments and return values should be arrays, scalars, or nested Python containers.",
      "static_argnums specifies positional arguments to treat as static (compile-time constant).",
      "static arguments must be hashable and immutable.",
      "in_shardings specifies resource assignment for function inputs as a pytree.",
      "Specifying a pytree prefix in in_shardings broadcasts the leaves to the subtree.",
      "in_shardings is optional; JAX infers shardings or defaults to replicating the input.",
      "Sharding specifies how a value will be partitioned.",
      "PartitionSpec is used for backwards compatibility with the mesh context manager.",
      "out_shardings specifies resource assignment for function outputs as a pytree.",
      "out_shardings is optional; jax.jit() uses GSPMD to determine output shardings.",
      "static_argnames specifies named arguments to treat as static (compile-time constant).",
      "donate_argnums specifies positional argument buffers that are \u201cdonated\u201d to the computation. Donated buffers should not be re-used.",
      "donate_argnames specifies named arguments that are donated to the computation.",
      "keep_unused controls whether unused arguments are dropped from the compiled XLA executable.",
      "device and backend are deprecated; place arguments on the desired device/backend before passing to jit.",
      "compiler_options allows passing compiler options to the XLA compiler.",
      "inline enables or disables inlining of the function."
    ],
    "code_examples": []
  },
  {
    "title": "Usage Example with Convolution",
    "concepts": [
      "Example demonstrates partitioning a convolution operator over devices using pjit."
    ],
    "code_examples": [
      {
        "description": "Example of using pjit to partition a convolution operation across multiple devices.",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax.sharding import Mesh, PartitionSpec\nfrom jax.experimental.pjit import pjit\n\nx = jnp.arange(8, dtype=jnp.float32)\nf = pjit(\n    lambda x: jax.numpy.convolve(x, jnp.asarray([0.5, 1.0, 0.5]), 'same'),\n    in_shardings=None,\n    out_shardings=PartitionSpec('devices')\n)\nwith Mesh(np.array(jax.devices()), ('devices',)):\n  print(f(x))"
      },
      {
        "description": "Duplicate of the previous code example, included in the original document.",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax.sharding import Mesh, PartitionSpec\nfrom jax.experimental.pjit import pjit\n\nx = jnp.arange(8, dtype=jnp.float32)\nf = pjit(\n    lambda x: jax.numpy.convolve(x, jnp.asarray([0.5, 1.0, 0.5]), 'same'),\n    in_shardings=None,\n    out_shardings=PartitionSpec('devices')\n)\nwith Mesh(np.array(jax.devices()), ('devices',)):\n  print(f(x))"
      }
    ]
  },
  {
    "title": "Pickling Support for Precompiled Binaries",
    "concepts": [
      "Serializing compiled binaries using pickling.",
      "Deserializing and loading compiled binaries."
    ],
    "code_examples": []
  },
  {
    "title": "Serialization",
    "concepts": [
      "The `serialize` function serializes a compiled binary."
    ],
    "code_examples": []
  },
  {
    "title": "Deserialization and Loading",
    "concepts": [
      "The `deserialize_and_load` function constructs a jax.stages.Compiled object from a serialized executable.",
      "The `deserialize_and_load` function takes parameters `serialized` and `in_tree`."
    ],
    "code_examples": []
  },
  {
    "title": "Serialization of Compiled Binaries",
    "concepts": [
      "Compiled binaries can be serialized.",
      "Pytrees within compiled binaries are not directly serializable.",
      "Pytrees are returned separately for user handling."
    ],
    "code_examples": []
  },
  {
    "title": "Compiled from Serialized Executable",
    "concepts": [
      "Constructs a jax.stages.Compiled from a serialized executable.",
      "The `backend` argument can be a string, xc.Client, or None."
    ],
    "code_examples": []
  },
  {
    "title": "Shard Mapping Function",
    "concepts": [
      "shard_map is a function to map over shards of data.",
      "It takes a function f, a mesh, input specs in_specs, and output specs out_specs as arguments."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to shard_map",
    "concepts": [
      "shard_map is an experimental API for mapping a function over shards of data.",
      "shard_map is subject to change.",
      "It requires understanding of sharded data and parallel programming.",
      "Refer to documentation for introduction to parallel programming and SPMD multi-device parallelism with shard_map."
    ],
    "code_examples": []
  },
  {
    "title": "Function Arguments of shard_map",
    "concepts": [
      "f: Callable to be mapped; each instance takes a shard of the input and produces a shard of the output.",
      "mesh: A jax.sharding.Mesh representing the array of devices for sharding and execution.",
      "in_specs: A pytree with PartitionSpec instances specifying how the input arguments should be sharded along the mesh axes.",
      "out_specs: A pytree with PartitionSpec instances specifying how the output shards should be concatenated.",
      "check_rep: A boolean flag to enable validity checks and differentiation optimizations.",
      "auto: A set of axis names for compiler-controlled sharding."
    ],
    "code_examples": []
  },
  {
    "title": "Usage and Examples",
    "concepts": [
      "shard_map applies a function across data sharded according to the mesh and in_specs.",
      "Refer to Introduction to parallel programming or SPMD multi-device parallelism with shard_map for examples."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Sparse Arrays",
    "concepts": [
      "jax.experimental.sparse provides experimental support for sparse matrix operations in JAX.",
      "The API is under active development and subject to change.",
      "The primary interfaces are the BCOO sparse array type and the sparsify() transform.",
      "The main high-level sparse object is BCOO (Batched Coordinate sparse array).",
      "BCOO offers a compressed storage format compatible with JAX transformations like JIT, batching, and autodiff."
    ],
    "code_examples": []
  },
  {
    "title": "Creating and Using BCOO Sparse Arrays",
    "concepts": [
      "BCOO sparse arrays can be created from dense arrays using `sparse.BCOO.fromdense()`.",
      "Sparse arrays can be converted back to dense arrays using the `todense()` method.",
      "The `data` attribute contains the explicitly stored data.",
      "The `indices` attribute contains the indices of the stored data.",
      "BCOO objects have array-like attributes like `ndim`, `shape`, `dtype`, and sparse-specific attributes like `nse` (number of specified elements).",
      "BCOO objects implement array-like methods, allowing their use in JAX programs.",
      "BCOO objects are compatible with JAX transforms like `jax.jit()`, `jax.vmap()`, and `jax.grad()`."
    ],
    "code_examples": [
      {
        "description": "Creating a sparse array from a dense array and inspecting its properties.",
        "code": "from jax.experimental import sparse\nimport jax.numpy as jnp\nimport numpy as np\n\nM = jnp.array([[0., 1., 0., 2.],\n               [3., 0., 0., 0.],\n               [0., 0., 4., 0.]])\nM_sp = sparse.BCOO.fromdense(M)\nprint(M_sp)\nprint(M_sp.todense())\nprint(M_sp.data)\nprint(M_sp.indices)\nprint(M_sp.ndim)\nprint(M_sp.shape)\nprint(M_sp.dtype)\nprint(M_sp.nse)"
      },
      {
        "description": "Performing a transposed matrix-vector product with a sparse matrix and comparing it to the dense version.",
        "code": "from jax.experimental import sparse\nimport jax.numpy as jnp\nimport numpy as np\n\nM = jnp.array([[0., 1., 0., 2.],\n               [3., 0., 0., 0.],\n               [0., 0., 4., 0.]])\nM_sp = sparse.BCOO.fromdense(M)\ny = jnp.array([3., 6., 5.])\nprint(M_sp.T @ y)\nprint(M.T @ y)"
      },
      {
        "description": "Using BCOO with jax.jit() and jax.grad().",
        "code": "from jax import grad, jit\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nimport numpy as np\n\nM = jnp.array([[0., 1., 0., 2.],\n               [3., 0., 0., 0.],\n               [0., 0., 4., 0.]])\nM_sp = sparse.BCOO.fromdense(M)\ny = jnp.array([3., 6., 5.])\n\ndef f(y):\n  return (M_sp.T @ y).sum()\n\nprint(jit(grad(f))(y))"
      }
    ]
  },
  {
    "title": "The sparsify() Transform",
    "concepts": [
      "The `sparsify()` transform allows switching from dense to sparse computation seamlessly.",
      "It enables using sparse matrices as input to functions written for dense matrices.",
      "Many common JAX primitives are supported within the `sparsify()` transform.",
      "The supported primitives include matrix products, elementwise operations, reductions, indexing, concatenation, transposition, reshaping, and some higher-order functions."
    ],
    "code_examples": [
      {
        "description": "Demonstrates sparsify() with a simple function.",
        "code": "from jax.experimental import sparse\nimport jax.numpy as jnp\nimport numpy as np\n\nM = jnp.array([[0., 1., 0., 2.],\n               [3., 0., 0., 0.],\n               [0., 0., 4., 0.]])\nM_sp = sparse.BCOO.fromdense(M)\ny = jnp.array([3., 6., 5.])\n\ndef f(M, v):\n  return 2 * jnp.dot(jnp.log1p(M.T), v) + 1\n\nf_sp = sparse.sparsify(f)\nprint(f(M, y))\nprint(f_sp(M_sp, y))"
      }
    ]
  },
  {
    "title": "Logistic Regression with Sparse Data",
    "concepts": [
      "A logistic regression model is implemented in JAX without explicit references to sparsity.",
      "The `sparsify()` transform can be used to fit the same model on sparse data.",
      "This allows using a dense implementation with sparse data by applying `sparsify()` to the fitting function."
    ],
    "code_examples": [
      {
        "description": "Logistic regression implemented with dense arrays.",
        "code": "import functools\nfrom sklearn.datasets import make_classification\nfrom jax.scipy import optimize\nimport jax.numpy as jnp\n\ndef sigmoid(x):\n  return 0.5 * (jnp.tanh(x / 2) + 1)\n\ndef y_model(params, X):\n  return sigmoid(jnp.dot(X, params[1:]) + params[0])\n\ndef loss(params, X, y):\n  y_hat = y_model(params, X)\n  return -jnp.mean(y * jnp.log(y_hat) + (1 - y) * jnp.log(1 - y_hat))\n\ndef fit_logreg(X, y):\n  params = jnp.zeros(X.shape[1] + 1)\n  result = optimize.minimize(\n      functools.partial(loss, X=X, y=y),\n      x0=params, method='BFGS'\n  )\n  return result.x\n\nX, y = make_classification(n_classes=2, random_state=1701)\nparams_dense = fit_logreg(X, y)\nprint(params_dense)"
      },
      {
        "description": "Applying sparsify() to fit_logreg function.",
        "code": "import functools\nfrom sklearn.datasets import make_classification\nfrom jax.scipy import optimize\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\n\ndef sigmoid(x):\n  return 0.5 * (jnp.tanh(x / 2) + 1)\n\ndef y_model(params, X):\n  return sigmoid(jnp.dot(X, params[1:]) + params[0])\n\ndef loss(params, X, y):\n  y_hat = y_model(params, X)\n  return -jnp.mean(y * jnp.log(y_hat) + (1 - y) * jnp.log(1 - y_hat))\n\ndef fit_logreg(X, y):\n  params = jnp.zeros(X.shape[1] + 1)\n  result = optimize.minimize(\n      functools.partial(loss, X=X, y=y),\n      x0=params, method='BFGS'\n  )\n  return result.x\n\nX, y = make_classification(n_classes=2, random_state=1701)\nXsp = sparse.BCOO.fromdense(X)\nfit_logreg_sp = sparse.sparsify(fit_logreg)\nparams_sparse = fit_logreg_sp(Xsp, y)\nprint(params_sparse)"
      }
    ]
  },
  {
    "title": "Sparse API Reference",
    "concepts": [
      "sparsify (f[,\u00a0use_tracer]) - Experimental sparsification transform.",
      "grad (fun[,\u00a0argnums,\u00a0has_aux]) - Sparse-aware version of jax.grad()",
      "value_and_grad (fun[,\u00a0argnums,\u00a0has_aux]) - Sparse-aware version of jax.value_and_grad()",
      "empty (shape[,\u00a0dtype,\u00a0index_dtype,\u00a0sparse_format]) - Create an empty sparse array.",
      "eye (N[,\u00a0M,\u00a0k,\u00a0dtype,\u00a0index_dtype,\u00a0sparse_format]) - Create 2D sparse identity matrix.",
      "todense (arr) - Convert input to a dense matrix.",
      "random_bcoo (key,\u00a0shape,\u00a0*[,\u00a0dtype,\u00a0...]) - Generate a random BCOO matrix.",
      "JAXSparse (args,\u00a0*,\u00a0shape) - Base class for high-level JAX sparse objects.",
      "BCOO (args,\u00a0*,\u00a0shape[,\u00a0indices_sorted,\u00a0...]) - Experimental batched COO matrix implemented in JAX",
      "bcoo_broadcast_in_dim (mat,\u00a0*,\u00a0shape,\u00a0...[,\u00a0...]) - Expand the size and rank of a BCOO array by duplicating the data.",
      "bcoo_concatenate (operands,\u00a0*,\u00a0dimension) - Sparse implementation of jax.lax.concatenate()",
      "bcoo_dot_general (lhs,\u00a0rhs,\u00a0*,\u00a0dimension_numbers) - A general contraction operation.",
      "bcoo_dot_general_sampled (A,\u00a0B,\u00a0indices,\u00a0*,\u00a0...) - A contraction operation with output computed at given sparse indices.",
      "bcoo_dynamic_slice (mat,\u00a0start_indices,\u00a0...) - Sparse implementation of {func}`jax.lax.dynamic_slice`.",
      "bcoo_extract (sparr,\u00a0arr,\u00a0*[,\u00a0assume_unique]) - Extract values from a dense array according to the sparse array's indices.",
      "bcoo_fromdense (mat,\u00a0*[,\u00a0nse,\u00a0n_batch,\u00a0...]) - Create BCOO-format sparse matrix from a dense matrix.",
      "bcoo_gather (operand,\u00a0start_indices,\u00a0...[,\u00a0...]) - BCOO version of lax.gather.",
      "bcoo_multiply_dense (sp_mat,\u00a0v) - An element-wise multiplication between a sparse and a dense array.",
      "bcoo_multiply_sparse (lhs,\u00a0rhs) - An element-wise multiplication of two sparse arrays.",
      "bcoo_update_layout (mat,\u00a0*[,\u00a0n_batch,\u00a0...]) - Update the storage layout (i.e. n_batch & n_dense) of a BCOO matrix.",
      "bcoo_reduce_sum (mat,\u00a0*,\u00a0axes) - Sum array element over given axes.",
      "bcoo_reshape (mat,\u00a0*,\u00a0new_sizes[,\u00a0...]) - Sparse implementation of {func}`jax.lax.reshape`.",
      "bcoo_slice (mat,\u00a0*,\u00a0start_indices,\u00a0limit_indices) - Sparse implementation of {func}`jax.lax.slice`.",
      "bcoo_sort_indices (mat) - Sort indices of a BCOO array.",
      "bcoo_squeeze (arr,\u00a0*,\u00a0dimensions) - Sparse implementation of {func}`jax.lax.squeeze`.",
      "bcoo_sum_duplicates (mat[,\u00a0nse]) - Sums duplicate indices within a BCOO array, returning an array with sorted indices.",
      "bcoo_todense (mat) - Convert batched sparse matrix to a dense matrix.",
      "bcoo_transpose (mat,\u00a0*,\u00a0permutation) - Transpose a BCOO-format array.",
      "BCSR (args,\u00a0*,\u00a0shape[,\u00a0indices_sorted,\u00a0...]) - Experimental batched CSR matrix implemented in JAX.",
      "bcsr_dot_general (lhs,\u00a0rhs,\u00a0*,\u00a0dimension_numbers) - A general contraction operation.",
      "bcsr_extract (indices,\u00a0indptr,\u00a0mat) - Extract values from a dense matrix at given BCSR (indices, indptr).",
      "bcsr_fromdense (mat,\u00a0*[,\u00a0nse,\u00a0n_batch,\u00a0...]) - Create BCSR-format sparse matrix from a dense matrix.",
      "bcsr_todense (mat) - Convert batched sparse matrix to a dense matrix.",
      "COO (args,\u00a0*,\u00a0shape[,\u00a0rows_sorted,\u00a0cols_sorted]) - Experimental COO matrix implemented in JAX.",
      "coo_fromdense (mat,\u00a0*[,\u00a0nse,\u00a0index_dtype]) - Create a COO-format sparse matrix from a dense matrix.",
      "coo_matmat (mat,\u00a0B,\u00a0*[,\u00a0transpose]) - Product of COO sparse matrix and a dense matrix.",
      "coo_matvec (mat,\u00a0v[,\u00a0transpose]) - Product of COO sparse matrix and a dense vector.",
      "coo_todense (mat) - Convert a COO-format sparse matrix to a dense matrix.",
      "CSR (args,\u00a0*,\u00a0shape) - Experimental CSR matrix implemented in JAX.",
      "csr_fromdense (mat,\u00a0*[,\u00a0nse,\u00a0index_dtype]) - Create a CSR-format sparse matrix from a dense matrix.",
      "csr_matmat (mat,\u00a0B,\u00a0*[,\u00a0transpose]) - Product of CSR sparse matrix and a dense matrix.",
      "csr_matvec (mat,\u00a0v[,\u00a0transpose]) - Product of CSR sparse matrix and a dense vector.",
      "csr_todense (mat) - Convert a CSR-format sparse matrix to a dense matrix.",
      "CSC (args,\u00a0*,\u00a0shape) - Experimental CSC matrix implemented in JAX; API subject to change.",
      "spsolve (data,\u00a0indices,\u00a0indptr,\u00a0b[,\u00a0tol,\u00a0reorder]) - A sparse direct solver using QR factorization.",
      "lobpcg_standard (A,\u00a0X[,\u00a0m,\u00a0tol]) - Compute the top-k standard eigenvalues using the LOBPCG routine."
    ],
    "code_examples": []
  },
  {
    "title": "Sparsification Transform Examples",
    "concepts": [
      "The document introduces an experimental sparsification transform.",
      "The transform allows JAX functions to work with sparse matrices (jax.experimental.sparse.BCOO).",
      "The @sparse.sparsify decorator makes a function compatible with sparse matrices."
    ],
    "code_examples": [
      {
        "description": "Decorate a JAX function to make it compatible with sparse matrices using @sparse.sparsify. This example defines a function 'f' that computes 2 * M.T @ v, where M is a sparse matrix.",
        "code": "from jax.experimental import sparse\n\n@sparse.sparsify\ndef f(M, v):\n  return 2 * M.T @ v"
      },
      {
        "description": "This example demonstrates the usage of the sparsified function 'f' with a BCOO sparse matrix and a dense vector.  It creates a BCOO matrix from a dense JAX array and then calls the decorated function 'f'.",
        "code": "from jax.experimental import sparse\nimport jax.numpy as jnp\n\n@sparse.sparsify\ndef f(M, v):\n  return 2 * M.T @ v\n\nM = sparse.BCOO.fromdense(jnp.arange(12).reshape(3, 4))\nv = jnp.array([3, 4, 2])\nf(M, v)"
      }
    ]
  },
  {
    "title": "Sparse-aware Gradient Computation",
    "concepts": [
      "This function computes the gradient of a function with respect to its arguments.",
      "It is similar to jax.grad() but is specifically designed for sparse arrays.",
      "When taking the gradient with respect to a jax.experimental.sparse array, the gradient is computed only in the subspace defined by the array\u2019s sparsity pattern."
    ],
    "code_examples": [
      {
        "description": "Compute the gradient of a function involving a sparse BCOO array.",
        "code": "from jax.experimental import sparse\nimport jax.numpy as jnp\n\nX = sparse.BCOO.fromdense(jnp.arange(6.))\ny = jnp.ones(6)\n\ndef f(X, y):\n  return X @ y\n\ngrad_f = sparse.grad(f)\nresult = grad_f(X, y)\nprint(result)"
      },
      {
        "description": "Demonstrates using sparse.grad with a BCOO sparse array.",
        "code": "from jax.experimental import sparse\nimport jax.numpy as jnp\n\nX = sparse.BCOO.fromdense(jnp.arange(6.))\ny = jnp.ones(6)\n\ndef f(X, y):\n  return X @ y\n\ngrad_f = sparse.grad(f)\nresult = grad_f(X, y)\nprint(result)"
      }
    ]
  },
  {
    "title": "Sparse-aware value_and_grad",
    "concepts": [
      "This is a sparse-aware version of jax.value_and_grad().",
      "It computes the gradient with respect to a jax.experimental.sparse array.",
      "The gradient is computed in the subspace defined by the array\u2019s sparsity pattern.",
      "Arguments and return values are the same as jax.value_and_grad()."
    ],
    "code_examples": [
      {
        "description": "Example of using sparse.value_and_grad with a BCOO sparse array.",
        "code": "from jax.experimental import sparse\nimport jax.numpy as jnp\n\nX = sparse.BCOO.fromdense(jnp.arange(6.))\ny = jnp.ones(6)\nsparse.value_and_grad(lambda X, y: X @ y)(X, y)"
      },
      {
        "description": "Example of using sparse.value_and_grad with a BCOO sparse array.",
        "code": "from jax.experimental import sparse\nimport jax.numpy as jnp\n\nX = sparse.BCOO.fromdense(jnp.arange(6.))\ny = jnp.ones(6)\nsparse.value_and_grad(lambda X, y: X @ y)(X, y)"
      }
    ]
  },
  {
    "title": "Creating an Empty Sparse Array",
    "concepts": [
      "An empty sparse array can be created.",
      "The shape of the array is specified by a sequence of integers.",
      "The data type of the array can be specified.",
      "The data type of the index arrays can be specified.",
      "The matrix format can be specified as a string (e.g., 'bcoo').",
      "Additional keywords can be passed to the format-specific constructor."
    ],
    "code_examples": []
  },
  {
    "title": "Sparse Format Attribute",
    "concepts": [
      "The `sparse_format` attribute represents an empty sparse matrix."
    ],
    "code_examples": []
  },
  {
    "title": "2D Sparse Identity Matrix Creation",
    "concepts": [
      "The function creates a two-dimensional sparse matrix.",
      "The matrix has ones along the k-th diagonal.",
      "N specifies the number of rows.",
      "M specifies the number of columns, defaulting to N if None.",
      "k specifies the diagonal to place ones on, with 0 as the main diagonal.",
      "dtype specifies the data type of the matrix.",
      "index_dtype specifies the data type of the index arrays.",
      "format specifies the matrix format (e.g., 'bcoo').",
      "**kwds allows passing additional keywords to the format-specific _empty constructor."
    ],
    "code_examples": []
  },
  {
    "title": "Dense Matrix Conversion",
    "concepts": [
      "Convert input to a dense matrix.",
      "If input is already dense, it passes through."
    ],
    "code_examples": []
  },
  {
    "title": "Random BCOO Matrix Generation",
    "concepts": [
      "Function to generate a random BCOO matrix.",
      "The key argument is a PRNG key.",
      "The shape argument specifies the output matrix shape.",
      "The dtype argument specifies the output matrix dtype.",
      "indices_dtype argument specifies the dtype of the BCOO indices.",
      "nse argument specifies the number of specified elements, or a fraction of sparse dimensions.",
      "n_batch argument specifies the number of batch dimensions.",
      "n_dense argument specifies the number of dense dimensions.",
      "unique_indices argument specifies whether indices should be unique.",
      "sorted_indices argument specifies whether indices should be row-sorted.",
      "generator argument is a function for generating random values.",
      "The function returns a sparse.BCOO array."
    ],
    "code_examples": []
  },
  {
    "title": "Base class for JAX sparse objects",
    "concepts": [
      "Defines the base class for high-level JAX sparse objects.",
      "Includes attributes like shape, ndim, size, data, nse, and dtype.",
      "Includes methods like __init__, block_until_ready, sum, transpose, tree_flatten, and tree_unflatten."
    ],
    "code_examples": []
  },
  {
    "title": "BCOO Matrix Overview",
    "concepts": [
      "BCOO is an experimental batched COO matrix implemented in JAX.",
      "It stores data and indices in a batched COO format.",
      "The matrix has a defined shape.",
      "It has parameters like `indices_sorted` and `unique_indices`.",
      "`data` is an ndarray containing the explicitly stored data.",
      "`indices` is an ndarray containing the indices of the stored data. Duplicate entries will be summed."
    ],
    "code_examples": []
  },
  {
    "title": "Creating a Sparse Array from a Dense Array",
    "concepts": [
      "The `fromdense` method creates a sparse BCOO array from a dense JAX array.",
      "The example demonstrates creating a sparse array from a 2x3 dense array."
    ],
    "code_examples": [
      {
        "description": "Creating a sparse array from a dense array using BCOO.fromdense().",
        "code": "import jax.numpy as jnp\nfrom jax.experimental.sparse import BCOO\n\nM = jnp.array([[0., 2., 0.], [1., 0., 4.]])\nM_sp = BCOO.fromdense(M)\nprint(M_sp)"
      }
    ]
  },
  {
    "title": "Examining the Internal Representation",
    "concepts": [
      "The `data` attribute stores the non-zero values.",
      "The `indices` attribute stores the corresponding indices of the non-zero values.",
      "These attributes provide access to the underlying COO representation."
    ],
    "code_examples": [
      {
        "description": "Accessing the data and indices of a BCOO matrix.",
        "code": "import jax.numpy as jnp\nfrom jax.experimental.sparse import BCOO\n\nM = jnp.array([[0., 2., 0.], [1., 0., 4.]])\nM_sp = BCOO.fromdense(M)\n\nprint(M_sp.data)\nprint(M_sp.indices)"
      }
    ]
  },
  {
    "title": "Creating a Dense Array from a Sparse Array",
    "concepts": [
      "The `todense` method converts a sparse BCOO array back into a dense JAX array."
    ],
    "code_examples": [
      {
        "description": "Converting a sparse array to a dense array using BCOO.todense().",
        "code": "import jax.numpy as jnp\nfrom jax.experimental.sparse import BCOO\n\nM = jnp.array([[0., 2., 0.], [1., 0., 4.]])\nM_sp = BCOO.fromdense(M)\n\nprint(M_sp.todense())"
      }
    ]
  },
  {
    "title": "Creating a Sparse Array from COO Data & Indices",
    "concepts": [
      "The BCOO constructor can be used directly with data and indices.",
      "The `shape` parameter specifies the shape of the sparse matrix.",
      "This method allows explicit construction of a BCOO matrix."
    ],
    "code_examples": [
      {
        "description": "Creating a sparse array from data and indices using the BCOO constructor.",
        "code": "import jax.numpy as jnp\nfrom jax.experimental.sparse import BCOO\n\ndata = jnp.array([1., 3., 5.])\nindices = jnp.array([[0, 0], [1, 1], [2, 2]])\nmat = BCOO((data, indices), shape=(3, 3))\n\nprint(mat)\nprint(mat.todense())"
      }
    ]
  },
  {
    "title": "BCOO Parameters and Methods",
    "concepts": [
      "BCOO accepts `shape`, `indices_sorted`, and `unique_indices` parameters.",
      "It provides methods like `astype`, `block_until_ready`, `from_scipy_sparse`, `reshape`, `sort_indices`, `sum`, `sum_duplicates`, `todense`, `transpose`, `tree_flatten`, `tree_unflatten`, and `update_layout`.",
      "These methods offer various functionalities for manipulating the sparse matrix.",
      "It has attributes like `T`, `dtype`, `n_batch`, `n_dense`, `n_sparse`, `ndim`, `nse`, `size`, `data`, `indices`, `shape`, `indices_sorted`, `unique_indices`."
    ],
    "code_examples": []
  },
  {
    "title": "BCOO Array Expansion",
    "concepts": [
      "Expanding the size of a BCOO array.",
      "Increasing the rank of a BCOO array.",
      "Duplicating data for expansion.",
      "BCOO equivalence to jax.lax.broadcast_in_dim.",
      "Input: A BCOO-format array (mat).",
      "Input: The desired shape (shape).",
      "Input: Mapping of operand dimensions to target dimensions (broadcast_dimensions).",
      "Output: A BCOO-format array representing the expanded array."
    ],
    "code_examples": []
  },
  {
    "title": "Sparse Implementation of jax.lax.concatenate()",
    "concepts": [
      "The function implements a sparse version of jax.lax.concatenate() for BCOO arrays.",
      "It concatenates a sequence of BCOO arrays along a specified dimension.",
      "The arrays must have equal shapes except in the concatenation dimension.",
      "The arrays must have equivalent batch, sparse, and dense dimensions.",
      "Concatenation is supported along batch or sparse dimensions but not dense dimensions.",
      "The function returns a BCOO array containing the concatenation of the inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Description of Contraction Operation",
    "concepts": [
      "Contraction operation computes output at specified sparse indices.",
      "The operation involves two ndarray inputs: lhs and rhs.",
      "BCOO indices determine the output locations.",
      "DotDimensionNumbers specify contraction and batch dimensions.",
      "The result is a BCOO data array."
    ],
    "code_examples": []
  },
  {
    "title": "Sparse Dynamic Slice Implementation",
    "concepts": [
      "Implementation of `jax.lax.dynamic_slice` for BCOO arrays.",
      "The `mat` parameter is the BCOO array to slice.",
      "The `start_indices` parameter is a sequence of scalar indices, one per dimension.",
      "The `slice_sizes` parameter is the size of the slice and must be a sequence of non-negative integers.",
      "Inside a JIT compiled function, only static slice sizes are supported."
    ],
    "code_examples": []
  },
  {
    "title": "Extracting Values from a Dense Array Using Sparse Array Indices",
    "concepts": [
      "Extract values from a dense array based on the indices of a sparse array.",
      "The `sparr` parameter is a BCOO array providing the indices.",
      "The `arr` parameter is the dense ArrayLike from which values are extracted, with shape equal to `sparr.shape`.",
      "The `assume_unique` parameter controls how duplicate indices are handled; if True, values are extracted for every index, otherwise values are summed for duplicates.",
      "The function returns a BCOO array with the same sparsity pattern as `sparr` containing the extracted values."
    ],
    "code_examples": []
  },
  {
    "title": "BCOO Matrix Creation",
    "concepts": [
      "Creation of a BCOO-format sparse matrix from a dense matrix is possible.",
      "The 'mat' parameter represents the input array to be converted.",
      "The 'nse' parameter defines the number of specified elements in each batch.",
      "The 'n_batch' parameter defines the number of batch dimensions.",
      "The 'n_dense' parameter defines the number of block dimensions.",
      "The 'index_dtype' parameter defines the data type of sparse indices."
    ],
    "code_examples": []
  },
  {
    "title": "BCOO Gather Operation",
    "concepts": [
      "Describes a BCOO version of the lax.gather operation.",
      "The 'operand' is a BCOO array.",
      "Uses 'start_indices' as indices for gathering.",
      "GatherDimensionNumbers define how dimensions are gathered.",
      "slice_sizes defines the size of the slices to gather.",
      "unique_indices indicates if indices are unique.",
      "indices_are_sorted indicates if indices are sorted.",
      "mode defines the gather/scatter mode."
    ],
    "code_examples": []
  },
  {
    "title": "Element-wise Multiplication",
    "concepts": [
      "Element-wise multiplication between a sparse array (BCOO format) and a dense array (ndarray).",
      "lhs represents the sparse array in BCOO format.",
      "rhs represents the dense array (ndarray).",
      "The result is an ndarray."
    ],
    "code_examples": []
  },
  {
    "title": "Element-wise Multiplication of Sparse Arrays",
    "concepts": [
      "The operation performs element-wise multiplication.",
      "The input arrays `lhs` and `rhs` are in BCOO format.",
      "The output is a BCOO-format array containing the result of the multiplication."
    ],
    "code_examples": []
  },
  {
    "title": "BCOO Matrix Layout Update",
    "concepts": [
      "Updates the storage layout (n_batch & n_dense) of a BCOO matrix.",
      "Increasing mat.n_batch or mat.n_dense can lead to inefficient storage with many explicitly-stored zeros unless the new dimensions have size 0 or 1.",
      "SparseEfficiencyError is raised for inefficient reconfigurations.",
      "on_inefficient argument can be used to specify the behavior in case of an inefficient reconfiguration."
    ],
    "code_examples": []
  },
  {
    "title": "Function Parameters and Return Value",
    "concepts": [
      "mat (BCOO): Input BCOO array.",
      "n_batch (int | None): Optional number of batch dimensions. Defaults to mat.n_batch if None.",
      "n_dense (int | None): Optional number of dense dimensions. Defaults to mat.n_dense if None.",
      "on_inefficient (str | None): Optional string, one of ['error', 'warn', None]. Specifies behavior for inefficient reconfigurations.",
      "Returns: A BCOO array with the specified layout representing the same sparse array as the input."
    ],
    "code_examples": []
  },
  {
    "title": "Summation of BCOO Array Elements",
    "concepts": [
      "Summation of array elements over specified axes is performed.",
      "The input is a BCOO-format array.",
      "The shape of the resulting array needs to be specified.",
      "A sequence of axes is provided to define the summation dimensions.",
      "The output is also a BCOO-format array."
    ],
    "code_examples": []
  },
  {
    "title": "Sparse Reshape Implementation",
    "concepts": [
      "The function `jax.lax.reshape` is implemented for sparse arrays (BCOO).",
      "The `operand` is the BCOO array to be reshaped.",
      "`new_sizes` specifies the resulting shape of the reshaped array.",
      "The size of the final array must match the size of the input array.",
      "`new_sizes` must be specified so that batch, sparse, and dense dimensions do not mix.",
      "`dimensions` specifies the permutation order of the input shape.",
      "If specified, the length of `dimensions` must match the shape of the operand.",
      "`dimensions` must only permute among like dimensions: batch, sparse, and dense.",
      "The function returns the reshaped BCOO array."
    ],
    "code_examples": []
  },
  {
    "title": "Sparse Slice Implementation",
    "concepts": [
      "Sparse implementation of jax.lax.slice function.",
      "Takes a BCOO array as input.",
      "Requires start_indices and limit_indices to define the slice.",
      "Strides are not implemented.",
      "Returns a BCOO array containing the slice."
    ],
    "code_examples": []
  },
  {
    "title": "Sorting BCOO Array Indices",
    "concepts": [
      "The function sorts indices of a BCOO array.",
      "The input is a BCOO array.",
      "The output is a BCOO array with sorted indices."
    ],
    "code_examples": []
  },
  {
    "title": "Sparse Squeeze Implementation",
    "concepts": [
      "Implements a sparse version of jax.lax.squeeze.",
      "Squeezes dimensions of size 1 from a BCOO array.",
      "The input is a BCOO array.",
      "The dimensions argument specifies which dimensions to squeeze."
    ],
    "code_examples": []
  },
  {
    "title": "Summing Duplicate Indices in BCOO Arrays",
    "concepts": [
      "The function sums duplicate indices within a BCOO array.",
      "The function returns a BCOO array with sorted indices.",
      "The input is a BCOO array.",
      "The nse parameter specifies the number of specified elements in the output matrix, and must be specified for JIT compatibility.",
      "If nse is not specified, it will be computed based on data and index arrays.",
      "If nse is larger than necessary, data and index arrays will be padded with standard fill values.",
      "If nse is smaller than necessary, data elements will be dropped from the output matrix."
    ],
    "code_examples": []
  },
  {
    "title": "BCOO to Dense Matrix Conversion",
    "concepts": [
      "The purpose is to convert a batched sparse matrix (BCOO) into a dense matrix.",
      "The input 'mat' is a BCOO matrix.",
      "The output is the dense representation of the input matrix 'mat'.",
      "The dense version is named 'mat_dense'."
    ],
    "code_examples": []
  },
  {
    "title": "Transposing a BCOO Array",
    "concepts": [
      "Transposing a BCOO-format array rearranges its axes.",
      "The `mat` parameter is the BCOO array to be transposed.",
      "The `permutation` parameter specifies the new order of axes.",
      "The `permutation` should be a sequence of integers representing the desired axis order.",
      "The returned value is a BCOO-format array with transposed axes.",
      "Currently, batch axes cannot be permuted with non-batch axes.",
      "Currently, dense axes cannot be permuted with non-dense axes."
    ],
    "code_examples": []
  },
  {
    "title": "BCSR Matrix Overview",
    "concepts": [
      "Experimental batched CSR matrix implemented in JAX.",
      "The matrix is defined by arguments (args), shape, indices_sorted, and unique_indices."
    ],
    "code_examples": []
  },
  {
    "title": "BCSR Matrix Initialization",
    "concepts": [
      "The matrix is initialized with arguments (args), shape, and optional keyword arguments (indices_sorted, etc.).",
      "The __init__ method is used for initialization."
    ],
    "code_examples": []
  },
  {
    "title": "BCSR Matrix Creation Methods",
    "concepts": [
      "from_bcoo: Creates a BCSR array from a BCOO array.",
      "from_scipy_sparse: Creates a BCSR array from a scipy.sparse array.",
      "fromdense: Creates a BCSR array from a dense JAX Array."
    ],
    "code_examples": []
  },
  {
    "title": "BCSR Matrix Manipulation Methods",
    "concepts": [
      "block_until_ready(): Blocks until the value of the array is known.",
      "sum(*args, **kwargs): Computes the sum of the elements in the array.",
      "sum_duplicates([nse, remove_zeros]): Returns a copy of the array with duplicate indices summed.",
      "to_bcoo(): Converts the BCSR array to a BCOO array.",
      "todense(): Converts the BCSR array to a dense JAX Array.",
      "transpose(*args, **kwargs): Transposes the array.",
      "tree_flatten(): Flattens the array for JAX transformations.",
      "tree_unflatten(aux_data, children): Unflattens the array for JAX transformations."
    ],
    "code_examples": []
  },
  {
    "title": "BCSR Matrix Attributes",
    "concepts": [
      "T: Transpose of the matrix.",
      "dtype: Data type of the matrix elements.",
      "n_batch: Number of batches.",
      "n_dense: Number of dense dimensions.",
      "n_sparse: Number of sparse dimensions.",
      "ndim: Number of dimensions.",
      "nse: Number of specified elements.",
      "size: Number of elements in the matrix.",
      "data: Data array.",
      "indices: Indices array.",
      "indptr: Index pointer array.",
      "shape: Shape of the matrix.",
      "indices_sorted: Indicates if indices are sorted.",
      "unique_indices: Indicates if indices are unique."
    ],
    "code_examples": []
  },
  {
    "title": "Contraction Operation",
    "concepts": [
      "General contraction operation is described.",
      "lhs can be a BCSR sparse matrix or a dense array.",
      "rhs can be a BCSR sparse matrix or a dense array.",
      "dimension_numbers specifies contracting and batch dimensions.",
      "precision and preferred_element_type are unused.",
      "Result type depends on input types (BCSR if both inputs are sparse, ndarray otherwise)."
    ],
    "code_examples": []
  },
  {
    "title": "BCSR Matrix Value Extraction",
    "concepts": [
      "Extract values from a dense matrix.",
      "The matrix is represented in BCSR format using indices, indptr, and data.",
      "Indices array stores the column indices of the non-zero elements.",
      "Indptr array stores the index to the first element of each row in the indices array.",
      "Mat is a dense matrix from which to extract values."
    ],
    "code_examples": []
  },
  {
    "title": "BCSR Matrix Creation",
    "concepts": [
      "Convert a dense matrix to BCSR sparse matrix format.",
      "The input is an ArrayLike object.",
      "The 'nse' parameter specifies the number of stored elements in each batch.",
      "The 'n_batch' parameter represents the number of batch dimensions (default is 0).",
      "The 'n_dense' parameter represents the number of dense dimensions (default is 0).",
      "The 'index_dtype' parameter specifies the data type of sparse indices (default is int32).",
      "The output is a BCSR representation of the matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Sparse Matrix Conversion",
    "concepts": [
      "Conversion of batched sparse matrix (BCSR) to a dense matrix.",
      "Input is a BCSR matrix named 'mat'.",
      "Output is the dense version of the input matrix.",
      "The dense version is an array."
    ],
    "code_examples": []
  },
  {
    "title": "COO Matrix Overview",
    "concepts": [
      "Experimental COO matrix implementation in JAX.",
      "Limited compatibility with JAX transformations like grad and autodiff.",
      "`jax.experimental.sparse.BCOO` is generally preferred.",
      "Known failures may occur when `nse` exceeds the true number of nonzeros.",
      "The `BCOO` format handles this situation better."
    ],
    "code_examples": []
  },
  {
    "title": "COO Matrix Parameters",
    "concepts": [
      "args: Tuple of arrays representing the non-zero values, row indices, and column indices.",
      "shape: Tuple of integers representing the matrix dimensions.",
      "rows_sorted: Boolean indicating if row indices are sorted.",
      "cols_sorted: Boolean indicating if column indices are sorted."
    ],
    "code_examples": []
  },
  {
    "title": "COO Matrix Methods",
    "concepts": [
      "__init__: Constructor method to initialize the COO matrix.",
      "block_until_ready: Blocks until the computation is complete.",
      "fromdense: Creates a COO matrix from a dense matrix.",
      "sum: Computes the sum of elements along specified axes.",
      "todense: Converts the COO matrix to a dense matrix.",
      "transpose: Transposes the matrix.",
      "tree_flatten: Flattens the COO matrix for JAX transformations.",
      "tree_unflatten: Unflattens the COO matrix after JAX transformations."
    ],
    "code_examples": []
  },
  {
    "title": "COO Matrix Attributes",
    "concepts": [
      "T: Transpose of the matrix.",
      "dtype: Data type of the matrix elements.",
      "ndim: Number of dimensions (always 2 for a matrix).",
      "nse: Number of stored elements (non-zeros).",
      "size: Total number of elements in the matrix.",
      "data: Array of non-zero values.",
      "row: Array of row indices.",
      "col: Array of column indices.",
      "shape: Shape of the matrix."
    ],
    "code_examples": []
  },
  {
    "title": "Overview",
    "concepts": [
      "Experimental CSC matrix implementation in JAX.",
      "API is subject to change.",
      "Defines attributes like shape, dtype, ndim, nse, size, data, indices, and indptr.",
      "Provides methods for operations like fromdense, sum, todense, and transpose."
    ],
    "code_examples": []
  },
  {
    "title": "Methods",
    "concepts": [
      "__init__(args, *, shape): Initializes the CSC matrix.",
      "block_until_ready(): Blocks until the computation is complete.",
      "fromdense(mat, *[, nse, index_dtype]): Creates a CSC matrix from a dense matrix.",
      "sum(*args, **kwargs): Computes the sum of the matrix elements.",
      "todense(): Converts the CSC matrix to a dense matrix.",
      "transpose([axes]): Transposes the matrix.",
      "tree_flatten(): Flattens the matrix for JAX transformations.",
      "tree_unflatten(aux_data, children): Unflattens the matrix for JAX transformations."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes",
    "concepts": [
      "T: Transpose of the matrix.",
      "dtype: Data type of the matrix elements.",
      "ndim: Number of dimensions of the matrix.",
      "nse: Number of stored elements in the CSC matrix.",
      "size: Total number of elements in the matrix.",
      "data: Array containing the non-zero elements.",
      "indices: Row indices of the corresponding non-zero elements.",
      "indptr: Array indicating the start and end of each column in the data and indices arrays.",
      "shape: Shape of the matrix (tuple of integers)."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Experimental CSR Matrix in JAX",
    "concepts": [
      "This is an experimental CSR matrix implementation in JAX.",
      "It has minimal compatibility with JAX transforms like grad and autodiff.",
      "jax.experimental.sparse.BCOO is generally preferred over this class.",
      "There are known failures when nse is larger than the true number of nonzeros.",
      "BCOO handles the over-estimation of nse better."
    ],
    "code_examples": []
  },
  {
    "title": "CSR Matrix Attributes",
    "concepts": [
      "The CSR matrix has attributes such as T (transpose), dtype, ndim, nse (number of specified elements), size, data, indices, indptr (index pointer), and shape."
    ],
    "code_examples": []
  },
  {
    "title": "CSR Matrix Methods",
    "concepts": [
      "The CSR matrix provides methods for initialization (__init__), blocking until ready (block_until_ready), creating from a dense matrix (fromdense), summing elements (sum), converting to a dense matrix (todense), transposing (transpose), flattening for tree structures (tree_flatten), and unflattening for tree structures (tree_unflatten)."
    ],
    "code_examples": []
  },
  {
    "title": "COO Sparse Matrix Creation",
    "concepts": [
      "Create a COO-format sparse matrix from a dense matrix.",
      "The input 'mat' is the array to be converted to COO.",
      "'nse' represents the number of specified entries in 'mat'. It can be computed if not specified.",
      "'index_dtype' is the data type of sparse indices.",
      "The output is a COO representation of the matrix as 'mat_coo'."
    ],
    "code_examples": []
  },
  {
    "title": "COO Sparse Matrix Multiplication with a Dense Matrix",
    "concepts": [
      "Describes the product of a COO sparse matrix and a dense matrix.",
      "Specifies the input types: COO matrix and a dense array.",
      "Explains the shape and dtype requirements of the dense array.",
      "Introduces the transpose parameter to control sparse matrix transposition before multiplication.",
      "Defines the shape of the resulting matrix product."
    ],
    "code_examples": []
  },
  {
    "title": "COO Sparse Matrix and Dense Vector Product",
    "concepts": [
      "Describes the product of a COO sparse matrix and a dense vector.",
      "The `mat` parameter represents the COO matrix.",
      "The `v` parameter represents the one-dimensional dense array.",
      "The `transpose` parameter specifies whether to transpose the sparse matrix before computing the product.",
      "The output is an array representing the matrix-vector product."
    ],
    "code_examples": []
  },
  {
    "title": "COO to Dense Matrix Conversion",
    "concepts": [
      "Converting a COO-format sparse matrix to a dense matrix.",
      "Input is a COO matrix named 'mat'.",
      "Output is the dense matrix version of 'mat', named 'mat_dense'."
    ],
    "code_examples": []
  },
  {
    "title": "CSR Matrix Creation from Dense Matrix",
    "concepts": [
      "Create a CSR-format sparse matrix from a dense matrix.",
      "The 'mat' parameter is the dense array to be converted.",
      "The 'nse' parameter represents the number of specified entries in 'mat'; if not provided, it's computed.",
      "The 'index_dtype' parameter specifies the data type of sparse indices.",
      "The function returns a CSR representation of the input matrix."
    ],
    "code_examples": []
  },
  {
    "title": "CSR Sparse Matrix Multiplication",
    "concepts": [
      "Multiplication of a CSR sparse matrix with a dense matrix.",
      "Input: CSR matrix (mat) and a dense array (B).",
      "The shape of B depends on whether the sparse matrix is transposed.",
      "Transpose parameter specifies whether to transpose the sparse matrix before multiplication.",
      "Output: Array representing the matrix-vector product, with shape dependent on the transpose parameter."
    ],
    "code_examples": []
  },
  {
    "title": "CSR Sparse Matrix and Dense Vector Product",
    "concepts": [
      "The function computes the product of a CSR sparse matrix and a dense vector.",
      "It takes a CSR matrix (mat) and a one-dimensional array (v) as input.",
      "The transpose parameter specifies whether to transpose the sparse matrix before computing the product.",
      "The function returns an array representing the matrix-vector product.",
      "The size of the array depends on whether the matrix is transposed."
    ],
    "code_examples": []
  },
  {
    "title": "CSR to Dense Matrix Conversion",
    "concepts": [
      "Conversion of a CSR-format sparse matrix to a dense matrix.",
      "The input is a CSR matrix (mat).",
      "The output is the dense version of the input matrix (mat_dense)."
    ],
    "code_examples": []
  },
  {
    "title": "Sparse Direct Solver with QR Factorization",
    "concepts": [
      "The solver uses QR factorization for sparse matrices.",
      "The solver accepts sparse matrices in CSR format (data, indices, indptr arrays).",
      "Currently, only CUDA GPU backend is implemented.",
      "CPU backend falls back to scipy.sparse.linalg.spsolve.",
      "Neither CPU nor GPU support batching with vmap.",
      "The `data` parameter holds the non-zero entries of the CSR matrix.",
      "The `indices` parameter contains the column indices.",
      "The `indptr` parameter is the row pointer array.",
      "`b` is the right-hand side of the linear system.",
      "`tol` is the tolerance for singularity detection (defaults to 1e-6).",
      "`reorder` specifies the reordering scheme to reduce fill-in (0: no reordering, 1: symrcm, 2: symamd, 3: csrmetisnd; defaults to symrcm)."
    ],
    "code_examples": []
  },
  {
    "title": "LOBPCG Overview",
    "concepts": [
      "LOBPCG stands for Locally Optimal Block Preconditioned Conjugate Gradient.",
      "The method finds top-k eigenvectors.",
      "Only the standard eigenvalue problem A U = lambda U is supported.",
      "General eigenvalue problems are not supported.",
      "Gradient code is not available.",
      "Finding the smallest eigenvectors is not yet supported.",
      "Preconditioning is not yet supported.",
      "The implementation maintains an orthonormal basis for the block search directions.",
      "A specific convergence criterion is used, based on residual L2 norm.",
      "Soft locking is intentionally not implemented."
    ],
    "code_examples": []
  },
  {
    "title": "Arguments of LOBPCG",
    "concepts": [
      "A represents a square Hermitian matrix or a callable with its action.",
      "X represents the initial search directions for the k desired top eigenvectors.",
      "X needs to be numerically linearly independent and will be orthonormalized.",
      "The condition 0 < k * 5 < n must be satisfied.",
      "m is the maximum integer iteration count, limiting the Krylov basis explored.",
      "tol is a float convergence tolerance based on the residual L2 norm.",
      "An eigenpair is considered converged when its residual L2 norm r = |A v - lambda v| is below tol * 10 * n * (lambda + |A v|).",
      "If tol is None, it is set to the float epsilon of A.dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Returns of LOBPCG",
    "concepts": [
      "LOBPCG returns theta, U, and i.",
      "theta is a (k,) array of eigenvalues.",
      "U is a (n, k) array of eigenvectors.",
      "i is the number of iterations performed."
    ],
    "code_examples": []
  },
  {
    "title": "Error Handling",
    "concepts": [
      "ValueError is raised if A, X dtypes or n dimensions do not match.",
      "ValueError is raised if k is too large (k * 5 < n is not satisfied).",
      "ValueError is raised if k == 0."
    ],
    "code_examples": []
  },
  {
    "title": "X64 Mode Context Manager",
    "concepts": [
      "Experimental context manager to temporarily enable X64 mode.",
      "The context manager allows for operations to be performed using float64 precision.",
      "It affects the dtype of jnp.asarray conversions within the context."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of the enable_x64 context manager.",
        "code": "x = np.arange(5, dtype='float64')\nwith enable_x64():\n    print(jnp.asarray(x).dtype)"
      },
      {
        "description": "Another example to show the effect of the enable_x64 context manager.",
        "code": "x = np.arange(5, dtype='float64')\nwith enable_x64():\n    print(jnp.asarray(x).dtype)"
      }
    ]
  },
  {
    "title": "Disabling X64 Mode Temporarily",
    "concepts": [
      "The context manager `disable_x64` temporarily disables X64 mode.",
      "This affects the default dtype of JAX arrays."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the use of the `disable_x64` context manager to change the default dtype to float32.",
        "code": "x = np.arange(5, dtype='float64')\nwith disable_x64():\n    print(jnp.asarray(x).dtype)"
      },
      {
        "description": "Another example demonstrating the usage of `disable_x64` context manager.",
        "code": "x = np.arange(5, dtype='float64')\nwith disable_x64():\n    print(jnp.asarray(x).dtype)"
      }
    ]
  },
  {
    "title": "Introduction to jax.lib",
    "concepts": [
      "jax.lib is a set of internal tools and types.",
      "It bridges JAX\u2019s Python frontend and its XLA backend."
    ],
    "code_examples": []
  },
  {
    "title": "get_backend Function",
    "concepts": [
      "get_backend([platform]) retrieves the backend."
    ],
    "code_examples": []
  },
  {
    "title": "get_compile_options Function",
    "concepts": [
      "get_compile_options(num_replicas, num_partitions) returns compile options derived from flag values.",
      "It takes the number of replicas and partitions as input."
    ],
    "code_examples": []
  },
  {
    "title": "register_custom_call_target Function",
    "concepts": [
      "register_custom_call_target(name, fn[, ...]) registers a custom call target.",
      "It takes the name and the function as input."
    ],
    "code_examples": []
  },
  {
    "title": "Platform Specification",
    "concepts": [
      "The platform can be None, a string, or an xla_client.Client object.",
      "xla_client.Client is a valid type for the platform."
    ],
    "code_examples": []
  },
  {
    "title": "Compile Options and Parameters",
    "concepts": [
      "The function returns compile options derived from flag values.",
      "num_replicas specifies the number of replicas for compilation.",
      "num_partitions specifies the number of partitions for compilation.",
      "device_assignment assigns logical replicas to physical devices.",
      "use_spmd_partitioning enables SPMD or MPMD partitioning in XLA.",
      "use_shardy_partitioner enables the Shardy partitioner in XLA (experimental).",
      "use_auto_spmd_partitioning enables automatic XLA sharding generation for SPMD.",
      "auto_spmd_partitioning_mesh_shape defines the device mesh shape for auto-spmd partitioning search.",
      "auto_spmd_partitioning_mesh_ids defines the device ids for auto-spmd partitioning search.",
      "env_options_overrides is a dict of additional compiler options.",
      "fdo_profile is an optional profile for feedback-directed optimization.",
      "detailed_logging indicates if detailed compilation logging is desired.",
      "backend is the client, if available."
    ],
    "code_examples": []
  },
  {
    "title": "Registering a Custom Call Target",
    "concepts": [
      "Registers a custom call target for use in XLA.",
      "The 'name' parameter specifies the function's name as a string.",
      "The 'fn' parameter is a PyCapsule object containing the function pointer.",
      "The 'platform' parameter specifies the target platform.",
      "The 'api_version' parameter indicates the XLA FFI version (0 for untyped, 1 for typed).",
      "The 'traits' parameter represents custom call traits corresponding to XLA FFI handler traits."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to the Document",
    "concepts": [
      "This is a dummy technical document.",
      "The document serves as a sample for information extraction.",
      "The goal is to parse the document and structure its content into a JSON format."
    ],
    "code_examples": []
  },
  {
    "title": "Simple Python Function",
    "concepts": [
      "A simple Python function is defined.",
      "The function takes two numbers as input.",
      "The function returns the sum of the two numbers."
    ],
    "code_examples": [
      {
        "description": "A simple Python function that adds two numbers.",
        "code": "def add(x, y):\n  return x + y"
      }
    ]
  },
  {
    "title": "Using the Addition Function",
    "concepts": [
      "The previously defined addition function is used.",
      "The function is called with the arguments 5 and 3.",
      "The result of the function call is printed to the console."
    ],
    "code_examples": [
      {
        "description": "Calling the add function and printing the result.",
        "code": "def add(x, y):\n  return x + y\n\nresult = add(5, 3)\nprint(result)"
      }
    ]
  },
  {
    "title": "Working with Lists",
    "concepts": [
      "A list of numbers is created.",
      "The sum of all numbers in the list is calculated.",
      "The sum is printed to the console."
    ],
    "code_examples": [
      {
        "description": "Calculating the sum of numbers in a list.",
        "code": "numbers = [1, 2, 3, 4, 5]\nsum_of_numbers = sum(numbers)\nprint(sum_of_numbers)"
      }
    ]
  },
  {
    "title": "Conditional Statements",
    "concepts": [
      "Demonstrates an if-else conditional statement.",
      "Checks if a number is even or odd.",
      "Prints the corresponding message."
    ],
    "code_examples": [
      {
        "description": "Checking if a number is even or odd.",
        "code": "number = 7\n\nif number % 2 == 0:\n  print(\"The number is even.\")\nelse:\n  print(\"The number is odd.\")"
      }
    ]
  },
  {
    "title": "Context Manager for jax_check_tracer_leaks",
    "concepts": [
      "The context manager is for the jax_check_tracer_leaks configuration option.",
      "It turns on checking for leaked tracers as soon as a trace completes.",
      "Enabling leak checking may have performance impacts.",
      "Some Python debuggers can cause false positives when leak checking is enabled."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for NaN Debugging",
    "concepts": [
      "This context manager configures NaN checks in JAX.",
      "It adds NaN checks to every JAX operation.",
      "It helps identify the source of NaNs in JIT-compiled computations by calling the un-compiled version of the code."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for `jax_debug_infs`",
    "concepts": [
      "Context manager for managing the `jax_debug_infs` configuration option.",
      "Adding inf checks to every operation in JAX.",
      "Debugging JIT-compiled computations by identifying the source of infinites.",
      "Falling back to un-compiled versions to pinpoint the problematic operation."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for jax_default_device",
    "concepts": [
      "Context manager is used to configure the default device for JAX operations.",
      "Setting the value to a Device object makes that device the default.",
      "Setting the value to None uses the system default device.",
      "This configuration affects JIT compiled functions.",
      "It does not affect multi-device computations."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for Matmul Precision",
    "concepts": [
      "The `jax_default_matmul_precision` context manager controls the default matmul and conv precision for 32bit inputs.",
      "TPUs offer configurable precision levels, balancing accuracy and speed.",
      "Precision can be controlled for each operation individually.",
      "This option controls the default precision level for matrix multiplication and convolution on 32bit inputs when a specific precision is not given.",
      "Precision levels include 'bfloat16' (fastest, least precise), 'float32' (full precision), and 'tensorfloat32' (intermediate).",
      "This parameter can also specify an accumulation algorithm for matrix multiplications using `DotAlgorithmPreset`."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for jax_default_prng_impl",
    "concepts": [
      "This section describes a context manager.",
      "The context manager configures the jax_default_prng_impl option.",
      "The option selects the default PRNG implementation.",
      "The implementation is used when one is not explicitly provided at seeding time.",
      "The new_val parameter represents the new PRNG implementation value."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for jax_enable_checks",
    "concepts": [
      "Context manager for managing the jax_enable_checks configuration option.",
      "Enables invariant checking for JAX internals.",
      "Invariant checking slows down the execution."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for jax_enable_custom_prng",
    "concepts": [
      "Context manager for enabling or disabling custom PRNG.",
      "Enables an internal upgrade allowing custom PRNG implementations.",
      "The option will be enabled by default in future JAX versions.",
      "Using the flag will be deprecated after default enablement."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for jax_enable_custom_vjp_by_custom_transpose",
    "concepts": [
      "Context manager for enabling or disabling the jax_enable_custom_vjp_by_custom_transpose configuration option.",
      "The configuration option is transient, meaning it only applies within the context.",
      "This upgrade implements jax.custom_vjp by reduction to jax.custom_jvp and jax.custom_transpose.",
      "This functionality will become the default behavior in future versions of JAX.",
      "Using this flag will be deprecated in future versions of JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for jax_log_compiles",
    "concepts": [
      "This context manager configures the jax_log_compiles option.",
      "It logs a message each time jit or pmap compiles an XLA computation.",
      "Logging is performed using the Python logging module.",
      "The log level is WARNING when the option is set, otherwise it's DEBUG."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for jax_numpy_rank_promotion",
    "concepts": [
      "Context manager is used for jax_numpy_rank_promotion config option.",
      "Controls NumPy-style automatic rank promotion broadcasting.",
      "Possible values: 'allow', 'warn', or 'raise'."
    ],
    "code_examples": []
  },
  {
    "title": "Context Manager for Transfer Guard Level",
    "concepts": [
      "A contextmanager is used to control the transfer guard level for all transfers.",
      "The transfer guard level is thread-local.",
      "The `new_val` parameter specifies the new transfer guard level.",
      "Refer to https://jax.readthedocs.io/en/latest/transfer_guard.html for more information."
    ],
    "code_examples": []
  },
  {
    "title": "Disable JIT Compilation with `jax.disable_jit()`",
    "concepts": [
      "The `jax.disable_jit()` context manager disables JIT compilation within its scope.",
      "It disables explicit `jit()` calls and implicit JIT compilation used by JAX.",
      "Individual primitive operations are still compiled by XLA.",
      "Values dependent on jitted function arguments are traced and abstracted.",
      "Abstracted values are represented by instances like ShapedArray.",
      "`disable_jit()` allows observing concrete values during debugging.",
      "The `disable` parameter (boolean) can enable or disable the context manager."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how JIT compilation abstracts values when printing inside a jitted function, resulting in a 'Traced<ShapedArray>' output.",
        "code": "import jax\n\n@jax.jit\ndef f(x):\n  y = x * 2\n  print(\"Value of y is\", y)\n  return y + 3\n\nprint(f(jax.numpy.array([1, 2, 3])))"
      },
      {
        "description": "Illustrates the use of `jax.disable_jit()` to see concrete values by disabling JIT compilation and printing the actual array.",
        "code": "import jax\n\n@jax.jit\ndef f(x):\n  y = x * 2\n  print(\"Value of y is\", y)\n  return y + 3\n\nwith jax.disable_jit():\n  print(f(jax.numpy.array([1, 2, 3])))"
      }
    ]
  },
  {
    "title": "Introduction to ensure_compile_time_eval",
    "concepts": [
      "jax.jit() and jax.lax.scan() involve staging, which delays numerical expression evaluation.",
      "Delayed evaluation can be undesirable in certain situations.",
      "The context manager ensures eager evaluation of JAX computations.",
      "A ConcretizationTypeError is raised if eager evaluation is not possible."
    ],
    "code_examples": []
  },
  {
    "title": "Contrived Example of ensure_compile_time_eval Usage",
    "concepts": [
      "The context manager jax.ensure_compile_time_eval forces eager evaluation within its scope.",
      "This allows values computed within the context to be used in Python control flow.",
      "The example shows how a boolean value `z_positive` derived from JAX operations can be used in an `if` statement."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using jax.ensure_compile_time_eval to allow a JAX-computed boolean value to control Python control flow.",
        "code": "import jax\nimport jax.numpy as jnp\n@jax.jit\ndef f(x):\n    with jax.ensure_compile_time_eval():\n        y = jnp.sin(3.0)\n        z = jnp.sin(y)\n        z_positive = z > 0\n        if z_positive:\n            # z_positive is usable in Python control flow\n            return jnp.sin(x)\n        else:\n            return jnp.cos(x)"
      },
      {
        "description": "Demonstrates using jax.ensure_compile_time_eval to allow a JAX-computed boolean value to control Python control flow.",
        "code": "import jax\nimport jax.numpy as jnp\n@jax.jit\ndef f(x):\n    with jax.ensure_compile_time_eval():\n        y = jnp.sin(3.0)\n        z = jnp.sin(y)\n        z_positive = z > 0\n        if z_positive:\n            # z_positive is usable in Python control flow\n            return jnp.sin(x)\n        else:\n            return jnp.cos(x)"
      }
    ]
  },
  {
    "title": "Real-World Example: Matrix Multiplication",
    "concepts": [
      "This example showcases a practical use case from jax-ml/jax#3974.",
      "It performs matrix multiplication and other operations within the context manager.",
      "The results are used to compute a final value."
    ],
    "code_examples": [
      {
        "description": "Example from jax-ml/jax#3974 demonstrating the use of jax.ensure_compile_time_eval with random number generation and matrix multiplication.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import random\n@jax.jit\ndef jax_fn(x):\n    with jax.ensure_compile_time_eval():\n        y = random.randint(random.key(0), (1000, 1000), 0, 100)\n        y2 = y @ y\n        x2 = jnp.sum(y2) * x\n    return x2"
      },
      {
        "description": "Example from jax-ml/jax#3974 demonstrating the use of jax.ensure_compile_time_eval with random number generation and matrix multiplication.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import random\n@jax.jit\ndef jax_fn(x):\n    with jax.ensure_compile_time_eval():\n        y = random.randint(random.key(0), (1000, 1000), 0, 100)\n        y2 = y @ y\n        x2 = jnp.sum(y2) * x\n    return x2"
      }
    ]
  },
  {
    "title": "Alternative: Hoisting Constant Expressions",
    "concepts": [
      "An alternative to using the context manager is to 'hoist' constant expressions out of the staging API.",
      "This can achieve similar behavior in many cases.",
      "Hoisting involves pre-computing the constant values outside of the jitted function."
    ],
    "code_examples": [
      {
        "description": "Demonstrates hoisting the random number generation outside the jitted function as an alternative to using jax.ensure_compile_time_eval.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import random\n\ny = random.randint(random.key(0), (1000, 1000), 0, 100)\n@jax.jit\ndef jax_fn(x):\n    y2 = y @ y\n    x2 = jnp.sum(y2) * x\n    return x2"
      },
      {
        "description": "Demonstrates hoisting the random number generation outside the jitted function as an alternative to using jax.ensure_compile_time_eval.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import random\n\ny = random.randint(random.key(0), (1000, 1000), 0, 100)\n@jax.jit\ndef jax_fn(x):\n    y2 = y @ y\n    x2 = jnp.sum(y2) * x\n    return x2"
      }
    ]
  },
  {
    "title": "Conclusion",
    "concepts": [
      "The context manager can be more convenient than hoisting in some situations."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to eval_shape",
    "concepts": [
      "eval_shape computes the shape and dtype of a function's output without performing any FLOPs.",
      "It is useful for shape inference.",
      "It uses JAX's abstract interpretation machinery.",
      "It can catch shape errors, similar to evaluating the function directly.",
      "Arguments to the function can be arrays, scalars, or nested Python containers.",
      "Array arguments need only be duck-typed to have shape and dtype attributes, such as jax.ShapeDtypeStruct.",
      "It returns a nested PyTree containing jax.ShapeDtypeStruct objects as leaves."
    ],
    "code_examples": [
      {
        "description": "Definition of the eval_shape function using JAX's shape and dtype inference.",
        "code": "def eval_shape(fun, *args, **kwargs):\n    out = fun(*args, **kwargs)\n    shape_dtype_struct = lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype)\n    return jax.tree_util.tree_map(shape_dtype_struct, out)"
      }
    ]
  },
  {
    "title": "Basic Usage of eval_shape with jax.ShapeDtypeStruct",
    "concepts": [
      "This example demonstrates how to use eval_shape with jax.ShapeDtypeStruct to infer the output shape and dtype of a simple dot product and tanh operation.",
      "jax.ShapeDtypeStruct is used to represent the input arrays without actual data."
    ],
    "code_examples": [
      {
        "description": "Example using eval_shape to determine the output shape of a jnp.tanh(jnp.dot(A, x)) operation, using ShapeDtypeStruct for inputs.",
        "code": "import jax\nimport jax.numpy as jnp\n\nf = lambda A, x: jnp.tanh(jnp.dot(A, x))\n\nA = jax.ShapeDtypeStruct((2000, 3000), jnp.float32)\nx = jax.ShapeDtypeStruct((3000, 1000), jnp.float32)\n\nout = jax.eval_shape(f, A, x) # no FLOPs performed\n\nprint(out.shape)\nprint(out.dtype)"
      }
    ]
  },
  {
    "title": "Using eval_shape with functools.partial for static arguments",
    "concepts": [
      "This example demonstrates how to use functools.partial() to include static arguments when using eval_shape().",
      "All arguments passed directly to eval_shape are treated as dynamic.",
      "Static arguments can be passed by closing over them using partial."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the use of functools.partial with eval_shape, using lax.conv_general_dilated with predefined window_strides and padding.",
        "code": "import jax\nfrom jax import lax\nfrom functools import partial\nimport jax.numpy as jnp\n\nx = jax.ShapeDtypeStruct((1, 1, 28, 28), jnp.float32)\nkernel = jax.ShapeDtypeStruct((32, 1, 3, 3), jnp.float32)\n\nconv_same = partial(lax.conv_general_dilated, window_strides=(1, 1), padding=\"SAME\")\n\nout = jax.eval_shape(conv_same, x, kernel)\n\nprint(out.shape)\nprint(out.dtype)"
      }
    ]
  },
  {
    "title": "ShapeDtypeStruct Overview",
    "concepts": [
      "ShapeDtypeStruct is a container for array attributes.",
      "It stores the shape, dtype, and sharding of an array.",
      "It is often used with jax.eval_shape().",
      "The attributes of a ShapeDtypeStruct are shape, dtype, sharding and weak_type."
    ],
    "code_examples": []
  },
  {
    "title": "ShapeDtypeStruct Attributes",
    "concepts": [
      "The shape attribute represents the array shape as a sequence of integers.",
      "The dtype attribute represents the data type of the array.",
      "The sharding attribute represents how the array is distributed across devices.",
      "The weak_type attribute indicates if the type is weakly inferred.",
      "The layout attribute represents the memory layout of the array.",
      "The ndim attribute represents the number of array dimensions.",
      "The size attribute represents the total number of elements in the array."
    ],
    "code_examples": []
  },
  {
    "title": "Transferring Data to a Device",
    "concepts": [
      "Transfers an array, scalar, or container to a specified device.",
      "The 'device' parameter specifies the destination device, sharding, or layout.",
      "The 'src' parameter specifies the source device or sharding.",
      "The 'donate' parameter indicates whether the input can be overwritten.",
      "The 'may_alias' parameter controls whether the input can be aliased.",
      "If 'device' is None, transfers to the default device if not already on any device.",
      "The function operates asynchronously."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jax.device_get",
    "concepts": [
      "Transfers an array or pytree `x` to the host (CPU) memory.",
      "If `x` is a PyTree, its buffers are copied in parallel.",
      "The function accepts an array, scalar, or nested Python container of arrays as input.",
      "It returns an array or nested Python container representing the value of `x` on the host."
    ],
    "code_examples": []
  },
  {
    "title": "Example: Passing a JAX Array",
    "concepts": [
      "Demonstrates how to transfer a JAX array from a device (e.g., GPU) to the host.",
      "The `jax.device_get` function retrieves the array's value on the host."
    ],
    "code_examples": [
      {
        "description": "Transfers a JAX array to the host and prints its value.",
        "code": "import jax\nimport jax.numpy\n\nx = jax.numpy.array([1., 2., 3.])\njax.device_get(x)"
      },
      {
        "description": "Transfers a JAX array to the host and prints its value.",
        "code": "import jax\nimport jax.numpy\n\nx = jax.numpy.array([1., 2., 3.])\njax.device_get(x)"
      }
    ]
  },
  {
    "title": "Example: Passing a Scalar",
    "concepts": [
      "Illustrates the behavior of `jax.device_get` when a scalar is passed as input.",
      "Passing a scalar has no effect, as scalars are typically already on the host."
    ],
    "code_examples": [
      {
        "description": "Demonstrates passing a scalar to jax.device_get.",
        "code": "import jax\n\njax.device_get(1)"
      },
      {
        "description": "Demonstrates passing a scalar to jax.device_get.",
        "code": "import jax\n\njax.device_get(1)"
      }
    ]
  },
  {
    "title": "Related Functions",
    "concepts": [
      "References to related functions: `device_put`, `device_put_sharded`, and `device_put_replicated`."
    ],
    "code_examples": []
  },
  {
    "title": "XLA Backend Platform Name",
    "concepts": [
      "The function returns the platform name of the default XLA backend."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to named_scope",
    "concepts": [
      "named_scope adds user-specified names to the JAX name stack.",
      "JAX does not preserve names of Python functions during JIT compilation by default.",
      "named_scope annotates underlying operations with names for debugging.",
      "Names are preserved in XLA compilation and jax2tf conversion.",
      "The name argument is a prefix for operations within the scope.",
      "named_scope can be used as a context manager or a decorator."
    ],
    "code_examples": []
  },
  {
    "title": "named_scope as a Context Manager",
    "concepts": [
      "named_scope can be used as a context manager inside compiled functions.",
      "It allows for naming specific operations within a function.",
      "Names are appended to the active name stack."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating named_scope as a context manager within a jitted function to name dot product and activation operations.",
        "code": "import jax\n\n@jax.jit\ndef layer(w, x):\n  with jax.named_scope(\"dot_product\"):\n    logits = w.dot(x)\n  with jax.named_scope(\"activation\"):\n    return jax.nn.relu(logits)"
      },
      {
        "description": "Example demonstrating named_scope as a context manager within a jitted function to name dot product and activation operations.",
        "code": "import jax\n\n@jax.jit\ndef layer(w, x):\n  with jax.named_scope(\"dot_product\"):\n    logits = w.dot(x)\n  with jax.named_scope(\"activation\"):\n    return jax.nn.relu(logits)"
      }
    ]
  },
  {
    "title": "named_scope as a Decorator",
    "concepts": [
      "named_scope can also be used as a decorator for functions.",
      "This applies the naming scope to the entire function."
    ],
    "code_examples": [
      {
        "description": "Example showing named_scope used as a decorator on a jitted function to name the entire layer.",
        "code": "import jax\n\n@jax.jit\n@jax.named_scope(\"layer\")\ndef layer(w, x):\n  logits = w.dot(x)\n  return jax.nn.relu(logits)"
      },
      {
        "description": "Example showing named_scope used as a decorator on a jitted function to name the entire layer.",
        "code": "import jax\n\n@jax.jit\n@jax.named_scope(\"layer\")\ndef layer(w, x):\n  logits = w.dot(x)\n  return jax.nn.relu(logits)"
      }
    ]
  },
  {
    "title": "Description of `block_until_ready` on PyTree Leaves",
    "concepts": [
      "The function attempts to call `block_until_ready` on the leaves of a PyTree.",
      "The input `x` is a PyTree, typically containing JAX array instances at its leaves.",
      "The output is a PyTree with the same structure and values as the input.",
      "In the output PyTree, all JAX array leaves are ready."
    ],
    "code_examples": []
  },
  {
    "title": "Asynchronous Copy to Host for PyTree Leaves",
    "concepts": [
      "The function attempts to initiate asynchronous data transfer to the host for each leaf in a PyTree.",
      "It targets JAX array leaves specifically, invoking the `copy_to_host_async` method if available.",
      "Non-JAX array leaves or leaves lacking the `copy_to_host_async` method are ignored.",
      "The input is a PyTree (x), potentially containing JAX array instances.",
      "The output is a PyTree with the same structure and values, but with host copies initiated for JAX array leaves."
    ],
    "code_examples": []
  },
  {
    "title": "Mesh Creation with jax.make_mesh",
    "concepts": [
      "Creates an efficient mesh with specified shape and axis names.",
      "Automatically computes a mapping from logical axes to a physical mesh.",
      "Device ordering takes into account the physical topology of the TPU.",
      "Logical axes affect the ordering of devices."
    ],
    "code_examples": [
      {
        "description": "Creating a mesh with 8 devices and a single axis 'x'.",
        "code": "mesh = jax.make_mesh((8,), ('x'))\n[d.id for d in mesh.devices.flat]"
      },
      {
        "description": "Creating a mesh with 16 devices and two axes 'x' and 'y' (2x8).",
        "code": "mesh = jax.make_mesh((2, 8), ('x', 'y'))\n[d.id for d in mesh.devices.flat]"
      },
      {
        "description": "Creating a mesh with 16 devices and two axes 'x' and 'y' (4x4).",
        "code": "mesh = jax.make_mesh((4, 4), ('x', 'y'))\n[d.id for d in mesh.devices.flat]"
      },
      {
        "description": "Creating a mesh with 16 devices and two axes 'x' and 'y' (2x8) - repeated.",
        "code": "mesh = jax.make_mesh((2, 8), ('x', 'y'))\n[d.id for d in mesh.devices.flat]"
      },
      {
        "description": "Creating a mesh with 16 devices and two axes 'x' and 'y' (4x4) - repeated.",
        "code": "mesh = jax.make_mesh((4, 4), ('x', 'y'))\n[d.id for d in mesh.devices.flat]"
      }
    ]
  },
  {
    "title": "Alternative Mesh Creation",
    "concepts": [
      "jax.experimental.mesh_utils.create_device_mesh can be used for extra arguments.",
      "Extra arguments include contiguous_submeshes and allow_split_physical_axes."
    ],
    "code_examples": []
  },
  {
    "title": "Arguments of jax.make_mesh",
    "concepts": [
      "axis_shapes: Shape of the mesh (Sequence[int]).",
      "axis_names: Names of the mesh axes (Sequence[str]).",
      "devices: Optional keyword only argument to specify the devices.",
      "auto_axes: TypeOfAxis | None",
      "explicit_axes: TypeOfAxis | None",
      "manual_axes: TypeOfAxis | None"
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "Returns a jax.sharding.Mesh object.",
      "The returned object is a mesh_lib.Mesh"
    ],
    "code_examples": []
  },
  {
    "title": "Function Gradient Evaluation",
    "concepts": [
      "The document describes a function to evaluate the gradient of a given function.",
      "The function to be differentiated, `fun`, should return a scalar value.",
      "The `argnums` parameter specifies which arguments to differentiate with respect to.",
      "The `has_aux` parameter indicates whether the function returns auxiliary data.",
      "The `holomorphic` parameter indicates if the function is holomorphic.",
      "The `allow_int` parameter controls differentiation with respect to integer inputs.",
      "The `reduce_axes` parameter accepts a sequence of axis names."
    ],
    "code_examples": [
      {
        "description": "Calculates the gradient of the hyperbolic tangent function at 0.2 using JAX.",
        "code": "import jax\n\ngrad_tanh = jax.grad(jax.numpy.tanh)\nprint(grad_tanh(0.2))"
      },
      {
        "description": "Calculates the gradient of the hyperbolic tangent function at 0.2 using JAX. This example is duplicated from the previous one in the document.",
        "code": "import jax\n\ngrad_tanh = jax.grad(jax.numpy.tanh)\nprint(grad_tanh(0.2))"
      }
    ]
  },
  {
    "title": "Function and Gradient Evaluation",
    "concepts": [
      "The document describes a function that evaluates a given function and its gradient.",
      "The function takes a callable 'fun' as input, which represents the function to be differentiated.",
      "The 'argnums' parameter specifies which positional arguments to differentiate with respect to.",
      "The 'has_aux' parameter indicates whether the function returns auxiliary data.",
      "The 'holomorphic' parameter indicates whether the function is holomorphic.",
      "The 'allow_int' parameter controls differentiation with respect to integer inputs.",
      "The 'reduce_axes' parameter takes a sequence of AxisName.",
      "The function returns a pair containing the function's value and its gradient."
    ],
    "code_examples": []
  },
  {
    "title": "Alias of jax.jacrev()",
    "concepts": [
      "This entry describes an alias for the jax.jacrev() function.",
      "The function signature includes a Callable argument.",
      "The function signature includes an argnums argument which is an int or a sequence of ints.",
      "The function signature includes a has_aux argument which is a boolean.",
      "The function signature includes a holomorphic argument which is a boolean.",
      "The function signature includes an allow_int argument which is a boolean."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jacrev",
    "concepts": [
      "The `jacrev` function computes the Jacobian of a function using reverse-mode automatic differentiation.",
      "The `fun` argument is the function whose Jacobian is to be computed.",
      "The `argnums` argument specifies which positional argument(s) to differentiate with respect to (default 0).",
      "The `has_aux` argument indicates whether the function returns a pair of (output, auxiliary_data).",
      "The `holomorphic` argument indicates whether fun is promised to be holomorphic.",
      "The `allow_int` argument specifies whether to allow differentiating with respect to integer valued inputs.",
      "The function returns the Jacobian of the input function.",
      "The Jacobian is evaluated row-by-row."
    ],
    "code_examples": []
  },
  {
    "title": "Example usage of jacrev",
    "concepts": [
      "This example demonstrates how to use `jax.jacrev` to compute the Jacobian of a function.",
      "The function `f(x)` takes a NumPy array `x` as input and returns a NumPy array.",
      "The `jax.jacrev(f)(x)` expression computes the Jacobian of `f` evaluated at `x`.",
      "The result is a 2D NumPy array representing the Jacobian matrix."
    ],
    "code_examples": [
      {
        "description": "Computes the Jacobian of a function f(x) using jax.jacrev.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x):\n    return jnp.asarray(\n    [x[0], 5 * x[2], 4 * x[1]**2 - 2 * x[2], x[2] * jnp.sin(x[0])])\n\nprint(jax.jacrev(f)(jnp.array([1., 2., 3.])))"
      },
      {
        "description": "Computes the Jacobian of a function f(x) using jax.jacrev.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef f(x):\n    return jnp.asarray(\n    [x[0], 5 * x[2], 4 * x[1]**2 - 2 * x[2], x[2] * jnp.sin(x[0])])\n\nprint(jax.jacrev(f)(jnp.array([1., 2., 3.])))"
      }
    ]
  },
  {
    "title": "Hessian Computation with JAX",
    "concepts": [
      "The `jax.hessian` function computes the Hessian of a given function.",
      "The function to be differentiated (`fun`) should accept arrays, scalars, or Python containers as arguments.",
      "The `argnums` parameter specifies which positional arguments to differentiate with respect to.",
      "The `has_aux` parameter indicates if the function returns auxiliary data.",
      "The `holomorphic` parameter indicates if the function is holomorphic.",
      "`jax.hessian` supports nested Python containers (pytrees) as inputs and outputs.",
      "The tree structure of `jax.hessian(fun)(x)` is a tree product of `fun(x)` and two copies of `x`.",
      "The shape of the Hessian leaf is determined by the shapes of the corresponding leaves in `fun(x)` and `x`."
    ],
    "code_examples": [
      {
        "description": "Example of computing the Hessian of a function `g` with respect to a vector input.",
        "code": "import jax\nimport jax.numpy\n\ng = lambda x: x[0]**3 - 2*x[0]*x[1] - x[1]**6\nprint(jax.hessian(g)(jax.numpy.array([1., 2.])))"
      },
      {
        "description": "Example of computing the Hessian of a function `f` with a dictionary input containing jax arrays.",
        "code": "import jax.numpy as jnp\n\nf = lambda dct: {\"c\": jnp.power(dct[\"a\"], dct[\"b\"])}\nprint(jax.hessian(f)({\"a\": jnp.arange(2.) + 1., \"b\": jnp.arange(2.) + 2.}))"
      }
    ]
  },
  {
    "title": "Jacobian-Vector Product Computation",
    "concepts": [
      "Computes a forward-mode Jacobian-vector product.",
      "The function to be differentiated should accept arrays, scalars, or standard Python containers.",
      "The primals are the evaluation points for the Jacobian.",
      "The tangents are the tangent vector for the Jacobian-vector product.",
      "The has_aux parameter indicates if the function returns auxiliary data."
    ],
    "code_examples": [
      {
        "description": "Computes the Jacobian-vector product of the sine function at a point.",
        "code": "import jax\n\nprimals, tangents = jax.jvp(jax.numpy.sin, (0.1,), (0.2,))\nprint(primals)\nprint(tangents)"
      },
      {
        "description": "Computes the Jacobian-vector product of the sine function at a point - repeated example.",
        "code": "import jax\n\nprimals, tangents = jax.jvp(jax.numpy.sin, (0.1,), (0.2,))\nprint(primals)\nprint(tangents)"
      }
    ]
  },
  {
    "title": "Introduction to linearize()",
    "concepts": [
      "linearize() produces a linear approximation of a function using jvp() and partial evaluation.",
      "The function takes the function to be differentiated and the primal values as input.",
      "It returns the value of the function at the primal values and a function that evaluates the Jacobian-vector product.",
      "linearize() uses partial evaluation to avoid re-linearizing the function on subsequent calls to the Jacobian-vector product function.",
      "linearize() has a similar signature to vjp().",
      "linearize() is useful for evaluating a pushforward for many different input tangent vectors at the same linearization point."
    ],
    "code_examples": []
  },
  {
    "title": "Comparison with jvp() and vmap()",
    "concepts": [
      "linearize() behaves like a curried jvp() but uses partial evaluation.",
      "Using vmap() and jvp() together can be more efficient than linearize() if all input tangent vectors are known at once.",
      "vmap() and jvp() avoid the stored-linearization memory cost incurred by linearize() and vjp()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates that linearize and jvp compute the same values",
        "code": "y,\nout_tangent = jax.jvp(f, (x,), (in_tangent,))\ny, f_jvp = jax.linearize(f, x)\nout_tangent = f_jvp(in_tangent)"
      },
      {
        "description": "Shows how to use vmap with jvp as an alternative to linearize when all input tangents are known.",
        "code": "pushfwd = partial(jvp, f, (x,))\ny, out_tangents = vmap(pushfwd, out_axes=(None, 0))((in_tangents,))"
      }
    ]
  },
  {
    "title": "Example Usage of linearize()",
    "concepts": [
      "Demonstrates how to use linearize() to approximate a function and evaluate its Jacobian-vector product at different points.",
      "The example defines a simple function and uses linearize() to obtain a linear approximation around a specific point.",
      "The returned function can then be used to efficiently compute the Jacobian-vector product for different tangent vectors."
    ],
    "code_examples": [
      {
        "description": "Illustrates a complete example of using `jax.linearize`.",
        "code": "import jax\nimport jax.numpy as jnp\n\n\ndef f(x):\n    return 3. * jnp.sin(x) + jnp.cos(x / 2.)\n\n\njax.jvp(f, (2.,), (3.,))\n\ny, f_jvp = jax.linearize(f, 2.)\nprint(y)\nprint(f_jvp(3.))\nprint(f_jvp(4.))"
      }
    ]
  },
  {
    "title": "Introduction to Vector-Jacobian Product (vjp)",
    "concepts": [
      "vjp() computes the reverse-mode vector-Jacobian product of a function.",
      "grad() is a special case of vjp().",
      "The function to be differentiated should accept arrays, scalars, or standard Python containers of arrays or scalars.",
      "The function should return an array, scalar, or standard Python container of arrays or scalars.",
      "Primals are the values at which the Jacobian of the function is evaluated.",
      "has_aux indicates whether the function returns auxiliary data alongside the output.",
      "vjpfun is a function that maps a cotangent vector to a tuple of cotangent vectors, representing the vector-Jacobian product."
    ],
    "code_examples": []
  },
  {
    "title": "Basic vjp() Usage Example",
    "concepts": [
      "Demonstrates how to use jax.vjp() to compute the vector-Jacobian product of a function.",
      "The function f(x, y) returns a tuple of sine of x and cosine of y.",
      "jax.vjp() returns the primal values and a vjp function.",
      "The vjp function is then used to compute the vector-Jacobian product with a cotangent vector.",
      "The result is a tuple of cotangent vectors corresponding to x and y."
    ],
    "code_examples": [
      {
        "description": "Compute the vector-Jacobian product of a function f(x, y) = (sin(x), cos(y)) at x=0.5 and y=1.0.",
        "code": "import jax\nimport jax.numpy\n\ndef f(x, y):\n    return jax.numpy.sin(x), jax.numpy.cos(y)\n\nprimals, f_vjp = jax.vjp(f, 0.5, 1.0)\nxbar, ybar = f_vjp((-0.7, 0.3))\nprint(xbar)\nprint(ybar)"
      },
      {
        "description": "Compute the vector-Jacobian product of a function f(x, y) = (sin(x), cos(y)) at x=0.5 and y=1.0.",
        "code": "import jax\nimport jax.numpy\n\ndef f(x, y):\n    return jax.numpy.sin(x), jax.numpy.cos(y)\n\nprimals, f_vjp = jax.vjp(f, 0.5, 1.0)\nxbar, ybar = f_vjp((-0.7, 0.3))\nprint(xbar)\nprint(ybar)"
      }
    ]
  },
  {
    "title": "Custom VJP Rules with custom_gradient",
    "concepts": [
      "The `custom_gradient` wrapper provides a convenient way to define custom VJP rules.",
      "It is an alternative to `jax.custom_vjp` and inspired by TensorFlow's `tf.custom_gradient` API.",
      "The decorated function should return a pair: the primal value and the VJP function.",
      "The VJP function takes a cotangent and returns cotangents for the inputs.",
      "Lexical closure can be used to share work between the forward and backward passes.",
      "Control flow depending on closed-over values or cotangent arguments is prohibited in the VJP function."
    ],
    "code_examples": [
      {
        "description": "Example of using `custom_gradient` to define the gradient of x**2.",
        "code": "@jax.custom_gradient\ndef f(x):\n  return x**2, lambda g: (g * x,)\n\nprint(f(3.))\nprint(jax.grad(f)(3.))"
      },
      {
        "description": "Example of using `custom_gradient` to define the gradient of x*y.",
        "code": "@jax.custom_gradient\ndef f(x, y):\n  return x * y, lambda g: (g * y, g * x)\n\nprint(f(3., 4.))\nprint(jax.grad(f, argnums=(0, 1))(3., 4.))"
      }
    ]
  },
  {
    "title": "Introduction to jax.checkpoint()",
    "concepts": [
      "jax.checkpoint() (aliased as jax.remat()) is used to trade off computation time and memory in automatic differentiation.",
      "It's beneficial with reverse-mode autodiff like jax.grad() and jax.vjp(), and jax.linearize().",
      "Reverse-mode differentiation stores linearization points (inputs to nonlinear operations) during the forward pass for use in the backward pass.",
      "Storing all linearization points can lead to high memory costs.",
      "jax.checkpoint() allows recomputing linearization points instead of storing them.",
      "Recomputation reduces memory usage but increases computation.",
      "The decorator returns a new function that recomputes linearization points when differentiated, potentially saving memory."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of jax.checkpoint()",
    "concepts": [
      "fun: The function to be decorated, arguments and return values should be arrays, scalars, or (nested) standard Python containers.",
      "prevent_cse: A boolean that prevents common subexpression elimination (CSE) optimizations in the HLO generated from differentiation, defaults to True.",
      "prevent_cse=False might be needed inside a scan().",
      "static_argnums: An int or sequence of ints indicating which argument values to specialize for tracing and caching purposes.",
      "static_argnums can avoid ConcretizationTypeErrors when tracing, but it introduces re-tracing overheads.",
      "policy: A callable, should be one of the attributes of jax.checkpoint_policies. It takes as input a type-level specification of a first-order primitive application and returns a boolean indicating whether the corresponding output value(s) can be saved as residuals (or instead must be recomputed in the (co)tangent computation if needed)."
    ],
    "code_examples": []
  },
  {
    "title": "Simple Example of jax.checkpoint() Usage",
    "concepts": [
      "jax.checkpoint() can be used to reduce memory usage during differentiation.",
      "The decorator doesn't change the output value or gradient, but it alters the computation strategy.",
      "Without the decorator, intermediate values (e.g., jnp.cos(x)) are stored for the backward pass.",
      "With the decorator, only the primal inputs are stored, and intermediate values are recomputed during the backward pass."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the basic usage of jax.checkpoint(). The same value is produced whether or not the decorator is present, but the computation strategy differs.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.checkpoint\ndef g(x):\n    y = jnp.sin(x)\n    z = jnp.sin(y)\n    return z\n\njax.value_and_grad(g)(2.0)"
      },
      {
        "description": "Example with no checkpointing",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef g(x):\n    y = jnp.sin(x)\n    z = jnp.sin(y)\n    return z\n\njax.value_and_grad(g)(2.0)"
      }
    ]
  },
  {
    "title": "Recursive Application of jax.checkpoint()",
    "concepts": [
      "jax.checkpoint() can be applied recursively to express sophisticated autodiff rematerialization strategies."
    ],
    "code_examples": [
      {
        "description": "Shows how to apply jax.checkpoint recursively.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef recursive_checkpoint(funs):\n    if len(funs) == 1:\n        return funs[0]\n    elif len(funs) == 2:\n        f1, f2 = funs\n        return lambda x: f1(f2(x))\n    else:\n        f1 = recursive_checkpoint(funs[:len(funs) // 2])\n        f2 = recursive_checkpoint(funs[len(funs) // 2:])\n        return lambda x: f1(jax.checkpoint(f2)(x))"
      }
    ]
  },
  {
    "title": "Using static_argnums with jax.checkpoint()",
    "concepts": [
      "The static_argnums parameter is necessary when Python control flow depends on argument values.",
      "Using static_argnums allows the if statement\u2019s condition to depend on the value of an argument.",
      "The cost to using static_argnums is that it introduces re-tracing overheads across calls.",
      "jax.ensure_compile_time_eval is sometimes needed in conjunction with static_argnums."
    ],
    "code_examples": [
      {
        "description": "Illustrates using static_argnums to handle control flow that depends on argument values.",
        "code": "from functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.checkpoint, static_argnums=(1,))\ndef foo(x, is_training):\n    if is_training:\n        x = x + 1\n    else:\n        x = x - 1\n    return x\n\njax.grad(foo)(jnp.array(2.0), True)"
      },
      {
        "description": "Example using jax.ensure_compile_time_eval",
        "code": "from functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.checkpoint, static_argnums=(1,))\ndef foo(x, y):\n    with jax.ensure_compile_time_eval():\n        y_pos = y > 0\n    if y_pos:\n        x = x + 1\n    else:\n        x = x - 1\n    return x\n\njax.grad(foo)(jnp.array(2.0), jnp.array(1.0))"
      }
    ]
  },
  {
    "title": "Alternative to static_argnums",
    "concepts": [
      "Instead of using static_argnums and jax.ensure_compile_time_eval, compute values outside the jax.checkpoint()-decorated function and close over them."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to custom_jvp",
    "concepts": [
      "The `custom_jvp` class is used as a function decorator to define custom JVP rules.",
      "It allows overriding the default automatic differentiation behavior of JAX.",
      "It provides a way to specify how the JVP (Jacobian-vector product) is computed for a function.",
      "The `defjvp()` method is used to define a single JVP rule for all inputs.",
      "The `defjvps()` method is a convenience wrapper to define JVPs for each argument separately."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the use of `custom_jvp` decorator and `defjvp` method to define a custom JVP rule for a function.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.custom_jvp\ndef f(x, y):\n  return jnp.sin(x) * y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, y = primals\n  x_dot, y_dot = tangents\n  primal_out = f(x, y)\n  tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n  return primal_out, tangent_out"
      },
      {
        "description": "Redundant Example demonstrating the use of `custom_jvp` decorator and `defjvp` method to define a custom JVP rule for a function.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.custom_jvp\ndef f(x, y):\n  return jnp.sin(x) * y\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n  x, y = primals\n  x_dot, y_dot = tangents\n  primal_out = f(x, y)\n  tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n  return primal_out, tangent_out"
      }
    ]
  },
  {
    "title": "Methods and Attributes",
    "concepts": [
      "__init__(fun[, nondiff_argnums]): Constructor for the custom_jvp class.",
      "defjvp(jvp[, symbolic_zeros]): Defines the custom JVP rule.",
      "defjvps(*jvps): Defines separate JVPs for each argument.",
      "jvp: Attribute representing the JVP rule.",
      "symbolic_zeros: Attribute related to symbolic zeros.",
      "fun: The underlying function being decorated.",
      "nondiff_argnums: Specifies arguments that should not be differentiated."
    ],
    "code_examples": []
  },
  {
    "title": "Custom JVP Rule Definition",
    "concepts": [
      "Defines a custom JVP (Jacobian-vector product) rule for a function.",
      "The `jvp` argument is a callable that computes the primal and tangent outputs.",
      "The `jvp` function accepts a tuple of primals and a tuple of tangents as input.",
      "The `jvp` function returns a tuple containing the primal output and the tangent output.",
      "The `symbolic_zeros` argument controls whether symbolic zeros are passed as tangents.",
      "Setting `symbolic_zeros` to True allows the JVP rule to detect non-differentiated inputs.",
      "Using symbolic zeros requires special handling for these objects."
    ],
    "code_examples": [
      {
        "description": "Defining a custom JVP rule for the function f(x, y) = sin(x) * y.  The f_jvp function calculates both the primal and tangent outputs using the chain rule.",
        "code": ">>>\n@jax\n.\ncustom_jvp\n...\ndef f(x, y):\n...  return jnp.sin(x) * y\n...\n>>>\n@f.defjvp\n...\ndef f_jvp(primals, tangents):\n...  x, y = primals\n...  x_dot, y_dot = tangents\n...  primal_out = f(x, y)\n...  tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n...  return primal_out, tangent_out"
      },
      {
        "description": "Defining a custom JVP rule for the function f(x, y) = sin(x) * y.  The f_jvp function calculates both the primal and tangent outputs using the chain rule.  Demonstration of using the value_and_grad function after defining the custom JVP.",
        "code": ">>>\n@jax\n.\ncustom_jvp\n...\ndef f(x, y):\n...  return jnp.sin(x) * y\n...\n>>>\n@f.defjvp\n...\ndef f_jvp(primals, tangents):\n...  x, y = primals\n...  x_dot, y_dot = tangents\n...  primal_out = f(x, y)\n...  tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n...  return primal_out, tangent_out\n>>> x = jnp.float32(1.0)\n>>> y = jnp.float32(2.0)\n>>> with jnp.printoptions(precision=2):\n...  print(jax.value_and_grad(f)(x, y))\n(Array(1.68, dtype=float32), Array(1.08, dtype=float32))"
      },
      {
        "description": "Defining a custom JVP rule for the function f(x, y) = sin(x) * y.  The f_jvp function calculates both the primal and tangent outputs using the chain rule.  Demonstration of using the value_and_grad function after defining the custom JVP.",
        "code": ">>>\n@jax\n.\ncustom_jvp\n...\ndef f(x, y):\n...  return jnp.sin(x) * y\n...\n>>>\n@f.defjvp\n...\ndef f_jvp(primals, tangents):\n...  x, y = primals\n...  x_dot, y_dot = tangents\n...  primal_out = f(x, y)\n...  tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n...  return primal_out, tangent_out\n>>> x = jnp.float32(1.0)\n>>> y = jnp.float32(2.0)\n>>> with jnp.printoptions(precision=2):\n...  print(jax.value_and_grad(f)(x, y))\n(Array(1.68, dtype=float32), Array(1.08, dtype=float32))"
      }
    ]
  },
  {
    "title": "Convenience Wrapper for Defining JVPs with `defjvps`",
    "concepts": [
      "The `defjvps` method is a convenience wrapper for defining JVP (Jacobian-vector product) rules for each argument of a custom function separately.",
      "It cannot be used with `nondiff_argnums`.",
      "It takes a sequence of functions, one for each positional argument of the `custom_jvp` function.",
      "Each JVP function's arguments include the tangent value for the corresponding primal input, the primal output, and all primal inputs."
    ],
    "code_examples": [
      {
        "description": "Define a custom JVP rule for a function f(x, y) = sin(x) * y using `defjvps`. The JVPs are defined as d/dx (sin(x) * y) = cos(x) * x_dot * y and d/dy (sin(x) * y) = sin(x) * y_dot.",
        "code": "@jax.custom_jvp\ndef f(x, y):\n  return jnp.sin(x) * y\n\nf.defjvps(\n    lambda x_dot, primal_out, x, y: jnp.cos(x) * x_dot * y,\n    lambda y_dot, primal_out, x, y: jnp.sin(x) * y_dot\n)"
      },
      {
        "description": "Example usage of the defined function `f` with `jax.value_and_grad` to compute both the value and the gradient of the function at a specific point (x=1.0, y=2.0). The output is formatted to a precision of 2 decimal places.",
        "code": "x = jnp.float32(1.0)\ny = jnp.float32(2.0)\n\nwith jnp.printoptions(precision=2):\n  print(jax.value_and_grad(f)(x, y))"
      }
    ]
  },
  {
    "title": "Custom VJP Rule Definition",
    "concepts": [
      "Defines a custom VJP (Vector-Jacobian Product) rule for a function.",
      "The `fwd` function represents the forward pass and returns the primal output and residual values.",
      "The `bwd` function represents the backward pass and takes residual values and the output cotangent as input, returning a tuple of input cotangents.",
      "The output of `bwd` must match the structure of the primal input arguments."
    ],
    "code_examples": []
  },
  {
    "title": "Symbolic Zeros",
    "concepts": [
      "The `symbolic_zeros` option enables the detection of unperturbed inputs and zero cotangents.",
      "When `symbolic_zeros` is True, the `fwd` function receives `CustomVJPPrimal` objects with `value` and `perturbed` attributes.",
      "The `bwd` function receives symbolic zero objects in its cotangent argument corresponding to unperturbed values.",
      "Enabling `symbolic_zeros` requires special handling due to the modified input types of `fwd` and `bwd`."
    ],
    "code_examples": []
  },
  {
    "title": "Optimize Remat",
    "concepts": [
      "The `optimize_remat` option enables an automatic optimization when used under `jax.remat()`.",
      "This is most useful when the `fwd` rule is an opaque call like a Pallas kernel or custom call."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of Custom VJP",
    "concepts": [
      "Demonstrates how to define a custom VJP rule for a function using `jax.custom_vjp`.",
      "The example shows the definition of the `fwd` and `bwd` functions.",
      "It shows how to register the custom VJP rule with `f.defvjp(f_fwd, f_bwd)`.",
      "Uses `jax.value_and_grad` to compute the value and gradient of the function."
    ],
    "code_examples": [
      {
        "description": "Defines a custom VJP rule for the function f(x, y) = jnp.sin(x) * y using jax.custom_vjp.",
        "code": "@jax.custom_vjp\ndef f(x, y):\n  return jnp.sin(x) * y\n\ndef f_fwd(x, y):\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)"
      },
      {
        "description": "Calculates the value and gradient of the function f(x, y) at x=1.0 and y=2.0 using jax.value_and_grad.",
        "code": "x = jnp.float32(1.0)\ny = jnp.float32(2.0)\nwith jnp.printoptions(precision=2):\n  print(jax.value_and_grad(f)(x, y))"
      }
    ]
  },
  {
    "title": "Introduction to custom_vmap",
    "concepts": [
      "The custom_vmap decorator customizes the behavior of a JAX function under jax.vmap().",
      "A custom_vmap-decorated function behaves like the original except when batched using jax.vmap().",
      "The def_vmap() method defines the custom rule that will be applied when using jax.vmap().",
      "custom_vmap is part of the jax.custom_batching module.",
      "custom_vmap functions do not support reverse-mode autodiff."
    ],
    "code_examples": []
  },
  {
    "title": "Basic custom_vmap example",
    "concepts": [
      "Demonstrates how to use custom_vmap to override the default behavior of a function under jax.vmap().",
      "The def_vmap decorator is used to define a custom vmap rule.",
      "The example shows how to change element-wise addition to element-wise multiplication."
    ],
    "code_examples": [
      {
        "description": "Defines a function f that normally returns x + y, but returns x * y when vmapped.",
        "code": "@jax.custom_batching.custom_vmap\ndef f(x, y):\n  return x + y\n\n@f.def_vmap\ndef f_vmap_rule(axis_size, in_batched, xs, ys):\n  assert all(in_batched)\n  assert xs.shape[0] == axis_size\n  assert ys.shape[0] == axis_size\n  out_batched = True\n  return xs * ys, out_batched\n\nxs = jnp.arange(3)\nys = jnp.arange(1, 4)\njax.vmap(f)(xs, ys)"
      },
      {
        "description": "Defines a function f that normally returns x + y, but returns x * y when vmapped.",
        "code": "@jax.custom_batching.custom_vmap\ndef f(x, y):\n  return x + y\n\n@f.def_vmap\ndef f_vmap_rule(axis_size, in_batched, xs, ys):\n  assert all(in_batched)\n  assert xs.shape[0] == axis_size\n  assert ys.shape[0] == axis_size\n  out_batched = True\n  return xs * ys, out_batched\n\nxs = jnp.arange(3)\nys = jnp.arange(1, 4)\njax.vmap(f)(xs, ys)"
      }
    ]
  },
  {
    "title": "Combining custom_vmap with custom_vjp",
    "concepts": [
      "Demonstrates how to combine custom_vmap with jax.custom_vjp to customize both vmap and reverse-mode autodiff.",
      "The jax.custom_vjp decorator must wrap the custom_vmap-decorated function.",
      "The example shows how to define custom forward and backward pass functions."
    ],
    "code_examples": [
      {
        "description": "Combines custom_vmap with custom_vjp to customize both vmap and reverse-mode autodiff for the function f(x, y) = sin(x) * y.",
        "code": "@jax.custom_vjp\n@jax.custom_batching.custom_vmap\ndef f(x, y):\n  return jnp.sin(x) * y\n\n@f.def_vmap\ndef f_vmap_rule(axis_size, in_batched, xs, ys):\n  return jnp.cos(xs) * ys, True\n\ndef f_fwd(x, y):\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)\n\njax.vmap(f)(jnp.zeros(3), jnp.ones(3))\njax.grad(f)(jnp.zeros(()), jnp.ones(()))"
      },
      {
        "description": "Combines custom_vmap with custom_vjp to customize both vmap and reverse-mode autodiff for the function f(x, y) = sin(x) * y.",
        "code": "@jax.custom_vjp\n@jax.custom_batching.custom_vmap\ndef f(x, y):\n  return jnp.sin(x) * y\n\n@f.def_vmap\ndef f_vmap_rule(axis_size, in_batched, xs, ys):\n  return jnp.cos(xs) * ys, True\n\ndef f_fwd(x, y):\n  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n\ndef f_bwd(res, g):\n  cos_x, sin_x, y = res\n  return (cos_x * g * y, sin_x * g)\n\nf.defvjp(f_fwd, f_bwd)\n\njax.vmap(f)(jnp.zeros(3), jnp.ones(3))\njax.grad(f)(jnp.zeros(()), jnp.ones(()))"
      }
    ]
  },
  {
    "title": "Method details",
    "concepts": [
      "__init__(fun): Initializes a custom_vmap function.",
      "def_vmap(vmap_rule): Defines the vmap rule for the custom_vmap function."
    ],
    "code_examples": []
  },
  {
    "title": "Attribute details",
    "concepts": [
      "fun: The original function being wrapped.",
      "vmap_rule: The custom vmap rule defined for the function."
    ],
    "code_examples": []
  },
  {
    "title": "Custom vmap Rule Definition",
    "concepts": [
      "The `vmap_rule` is a function that defines how a custom function is vectorized.",
      "The `vmap_rule` takes `axis_size`, a pytree of booleans indicating batched arguments, and the batched arguments themselves as input.",
      "The `vmap_rule` returns a tuple of the batched output and a pytree of booleans indicating which output elements are batched.",
      "This method passes the rule through, returning vmap_rule unchanged."
    ],
    "code_examples": []
  },
  {
    "title": "Sequential Vmap Explanation",
    "concepts": [
      "sequential_vmap is a special case of custom_vmap that uses a loop.",
      "Decorating a function with sequential_vmap causes it to be called sequentially within a loop when batched.",
      "This is useful for functions that don't natively support batch dimensions."
    ],
    "code_examples": [
      {
        "description": "Example of sequential_vmap usage with print statements to demonstrate sequential execution.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.custom_batching.sequential_vmap\ndef f(x):\n  jax.debug.print(\"{}\", x)\n  return x + 1\n\njax.vmap(f)(jnp.arange(3))"
      },
      {
        "description": "Another example of sequential_vmap usage with print statements to demonstrate sequential execution.",
        "code": "import jax\nimport jax.numpy as jnp\n\n@jax.custom_batching.sequential_vmap\ndef f(x):\n  jax.debug.print(\"{}\", x)\n  return x + 1\n\njax.vmap(f)(jnp.arange(3))"
      }
    ]
  },
  {
    "title": "Introduction to jax.Array",
    "concepts": [
      "jax.Array is the public interface for instance checks and type annotation of JAX arrays and tracers.",
      "jax.Array is used for instance checks using isinstance().",
      "jax.Array is used for type annotations in JAX functions.",
      "Array creation routines from jax.numpy should be used instead of directly creating jax.Array instances."
    ],
    "code_examples": [
      {
        "description": "Demonstrates instance check using isinstance() with jax.Array.",
        "code": "import jax.numpy as jnp\nimport jax\n\nx = jnp.arange(5)\nisinstance(x, jax.Array)"
      },
      {
        "description": "Demonstrates using jax.Array for type annotations in a JAX function.",
        "code": "import jax.numpy as jnp\nimport jax\nfrom jax import Array\n\ndef f(x: Array) -> Array:\n  return x"
      },
      {
        "description": "Demonstrates instance check using isinstance() with jax.Array.",
        "code": "import jax.numpy as jnp\nimport jax\n\nx = jnp.arange(5)\nisinstance(x, jax.Array)"
      },
      {
        "description": "Demonstrates using jax.Array for type annotations in a JAX function.",
        "code": "import jax.numpy as jnp\nimport jax\nfrom jax import Array\n\ndef f(x: Array) -> Array:\n  return x"
      }
    ]
  },
  {
    "title": "Methods of jax.Array",
    "concepts": [
      "jax.Array has a variety of methods for array manipulation and computation.",
      "Methods include mathematical operations like all, any, argmax, argmin, cumsum, prod, sum, mean, std, var, dot, etc.",
      "Methods include array manipulation operations like reshape, flatten, transpose, swapaxes, take, repeat, etc.",
      "Methods include data access and conversion operations like astype, copy, item, view, etc.",
      "Methods include searching and sorting operations like argpartition, argsort, searchsorted, sort, etc."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes of jax.Array",
    "concepts": [
      "jax.Array has various attributes providing information about the array.",
      "Attributes include shape, dtype, size, ndim, nbytes, itemsize, etc.",
      "Attributes related to memory and sharding include sharding, addressable_shards, global_shards, is_fully_addressable, is_fully_replicated, committed, etc.",
      "Attributes for accessing transposed views (T, mT).",
      "Attributes for accessing real and imaginary parts (real, imag).",
      "Attributes for device placement (device)."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jax.make_array_from_single_device_arrays",
    "concepts": [
      "This function constructs a global jax.Array from single-device arrays, sharded according to a given sharding scheme.",
      "The input 'arrays' should be a sequence of jax.Arrays, each residing on a single device.",
      "The length of 'arrays' must match the number of addressable devices in the sharding.",
      "The shape of each array in 'arrays' must be the same.",
      "For multi-process code, each process provides its own set of arrays corresponding to its local data.",
      "Shape is the shape of the output jax.Array, serving as a double-check.",
      "Sharding describes how the output jax.Array is laid out across devices.",
      "Arrays are commonly created via jax.device_put."
    ],
    "code_examples": [
      {
        "description": "Creates a sharded jax.Array using make_array_from_single_device_arrays.  It defines a mesh, sharding, and some input data, and then uses jax.device_put to place parts of the data on different devices according to the sharding. Finally, it creates a global sharded array using make_array_from_single_device_arrays.",
        "code": "import math\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\nmesh_rows = 2\nmesh_cols = jax.device_count() // 2\nglobal_shape = (8, 8)\nmesh = Mesh(np.array(jax.devices()).reshape(mesh_rows, mesh_cols), ('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\ninp_data = np.arange(math.prod(global_shape)).reshape(global_shape)\n\narrays = [\n    jax.device_put(inp_data[index], d)\n    for d, index in sharding.addressable_devices_indices_map(global_shape).items()\n]\n\narr = jax.make_array_from_single_device_arrays(global_shape, sharding, arrays)\nassert arr.shape == (8, 8)\n# arr.shape is (8,8) regardless of jax.device_count()"
      },
      {
        "description": "Creates a sharded jax.Array using make_array_from_single_device_arrays.  It defines a mesh, sharding, and some input data, and then uses jax.device_put to place parts of the data on different devices according to the sharding. Finally, it creates a global sharded array using make_array_from_single_device_arrays.",
        "code": "import math\nfrom jax.sharding import Mesh\nfrom jax.sharding import PartitionSpec as P\nimport numpy as np\nimport jax\n\nmesh_rows = 2\nmesh_cols = jax.device_count() // 2\nglobal_shape = (8, 8)\nmesh = Mesh(np.array(jax.devices()).reshape(mesh_rows, mesh_cols), ('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\ninp_data = np.arange(math.prod(global_shape)).reshape(global_shape)\n\narrays = [\n    jax.device_put(inp_data[index], d)\n    for d, index in sharding.addressable_devices_indices_map(global_shape).items()\n]\n\narr = jax.make_array_from_single_device_arrays(global_shape, sharding, arrays)\nassert arr.shape == (8, 8)\n# arr.shape is (8,8) regardless of jax.device_count()"
      }
    ]
  },
  {
    "title": "Related Function",
    "concepts": [
      "jax.make_array_from_process_local_data can be used to convert a local array to a global jax.Array."
    ],
    "code_examples": []
  },
  {
    "title": "Addressable Shards",
    "concepts": [
      "Document contains a list of addressable shards."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of Array Element Testing",
    "concepts": [
      "Test if all array elements along a given axis evaluate to True.",
      "Refer to jax.numpy.all() for full documentation.",
      "Arguments include the array itself (self), the axis for reduction, output array (out), keepdims option, and where condition."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of `any` Function",
    "concepts": [
      "Tests whether any array elements along a given axis evaluate to True.",
      "Refer to jax.numpy.any() for the full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "self (Array): The input array.",
      "axis (reductions.Axis): The axis along which to perform the reduction.",
      "out (None): Not applicable.",
      "keepdims (bool): Determines if the reduced axes should be kept in the result as dimensions with size one.",
      "where (ArrayLike | None): Elements to include in checking for True."
    ],
    "code_examples": []
  },
  {
    "title": "Description of argmax Function",
    "concepts": [
      "The function returns the index of the maximum value in an array.",
      "It refers to jax.numpy.argmax() for complete documentation.",
      "The function accepts Array as self argument.",
      "The function accepts axis (int | None) argument.",
      "The function accepts out (None) argument.",
      "The function accepts keepdims (bool | None) argument."
    ],
    "code_examples": []
  },
  {
    "title": "Description of argmin function",
    "concepts": [
      "Returns the index of the minimum value in an array.",
      "Refers to jax.numpy.argmin() for full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of argmin function",
    "concepts": [
      "self represents the input array.",
      "axis specifies the axis along which to find the minimum index (int or None).",
      "out is not supported (None).",
      "keepdims specifies whether to keep the reduced dimensions (bool or None).",
      "Array is the return type of the function."
    ],
    "code_examples": []
  },
  {
    "title": "Partial Sorting with Indices",
    "concepts": [
      "The function returns indices that partially sort an array.",
      "Refer to jax.numpy.argpartition() for the full documentation.",
      "The function operates on an array and takes kth and axis as arguments."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Array Sorting Indices",
    "concepts": [
      "The document discusses how to return the indices that would sort an array.",
      "It refers to jax.numpy.argsort() for complete documentation.",
      "The method takes an array as input (self).",
      "It accepts an optional axis argument (int | None).",
      "It takes optional arguments kind (None) and order (None).",
      "It takes an optional stable argument (bool).",
      "It takes an optional descending argument (bool)."
    ],
    "code_examples": []
  },
  {
    "title": "Array Copy and Type Casting",
    "concepts": [
      "Copying an array to a new memory location.",
      "Casting an array to a specified data type.",
      "Using jax.lax.convert_element_type() for type conversion.",
      "Potential differences between jax.lax.convert_element_type() and numpy.ndarray.astype().",
      "Implementation-dependent behavior of float-to-int and int-to-float casts."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of the 'at' Property",
    "concepts": [
      "The 'at' property provides a functionally pure equivalent of in-place array modifications.",
      "The 'at' property returns a modified copy of the array, not modifying the original.",
      "Inside a jit() compiled function, x = x.at[idx].set(y) is applied in-place.",
      "Multiple updates to the same index are all applied, unlike NumPy.",
      "The order of conflicting updates is implementation-defined and may be nondeterministic."
    ],
    "code_examples": [
      {
        "description": "Demonstrates equivalent in-place expressions using the 'at' property for setting, adding, subtracting, multiplying, dividing, exponentiating, finding the minimum, finding the maximum, and applying a ufunc.",
        "code": "x = x.at[idx].set(y)\nx = x.at[idx].add(y)\nx = x.at[idx].subtract(y)\nx = x.at[idx].multiply(y)\nx = x.at[idx].divide(y)\nx = x.at[idx].power(y)\nx = x.at[idx].min(y)\nx = x.at[idx].max(y)\nx = x.at[idx].apply(ufunc)\nx = x.at[idx].get()"
      }
    ]
  },
  {
    "title": "Out-of-Bounds Indexing Modes",
    "concepts": [
      "By default, JAX assumes all indices are in-bounds.",
      "The mode parameter specifies out-of-bound indexing semantics.",
      "The 'promise_in_bounds' mode assumes indices are in bounds and performs no additional checking.",
      "The 'clip' mode clamps out-of-bounds indices into a valid range.",
      "The 'drop' mode ignores out-of-bound indices.",
      "The 'fill' mode (alias for 'drop') can fill out-of-bounds get() with a fill_value."
    ],
    "code_examples": []
  },
  {
    "title": "Optimization Flags",
    "concepts": [
      "indices_are_sorted=True assumes indices are sorted in ascending order for potentially more efficient execution.",
      "unique_indices=True assumes indices are unique for potentially more efficient execution."
    ],
    "code_examples": []
  },
  {
    "title": "Fill Value for Out-of-Bounds 'get' Operations",
    "concepts": [
      "fill_value only applies to the get() method when mode is 'fill'.",
      "It specifies the value returned for out-of-bounds slices.",
      "The default fill_value depends on the data type (NaN for inexact, largest negative for signed, largest positive for unsigned, True for booleans)."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of 'at' Property Usage",
    "concepts": [
      "Demonstrates how to use the 'at' property with add() and get() methods.",
      "Shows how out-of-bounds indices are handled by default (ignored for add(), clipped for get()).",
      "Illustrates the use of the 'clip' mode for add() and 'fill' mode for get().",
      "Demonstrates setting a custom fill value for the 'fill' mode."
    ],
    "code_examples": [
      {
        "description": "Example usage of 'at' property with add() and default out-of-bounds handling.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(5.0)\nprint(x)\nx = x.at[2].add(10)\nprint(x)\nx = x.at[10].add(10)\nprint(x)"
      },
      {
        "description": "Example using the 'clip' mode for out-of-bounds indices in add().",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(5.0)\nx = x.at[20].add(10, mode='clip')\nprint(x)"
      },
      {
        "description": "Example of get() and its default clipping of out-of-bounds indices.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(5.0)\nprint(x.at[2].get())\nprint(x.at[20].get())"
      },
      {
        "description": "Example using the 'fill' mode with get() and the default NaN fill value.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(5.0)\nprint(x.at[20].get(mode='fill'))"
      },
      {
        "description": "Example using the 'fill' mode with get() and a custom fill value.",
        "code": "import jax.numpy as jnp\n\nx = jnp.arange(5.0)\nprint(x.at[20].get(mode='fill', fill_value=-1))"
      }
    ]
  },
  {
    "title": "Array Construction with Element Selection",
    "concepts": [
      "Constructing an array by choosing elements from multiple arrays.",
      "Functionality similar to jax.numpy.choose().",
      "The function takes a sequence of ArrayLike objects as choices.",
      "The 'out' parameter is currently set to None.",
      "The 'mode' parameter determines how out-of-bounds indexes will be handled."
    ],
    "code_examples": []
  },
  {
    "title": "Array Clipping",
    "concepts": [
      "The function returns an array with values limited to a specified range.",
      "The function is related to jax.numpy.clip().",
      "The 'self' parameter represents the input array.",
      "The 'min' parameter represents the minimum value for clipping (ArrayLike | None).",
      "The 'max' parameter represents the maximum value for clipping (ArrayLike | None)."
    ],
    "code_examples": []
  },
  {
    "title": "Array Slicing with Conditions",
    "concepts": [
      "Returns selected slices of an array along a given axis based on a condition.",
      "The functionality is similar to jax.numpy.compress().",
      "The input array is referred to as 'self'.",
      "A condition array ('condition') determines which slices are selected.",
      "The 'axis' parameter specifies the axis along which to select slices."
    ],
    "code_examples": []
  },
  {
    "title": "Array Commitment in JAX",
    "concepts": [
      "An array is committed when explicitly placed on a device using JAX APIs.",
      "jax.device_put() can commit an array to a specific device or the default device.",
      "Computations with committed inputs occur on the committed device(s).",
      "Operations on arguments committed to different devices raise an error."
    ],
    "code_examples": [
      {
        "description": "Example showing an error when adding arrays committed to different devices.",
        "code": "a = jax.device_put(np.arange(8), jax.devices()[0])\nb = jax.device_put(np.arange(8), jax.devices()[1])\na + b # Raises an error"
      }
    ]
  },
  {
    "title": "Complex Conjugate of an Array",
    "concepts": [
      "Returns the complex conjugate of the array.",
      "Refer to jax.numpy.conj() for full documentation.",
      "The input is an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Complex Conjugate",
    "concepts": [
      "Returns the complex conjugate of an array.",
      "Refer to jax.numpy.conjugate() for full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Array Copying",
    "concepts": [
      "The function returns a copy of an array.",
      "Refer to jax.numpy.copy() for more details."
    ],
    "code_examples": []
  },
  {
    "title": "Cumulative Product of an Array",
    "concepts": [
      "The document refers to the cumulative product of an array.",
      "It points to jax.numpy.cumprod() for full documentation.",
      "The function likely takes an array as input.",
      "The axis along which to compute the cumulative product can be specified.",
      "The data type of the result can be specified.",
      "The 'out' parameter is mentioned, but its usage is unclear without further context."
    ],
    "code_examples": []
  },
  {
    "title": "Cumulative Sum of an Array",
    "concepts": [
      "The function computes the cumulative sum of an array.",
      "Refer to jax.numpy.cumsum() for complete documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Array API-Compatible Device Attribute",
    "concepts": [
      "For single-device arrays, the attribute returns a Device object.",
      "For sharded arrays, the attribute returns a Sharding object."
    ],
    "code_examples": []
  },
  {
    "title": "Diagonal Extraction from Array",
    "concepts": [
      "Extracting a specific diagonal from an array.",
      "Referencing jax.numpy.diagonal() for complete documentation.",
      "self represents the input array.",
      "offset specifies the offset of the diagonal.",
      "axis1 specifies the first axis.",
      "axis2 specifies the second axis."
    ],
    "code_examples": []
  },
  {
    "title": "Dot Product Computation",
    "concepts": [
      "Computes the dot product of two arrays.",
      "Refer to jax.numpy.dot() for full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "self is an Array.",
      "b is an ArrayLike.",
      "precision is lax_internal.PrecisionLike",
      "preferred_element_type is DTypeLike | None"
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "Returns an Array."
    ],
    "code_examples": []
  },
  {
    "title": "Data Type of an Array",
    "concepts": [
      "The data type of a NumPy array is represented by numpy.dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Recommendation to Use flatten()",
    "concepts": [
      "The document recommends using the `flatten()` function.",
      "The functionality being discussed is currently not implemented."
    ],
    "code_examples": []
  },
  {
    "title": "Array Flattening",
    "concepts": [
      "Flattening an array transforms it into a 1-dimensional shape.",
      "Use jax.numpy.ravel() for complete documentation on array flattening."
    ],
    "code_examples": []
  },
  {
    "title": "Global Shards",
    "concepts": [
      "The document mentions a list of global shards."
    ],
    "code_examples": []
  },
  {
    "title": "Imaginary Part of an Array",
    "concepts": [
      "The imaginary part of an array represents the imaginary components of complex numbers within the array.",
      "The function to retrieve the imaginary part is not explicitly defined in this document."
    ],
    "code_examples": []
  },
  {
    "title": "Addressability of jax.Array",
    "concepts": [
      "A jax.Array is fully addressable if the current process can address all devices named in the Sharding.",
      "is_fully_addressable is equivalent to \u201cis_local\u201d in multi-process JAX.",
      "Fully replicated is not equal to fully addressable; a fully replicated jax.Array can span multiple hosts and not be fully addressable."
    ],
    "code_examples": []
  },
  {
    "title": "Array Replication Verification",
    "concepts": [
      "The document aims to verify if an array is fully replicated."
    ],
    "code_examples": []
  },
  {
    "title": "Array Element Copying",
    "concepts": [
      "Copies an element from an array to a Python scalar.",
      "The function takes an array and an integer as input.",
      "The function returns a boolean, integer, float, or complex number."
    ],
    "code_examples": []
  },
  {
    "title": "Array Element Length",
    "concepts": [
      "The document specifies the length of one array element in bytes."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Finding the Maximum Value in an Array",
    "concepts": [
      "The document refers to finding the maximum value of array elements along a specified axis.",
      "The function jax.numpy.max() provides full documentation on this operation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Max Function",
    "concepts": [
      "self: Represents the input array.",
      "axis: Specifies the axis along which the maximum is computed.",
      "out: Indicates the output array (currently set to None).",
      "keepdims: Determines if the output should retain the same number of dimensions as the input.",
      "initial: Represents the initial value for the reduction.",
      "where: An array-like object to select elements to include in the reduction."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.mean()",
    "concepts": [
      "Calculates the mean of array elements along a specified axis.",
      "Refer to jax.numpy.mean() for comprehensive documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of jax.numpy.mean()",
    "concepts": [
      "self (Array): Input array.",
      "axis (reductions.Axis): Axis or axes along which to compute the mean.",
      "dtype (DTypeLike | None): Data type to use in performing the mean. If None, the data type is inferred.",
      "out (None): Not supported.",
      "keepdims (bool): If True, the axes which are reduced are left in the result as dimensions with size one.",
      "where (ArrayLike | None): Elements to include in the mean."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of Array Minimum Reduction",
    "concepts": [
      "Returns the minimum of array elements along a given axis.",
      "Refer to jax.numpy.min() for full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Minimum Function",
    "concepts": [
      "self: The input array.",
      "axis: The axis along which to reduce.",
      "out: Not applicable.",
      "keepdims: Boolean indicating whether to keep the reduced dimensions.",
      "initial: Initial value for the reduction.",
      "where: ArrayLike indicating elements to compare for finding the minimum."
    ],
    "code_examples": []
  },
  {
    "title": "Array Memory Consumption",
    "concepts": [
      "Total bytes consumed by the elements of the array can be calculated.",
      "This refers to the memory footprint of the data stored within the array."
    ],
    "code_examples": []
  },
  {
    "title": "Array Dimensions",
    "concepts": [
      "The document refers to the number of dimensions in an array."
    ],
    "code_examples": []
  },
  {
    "title": "Description",
    "concepts": [
      "Returns indices of nonzero elements of an array.",
      "Refer to jax.numpy.nonzero() for the full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "self (Array): The input array.",
      "fill_value (None | ArrayLike | tuple[ArrayLike, ...]): The fill value.",
      "size (int | None): The size of the output."
    ],
    "code_examples": []
  },
  {
    "title": "Returns",
    "concepts": [
      "tuple[Array, \u2026]: Indices of nonzero elements."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Array Product",
    "concepts": [
      "Calculates the product of array elements along a specified axis.",
      "Mirrors functionality of jax.numpy.prod().",
      "Takes an array as input.",
      "Allows specifying the axis for reduction.",
      "Supports specifying the data type of the result.",
      "Supports keeping the reduced dimensions.",
      "Supports an initial value for the product.",
      "Supports a where condition for selective product calculation.",
      "Supports promoting integers during calculation."
    ],
    "code_examples": []
  },
  {
    "title": "Peak-to-Peak Range",
    "concepts": [
      "Calculate the range between the maximum and minimum values along a specified axis.",
      "The function mirrors the functionality of jax.numpy.ptp().",
      "It accepts an array as input.",
      "The axis along which to calculate the peak-to-peak range can be specified.",
      "The 'out' parameter is currently not supported and should be None.",
      "The 'keepdims' parameter controls whether the reduced axis is kept in the result."
    ],
    "code_examples": []
  },
  {
    "title": "Array Flattening",
    "concepts": [
      "Flattening an array transforms it into a 1-dimensional shape.",
      "Refer to jax.numpy.ravel() for detailed documentation on array flattening."
    ],
    "code_examples": []
  },
  {
    "title": "Real Part of an Array",
    "concepts": [
      "The document states the intention to return the real part of an array."
    ],
    "code_examples": []
  },
  {
    "title": "Constructing Arrays from Repeated Elements",
    "concepts": [
      "This section describes how to construct an array by repeating elements.",
      "Refer to jax.numpy.repeat() for full documentation.",
      "The input array is referred to as 'self'.",
      "'repeats' is an ArrayLike object that specifies how many times each element should be repeated.",
      "'axis' is an optional integer specifying the axis along which to repeat.",
      "'total_repeat_length' is an optional integer specifying the total length of the repeated axis."
    ],
    "code_examples": []
  },
  {
    "title": "Array Reshaping Overview",
    "concepts": [
      "Returns an array with a new shape.",
      "Uses `jax.numpy.reshape()` for complete documentation.",
      "Takes `self` (the array), `args` (Any arguments), and `order` (str) as input."
    ],
    "code_examples": []
  },
  {
    "title": "Rounding Array Elements",
    "concepts": [
      "Rounds array elements to a given decimal.",
      "Refer to jax.numpy.round() for full documentation.",
      "The function takes an array and the number of decimals as input."
    ],
    "code_examples": []
  },
  {
    "title": "Binary Search in Sorted Array",
    "concepts": [
      "Binary search is performed within a sorted array.",
      "Refer to jax.numpy.searchsorted() for full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of Searchsorted Function",
    "concepts": [
      "self (Array): The sorted array to search within.",
      "v (ArrayLike): The value(s) to insert into self.",
      "side (str): 'left' or 'right', indicating which index to return in case of duplicates.",
      "sorter (ArrayLike | None): Optional array of indices that sort self.",
      "method (str): The search method, e.g., 'binary'."
    ],
    "code_examples": []
  },
  {
    "title": "Array Shape",
    "concepts": [
      "Arrays have a shape attribute.",
      "The shape attribute represents the dimensions of the array."
    ],
    "code_examples": []
  },
  {
    "title": "Array Sharding",
    "concepts": [
      "The document mentions sharding for an array."
    ],
    "code_examples": []
  },
  {
    "title": "Array Length",
    "concepts": [
      "The total number of elements in the array."
    ],
    "code_examples": []
  },
  {
    "title": "Array Sorting",
    "concepts": [
      "Returns a sorted copy of an array.",
      "Refer to jax.numpy.sort() for full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Array Squeezing",
    "concepts": [
      "Removes length-1 axes from an array.",
      "Refer to jax.numpy.squeeze() for full documentation.",
      "The function operates on an input array.",
      "The axis argument specifies the axes to squeeze."
    ],
    "code_examples": []
  },
  {
    "title": "Standard Deviation Computation",
    "concepts": [
      "Compute the standard deviation along a specified axis.",
      "Refer to jax.numpy.std() for full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "self: Input array.",
      "axis: Axis or axes along which to compute the standard deviation.",
      "dtype: Data type of the result. If None, the data type of the input is used.",
      "out: Not supported.",
      "ddof: \u201cDelta Degrees of Freedom\u201d: the divisor used in the calculation is N - ddof, where N represents the number of elements.",
      "keepdims: If this is set to True, the axes which are reduced are left in the result as dimensions with size one.",
      "where: Elements to include in the standard deviation.",
      "correction: Alias for ddof.",
      "Array: Array"
    ],
    "code_examples": []
  },
  {
    "title": "Sum of Array Elements",
    "concepts": [
      "The document refers to the sum of array elements over a given axis.",
      "The function is related to jax.numpy.sum().",
      "Parameters such as axis, dtype, out, keepdims, initial and where can be specified.",
      "There is a parameter promote_integers to handle integer promotion."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Array Axis Swapping",
    "concepts": [
      "Swapping two axes of an array.",
      "Refer to jax.numpy.swapaxes() for full documentation.",
      "Relevant parameters are self (Array), axis1 (int), and axis2 (int)."
    ],
    "code_examples": []
  },
  {
    "title": "Taking Elements from an Array",
    "concepts": [
      "This section describes how to take elements from an array.",
      "The function jax.numpy.take() is referenced for full documentation.",
      "Parameters include the array to take elements from, indices to take, axis, out, mode, unique_indices, indices_are_sorted, and fill_value."
    ],
    "code_examples": []
  },
  {
    "title": "Array Copying to a Device",
    "concepts": [
      "Creates a copy of an array on a specified device.",
      "The 'device' argument specifies the target device or sharding configuration.",
      "The 'stream' argument is not currently implemented.",
      "The method returns a copy of the array.",
      "The method is invoked on an Array object (self)."
    ],
    "code_examples": []
  },
  {
    "title": "Diagonal Summation",
    "concepts": [
      "Returns the sum along the diagonal of an array.",
      "Refer to jax.numpy.trace() for full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Array Transposition",
    "concepts": [
      "The function returns a copy of the array.",
      "The function transposes the axes of the array.",
      "Refer to jax.numpy.transpose() for full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Variance Calculation",
    "concepts": [
      "Compute the variance along a specified axis.",
      "Refer to jax.numpy.var() for full documentation."
    ],
    "code_examples": []
  },
  {
    "title": "Parameter Definitions",
    "concepts": [
      "self: The input array.",
      "axis: The axis or axes along which to compute the variance.",
      "dtype: The data type of the result.",
      "out: Not applicable (None).",
      "ddof: The delta degrees of freedom.",
      "keepdims: Whether to keep the dimensions of the input array in the output.",
      "where: Elements to include in the variance calculation.",
      "correction: Correction factor for variance calculation."
    ],
    "code_examples": []
  },
  {
    "title": "Bitwise Copy and Dtype Conversion using view()",
    "concepts": [
      "The `view()` method returns a bitwise copy of the array, viewed as a new dtype.",
      "It is a wrapper around `jax.lax.bitcast_convert_type()`.",
      "If the source and target dtype have the same bitwidth, the result has the same shape as the input array.",
      "If the bitwidth of the target dtype is different from the source, the size of the last axis of the result is adjusted accordingly."
    ],
    "code_examples": [
      {
        "description": "Demonstrates changing the dtype from int16 to int8, altering the shape.",
        "code": "jnp.zeros([1, 2, 3], dtype=jnp.int16).view(jnp.int8).shape"
      },
      {
        "description": "Demonstrates changing the dtype from int8 to int16, altering the shape.",
        "code": "jnp.zeros([1, 2, 4], dtype=jnp.int8).view(jnp.int16).shape"
      },
      {
        "description": "Demonstrates changing the dtype from int16 to int8, altering the shape.",
        "code": "jnp.zeros([1, 2, 3], dtype=jnp.int16).view(jnp.int8).shape"
      },
      {
        "description": "Demonstrates changing the dtype from int8 to int16, altering the shape.",
        "code": "jnp.zeros([1, 2, 4], dtype=jnp.int8).view(jnp.int16).shape"
      }
    ]
  },
  {
    "title": "Boolean Conversions and Safe Practices",
    "concepts": [
      "Conversions involving booleans are not well-defined in all situations.",
      "Booleans are treated as having a bitwidth of 8.",
      "When converting to a boolean array, the input should only contain 0 or 1 bytes.",
      "Results of boolean conversions of arbitrary byte values are unpredictable.",
      "Comparing with 0 is the safe way to convert integer arrays to boolean arrays."
    ],
    "code_examples": [
      {
        "description": "Shows a guaranteed and safe conversion from int8 to boolean when the int8 array contains only 0 and 1.",
        "code": "jnp.array([1, 0, 1], dtype=jnp.int8).view(jnp.bool_)"
      },
      {
        "description": "Shows a guaranteed and safe conversion from int8 to boolean when the int8 array contains only 0 and 1.",
        "code": "jnp.array([1, 0, 1], dtype=jnp.int8).view(jnp.bool_)"
      },
      {
        "description": "Demonstrates the safe way to convert an int8 array to boolean by comparing with 0.",
        "code": "jnp.array([1, 2, 0], dtype=jnp.int8) != 0"
      },
      {
        "description": "Demonstrates the safe way to convert an int8 array to boolean by comparing with 0.",
        "code": "jnp.array([1, 2, 0], dtype=jnp.int8) != 0"
      }
    ]
  },
  {
    "title": "Parameters",
    "concepts": [
      "self (Array): The array to view.",
      "dtype (DTypeLike | None): The desired data type.",
      "type (None): Type parameter, its purpose is not clear from context."
    ],
    "code_examples": []
  },
  {
    "title": "All-Axis Array Transpose",
    "concepts": [
      "Computes the transpose of an array across all axes.",
      "Refers to jax.numpy.transpose() for details."
    ],
    "code_examples": []
  },
  {
    "title": "Matrix Transpose",
    "concepts": [
      "Compute the (batched) matrix transpose.",
      "Refer to jax.numpy.matrix_transpose() for details."
    ],
    "code_examples": []
  },
  {
    "title": "Vectorizing with vmap",
    "concepts": [
      "vmap creates a function that maps a given function over specified argument axes.",
      "The `fun` argument is the function to be vectorized.",
      "`in_axes` specifies which input array axes to map over, accepting integers, None, or sequences.",
      "If positional arguments are pytrees, `in_axes` should match the pytree structure.",
      "`out_axes` specifies where the mapped axis should appear in the output.",
      "`axis_name` is used for identifying the mapped axis for parallel collectives.",
      "`axis_size` is an optional integer indicating the size of the axis to be mapped.",
      "Arguments passed as keywords are always mapped over their leading axis (axis 0)."
    ],
    "code_examples": []
  },
  {
    "title": "Matrix Multiplication Example",
    "concepts": [
      "vmap can be used to implement matrix-matrix multiplication using a vector dot product.",
      "The `in_axes` argument controls which axes are mapped over.",
      "The `out_axes` argument controls the output array axes."
    ],
    "code_examples": [
      {
        "description": "Matrix-matrix product using a vector dot product with vmap. Demonstrates the use of `in_axes` to specify the axes to map over for different arguments. Showing different configurations of input matrix dimensions.",
        "code": "import jax.numpy as jnp\n\nvv = lambda x, y: jnp.vdot(x, y)  # ([a], [a]) -> []\nmv = vmap(vv, (0, None), 0)  # ([b,a], [a]) -> [b]      (b is the mapped axis)\nmm = vmap(mv, (None, 1), 1)  # ([b,a], [a,c]) -> [b,c]  (c is the mapped axis)\n\nmv1 = vmap(vv, (0, 0), 0)  # ([b,a], [b,a]) -> [b]        (b is the mapped axis)\nmv2 = vmap(vv, (0, 1), 0)  # ([b,a], [a,b]) -> [b]        (b is the mapped axis)\nmm2 = vmap(mv2, (1, 1), 0)  # ([b,c,a], [a,c,b]) -> [c,b]  (c is the mapped axis)"
      }
    ]
  },
  {
    "title": "Handling PyTrees with vmap",
    "concepts": [
      "vmap can handle container types (pytrees) in `in_axes` to specify axes of container elements to map over.",
      "The structure of `in_axes` must match the pytree structure of the arguments.",
      "This allows for batching operations on nested data structures."
    ],
    "code_examples": [
      {
        "description": "Illustrates how to use container types (specifically, tuples) within `in_axes` to specify which axes of the container elements should be mapped over when using vmap. It defines a function `foo` that operates on a nested tuple of arrays and then uses `vmap` with a carefully structured `in_axes` argument to batch the operation.",
        "code": "import jax.numpy as jnp\nfrom jax import vmap\n\nA, B, C, D = 2, 3, 4, 5\nx = jnp.ones((A, B))\ny = jnp.ones((B, C))\nz = jnp.ones((C, D))\n\ndef foo(tree_arg):\n    x, (y, z) = tree_arg\n    return jnp.dot(x, jnp.dot(y, z))\n\ntree = (x, (y, z))\n\nK = 6  # batch size\nx = jnp.ones((K, A, B))  # batch axis in different locations\ny = jnp.ones((B, K, C))\nz = jnp.ones((C, D, K))\ntree = (x, (y, z))\nvfoo = vmap(foo, in_axes=((0, (1, 2)),))\n\nprint(vfoo(tree).shape)"
      },
      {
        "description": "Demonstrates how to use a dictionary as part of the `in_axes` argument to vmap, which allows specifying how to map over elements within a dictionary. The example defines a function `foo` that accesses specific keys in a dictionary and then uses `vmap` with a dictionary-based `in_axes` to apply `foo` over a specific dimension of the 'b' key in the dictionary.",
        "code": "import jax.numpy as jnp\nfrom jax import vmap\n\ndct = {'a': 0., 'b': jnp.arange(5.)}\nx = 1.\n\ndef foo(dct, x):\n    return dct['a'] + dct['b'] + x\n\nout = vmap(foo, in_axes=({'a': None, 'b': 0}, None))(dct, x)\nprint(out)"
      }
    ]
  },
  {
    "title": "Mapping and Unmapping Results",
    "concepts": [
      "The results of a vectorized function can be either mapped or unmapped using `out_axes`.",
      "Specifying `out_axes=None` keeps the result unmapped.",
      "Specifying `out_axes` for an unmapped result broadcasts the result across the mapped axis."
    ],
    "code_examples": [
      {
        "description": "Illustrates how to map or unmap the results of a vectorized function using the `out_axes` argument.  The first element of the returned tuple is mapped while the second is unmapped, and it demonstrates broadcasting the second result across the mapped axis when out_axes is set to 0.",
        "code": "import jax.numpy as jnp\nfrom jax import vmap\n\nprint(vmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None), out_axes=(0, None))(jnp.arange(2.), 4.))\nprint(vmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None), out_axes=0)(jnp.arange(2.), 4.))"
      }
    ]
  },
  {
    "title": "Using axis_name with Collectives",
    "concepts": [
      "`axis_name` can be used with collectives like `lax.psum` to perform parallel reductions along the mapped axis.",
      "This allows for communication between different mapped elements."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use the `axis_name` argument in conjunction with `lax.psum` to perform a parallel sum along the mapped axis.  The `axis_name` argument is used to identify the mapped axis so that the collective operation `lax.psum` can be applied correctly.",
        "code": "import jax.numpy as jnp\nfrom jax import vmap\nfrom jax import lax\n\nxs = jnp.arange(3. * 4.).reshape(3, 4)\nprint(vmap(lambda x: lax.psum(x, 'i'), axis_name='i')(xs))"
      }
    ]
  },
  {
    "title": "Introduction to pmap",
    "concepts": [
      "pmap() is used for single-program multiple-data (SPMD) programs.",
      "pmap() compiles and executes a function in parallel on XLA devices.",
      "pmap() is semantically similar to vmap(), but replicates the function for parallel execution instead of vectorizing.",
      "The mapped axis size should be less than or equal to the number of local XLA devices.",
      "pmap() requires that all participating devices are identical.",
      "pmap() can be combined with jit(), but it\u2019s usually unnecessary as pmap() compiles fun.",
      "On multi-process platforms, pmap() is used in SPMD Python programs where all processes run the same pmapped function in the same order.",
      "Collective operations in fun will be computed over all participating devices, including those on other processes, via device-to-device communication."
    ],
    "code_examples": []
  },
  {
    "title": "Basic pmap usage",
    "concepts": [
      "pmap() can be used as a map along a leading array axis.",
      "When the leading dimension is smaller than the number of available devices, JAX will run on a subset of devices.",
      "If the leading dimension is larger than the number of available devices, a ValueError is raised.",
      "Using None in in_axes indicates that an argument should be broadcasted, rather than mapped, across the replicas."
    ],
    "code_examples": [
      {
        "description": "Demonstrates basic usage of pmap to square a range of numbers.",
        "code": "import jax.numpy as jnp\nimport jax\n\nout = jax.pmap(lambda x: x**2)(jnp.arange(8))\nprint(out)"
      },
      {
        "description": "Demonstrates pmap with matrix multiplication when the leading dimension is smaller than the number of available devices.",
        "code": "import jax.numpy as jnp\nimport jax\n\nx = jnp.arange(3*2*2.).reshape((3, 2, 2))\ny = jnp.arange(3*2*2.).reshape((3, 2, 2))**2\nout = jax.pmap(jnp.dot)(x, y)\nprint(out)"
      },
      {
        "description": "Demonstrates the use of `in_axes=None` to broadcast an argument across replicas.",
        "code": "import jax.numpy as jnp\nimport jax\n\nx, y = jnp.arange(2.), 4.\nout = jax.pmap(lambda x, y: (x + y, y * 2.), in_axes=(0, None))(x, y)\nprint(out)"
      }
    ]
  },
  {
    "title": "Collective operations with pmap",
    "concepts": [
      "pmap() can be used to express parallel single-program multiple-data (SPMD) programs that communicate via collective operations.",
      "The `axis_name` argument names the mapped axis so that collective operations can refer to it.",
      "Axis names are important in nested pmap() functions, where collective operations can operate over distinct axes.",
      "On multi-process platforms, collective operations operate over all devices, including those on other processes."
    ],
    "code_examples": [
      {
        "description": "Demonstrates a simple collective operation (psum) with pmap.",
        "code": "import jax.numpy as jnp\nimport jax\n\nf = lambda x: x / jax.lax.psum(x, axis_name='i')\nout = jax.pmap(f, axis_name='i')(jnp.arange(4.))\nprint(out)\nprint(out.sum())"
      },
      {
        "description": "Demonstrates nested pmap functions with collective operations over distinct axes.",
        "code": "from functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.pmap, axis_name='rows')\n@partial(jax.pmap, axis_name='cols')\ndef normalize(x):\n    row_normed = x / jax.lax.psum(x, 'rows')\n    col_normed = x / jax.lax.psum(x, 'cols')\n    doubly_normed = x / jax.lax.psum(x, ('rows', 'cols'))\n    return row_normed, col_normed, doubly_normed\n\nx = jnp.arange(8.).reshape((4, 2))\nrow_normed, col_normed, doubly_normed = normalize(x)\nprint(row_normed.sum(0))\nprint(col_normed.sum(1))\nprint(doubly_normed.sum((0, 1)))"
      },
      {
        "description": "Demonstrates a collective operation (psum) on a multi-process platform.",
        "code": "import jax\nimport jax.numpy as jnp\n\nf = lambda x: x + jax.lax.psum(x, axis_name='i')\ndata = jnp.arange(4) if jax.process_index() == 0 else jnp.arange(4, 8)\nout = jax.pmap(f, axis_name='i')(data)\nprint(out)"
      }
    ]
  },
  {
    "title": "Specifying Devices",
    "concepts": [
      "The `devices` argument can be used to specify exactly which devices are used to run the parallel computation."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how to use the `devices` argument to run parallel computations on specific devices.",
        "code": "from functools import partial\nimport jax\nimport jax.numpy as jnp\n\n@partial(jax.pmap, axis_name='i', devices=jax.devices()[:6])\ndef f1(x):\n    return x / jax.lax.psum(x, axis_name='i')\n\n@partial(jax.pmap, axis_name='i', devices=jax.devices()[-2:])\ndef f2(x):\n    return jax.lax.psum(x**2, axis_name='i')\n\nprint(f1(jnp.arange(6.)))\nprint(f2(jnp.array([2., 3.])))"
      }
    ]
  },
  {
    "title": "Device Retrieval",
    "concepts": [
      "Returns a list of all devices for a given backend.",
      "Devices are represented by subclasses of Device (e.g., CpuDevice, GpuDevice).",
      "The length of the list equals device_count(backend).",
      "Local devices are identified by comparing Device.process_index to jax.process_index().",
      "If backend is None, all devices from the default backend are returned.",
      "The default backend is typically 'gpu' or 'tpu' if available, otherwise 'cpu'."
    ],
    "code_examples": []
  },
  {
    "title": "Backend Parameter",
    "concepts": [
      "The backend parameter specifies the XLA backend to use.",
      "Acceptable values are 'cpu', 'gpu', or 'tpu'.",
      "Passing None defaults to the default backend."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "Returns a list of Device subclasses (xla_client.Device)."
    ],
    "code_examples": []
  },
  {
    "title": "Local Devices Retrieval",
    "concepts": [
      "Retrieves devices local to a specific process.",
      "Uses `jax.devices()` to get devices.",
      "Allows specifying a `process_index` to target a particular process.",
      "If `process_index` is None, it defaults to the current process.",
      "Provides an option to specify the XLA backend (CPU, GPU, or TPU).",
      "Returns a list of `xla_client.Device` subclasses."
    ],
    "code_examples": []
  },
  {
    "title": "Process Index",
    "concepts": [
      "Returns the integer process index of the current process.",
      "On most platforms, the process index will be 0.",
      "The process index can vary on multi-process platforms."
    ],
    "code_examples": []
  },
  {
    "title": "Backend Selection",
    "concepts": [
      "The backend parameter is an experimental feature.",
      "The backend API is likely to change.",
      "The backend can be specified as a string ('cpu', 'gpu', or 'tpu') or an xla_client.Client object.",
      "The backend parameter is optional."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "The function returns the integer process index.",
      "The return type is int."
    ],
    "code_examples": []
  },
  {
    "title": "Device Count and Backend Specification",
    "concepts": [
      "Returns the total number of devices.",
      "On most platforms, it's the same as jax.local_device_count().",
      "In multi-process environments, it represents the total device count across all processes.",
      "The backend parameter is experimental and subject to change.",
      "The backend parameter specifies the XLA backend (e.g., 'cpu', 'gpu', 'tpu').",
      "The function returns the number of devices as an integer."
    ],
    "code_examples": []
  },
  {
    "title": "Addressable Devices Count",
    "concepts": [
      "The function returns the number of devices addressable by the current process.",
      "The `backend` argument specifies the XLA client to use."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Process Count",
    "concepts": [
      "The function returns the number of JAX processes.",
      "It is associated with a specific backend.",
      "The backend can be a string, an xla_client.Client, or None."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Process Indices Retrieval",
    "concepts": [
      "Returns the list of all JAX process indices.",
      "Associated with the specified backend.",
      "The backend can be a string ('cpu', 'gpu', 'tpu') or an xla_client.Client object.",
      "The backend parameter is optional.",
      "The return value is a list of integer process indices."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to pure_callback",
    "concepts": [
      "pure_callback enables calling pure Python functions in JIT-ed JAX functions.",
      "Input callbacks receive JAX arrays on CPU and should return JAX arrays on CPU.",
      "The callback is treated as functionally pure, having no side effects and its output depends only on its arguments.",
      "Pure callbacks may be called multiple times, reordered, or not called at all depending on data dependencies.",
      "Python exceptions within pure_callback are considered side effects and lead to undefined behavior.",
      "Calling vmap() on a callback without an explicit vmap_method is deprecated and will eventually raise NotImplementedError."
    ],
    "code_examples": []
  },
  {
    "title": "vmap_method Options",
    "concepts": [
      "vmap_method=\"sequential\" uses map() to loop over batched arguments, calling the callback once per element.",
      "vmap_method=\"sequential_unrolled\" is like sequential but unrolls the loop.",
      "vmap_method=\"expand_dims\" adds new axes of size 1 to unbatched inputs.",
      "vmap_method=\"broadcast_all\" behaves like expand_dims, but tiles inputs to the expected shape.",
      "vmap_method=\"legacy_vectorized\" recovers the deprecated vectorized=True behavior."
    ],
    "code_examples": []
  },
  {
    "title": "pure_callback Function Definition",
    "concepts": [
      "callback: The Python function to execute on the host.",
      "result_shape_dtypes: A pytree with shape and dtype attributes matching the callback's expected output.",
      "*args: Arguments passed to the callback function.",
      "sharding: Optional sharding to specify the device for callback invocation.",
      "vmap_method: String specifying how the callback transforms under vmap().",
      "**kwargs: Keyword arguments passed to the callback function.",
      "vectorized: Deprecated argument."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of vmap_method Usage",
    "concepts": [
      "The behavior of pure_callback under vmap() is controlled by the vmap_method argument.",
      "The example demonstrates how vmap_method affects the shapes of the inputs passed to the callback function."
    ],
    "code_examples": [
      {
        "description": "Defines a callback function that prints the shapes of its inputs and returns their sum.",
        "code": "def callback(x, y):\n  print(jnp.shape(x), jnp.shape(y))\n  return x + y"
      },
      {
        "description": "Defines a function that uses pure_callback with a specified vmap_method.",
        "code": "def fun(x, y, *, vmap_method):\n  shape = jnp.broadcast_shapes(jnp.shape(x), jnp.shape(y))\n  dtype = jnp.result_type(x, y)\n  out_type = jax.ShapeDtypeStruct(shape, dtype)\n  return jax.pure_callback(callback, out_type, x, y, vmap_method=vmap_method)"
      },
      {
        "description": "Demonstrates the use of vmap_method=\"expand_dims\", adding a new axis of size 1 to y.",
        "code": "from functools import partial\n\nx = jnp.arange(4)\ny = 1.0\n\njax.vmap(partial(fun, vmap_method=\"expand_dims\"), in_axes=(0, None))(x, y)"
      },
      {
        "description": "Demonstrates the use of vmap_method=\"broadcast_all\", adding an axis of size 4 to y.",
        "code": "jax.vmap(partial(fun, vmap_method=\"broadcast_all\"),\n         in_axes=(0, None))(x, y)"
      }
    ]
  },
  {
    "title": "Impure Python Callback",
    "concepts": [
      "Calls an impure Python function on the host during JAX computation.",
      "Use `jax.pure_callback()` for pure functions for better efficiency.",
      "`callback` is the function to execute.",
      "`result_shape_dtypes` specifies the shape and dtype of the callback's output.",
      "`*args` are positional arguments passed to the callback.",
      "`sharding` specifies the device for callback invocation.",
      "`ordered` indicates whether sequential calls must be ordered.",
      "`**kwargs` are keyword arguments passed to the callback."
    ],
    "code_examples": []
  },
  {
    "title": "Device Descriptor Overview",
    "concepts": [
      "A device descriptor represents an available device.",
      "Subclasses represent specific device types like CPUs and GPUs.",
      "Subclasses can have device-specific properties."
    ],
    "code_examples": []
  },
  {
    "title": "Device Descriptor Methods",
    "concepts": [
      "__init__: Initializes a device descriptor instance.",
      "client: Returns the client associated with the device.",
      "default_memory: Returns the default memory of the device.",
      "device_kind: Returns the kind of device (e.g., CPU, GPU).",
      "get_stream_for_external_ready_events:",
      "platform: Returns the platform of the device (e.g., CUDA, ROCm)."
    ],
    "code_examples": []
  },
  {
    "title": "Device Descriptor Attributes",
    "concepts": [
      "addressable_memories: Returns all memories the device can address.",
      "host_id: Deprecated; use process_index instead.",
      "id: Integer ID of the device.",
      "live_buffers:",
      "local_hardware_id: Opaque hardware ID (e.g., CUDA device number).",
      "memory:",
      "memory_stats: Returns memory statistics keyed by name.",
      "process_index: Integer index of the device's process.",
      "task_id: Deprecated; use process_index instead.",
      "transfer_from_outfeed:",
      "transfer_to_infeed:"
    ],
    "code_examples": []
  },
  {
    "title": "Environment and JAX Installation Information",
    "concepts": [
      "The function returns a string with local environment and JAX installation information.",
      "This information is useful for debugging and reporting issues.",
      "The function can either print the string to stdout or return it.",
      "The `return_string` argument controls whether the string is returned or printed."
    ],
    "code_examples": []
  },
  {
    "title": "Live Array Retrieval",
    "concepts": [
      "Retrieves all live arrays in the backend.",
      "Uses the default backend if platform is None."
    ],
    "code_examples": []
  },
  {
    "title": "Cache Clearing",
    "concepts": [
      "Clears compilation and staging caches.",
      "Does not clear persistent cache.",
      "Persistent cache can be disabled via jax_enable_compilation_cache config option."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Project Overview",
    "concepts": [
      "The JAX project is led by the JAX core team and welcomes open-source contributions.",
      "The JAX core library focuses on the fundamentals of machine learning and numerical computing at scale.",
      "JAX emphasizes modularity to support domain-specific libraries and new hardware.",
      "Open development and continuous improvement are core values of the JAX project."
    ],
    "code_examples": []
  },
  {
    "title": "Modularity and Extensibility in JAX",
    "concepts": [
      "JAX is designed to be composable and extensible, allowing for domain-specific libraries to thrive.",
      "JAX's API focuses on basic building blocks, encouraging auxiliary libraries to develop utilities.",
      "JAX exposes advanced APIs for customization and extensibility, allowing libraries to integrate with its transformations.",
      "Projects across the JAX ecosystem are developed in a distributed and often open fashion."
    ],
    "code_examples": []
  },
  {
    "title": "Backend Modularity and Hardware Support",
    "concepts": [
      "JAX supports CPUs, GPUs, TPUs, and other hardware platforms through a modular backend.",
      "JAX uses the XLA compiler and the PJRT runtime for hardware device management, memory management, and compilation.",
      "XLA aims for interoperability across accelerators, and PJRT offers extensibility through a plug-in device API.",
      "Adding support for new devices involves implementing a backend lowering for XLA and implementing a plug-in device API defined by PJRT."
    ],
    "code_examples": []
  },
  {
    "title": "General Information and New Features",
    "concepts": [
      "JAX follows Effort-based versioning.",
      "Negative indices are now allowed in dynamic_slice and dynamic_update_slice functions by default; this can be disabled to improve code size.",
      "A custom_dce decorator has been added to customize dead code elimination behavior for opaque functions.",
      "Low-level reduction APIs have been added to jax.lax.",
      "Column-pivoting is now supported in jax.lax.linalg.qr() and jax.scipy.linalg.qr() on CPU and GPU.",
      "JAX_CPU_COLLECTIVES_IMPLEMENTATION and JAX_NUM_CPU_DEVICES can now be set via environment variables.",
      "The jax[tpu] TPU extra no longer depends on the libtpu-nightly package."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecations",
    "concepts": [
      "linear_util.wrap_init and core.Jaxpr now require a non-empty core.DebugInfo kwarg.",
      "Non-arraylike inputs to jax.numpy.ndim(), jax.numpy.shape(), and jax.numpy.size() are deprecated."
    ],
    "code_examples": []
  },
  {
    "title": "Bug Fixes and Performance Improvements",
    "concepts": [
      "TPU runtime startup and shutdown time have been improved on TPU v5e and newer.",
      "Persistent compilation cache no longer writes access time file if JAX_COMPILATION_CACHE_MAX_SIZE is unset or set to -1.",
      "This release makes a breaking change to PRNG key semantics that may require users to update their code."
    ],
    "code_examples": []
  },
  {
    "title": "Breaking Changes and Updates",
    "concepts": [
      "jax_threefry_partitionable is enabled by default.",
      "Support for Mac x86 wheels has been dropped.",
      "The minimum NumPy version is now 1.25.",
      "The minimum SciPy version is now 1.11.",
      "jax.numpy.einsum() now defaults to optimize='auto'.",
      "jax.numpy.linalg.solve() no longer supports batched 1D arguments on the right hand side."
    ],
    "code_examples": []
  },
  {
    "title": "New Features in FFT, FFI and AOT Lowering",
    "concepts": [
      "jax.numpy.fft functions now support transforms in more than 3 dimensions.",
      "Support added for user defined state in the FFI via the new jax.ffi.register_ffi_type_id() function.",
      "The AOT lowering .as_text() method now supports the debug_info option."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecations and Deletions in Internal Modules",
    "concepts": [
      "abstractify and pytype_aval_mappings are now deprecated.",
      "jax.scipy.special.lpmn() and jax.scipy.special.lpmn_values() are deprecated.",
      "The jax.extend.ffi submodule was moved to jax.ffi, and the previous import path is deprecated.",
      "jax_enable_memories flag has been deleted.",
      "Device and XlaRuntimeError symbols have been removed from jax.lib.xla_client.",
      "The jax.experimental.array_api module has been removed."
    ],
    "code_examples": []
  },
  {
    "title": "Breaking Changes to XlaExecutable and Tree Utilities",
    "concepts": [
      "XlaExecutable.cost_analysis now returns a dict[str, float].",
      "jax.tree.flatten_with_path and jax.tree.map_with_path are added as shortcuts."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecations and Removals in Core and Libraries",
    "concepts": [
      "A number of APIs in the internal jax.core namespace have been deprecated or removed.",
      "Several previously-deprecated APIs have been removed from jax.core and jax.lib.xla_bridge.",
      "jax.numpy.round_ has been removed."
    ],
    "code_examples": []
  },
  {
    "title": "New Features: Device-Polymorphic Export and Lax Split",
    "concepts": [
      "jax.export.export() can be used for device-polymorphic export with shardings constructed with jax.sharding.AbstractMesh().",
      "jax.lax.split() has been added."
    ],
    "code_examples": []
  },
  {
    "title": "Bug Fixes in JIT and While Loop",
    "concepts": [
      "Fixed a bug where jit would error if an argument was named f.",
      "Fixed a bug that will throw index out of range error in jax.lax.while_loop() under specific circumstances."
    ],
    "code_examples": []
  },
  {
    "title": "Internal Changes: Stackless Tracing and Breaking Changes",
    "concepts": [
      "This release introduces 'stackless', an internal change to JAX\u2019s tracing machinery.",
      "jax.experimental.jax2tf.convert() with native_serialization=False or with enable_xla=False has been removed.",
      "xb, xc, and xe symbols have been removed from jax.interpreters.xla.",
      "The deprecated module jax.experimental.export has been removed.",
      "The initial argument to jax.nn.softmax() and jax.nn.log_softmax() has been removed.",
      "Calling np.asarray on typed PRNG keys now raises an error."
    ],
    "code_examples": []
  },
  {
    "title": "Removals from jax.export and Refactoring of JAX Build CLI",
    "concepts": [
      "Deprecated methods and functions in jax.export have been removed.",
      "The kwargs symbolic_scope and symbolic_constraints from jax.export.symbolic_args_specs() have been removed.",
      "Hashing of tracers now results in a TypeError.",
      "JAX build CLI (build/build.py) now uses a subcommand structure."
    ],
    "code_examples": []
  },
  {
    "title": "Changes in jax.scipy.linalg and jax.scipy.special",
    "concepts": [
      "jax.scipy.linalg.toeplitz() now does implicit batching on multi-dimensional inputs.",
      "jax.scipy.special.gamma() and jax.scipy.special.gammasgn() now return NaN for negative integer inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Removals and New Features in JAX Functionality",
    "concepts": [
      "jax.clear_backends was removed.",
      "The custom call \"__gpu$xla.gpu.triton\" was removed from the list of custom calls that have guaranteed export stability.",
      "jax.jit() got a new compiler_options argument.",
      "jax.tree_util.register_dataclass() now allows metadata fields to be declared inline via dataclasses.field().",
      "jax.numpy.put_along_axis() was added.",
      "jax.lax.linalg.eig() and related jax.numpy functions are now supported on GPU.",
      "Two new configuration flags were added to control the effort the compiler spends minimizing execution time and memory usage."
    ],
    "code_examples": []
  },
  {
    "title": "Bug Fixes, Deprecations, and Breaking Changes",
    "concepts": [
      "Fixed a bug related to GPU implementations of LU and QR decomposition.",
      "jax.lib.xla_extension.ArrayImpl and jax.lib.xla_client.ArrayImpl are deprecated; use jax.Array instead.",
      "jax.lib.xla_extension.XlaRuntimeError is deprecated; use jax.errors.JaxRuntimeError instead.",
      "jax.numpy.isscalar() now returns True for any array-like object with zero dimensions.",
      "jax.experimental.host_callback has been removed."
    ],
    "code_examples": []
  },
  {
    "title": "Changes in LAX and TPU Support",
    "concepts": [
      "jax.lax.FftType was introduced as a public name for the enum of FFT operations.",
      "JAX now installs TPU support from the libtpu package rather than libtpu-nightly."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecations related to Internal APIs and Callbacks",
    "concepts": [
      "The semi-public API jax.lib.xla_client.PaddingType has been deprecated.",
      "The default behavior of jax.pure_callback() and jax.extend.ffi.ffi_call() under vmap has been deprecated and so has the vectorized parameter to those functions.",
      "The semi-public API jax.lib.xla_client.register_custom_call_target has been deprecated.",
      "The semi-public APIs jax.lib.xla_client.dtype_to_etype, jax.lib.xla_client.ops, jax.lib.xla_client.shape_from_pyval, jax.lib.xla_client.PrimitiveType, jax.lib.xla_client.Shape, jax.lib.xla_client.XlaBuilder, and jax.lib.xla_client.XlaComputation have been deprecated."
    ],
    "code_examples": []
  },
  {
    "title": "New Functionality: Python 3.13 Support and Error Handling",
    "concepts": [
      "This release includes wheels for Python 3.13.",
      "jax.errors.JaxRuntimeError has been added as a public alias for the formerly private XlaRuntimeError type."
    ],
    "code_examples": []
  },
  {
    "title": "Breaking Changes related to PMAP and Array Shapes",
    "concepts": [
      "jax_pmap_no_rank_reduction flag is set to True by default.",
      "array[0] on a pmap result now introduces a reshape.",
      "The per-shard shape now has a leading (1, \u2026).",
      "The default value of the --jax_host_callback_legacy configuration value is set to True."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecations in JAX Libraries and Removal of XLA Computation",
    "concepts": [
      "In jax.numpy.trim_zeros(), non-arraylike arguments or arraylike arguments with ndim != 1 are now deprecated.",
      "Internal pretty-printing tools jax.core.pp_* have been removed.",
      "jax.lib.xla_client.Device is deprecated; use jax.Device instead.",
      "jax.lib.xla_client.XlaRuntimeError is deprecated; use jax.errors.JaxRuntimeError instead.",
      "jax.xla_computation is deleted.",
      "jax.ShapeDtypeStruct no longer accepts the named_shape argument.",
      "jax.tree.map(f, None, non-None) now raises an error.",
      "jax.sharding.XLACompatibleSharding has been removed."
    ],
    "code_examples": []
  },
  {
    "title": "Bug Fixes and Patches in Jax 0.4.32",
    "concepts": [
      "Fixed a bug where jax.numpy.cumsum() would produce incorrect outputs.",
      "Edit implementation of jax.numpy.ldexp() to get correct gradient.",
      "A TPU-only data corruption bug was found and fixed by pinning a fixed version of libtpu.",
      "This release fixes an inaccurate result for F64 tanh on CPU."
    ],
    "code_examples": []
  },
  {
    "title": "New Functionality: Foreign Function Interface (FFI)",
    "concepts": [
      "Added jax.extend.ffi.ffi_call() and jax.extend.ffi.ffi_lowering() to support the use of the new Foreign function interface (FFI) to interface with custom C++ and CUDA code from JAX."
    ],
    "code_examples": []
  },
  {
    "title": "Changes in Flags, Array API Support, and Dispatching",
    "concepts": [
      "jax_enable_memories flag is set to True by default.",
      "jax.numpy now supports v2023.12 of the Python Array API Standard.",
      "Computations on the CPU backend may now be dispatched asynchronously in more cases.",
      "Added new jax.process_indices() function to replace the jax.host_ids() function.",
      "jax.numpy.fabs has been modified to no longer support complex dtypes.",
      "jax.tree_util.register_dataclass now checks that data_fields and meta_fields includes all dataclass fields with init=True and only them, if nodetype is a dataclass.",
      "Several jax.numpy functions now have full ufunc interfaces.",
      "Added jax.lax.optimization_barrier(), which allows users to prevent compiler optimizations and control scheduling."
    ],
    "code_examples": []
  },
  {
    "title": "Breaking Changes and Deprecations",
    "concepts": [
      "The MHLO MLIR dialect (jax.extend.mlir.mhlo) has been removed. Use the stablehlo dialect instead.",
      "Complex inputs to jax.numpy.clip() and jax.numpy.hypot() are no longer allowed.",
      "Deprecated the following APIs from jax.lib.xla_bridge.",
      "The jax.experimental.array_api module is deprecated.",
      "The internal utilities jax.core.check_eqn, jax.core.check_type, and jax.core.check_valid_jaxtype are now deprecated.",
      "jax.numpy.round_ has been deprecated.",
      "Passing a DLPack capsule to jax.dlpack.from_dlpack() is deprecated."
    ],
    "code_examples": []
  },
  {
    "title": "Breaking Changes and Hermetic CUDA Support",
    "concepts": [
      "This release of jaxlib switched to a new version of the CPU backend.",
      "Hermetic CUDA support is added. ",
      "Hermetic CUDA uses a specific downloadable version of CUDA instead of the user\u2019s locally installed CUDA."
    ],
    "code_examples": []
  },
  {
    "title": "New Features: SparseCore Profiling",
    "concepts": [
      "SparseCore profiling is added.",
      "JAX now supports profiling SparseCore on TPUv5p chips."
    ],
    "code_examples": []
  },
  {
    "title": "Changes to CuDNN, Python, NumPy, and SciPy Versions",
    "concepts": [
      "The minimum CuDNN version is v9.1.",
      "The minimum Python version is now 3.10.",
      "The minimum NumPy version is now 1.24.",
      "The minimum SciPy version is now 1.10.",
      "jax.numpy.ceil(), jax.numpy.floor() and jax.numpy.trunc() now return the output of the same dtype as the input.",
      "libdevice.10.bc is no longer bundled with CUDA wheels.",
      "jax.experimental.pallas.BlockSpec now expects block_shape to be passed before index_map.",
      "Updated the repr of gpu devices to be more consistent with TPUs/CPUs.",
      "Added the device property and to_device method to jax.Array, as part of JAX\u2019s Array API support."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecations and HLO Lowering Rules",
    "concepts": [
      "Removed a number of previously-deprecated internal APIs related to polymorphic shapes from jax.core.",
      "HLO lowering rules should no longer wrap singleton ir.Values in tuples.",
      "jax.experimental.jax2tf.convert() with native_serialization=False or enable_xla=False is now deprecated.",
      "The previously-deprecated function jax.random.shuffle has been removed."
    ],
    "code_examples": []
  },
  {
    "title": "Bug Fixes and ML dtypes Support",
    "concepts": [
      "Fixed a bug that meant that negative static_argnums to a jit were mishandled by the jit dispatch fast path.",
      "Fixed a bug that meant triangular solves of batches of singular matrices produce nonsensical finite values.",
      "JAX supports ml_dtypes >= 0.2."
    ],
    "code_examples": []
  },
  {
    "title": "Changes in Dependencies, Export API and Mesh Utilities",
    "concepts": [
      "jax.experimental.mesh_utils can now create an efficient mesh for TPU v5e.",
      "jax now depends on jaxlib directly.",
      "Added an API for exporting and serializing JAX functions, now located in jax.export."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecations in Internal Tools and Export API",
    "concepts": [
      "Internal pretty-printing tools jax.core.pp_* are deprecated.",
      "Hashing of tracers is deprecated.",
      "jax.experimental.export is deprecated. Use jax.export instead.",
      "Passing an array in place of a dtype is now deprecated.",
      "jax.xla_computation is deprecated and will be removed in a future release."
    ],
    "code_examples": []
  },
  {
    "title": "Monolithic CUDA JAXLIBS and ML dtypes Version",
    "concepts": [
      "Support for monolithic CUDA jaxlibs has been dropped.",
      "JAX now requires ml_dtypes version 0.4.0 or newer."
    ],
    "code_examples": []
  },
  {
    "title": "Backwards Compatibility and is_leaf Argument",
    "concepts": [
      "Removed backwards-compatibility support for old usage of the jax.experimental.export API.",
      "Added is_leaf argument to jax.tree.all() & jax.tree_util.tree_all()."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecations for Sharding and Exported APIs",
    "concepts": [
      "jax.sharding.XLACompatibleSharding is deprecated.",
      "jax.experimental.Exported.in_shardings has been renamed as jax.experimental.Exported.in_shardings_hlo. Same for out_shardings."
    ],
    "code_examples": []
  },
  {
    "title": "Removals from JAX Libraries",
    "concepts": [
      "Removed a number of previously-deprecated APIs from jax.core, jax.lax, jax.nn, jax.interpreters.xla.",
      "The tol argument of jax.numpy.linalg.matrix_rank() is being deprecated.",
      "The rcond argument of jax.numpy.linalg.pinv() is being deprecated.",
      "The deprecated jax.config submodule has been removed.",
      "jax.random APIs no longer accept batched keys.",
      "In jax.scipy.special.beta(), the x and y parameters have been renamed to a and b."
    ],
    "code_examples": []
  },
  {
    "title": "New Functionality: Shardings",
    "concepts": [
      "Added jax.experimental.Exported.in_shardings_jax() to construct shardings."
    ],
    "code_examples": []
  },
  {
    "title": "Bug Fixes in XLA and Compiler Crashes",
    "concepts": [
      "Fixed a bug where XLA sharded some concatenation operations incorrectly.",
      "Fixed a bug where XLA:CPU miscompiled certain matmul fusions.",
      "Fixes a compiler crash on GPU."
    ],
    "code_examples": []
  },
  {
    "title": "Tree Mapping Deprecation",
    "concepts": [
      "jax.tree.map(f, None, non-None) now emits a DeprecationWarning."
    ],
    "code_examples": []
  },
  {
    "title": "Deprecations & Removals: Sort, Reshape and Compute Capability",
    "concepts": [
      "The kind argument to jax.numpy.sort() and jax.numpy.argsort() is now removed.",
      "Removed get_compute_capability from the jax.experimental.pallas.gpu module.",
      "The newshape argument to jax.numpy.reshape() is being deprecated."
    ],
    "code_examples": []
  },
  {
    "title": "Bug Fixes: Memory Corruption and Compilation",
    "concepts": [
      "Fixes a memory corruption bug in the type name of Array and JIT Python objects in Python 3.10 or earlier.",
      "Fixed a warning '+ptx84' is not a recognized feature for this target under CUDA 12.4.",
      "Fixed a slow compilation problem on CPU."
    ],
    "code_examples": []
  },
  {
    "title": "Windows Build and New Functions",
    "concepts": [
      "The Windows build is now built with Clang instead of MSVC.",
      "Added jax.numpy.unstack() and jax.numpy.cumulative_sum().",
      "Added a new config option jax_cpu_collectives_implementation."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Array Usage and Complex Number Semantics",
    "concepts": [
      "jax.pure_callback() , jax.experimental.io_callback() and jax.debug.callback() now use jax.Array instead of np.ndarray.",
      "complex_arr.astype(bool) now follows the same semantics."
    ],
    "code_examples": []
  },
  {
    "title": "Arrays and Devices",
    "concepts": [
      "JAX's array object is analogous to NumPy's ndarray.",
      "CPUs are standard computational architecture.",
      "GPUs were originally for image rendering but are now general-purpose and offer faster array operations than CPUs.",
      "TPUs are engineered for fast tensor operations, especially in deep learning.",
      "JAX can target CPUs, GPUs, and TPUs for computations.",
      "Device refers to the CPU, GPU, or TPU used by JAX.",
      "XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra operations and the primary backend for JIT-compiled JAX code."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Transformations and Compilation",
    "concepts": [
      "JIT (Just In Time) compilation compiles array operations to XLA, often using jax.jit().",
      "jaxpr is an intermediate representation of a computation generated by JAX and forwarded to XLA.",
      "Primitives are fundamental units of computation in JAX programs.",
      "Transformation is a higher-order function that takes a function as input and returns a transformed function.",
      "SPMD (Single Program Multi Data) is a parallel computation technique implemented by jax.pmap().",
      "Tracer is an object used as a standin for a JAX Array to determine the sequence of operations.",
      "A static value is a value that is not traced in JIT compilation."
    ],
    "code_examples": []
  },
  {
    "title": "Automatic Differentiation",
    "concepts": [
      "JVP (Jacobian Vector Product) is forward-mode automatic differentiation, implemented via jax.jvp().",
      "VJP (Vector Jacobian Product) is reverse-mode automatic differentiation, implemented via jax.vjp()."
    ],
    "code_examples": []
  },
  {
    "title": "Programming Paradigms",
    "concepts": [
      "Functional programming involves defining programs with pure functions.",
      "A pure function's output depends only on its inputs and has no side-effects.",
      "Pytrees are abstractions that allow JAX to handle containers of array values uniformly."
    ],
    "code_examples": []
  },
  {
    "title": "Data Types",
    "concepts": [
      "Weakly-typed values in JAX have the same type promotion semantics as Python scalars."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Pytrees",
    "concepts": [
      "JAX has built-in support for objects that look like dictionaries of arrays, or lists of lists of dicts, or other nested structures called pytrees.",
      "A pytree is a container-like structure built out of container-like Python objects.",
      "A pytree can include lists, tuples, and dicts.",
      "A leaf is anything that\u2019s not a pytree, such as an array, but a single leaf is also a pytree.",
      "Pytrees can contain model parameters, dataset entries, and reinforcement learning agent observations."
    ],
    "code_examples": [
      {
        "description": "Example of a simple pytree and extracting the flattened leaves from the trees using `jax.tree.leaves()`.",
        "code": "import jax\nimport jax.numpy as jnp\n\nexample_trees = [\n    [1, 'a', object()],\n    (1, (2, 3), ()),\n    [1, {'k1': 2, 'k2': (3, 4)}, 5],\n    {'a': 2, 'b': (2, 3)},\n    jnp.array([1, 2, 3]),\n]\n\n# Print how many leaves the pytrees have.\nfor pytree in example_trees:\n    # This `jax.tree.leaves()` method extracts the flattened leaves from the pytrees.\n    leaves = jax.tree.leaves(pytree)\n    print(f\"{repr(pytree):<45} has {len(leaves)} leaves: {leaves}\")"
      }
    ]
  },
  {
    "title": "Pytree Utilities in JAX",
    "concepts": [
      "Classes are considered container-like if they are in the pytree registry, which by default includes lists, tuples, and dicts.",
      "Any object whose type is not in the pytree container registry will be treated as a leaf node in the tree.",
      "The pytree registry can be extended to include user-defined container classes.",
      "JAX provides utilities to operate over pytrees, found in the jax.tree_util subpackage (aliased as jax.tree)."
    ],
    "code_examples": []
  },
  {
    "title": "jax.tree.map()",
    "concepts": [
      "The most commonly used pytree function is jax.tree.map().",
      "It works analogously to Python\u2019s native map, but transparently operates over entire pytrees.",
      "jax.tree.map() allows mapping a N-ary function over multiple arguments.",
      "When using multiple arguments with jax.tree.map(), the structure of the inputs must exactly match."
    ],
    "code_examples": [
      {
        "description": "Example of using `jax.tree.map()` to apply a function to each element in a list of lists.",
        "code": "import jax\nlist_of_lists = [\n    [1, 2, 3],\n    [1, 2],\n    [1, 2, 3, 4]\n]\njax.tree.map(lambda x: x * 2, list_of_lists)"
      },
      {
        "description": "Example of using `jax.tree.map()` with multiple arguments to add corresponding elements in two lists of lists.",
        "code": "import jax\nlist_of_lists = [\n    [1, 2, 3],\n    [1, 2],\n    [1, 2, 3, 4]\n]\nanother_list_of_lists = list_of_lists\njax.tree.map(lambda x, y: x + y, list_of_lists, another_list_of_lists)"
      }
    ]
  },
  {
    "title": "Pytrees in Machine Learning: A Simple MLP",
    "concepts": [
      "Pytree operations can be useful when training a simple multi-layer perceptron (MLP).",
      "Model parameters can be organized as pytrees.",
      "jax.tree.map() can be used to inspect the shapes of parameters."
    ],
    "code_examples": [
      {
        "description": "Function to initialize MLP parameters.",
        "code": "import numpy as np\n\ndef init_mlp_params(layer_widths):\n    params = []\n    for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n        params.append(dict(\n            weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2/n_in),\n            biases=np.ones(shape=(n_out,))\n        ))\n    return params\n\nparams = init_mlp_params([1, 128, 128, 1])"
      },
      {
        "description": "Using jax.tree.map() to check the shapes of the initial parameters.",
        "code": "import jax\nimport numpy as np\n\ndef init_mlp_params(layer_widths):\n    params = []\n    for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n        params.append(dict(\n            weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2/n_in),\n            biases=np.ones(shape=(n_out,))\n        ))\n    return params\n\nparams = init_mlp_params([1, 128, 128, 1])\n\njax.tree.map(lambda x: x.shape, params)"
      },
      {
        "description": "Defining the forward pass, loss function, and parameter update function for training the MLP model.",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef init_mlp_params(layer_widths):\n    params = []\n    for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n        params.append(dict(\n            weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2/n_in),\n            biases=np.ones(shape=(n_out,))\n        ))\n    return params\n\nparams = init_mlp_params([1, 128, 128, 1])\n\n# Define the forward pass.\ndef forward(params, x):\n    *hidden, last = params\n    for layer in hidden:\n        x = jax.nn.relu(x @ layer['weights'] + layer['biases'])\n    return x @ last['weights'] + last['biases']\n\n# Define the loss function.\ndef loss_fn(params, x, y):\n    return jnp.mean((forward(params, x) - y) ** 2)\n\n# Set the learning rate.\nLEARNING_RATE = 0.0001\n\n# Using the stochastic gradient descent, define the parameter update function.\n# Apply `@jax.jit` for JIT compilation (speed).\n@jax.jit\ndef update(params, x, y):\n    # Calculate the gradients with `jax.grad`.\n    grads = jax.grad(loss_fn)(params, x, y)\n    # Note that `grads` is a pytree with the same structure as `params`.\n    # `jax.grad` is one of many JAX functions that has\n    # built-in support for pytrees.\n    # This is useful - you can apply the SGD update using JAX pytree utilities.\n    return jax.tree.map(lambda p, g: p - LEARNING_RATE * g, params, grads)"
      }
    ]
  },
  {
    "title": "Custom Pytree Nodes",
    "concepts": [
      "JAX allows extending the set of Python types that will be considered internal nodes in pytrees by using jax.tree_util.register_pytree_node().",
      "If you define your own container class, it will be considered a pytree leaf unless you register it with JAX.",
      "JAX maintains a global registry of types to be considered internal pytree nodes.",
      "Values of registered types are traversed recursively."
    ],
    "code_examples": [
      {
        "description": "Example of a custom class that is not automatically treated as a pytree node.",
        "code": "import jax\nclass Special(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\njax.tree.leaves([Special(0, 1), Special(2, 4),])"
      },
      {
        "description": "Example of registering a custom class as a pytree node using `jax.tree_util.register_pytree_node()`.",
        "code": "import jax\nfrom jax.tree_util import register_pytree_node\n\nclass Special(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\nclass RegisteredSpecial(Special):\n    def __repr__(self):\n        return \"RegisteredSpecial(x={}, y={})\" .format(self.x, self.y)\n\ndef special_flatten(v):\n    \"\"\"Specifies a flattening recipe.\n    Params:\n        v: The value of the registered type to flatten.\n    Returns:\n        A pair of an iterable with the children to be flattened recursively,\n        and some opaque auxiliary data to pass back to the unflattening recipe.\n        The auxiliary data is stored in the treedef for use during unflattening.\n        The auxiliary data could be used, for example, for dictionary keys.\n    \"\"\"\n    children = (v.x, v.y)\n    aux_data = None\n    return (children, aux_data)\n\ndef special_unflatten(aux_data, children):\n    \"\"\"Specifies an unflattening recipe.\n    Params:\n        aux_data: The opaque data that was specified during flattening of the\n            current tree definition.\n        children: The unflattened children\n    Returns:\n        A reconstructed object of the registered type, using the specified\n        children and auxiliary data.\n    \"\"\"\n    return RegisteredSpecial(*children)\n\n# Global registration\nregister_pytree_node(\n    RegisteredSpecial,\n    special_flatten,  # Instruct JAX what are the children nodes.\n    special_unflatten  # Instruct JAX how to pack back into a `RegisteredSpecial`.\n)"
      },
      {
        "description": "Example of using jax.tree.map after registering the custom class.",
        "code": "import jax\nfrom jax.tree_util import register_pytree_node\n\nclass Special(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\nclass RegisteredSpecial(Special):\n    def __repr__(self):\n        return \"RegisteredSpecial(x={}, y={})\" .format(self.x, self.y)\n\ndef special_flatten(v):\n    \"\"\"Specifies a flattening recipe.\n    Params:\n        v: The value of the registered type to flatten.\n    Returns:\n        A pair of an iterable with the children to be flattened recursively,\n        and some opaque auxiliary data to pass back to the unflattening recipe.\n        The auxiliary data is stored in the treedef for use during unflattening.\n        The auxiliary data could be used, for example, for dictionary keys.\n    \"\"\"\n    children = (v.x, v.y)\n    aux_data = None\n    return (children, aux_data)\n\ndef special_unflatten(aux_data, children):\n    \"\"\"Specifies an unflattening recipe.\n    Params:\n        aux_data: The opaque data that was specified during flattening of the\n            current tree definition.\n        children: The unflattened children\n    Returns:\n        A reconstructed object of the registered type, using the specified\n        children and auxiliary data.\n    \"\"\"\n    return RegisteredSpecial(*children)\n\n# Global registration\nregister_pytree_node(\n    RegisteredSpecial,\n    special_flatten,  # Instruct JAX what are the children nodes.\n    special_unflatten  # Instruct JAX how to pack back into a `RegisteredSpecial`.\n)\n\njax.tree.map(lambda x: x + 1, [RegisteredSpecial(0, 1), RegisteredSpecial(2, 4),])"
      },
      {
        "description": "Example of defining a container using NamedTuple subclass.",
        "code": "import jax\nfrom typing import NamedTuple, Any\n\nclass MyOtherContainer(NamedTuple):\n    name: str\n    a: Any\n    b: Any\n    c: Any\n\n# NamedTuple subclasses are handled as pytree nodes, so\n# this will work out-of-the-box.\njax.tree.leaves([MyOtherContainer('Alice', 1, 2, 3), MyOtherContainer('Bob', 4, 5, 6)])"
      },
      {
        "description": "Example of defining a container using @dataclass and registering it with `jax.tree_util.register_dataclass()`",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom dataclasses import dataclass\nimport functools\n\n@functools.partial(\n    jax.tree_util.register_dataclass,\n    data_fields=['a', 'b', 'c'],\n    meta_fields=['name']\n)\n@dataclass\nclass MyDataclassContainer(object):\n    name: str\n    a: Any\n    b: Any\n    c: Any\n\n# MyDataclassContainer is now a pytree node.\njax.tree.leaves([\n    MyDataclassContainer('apple', 5.3, 1.2, jnp.zeros([4])),\n    MyDataclassContainer('banana', np.array([3, 4]), -1., 0.)\n])"
      },
      {
        "description": "Example of using dataclass container in JIT-ed function, where name is considered static",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom dataclasses import dataclass\nimport functools\n\n@functools.partial(\n    jax.tree_util.register_dataclass,\n    data_fields=['a', 'b', 'c'],\n    meta_fields=['name']\n)\n@dataclass\nclass MyDataclassContainer(object):\n    name: str\n    a: Any\n    b: Any\n    c: Any\n\n@jax.jit\ndef f(x: MyDataclassContainer):\n    return x.a + x.b\n\n# Works fine! `mdc.name` is static.\nmdc = MyDataclassContainer('mdc', 1, 2, 3)\ny = f(mdc)"
      },
      {
        "description": "Example of using NamedTuple subclass in JIT-ed function, where name is considered a leaf, resulting in a TypeError",
        "code": "import jax\nfrom typing import NamedTuple, Any\n\nclass MyOtherContainer(NamedTuple):\n    name: str\n    a: Any\n    b: Any\n    c: Any\n\n@jax.jit\ndef f(x: MyOtherContainer):\n    return x.a + x.b\n\nmoc = MyOtherContainer('moc', 1, 2, 3)\ny = f(moc)"
      }
    ]
  },
  {
    "title": "Pytrees and JAX Function Transformations",
    "concepts": [
      "Many JAX functions, like jax.lax.scan(), operate over pytrees of arrays.",
      "All JAX function transformations can be applied to functions that accept as input and produce as output pytrees of arrays.",
      "Parameters like in_axes and out_axes in JAX transformations can be pytrees.",
      "The structure of the parameter pytrees must correspond to the pytree structure of the corresponding arguments.",
      "Parameter pytrees are often constrained to be tree prefixes of the argument pytrees."
    ],
    "code_examples": []
  },
  {
    "title": "Key Paths in Pytrees",
    "concepts": [
      "In a pytree each leaf has a key path.",
      "A key path for a leaf is a list of keys, where the length of the list is equal to the depth of the leaf in the pytree.",
      "Each key is a hashable object that represents an index into the corresponding pytree node type.",
      "JAX has jax.tree_util.tree_flatten_with_path(), jax.tree_util.tree_map_with_path(), and jax.tree_util.keystr() methods for working with key paths.",
      "The default key types for the built-in pytree node types are SequenceKey, DictKey, and GetAttrKey.",
      "You can define your own key types for your custom nodes."
    ],
    "code_examples": [
      {
        "description": "Example of printing debugging information related to a certain leaf value using jax.tree_util.tree_flatten_with_path and jax.tree_util.keystr.",
        "code": "import jax\nimport jax.tree_util\nimport collections\n\nATuple = collections.namedtuple(\"ATuple\", ('name'))\n\ntree = [\n    1,\n    {'k1': 2, 'k2': (3, 4)},\n    ATuple('foo')\n]\n\nflattened, _ = jax.tree_util.tree_flatten_with_path(tree)\n\nfor key_path, value in flattened:\n    print(f'Value of tree{jax.tree_util.keystr(key_path)}: {value}')"
      },
      {
        "description": "Example of how to print the key paths for each leaf value using jax.tree_util.tree_flatten_with_path and jax.tree_util.keystr.",
        "code": "import jax\nimport jax.tree_util\nimport collections\n\nATuple = collections.namedtuple(\"ATuple\", ('name'))\n\ntree = [\n    1,\n    {'k1': 2, 'k2': (3, 4)},\n    ATuple('foo')\n]\n\nflattened, _ = jax.tree_util.tree_flatten_with_path(tree)\n\nfor key_path, _ in flattened:\n    print(f'Key path of tree{jax.tree_util.keystr(key_path)}: {repr(key_path)}')"
      }
    ]
  },
  {
    "title": "Common Gotchas with Pytrees",
    "concepts": [
      "Accidentally introducing tree nodes instead of leaves is a common pitfall.",
      "The shape of an array is a tuple, which is a pytree node, with its elements as leaves.",
      "jax.tree_util functions treat None as the absence of a pytree node, not as a leaf.",
      "JAX transformations occasionally initialize user-defined pytree objects with unexpected values.",
      "The __init__ and __new__ methods of custom pytree classes should generally avoid doing any array conversion or other input validation, or else anticipate and handle these special cases.",
      "Structure your custom tree_unflatten function so that it avoids calling __init__."
    ],
    "code_examples": [
      {
        "description": "Example of accidentally introducing tree nodes instead of leaves when trying to create a pytree with ones instead of zeros, from existing pytree",
        "code": "import jax\nimport jax.numpy as jnp\na_tree = [\n    jnp.zeros((2, 3)),\n    jnp.zeros((3, 4))\n]\n\n# Try to make another pytree with ones instead of zeros.\nshapes = jax.tree.map(lambda x: x.shape, a_tree)\njax.tree.map(jnp.ones, shapes)"
      },
      {
        "description": "Using the `is_leaf` argument to treat `None` as a leaf.",
        "code": "import jax\n\njax.tree.leaves([None, None, None], is_leaf=lambda x: x is None)"
      },
      {
        "description": "Example of how JAX transformations occasionally initialize user-defined pytree objects with unexpected values",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import register_pytree_node\n\nclass MyTree:\n    def __init__(self, a):\n        self.a = jnp.asarray(a)\n\nregister_pytree_node(\n    MyTree,\n    lambda tree: ((tree.a,), None),\n    lambda _, args: MyTree(*args)\n)\n\ntree = MyTree(jnp.arange(5.0))\n# Results in Error because object() is passed to `MyTree`.\n# jax.vmap(lambda x: x)(tree)\n\n# Results in Error because MyTree(...) is passed to `MyTree`.\n# jax.jacobian(lambda x: x)(tree)"
      },
      {
        "description": "Potential solution 1: Modifying the __init__ method to anticipate and handle special cases.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import register_pytree_node\n\nclass MyTree:\n    def __init__(self, a):\n        if not (type(a) is object or a is None or isinstance(a, MyTree)):\n            a = jnp.asarray(a)\n        self.a = a"
      },
      {
        "description": "Potential solution 2: Modifying the tree_unflatten function so that it avoids calling __init__",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax.tree_util import register_pytree_node\n\nclass MyTree:\n    def __init__(self, a):\n        if not (type(a) is object or a is None or isinstance(a, MyTree)):\n            a = jnp.asarray(a)\n        self.a = a\n\ndef tree_unflatten(aux_data, children):\n    del aux_data  # Unused in this class.\n    obj = object.__new__(MyTree)\n    obj.a = a\n    return obj"
      }
    ]
  },
  {
    "title": "Common Patterns with Pytrees",
    "concepts": [
      "To transpose a pytree (turn a list of trees into a tree of lists), JAX has two functions: jax.tree.map() and jax.tree.transpose().",
      "jax.tree.transpose() is more flexible, complex and verbose."
    ],
    "code_examples": [
      {
        "description": "Example of transposing a pytree using jax.tree.map().",
        "code": "import jax\n\ndef tree_transpose(list_of_trees):\n    \"\"\"\n    Converts a list of trees of identical structure into a single tree of lists.\n    \"\"\"\n    return jax.tree.map(lambda *xs: list(xs), *list_of_trees)\n\n# Convert a dataset from row-major to column-major.\nepisode_steps = [\n    dict(t=1, obs=3),\n    dict(t=2, obs=4)\n]\ntree_transpose(episode_steps)"
      },
      {
        "description": "Example of transposing a pytree using jax.tree.transpose().",
        "code": "import jax\n\ndef tree_transpose(list_of_trees):\n    \"\"\"\n    Converts a list of trees of identical structure into a single tree of lists.\n    \"\"\"\n    return jax.tree.map(lambda *xs: list(xs), *list_of_trees)\n\n# Convert a dataset from row-major to column-major.\nepisode_steps = [\n    dict(t=1, obs=3),\n    dict(t=2, obs=4)\n]\n\njax.tree.transpose(\n    outer_treedef=jax.tree.structure([0 for e in episode_steps]),\n    inner_treedef=jax.tree.structure(episode_steps[0]),\n    pytree_to_transpose=episode_steps\n)"
      }
    ]
  },
  {
    "title": "Introduction to JAX Callbacks",
    "concepts": [
      "JAX callbacks allow execution of Python code on the host during JAX runtime.",
      "Examples of JAX callbacks include jax.pure_callback, jax.experimental.io_callback, and jax.debug.callback.",
      "Callbacks can be used even under JAX transformations like jit(), vmap(), and grad().",
      "Callbacks are a way to perform host-side execution of code at runtime."
    ],
    "code_examples": []
  },
  {
    "title": "Printing Values During JAX Computation without Callbacks",
    "concepts": [
      "Using a simple Python print() statement within a JAX-jitted function will print the trace-time abstract value, not the runtime value.",
      "The printed value represents the abstract shape and type during tracing, not the actual numerical value during execution."
    ],
    "code_examples": [
      {
        "description": "Demonstrates printing an intermediate value within a jitted function. The output shows the traced abstract value instead of the actual runtime value.",
        "code": "import jax\n\n@jax.jit\ndef f(x):\n  y = x + 1\n  print(\"intermediate value: {}\".format(y))\n  return y * 2\n\nresult = f(2)"
      }
    ]
  },
  {
    "title": "Printing Values Using jax.debug.print()",
    "concepts": [
      "jax.debug.print() allows printing the runtime value of a variable during JAX computation.",
      "It passes the runtime value to the host process for printing.",
      "jax.debug.print() is a wrapper around jax.debug.callback()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates printing an intermediate value within a jitted function using jax.debug.print(). The output shows the actual runtime value.",
        "code": "import jax\n\n@jax.jit\ndef f(x):\n  y = x + 1\n  jax.debug.print(\"intermediate value: {}\", y)\n  return y * 2\n\nresult = f(2)"
      }
    ]
  },
  {
    "title": "Evolution of JAX Callbacks: From host_callback to pure_callback, io_callback, and debug_callback",
    "concepts": [
      "jax.experimental.host_callback() is deprecated.",
      "New callbacks (jax.pure_callback(), jax.experimental.io_callback(), jax.debug.callback()) are designed for different situations.",
      "These callbacks are distinguished by the transformations and compiler optimizations they allow.",
      "jax.pure_callback() is suitable for pure functions (no side effects).",
      "jax.experimental.io_callback() is suitable for impure functions (e.g., reading/writing data).",
      "jax.debug.callback() is suitable for functions that reflect the execution behavior of the compiler."
    ],
    "code_examples": []
  },
  {
    "title": "Comparison of Callback Function Features",
    "concepts": [
      "A table summarizes the features of different callback functions, including support for return values, jit, vmap, grad, scan/while_loop, and guaranteed execution.",
      "jax.pure_callback() supports return values, jit, vmap, and scan/while_loop, but not guaranteed execution and grad directly (requires custom_jvp for autodiff).",
      "jax.experimental.io_callback() supports return values, jit, grad, scan/while_loop, and guaranteed execution, but vmap support is limited.",
      "jax.debug.callback() supports jit, vmap, grad, scan/while_loop but no return value and guaranteed execution."
    ],
    "code_examples": []
  },
  {
    "title": "Exploring jax.pure_callback()",
    "concepts": [
      "jax.pure_callback() is intended for pure functions (no side effects).",
      "It may be elided or called multiple times because JAX assumes the function is pure.",
      "It's compatible with jit, scan, and while_loop.",
      "jax.pure_callback has undefined autodiff semantics and requires `jax.custom_jvp` for gradients."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using jax.pure_callback() to call a NumPy function from JAX. It uses `jax.ShapeDtypeStruct` to define the shape and dtype of the result and specifies `vmap_method='sequential'`.",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef f_host(x):\n  # call a numpy (not jax.numpy) operation:\n  return np.sin(x).astype(x.dtype)\n\ndef f(x):\n  result_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)\n  return jax.pure_callback(f_host, result_shape, x, vmap_method='sequential')\n\nx = jnp.arange(5.0)\nf(x)"
      },
      {
        "description": "Demonstrates jit compatibility.",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef f_host(x):\n  # call a numpy (not jax.numpy) operation:\n  return np.sin(x).astype(x.dtype)\n\ndef f(x):\n  result_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)\n  return jax.pure_callback(f_host, result_shape, x, vmap_method='sequential')\n\nx = jnp.arange(5.0)\njax.jit(f)(x)"
      },
      {
        "description": "Demonstrates scan compatibility.",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef f_host(x):\n  # call a numpy (not jax.numpy) operation:\n  return np.sin(x).astype(x.dtype)\n\ndef f(x):\n  result_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)\n  return jax.pure_callback(f_host, result_shape, x, vmap_method='sequential')\n\nx = jnp.arange(5.0)\ndef body_fun(_, x):\n  return _, f(x)\njax.lax.scan(body_fun, None, jnp.arange(5.0))[1]"
      },
      {
        "description": "Demonstrates vmap compatibility.",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef f_host(x):\n  # call a numpy (not jax.numpy) operation:\n  return np.sin(x).astype(x.dtype)\n\ndef f(x):\n  result_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)\n  return jax.pure_callback(f_host, result_shape, x, vmap_method='sequential')\n\nx = jnp.arange(5.0)\njax.vmap(f)(x)"
      },
      {
        "description": "Demonstrates that grad is not directly compatible and produces ValueError",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef f_host(x):\n  # call a numpy (not jax.numpy) operation:\n  return np.sin(x).astype(x.dtype)\n\ndef f(x):\n  result_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)\n  return jax.pure_callback(f_host, result_shape, x, vmap_method='sequential')\n\nx = jnp.arange(5.0)\njax.grad(f)(x)"
      },
      {
        "description": "Demonstrates that compiler can eliminate the function call if the output of the callback is unused.",
        "code": "import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef print_something():\n  print('printing something')\n  return np.int32(0)\n\n@jax.jit\ndef f1():\n  return jax.pure_callback(print_something, np.int32(0))\n\nf1();\n\n@jax.jit\ndef f2():\n  jax.pure_callback(print_something, np.int32(0))\n  return 1.0\n\nf2();"
      },
      {
        "description": "Demonstrates that passing impure functions to pure_callback may result in unexpected behavior during transformations like jax.jit() or jax.vmap()",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef raise_via_callback(x):\n  def _raise(x):\n    raise ValueError(f\"value of x is {x}\")\n  return jax.pure_callback(_raise, x, x)\n\ndef raise_if_negative(x):\n  return jax.lax.cond(x < 0, raise_via_callback, lambda x: x, x)\n\nx_batch = jnp.arange(4)\n[raise_if_negative(x) for x in x_batch]\n# does not raise\njax.vmap(raise_if_negative)(x_batch)"
      }
    ]
  },
  {
    "title": "Exploring jax.experimental.io_callback()",
    "concepts": [
      "jax.experimental.io_callback() is meant to be used with impure functions (functions with side effects).",
      "It's compatible with vmap by default, but the order of execution is not guaranteed unless ordered=True is set.",
      "scan and while_loop work with io_callback regardless of ordering.",
      "Like pure_callback, io_callback fails under automatic differentiation if it's passed a differentiated variable."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using io_callback with a global host-side NumPy random generator.",
        "code": "from jax.experimental import io_callback\nfrom functools import partial\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\n\nglobal_rng = np.random.default_rng(0)\n\ndef host_side_random_like(x):\n  \"\"\"Generate a random array like x using the global_rng state\"\"\"\n  # We have two side-effects here:\n  # - printing the shape and dtype\n  # - calling global_rng, thus updating its state\n  print(f'generating {x.dtype}{list(x.shape)}')\n  return global_rng.uniform(size=x.shape).astype(x.dtype)\n\n@jax.jit\ndef numpy_random_like(x):\n  return io_callback(host_side_random_like, x, x)\n\nx = jnp.zeros(5)\nnumpy_random_like(x)"
      },
      {
        "description": "Demonstrates vmap compatibility.",
        "code": "from jax.experimental import io_callback\nfrom functools import partial\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\n\nglobal_rng = np.random.default_rng(0)\n\ndef host_side_random_like(x):\n  \"\"\"Generate a random array like x using the global_rng state\"\"\"\n  # We have two side-effects here:\n  # - printing the shape and dtype\n  # - calling global_rng, thus updating its state\n  print(f'generating {x.dtype}{list(x.shape)}')\n  return global_rng.uniform(size=x.shape).astype(x.dtype)\n\n@jax.jit\ndef numpy_random_like(x):\n  return io_callback(host_side_random_like, x, x)\n\nx = jnp.zeros(5)\njax.vmap(numpy_random_like)(x)"
      },
      {
        "description": "Demonstrates setting ordered=True to raise an error if vmap is used.",
        "code": "from jax.experimental import io_callback\nfrom functools import partial\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\n\nglobal_rng = np.random.default_rng(0)\n\ndef host_side_random_like(x):\n  \"\"\"Generate a random array like x using the global_rng state\"\"\"\n  # We have two side-effects here:\n  # - printing the shape and dtype\n  # - calling global_rng, thus updating its state\n  print(f'generating {x.dtype}{list(x.shape)}')\n  return global_rng.uniform(size=x.shape).astype(x.dtype)\n\n@jax.jit\ndef numpy_random_like_ordered(x):\n  return io_callback(host_side_random_like, x, x, ordered=True)\n\nx = jnp.zeros(5)\njax.vmap(numpy_random_like_ordered)(x)"
      },
      {
        "description": "Demonstrates scan compatibility.",
        "code": "from jax.experimental import io_callback\nfrom functools import partial\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\n\nglobal_rng = np.random.default_rng(0)\n\ndef host_side_random_like(x):\n  \"\"\"Generate a random array like x using the global_rng state\"\"\"\n  # We have two side-effects here:\n  # - printing the shape and dtype\n  # - calling global_rng, thus updating its state\n  print(f'generating {x.dtype}{list(x.shape)}')\n  return global_rng.uniform(size=x.shape).astype(x.dtype)\n\n@jax.jit\ndef numpy_random_like_ordered(x):\n  return io_callback(host_side_random_like, x, x, ordered=True)\n\nx = jnp.zeros(5)\ndef body_fun(_, x):\n  return _, numpy_random_like_ordered(x)\njax.lax.scan(body_fun, None, jnp.arange(5.0))[1]"
      },
      {
        "description": "Demonstrates that grad is not directly compatible and produces ValueError",
        "code": "from jax.experimental import io_callback\nfrom functools import partial\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\n\nglobal_rng = np.random.default_rng(0)\n\ndef host_side_random_like(x):\n  \"\"\"Generate a random array like x using the global_rng state\"\"\"\n  # We have two side-effects here:\n  # - printing the shape and dtype\n  # - calling global_rng, thus updating its state\n  print(f'generating {x.dtype}{list(x.shape)}')\n  return global_rng.uniform(size=x.shape).astype(x.dtype)\n\n@jax.jit\ndef numpy_random_like(x):\n  return io_callback(host_side_random_like, x, x)\n\nx = jnp.zeros(5)\njax.grad(numpy_random_like)(x)"
      },
      {
        "description": "Demonstrates that if the callback is not dependent on a differentiated variable, it will execute.",
        "code": "import jax\nfrom jax.experimental import io_callback\n\n@jax.jit\ndef f(x):\n  io_callback(lambda: print('hello'), None)\n  return x\n\njax.grad(f)(1.0);"
      }
    ]
  },
  {
    "title": "Exploring jax.debug.callback()",
    "concepts": [
      "jax.debug.callback() makes no assumptions about the callback function.",
      "The callback action reflects exactly what JAX is doing.",
      "debug.callback cannot return any value to the JAX program.",
      "It is compatible with vmap and grad, making it useful for debugging."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using jax.debug.callback() to log a value during JAX computation.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import debug\n\ndef log_value(x):\n  # This could be an actual logging call; we'll use\n  # print() for demonstration\n  print(\"log:\", x)\n\n@jax.jit\ndef f(x):\n  debug.callback(log_value, x)\n  return x\n\nf(1.0);"
      },
      {
        "description": "Demonstrates vmap compatibility.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import debug\n\ndef log_value(x):\n  # This could be an actual logging call; we'll use\n  # print() for demonstration\n  print(\"log:\", x)\n\n@jax.jit\ndef f(x):\n  debug.callback(log_value, x)\n  return x\n\nx = jnp.arange(5.0)\njax.vmap(f)(x);"
      },
      {
        "description": "Demonstrates grad compatibility.",
        "code": "import jax\nimport jax.numpy as jnp\nfrom jax import debug\n\ndef log_value(x):\n  # This could be an actual logging call; we'll use\n  # print() for demonstration\n  print(\"log:\", x)\n\n@jax.jit\ndef f(x):\n  debug.callback(log_value, x)\n  return x\n\njax.grad(f)(1.0);"
      }
    ]
  },
  {
    "title": "Combining jax.pure_callback() with jax.custom_jvp()",
    "concepts": [
      "jax.pure_callback() can be combined with jax.custom_jvp() to define custom gradient rules.",
      "This allows creating JAX-compatible wrappers for SciPy or NumPy functions that are not yet available in JAX.",
      "By defining a custom JVP rule, you can enable automatic differentiation for functions wrapped with pure_callback."
    ],
    "code_examples": [
      {
        "description": "Demonstrates creating a JAX-compatible wrapper for the SciPy Bessel function of the first kind (scipy.special.jv) using pure_callback.",
        "code": "import jax\nimport jax.numpy as jnp\nimport scipy.special\n\ndef jv(v, z):\n  v, z = jnp.asarray(v), jnp.asarray(z)\n  # Require the order v to be integer type: this simplifies\n  # the JVP rule below.\n  assert jnp.issubdtype(v.dtype, jnp.integer)\n  # Promote the input to inexact (float/complex).\n  # Note that jnp.result_type() accounts for the enable_x64 flag.\n  z = z.astype(jnp.result_type(float, z.dtype))\n  # Wrap scipy function to return the expected dtype.\n  _scipy_jv = lambda v, z: scipy.special.jv(v, z).astype(z.dtype)\n  # Define the expected shape & dtype of output.\n  result_shape_dtype = jax.ShapeDtypeStruct(\n      shape=jnp.broadcast_shapes(v.shape, z.shape),\n      dtype=z.dtype)\n  # Use vmap_method=\"broadcast_all\" because scipy.special.jv handles broadcasted inputs.\n  return jax.pure_callback(_scipy_jv, result_shape_dtype, v, z, vmap_method=\"broadcast_all\")"
      },
      {
        "description": "Demonstrates using the jv function with jit and vmap.",
        "code": "import jax\nimport jax.numpy as jnp\nimport scipy.special\n\ndef jv(v, z):\n  v, z = jnp.asarray(v), jnp.asarray(z)\n  # Require the order v to be integer type: this simplifies\n  # the JVP rule below.\n  assert jnp.issubdtype(v.dtype, jnp.integer)\n  # Promote the input to inexact (float/complex).\n  # Note that jnp.result_type() accounts for the enable_x64 flag.\n  z = z.astype(jnp.result_type(float, z.dtype))\n  # Wrap scipy function to return the expected dtype.\n  _scipy_jv = lambda v, z: scipy.special.jv(v, z).astype(z.dtype)\n  # Define the expected shape & dtype of output.\n  result_shape_dtype = jax.ShapeDtypeStruct(\n      shape=jnp.broadcast_shapes(v.shape, z.shape),\n      dtype=z.dtype)\n  # Use vmap_method=\"broadcast_all\" because scipy.special.jv handles broadcasted inputs.\n  return jax.pure_callback(_scipy_jv, result_shape_dtype, v, z, vmap_method=\"broadcast_all\")\n\nfrom functools import partial\nj1 = partial(jv, 1)\nz = jnp.arange(5.0)\n\nprint(jax.jit(j1)(z))\nprint(jax.vmap(j1)(z))"
      },
      {
        "description": "Demonstrates that grad on the raw pure_callback will produce ValueError.",
        "code": "import jax\nimport jax.numpy as jnp\nimport scipy.special\n\ndef jv(v, z):\n  v, z = jnp.asarray(v), jnp.asarray(z)\n  # Require the order v to be integer type: this simplifies\n  # the JVP rule below.\n  assert jnp.issubdtype(v.dtype, jnp.integer)\n  # Promote the input to inexact (float/complex).\n  # Note that jnp.result_type() accounts for the enable_x64 flag.\n  z = z.astype(jnp.result_type(float, z.dtype))\n  # Wrap scipy function to return the expected dtype.\n  _scipy_jv = lambda v, z: scipy.special.jv(v, z).astype(z.dtype)\n  # Define the expected shape & dtype of output.\n  result_shape_dtype = jax.ShapeDtypeStruct(\n      shape=jnp.broadcast_shapes(v.shape, z.shape),\n      dtype=z.dtype)\n  # Use vmap_method=\"broadcast_all\" because scipy.special.jv handles broadcasted inputs.\n  return jax.pure_callback(_scipy_jv, result_shape_dtype, v, z, vmap_method=\"broadcast_all\")\n\nfrom functools import partial\nj1 = partial(jv, 1)\nz = jnp.arange(5.0)\njax.grad(j1)(z)"
      },
      {
        "description": "Demonstrates defining a custom gradient rule for the Bessel function using jax.custom_jvp().",
        "code": "import jax\nimport jax.numpy as jnp\nimport scipy.special\n\ndef jv(v, z):\n  v, z = jnp.asarray(v), jnp.asarray(z)\n  # Require the order v to be integer type: this simplifies\n  # the JVP rule below.\n  assert jnp.issubdtype(v.dtype, jnp.integer)\n  # Promote the input to inexact (float/complex).\n  # Note that jnp.result_type() accounts for the enable_x64 flag.\n  z = z.astype(jnp.result_type(float, z.dtype))\n  # Wrap scipy function to return the expected dtype.\n  _scipy_jv = lambda v, z: scipy.special.jv(v, z).astype(z.dtype)\n  # Define the expected shape & dtype of output.\n  result_shape_dtype = jax.ShapeDtypeStruct(\n      shape=jnp.broadcast_shapes(v.shape, z.shape),\n      dtype=z.dtype)\n  # Use vmap_method=\"broadcast_all\" because scipy.special.jv handles broadcasted inputs.\n  return jax.pure_callback(_scipy_jv, result_shape_dtype, v, z, vmap_method=\"broadcast_all\")\n\njv = jax.custom_jvp(jv)\n\n@jv.defjvp\ndef _jv_jvp(primals, tangents):\n  v, z = primals\n  _, z_dot = tangents\n  # Note: v_dot is always 0 because v is integer.\n  jv_minus_1, jv_plus_1 = jv(v - 1, z), jv(v + 1, z)\n  djv_dz = jnp.where(v == 0, -jv_plus_1, 0.5 * (jv_minus_1 - jv_plus_1))\n  return jv(v, z), z_dot * djv_dz"
      },
      {
        "description": "Demonstrates computing the gradient of the custom jv function.",
        "code": "import jax\nimport jax.numpy as jnp\nimport scipy.special\n\ndef jv(v, z):\n  v, z = jnp.asarray(v), jnp.asarray(z)\n  # Require the order v to be integer type: this simplifies\n  # the JVP rule below.\n  assert jnp.issubdtype(v.dtype, jnp.integer)\n  # Promote the input to inexact (float/complex).\n  # Note that jnp.result_type() accounts for the enable_x64 flag.\n  z = z.astype(jnp.result_type(float, z.dtype))\n  # Wrap scipy function to return the expected dtype.\n  _scipy_jv = lambda v, z: scipy.special.jv(v, z).astype(z.dtype)\n  # Define the expected shape & dtype of output.\n  result_shape_dtype = jax.ShapeDtypeStruct(\n      shape=jnp.broadcast_shapes(v.shape, z.shape),\n      dtype=z.dtype)\n  # Use vmap_method=\"broadcast_all\" because scipy.special.jv handles broadcasted inputs.\n  return jax.pure_callback(_scipy_jv, result_shape_dtype, v, z, vmap_method=\"broadcast_all\")\n\njv = jax.custom_jvp(jv)\n\n@jv.defjvp\ndef _jv_jvp(primals, tangents):\n  v, z = primals\n  _, z_dot = tangents\n  # Note: v_dot is always 0 because v is integer.\n  jv_minus_1, jv_plus_1 = jv(v - 1, z), jv(v + 1, z)\n  djv_dz = jnp.where(v == 0, -jv_plus_1, 0.5 * (jv_minus_1 - jv_plus_1))\n  return jv(v, z), z_dot * djv_dz\n\nfrom functools import partial\nj1 = partial(jv, 1)\nprint(jax.grad(j1)(2.0))"
      },
      {
        "description": "Demonstrates computing the hessian of the custom jv function.",
        "code": "import jax\nimport jax.numpy as jnp\nimport scipy.special\n\ndef jv(v, z):\n  v, z = jnp.asarray(v), jnp.asarray(z)\n  # Require the order v to be integer type: this simplifies\n  # the JVP rule below.\n  assert jnp.issubdtype(v.dtype, jnp.integer)\n  # Promote the input to inexact (float/complex).\n  # Note that jnp.result_type() accounts for the enable_x64 flag.\n  z = z.astype(jnp.result_type(float, z.dtype))\n  # Wrap scipy function to return the expected dtype.\n  _scipy_jv = lambda v, z: scipy.special.jv(v, z).astype(z.dtype)\n  # Define the expected shape & dtype of output.\n  result_shape_dtype = jax.ShapeDtypeStruct(\n      shape=jnp.broadcast_shapes(v.shape, z.shape),\n      dtype=z.dtype)\n  # Use vmap_method=\"broadcast_all\" because scipy.special.jv handles broadcasted inputs.\n  return jax.pure_callback(_scipy_jv, result_shape_dtype, v, z, vmap_method=\"broadcast_all\")\n\njv = jax.custom_jvp(jv)\n\n@jv.defjvp\ndef _jv_jvp(primals, tangents):\n  v, z = primals\n  _, z_dot = tangents\n  # Note: v_dot is always 0 because v is integer.\n  jv_minus_1, jv_plus_1 = jv(v - 1, z), jv(v + 1, z)\n  djv_dz = jnp.where(v == 0, -jv_plus_1, 0.5 * (jv_minus_1 - jv_plus_1))\n  return jv(v, z), z_dot * djv_dz\n\nfrom functools import partial\nj1 = partial(jv, 1)\njax.hessian(j1)(2.0)"
      }
    ]
  },
  {
    "title": "Introduction to Convolution in JAX",
    "concepts": [
      "JAX provides multiple interfaces for computing convolutions.",
      "jax.numpy and jax.scipy operations are sufficient for basic convolutions.",
      "jax.lax.conv_general_dilated is used for more general batched multi-dimensional convolution."
    ],
    "code_examples": []
  },
  {
    "title": "One-Dimensional Convolution with jax.numpy.convolve",
    "concepts": [
      "jax.numpy.convolve() provides a JAX interface for numpy.convolve().",
      "The mode parameter controls how boundary conditions are handled.",
      "mode='same' ensures the output is the same size as the input."
    ],
    "code_examples": [
      {
        "description": "Example of 1D smoothing implemented via convolution using jax.numpy.convolve.",
        "code": "import matplotlib.pyplot as plt\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\nkey = random.key(1701)\nx = jnp.linspace(0, 10, 500)\ny = jnp.sin(x) + 0.2 * random.normal(key, shape=(500,))\nwindow = jnp.ones(10) / 10\ny_smooth = jnp.convolve(y, window, mode='same')\nplt.plot(x, y, 'lightgray')\nplt.plot(x, y_smooth, 'black');"
      },
      {
        "description": "Example of 1D smoothing implemented via convolution using jax.numpy.convolve.",
        "code": "import matplotlib.pyplot as plt\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\nkey = random.key(1701)\nx = jnp.linspace(0, 10, 500)\ny = jnp.sin(x) + 0.2 * random.normal(key, shape=(500,))\nwindow = jnp.ones(10) / 10\ny_smooth = jnp.convolve(y, window, mode='same')\nplt.plot(x, y, 'lightgray')\nplt.plot(x, y_smooth, 'black');"
      }
    ]
  },
  {
    "title": "N-Dimensional Convolution with jax.scipy.signal.convolve",
    "concepts": [
      "jax.scipy.signal.convolve() generalizes convolution to N dimensions.",
      "The mode parameter specifies how edges are handled."
    ],
    "code_examples": [
      {
        "description": "Example of de-noising an image based on convolution with a Gaussian filter using jax.scipy.signal.convolve.",
        "code": "from scipy import datasets\nimport jax.scipy as jsp\nfig, ax = plt.subplots(1, 3, figsize=(12, 5))\n# Load a sample image; compute mean() to convert from RGB to grayscale.\nimage = jnp.array(datasets.face().mean(-1))\nax[0].imshow(image, cmap='binary_r')\nax[0].set_title('original')\n# Create a noisy version by adding random Gaussian noise\nkey = random.key(1701)\nnoisy_image = image + 50 * random.normal(key, image.shape)\nax[1].imshow(noisy_image, cmap='binary_r')\nax[1].set_title('noisy')\n# Smooth the noisy image with a 2D Gaussian smoothing kernel.\nx = jnp.linspace(-3, 3, 7)\nwindow = jsp.stats.norm.pdf(x) * jsp.stats.norm.pdf(x[:, None])\nsmooth_image = jsp.signal.convolve(noisy_image, window, mode='same')\nax[2].imshow(smooth_image, cmap='binary_r')\nax[2].set_title('smoothed');"
      },
      {
        "description": "Example of de-noising an image based on convolution with a Gaussian filter using jax.scipy.signal.convolve.",
        "code": "from scipy import datasets\nimport jax.scipy as jsp\nfig, ax = plt.subplots(1, 3, figsize=(12, 5))\n# Load a sample image; compute mean() to convert from RGB to grayscale.\nimage = jnp.array(datasets.face().mean(-1))\nax[0].imshow(image, cmap='binary_r')\nax[0].set_title('original')\n# Create a noisy version by adding random Gaussian noise\nkey = random.key(1701)\nnoisy_image = image + 50 * random.normal(key, image.shape)\nax[1].imshow(noisy_image, cmap='binary_r')\nax[1].set_title('noisy')\n# Smooth the noisy image with a 2D Gaussian smoothing kernel.\nx = jnp.linspace(-3, 3, 7)\nwindow = jsp.stats.norm.pdf(x) * jsp.stats.norm.pdf(x[:, None])\nsmooth_image = jsp.signal.convolve(noisy_image, window, mode='same')\nax[2].imshow(smooth_image, cmap='binary_r')\nax[2].set_title('smoothed');"
      }
    ]
  },
  {
    "title": "General N-Dimensional Convolution with jax.lax.conv_general_dilated",
    "concepts": [
      "jax.lax.conv_general_dilated is used for more general batched convolutions.",
      "It offers very general N-dimensional convolution capabilities.",
      "A survey of the family of convolutional operators and convolutional arithmetic is highly recommended.",
      "The convenience lax.conv and lax.conv_with_general_padding helper functions assume NCHW images and OIHW kernels.",
      "The conv_dimension_numbers function helps define the layout of the input, kernel and output tensors."
    ],
    "code_examples": [
      {
        "description": "Define a simple diagonal edge kernel in HWIO layout.",
        "code": "# 2D kernel - HWIO layout\nkernel = jnp.zeros((3, 3, 3, 3), dtype=jnp.float32)\nkernel += jnp.array([[1, 1, 0],\n                       [1, 0, -1],\n                       [0, -1, -1]])[:, :, jnp.newaxis, jnp.newaxis]\nprint(\"Edge Conv kernel:\")\nplt.imshow(kernel[:, :, 0, 0]);"
      },
      {
        "description": "Define a simple diagonal edge kernel in HWIO layout.",
        "code": "# 2D kernel - HWIO layout\nkernel = jnp.zeros((3, 3, 3, 3), dtype=jnp.float32)\nkernel += jnp.array([[1, 1, 0],\n                       [1, 0, -1],\n                       [0, -1, -1]])[:, :, jnp.newaxis, jnp.newaxis]\nprint(\"Edge Conv kernel:\")\nplt.imshow(kernel[:, :, 0, 0]);"
      },
      {
        "description": "Create a simple synthetic image in NHWC layout.",
        "code": "# NHWC layout\nimg = jnp.zeros((1, 200, 198, 3), dtype=jnp.float32)\nfor k in range(3):\n  x = 30 + 60 * k\n  y = 20 + 60 * k\n  img = img.at[0, x:x+10, y:y+10, k].set(1.0)\nprint(\"Original Image:\")\nplt.imshow(img[0]);"
      },
      {
        "description": "Create a simple synthetic image in NHWC layout.",
        "code": "# NHWC layout\nimg = jnp.zeros((1, 200, 198, 3), dtype=jnp.float32)\nfor k in range(3):\n  x = 30 + 60 * k\n  y = 20 + 60 * k\n  img = img.at[0, x:x+10, y:y+10, k].set(1.0)\nprint(\"Original Image:\")\nplt.imshow(img[0]);"
      },
      {
        "description": "Using lax.conv with NCHW images and OIHW kernels.",
        "code": "from jax import lax\nout = lax.conv(\n    jnp.transpose(img,[0,3,1,2]),  # lhs = NCHW image tensor\n    jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n    (1,1),                      # window strides\n    'SAME'                       # padding mode\n)\nprint(\"out shape: \", out.shape)\nprint(\"First output channel:\")\nplt.figure(figsize=(10,10))\nplt.imshow(np.array(out)[0,0,:,:]);"
      },
      {
        "description": "Using lax.conv with NCHW images and OIHW kernels.",
        "code": "from jax import lax\nout = lax.conv(\n    jnp.transpose(img,[0,3,1,2]),  # lhs = NCHW image tensor\n    jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n    (1,1),                      # window strides\n    'SAME'                       # padding mode\n)\nprint(\"out shape: \", out.shape)\nprint(\"First output channel:\")\nplt.figure(figsize=(10,10))\nplt.imshow(np.array(out)[0,0,:,:]);"
      },
      {
        "description": "Using lax.conv_with_general_padding.",
        "code": "out = lax.conv_with_general_padding(\n    jnp.transpose(img,[0,3,1,2]),  # lhs = NCHW image tensor\n    jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n    (1,1),                      # window strides\n    ((2,2),(2,2)),               # general padding 2x2\n    (1,1),                      # lhs/image dilation\n    (1,1)                       # rhs/kernel dilation\n)\nprint(\"out shape: \", out.shape)\nprint(\"First output channel:\")\nplt.figure(figsize=(10,10))\nplt.imshow(np.array(out)[0,0,:,:]);"
      },
      {
        "description": "Using lax.conv_with_general_padding.",
        "code": "out = lax.conv_with_general_padding(\n    jnp.transpose(img,[0,3,1,2]),  # lhs = NCHW image tensor\n    jnp.transpose(kernel,[3,2,0,1]), # rhs = OIHW conv kernel tensor\n    (1,1),                      # window strides\n    ((2,2),(2,2)),               # general padding 2x2\n    (1,1),                      # lhs/image dilation\n    (1,1)                       # rhs/kernel dilation\n)\nprint(\"out shape: \", out.shape)\nprint(\"First output channel:\")\nplt.figure(figsize=(10,10))\nplt.imshow(np.array(out)[0,0,:,:]);"
      },
      {
        "description": "Demonstrates conv_dimension_numbers to define dimension layouts (NHWC, HWIO, NHWC).",
        "code": "dn = lax.conv_dimension_numbers(\n    img.shape, # only ndim matters, not shape\n    kernel.shape, # only ndim matters, not shape\n    ('NHWC', 'HWIO', 'NHWC') # the important bit\n)\nprint(dn)"
      },
      {
        "description": "Demonstrates conv_dimension_numbers to define dimension layouts (NHWC, HWIO, NHWC).",
        "code": "dn = lax.conv_dimension_numbers(\n    img.shape, # only ndim matters, not shape\n    kernel.shape, # only ndim matters, not shape\n    ('NHWC', 'HWIO', 'NHWC') # the important bit\n)\nprint(dn)"
      },
      {
        "description": "Demonstrates lax.conv_general_dilated with 'SAME' padding.",
        "code": "out = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,1),                      # window strides\n    'SAME',                     # padding mode\n    (1,1),                      # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape)\nprint(\"First output channel:\")\nplt.figure(figsize=(10,10))\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Demonstrates lax.conv_general_dilated with 'SAME' padding.",
        "code": "out = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,1),                      # window strides\n    'SAME',                     # padding mode\n    (1,1),                      # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape)\nprint(\"First output channel:\")\nplt.figure(figsize=(10,10))\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Demonstrates lax.conv_general_dilated with 'VALID' padding.",
        "code": "out = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,1),                      # window strides\n    'VALID',                    # padding mode\n    (1,1),                      # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape, \"DIFFERENT from above!\")\nprint(\"First output channel:\")\nplt.figure(figsize=(10,10))\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Demonstrates lax.conv_general_dilated with 'VALID' padding.",
        "code": "out = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,1),                      # window strides\n    'VALID',                    # padding mode\n    (1,1),                      # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape, \"DIFFERENT from above!\")\nprint(\"First output channel:\")\nplt.figure(figsize=(10,10))\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Demonstrates lax.conv_general_dilated with strides of (2,2).",
        "code": "out = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (2,2),                      # window strides\n    'SAME',                     # padding mode\n    (1,1),                      # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape, \" <-- half the size of above\")\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Demonstrates lax.conv_general_dilated with strides of (2,2).",
        "code": "out = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (2,2),                      # window strides\n    'SAME',                     # padding mode\n    (1,1),                      # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape, \" <-- half the size of above\")\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Demonstrates lax.conv_general_dilated with lhs/image dilation of (12, 12).",
        "code": "out = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,1),                      # window strides\n    'VALID',                    # padding mode\n    (12,12),                    # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape)\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Demonstrates lax.conv_general_dilated with lhs/image dilation of (12, 12).",
        "code": "out = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,1),                      # window strides\n    'VALID',                    # padding mode\n    (12,12),                    # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape)\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Demonstrates lax.conv_general_dilated with padding mode specified as a tuple of tuples ((0,0), (0,0)).",
        "code": "out = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,1),                      # window strides\n    ((0,0),(0,0)),                # padding mode\n    (2,2),                      # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape, \"<-- larger than original!\")\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Demonstrates lax.conv_general_dilated with padding mode specified as a tuple of tuples ((0,0), (0,0)).",
        "code": "out = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,1),                      # window strides\n    ((0,0),(0,0)),                # padding mode\n    (2,2),                      # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape, \"<-- larger than original!\")\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Implementing transposed convolutions using lax.conv_general_dilated.",
        "code": "# The following is equivalent to tensorflow:\n# N,H,W,C = img.shape\n# out = tf.nn.conv2d_transpose(img, kernel, (N,2*H,2*W,C), (1,2,2,1))\n# transposed conv = 180deg kernel rotation plus LHS dilation\n# rotate kernel 180deg:\nkernel_rot = jnp.rot90(jnp.rot90(kernel, axes=(0,1)), axes=(0,1))\n# need a custom output padding:\npadding = ((2,1),(2,1))\nout = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel_rot,                 # rhs = conv kernel tensor\n    (1,1),                      # window strides\n    padding,                      # padding mode\n    (2,2),                      # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape, \"<-- transposed_conv\")\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "Implementing transposed convolutions using lax.conv_general_dilated.",
        "code": "# The following is equivalent to tensorflow:\n# N,H,W,C = img.shape\n# out = tf.nn.conv2d_transpose(img, kernel, (N,2*H,2*W,C), (1,2,2,1))\n# transposed conv = 180deg kernel rotation plus LHS dilation\n# rotate kernel 180deg:\nkernel_rot = jnp.rot90(jnp.rot90(kernel, axes=(0,1)), axes=(0,1))\n# need a custom output padding:\npadding = ((2,1),(2,1))\nout = lax.conv_general_dilated(\n    img,                        # lhs = image tensor\n    kernel_rot,                 # rhs = conv kernel tensor\n    (1,1),                      # window strides\n    padding,                      # padding mode\n    (2,2),                      # lhs/image dilation\n    (1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape, \"<-- transposed_conv\")\nplt.figure(figsize=(10,10))\nprint(\"First output channel:\")\nplt.imshow(np.array(out)[0,:,:,0]);"
      },
      {
        "description": "1D convolution demo using lax.conv_general_dilated.",
        "code": "# 1D kernel - WIO layout\nkernel = jnp.array([[[1, 0, -1],\n                      [-1, 0, 1]],\n                     [[1, 1, 1],\n                      [-1, -1, -1]]],\n                   dtype=jnp.float32).transpose([2,1,0])\n# 1D data - NWC layout\ndata = np.zeros((1, 200, 2), dtype=jnp.float32)\nfor i in range(2):\n  for k in range(2):\n    x = 35 * i + 30 + 60 * k\n    data[0, x:x+30, k] = 1.0\nprint(\"in shapes:\", data.shape, kernel.shape)\nplt.figure(figsize=(10,5))\nplt.plot(data[0]);\n\ndn = lax.conv_dimension_numbers(\n    data.shape,\n    kernel.shape,\n    ('NWC', 'WIO', 'NWC'))\nprint(dn)\n\nout = lax.conv_general_dilated(\n    data,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,),                      # window strides\n    'SAME',                    # padding mode\n    (1,),                      # lhs/image dilation\n    (1,),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape)\nplt.figure(figsize=(10,5))\nplt.plot(out[0]);"
      },
      {
        "description": "1D convolution demo using lax.conv_general_dilated.",
        "code": "# 1D kernel - WIO layout\nkernel = jnp.array([[[1, 0, -1],\n                      [-1, 0, 1]],\n                     [[1, 1, 1],\n                      [-1, -1, -1]]],\n                   dtype=jnp.float32).transpose([2,1,0])\n# 1D data - NWC layout\ndata = np.zeros((1, 200, 2), dtype=jnp.float32)\nfor i in range(2):\n  for k in range(2):\n    x = 35 * i + 30 + 60 * k\n    data[0, x:x+30, k] = 1.0\nprint(\"in shapes:\", data.shape, kernel.shape)\nplt.figure(figsize=(10,5))\nplt.plot(data[0]);\n\ndn = lax.conv_dimension_numbers(\n    data.shape,\n    kernel.shape,\n    ('NWC', 'WIO', 'NWC'))\nprint(dn)\n\nout = lax.conv_general_dilated(\n    data,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,),                      # window strides\n    'SAME',                    # padding mode\n    (1,),                      # lhs/image dilation\n    (1,),                      # rhs/kernel dilation\n    dn                          # dimension_numbers = lhs, rhs, out dimension permutation\n)\nprint(\"out shape: \", out.shape)\nplt.figure(figsize=(10,5))\nplt.plot(out[0]);"
      },
      {
        "description": "3D convolution demo using lax.conv_general_dilated.",
        "code": "import matplotlib as mpl\n# Random 3D kernel - HWDIO layout\nkernel = jnp.array([[[0, 0, 0],\n                      [0, 1, 0],\n                      [0, 0, 0]],\n                     [[0, -1, 0],\n                      [-1, 0, -1],\n                      [0, -1, 0]],\n                     [[0, 0, 0],\n                      [0, 1, 0],\n                      [0, 0, 0]]],\n                   dtype=jnp.float32)[:,:,:,jnp.newaxis,jnp.newaxis]\n# 3D data - NHWDC layout\ndata = jnp.zeros((1, 30, 30, 30, 1), dtype=jnp.float32)\nx, y, z = np.mgrid[0:1:30j, 0:1:30j, 0:1:30j]\ndata += (jnp.sin(2 * x * jnp.pi) * jnp.cos(2 * y * jnp.pi) * jnp.cos(2 * z * jnp.pi))[None,:,:,:,None]\nprint(\"in shapes:\", data.shape, kernel.shape)\n\ndn = lax.conv_dimension_numbers(\n    data.shape,\n    kernel.shape,\n    ('NHWDC', 'HWDIO', 'NHWDC'))\nprint(dn)\n\nout = lax.conv_general_dilated(\n    data,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,1,1),                      # window strides\n    'SAME',                    # padding mode\n    (1,1,1),                      # lhs/image dilation\n    (1,1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers\n)\nprint(\"out shape: \", out.shape)\n\n# Make some simple 3d density plots:\ndef make_alpha(cmap):\n  my_cmap = cmap(jnp.arange(cmap.N))\n  my_cmap[:,-1] = jnp.linspace(0, 1, cmap.N)**3\n  return mpl.colors.ListedColormap(my_cmap)\n\nmy_cmap = make_alpha(plt.cm.viridis)\nfig = plt.figure()\nax = fig.add_subplot(projection = '3d')\nax.scatter(x.ravel(), y.ravel(), z.ravel(), c=data.ravel(), cmap=my_cmap)\nax.axis('off')\nax.set_title('input')\n\nfig = plt.figure()\nax = fig.add_subplot(projection = '3d')\nax.scatter(x.ravel(), y.ravel(), z.ravel(), c=out.ravel(), cmap=my_cmap)\nax.axis('off')\nax.set_title('3D conv output');"
      },
      {
        "description": "3D convolution demo using lax.conv_general_dilated.",
        "code": "import matplotlib as mpl\n# Random 3D kernel - HWDIO layout\nkernel = jnp.array([[[0, 0, 0],\n                      [0, 1, 0],\n                      [0, 0, 0]],\n                     [[0, -1, 0],\n                      [-1, 0, -1],\n                      [0, -1, 0]],\n                     [[0, 0, 0],\n                      [0, 1, 0],\n                      [0, 0, 0]]],\n                   dtype=jnp.float32)[:,:,:,jnp.newaxis,jnp.newaxis]\n# 3D data - NHWDC layout\ndata = jnp.zeros((1, 30, 30, 30, 1), dtype=jnp.float32)\nx, y, z = np.mgrid[0:1:30j, 0:1:30j, 0:1:30j]\ndata += (jnp.sin(2 * x * jnp.pi) * jnp.cos(2 * y * jnp.pi) * jnp.cos(2 * z * jnp.pi))[None,:,:,:,None]\nprint(\"in shapes:\", data.shape, kernel.shape)\n\ndn = lax.conv_dimension_numbers(\n    data.shape,\n    kernel.shape,\n    ('NHWDC', 'HWDIO', 'NHWDC'))\nprint(dn)\n\nout = lax.conv_general_dilated(\n    data,                        # lhs = image tensor\n    kernel,                     # rhs = conv kernel tensor\n    (1,1,1),                      # window strides\n    'SAME',                    # padding mode\n    (1,1,1),                      # lhs/image dilation\n    (1,1,1),                      # rhs/kernel dilation\n    dn                          # dimension_numbers\n)\nprint(\"out shape: \", out.shape)\n\n# Make some simple 3d density plots:\ndef make_alpha(cmap):\n  my_cmap = cmap(jnp.arange(cmap.N))\n  my_cmap[:,-1] = jnp.linspace(0, 1, cmap.N)**3\n  return mpl.colors.ListedColormap(my_cmap)\n\nmy_cmap = make_alpha(plt.cm.viridis)\nfig = plt.figure()\nax = fig.add_subplot(projection = '3d')\nax.scatter(x.ravel(), y.ravel(), z.ravel(), c=data.ravel(), cmap=my_cmap)\nax.axis('off')\nax.set_title('input')\n\nfig = plt.figure()\nax = fig.add_subplot(projection = '3d')\nax.scatter(x.ravel(), y.ravel(), z.ravel(), c=out.ravel(), cmap=my_cmap)\nax.axis('off')\nax.set_title('3D conv output');"
      }
    ]
  },
  {
    "title": "Introduction to Performance Regression Analysis in JAX",
    "concepts": [
      "Identify the commit that triggered a performance regression in JAX.",
      "Use a brute force method to test nightly containers between releases.",
      "Ensure XLA and JAX commits are compatible during testing.",
      "Limit XLA recompilation to speed up the investigation process.",
      "Verify findings with manual checks or git bisect."
    ],
    "code_examples": []
  },
  {
    "title": "Brute Force Testing of Nightly Containers",
    "concepts": [
      "Use NVIDIA JAX-Toolbox nightly containers for testing.",
      "Discard days with buggy containers or temporary regressions.",
      "Automate the testing process using shell scripts.",
      "Use `test_runner.sh` to start containers and tests.",
      "Use `test.sh` to install dependencies and run the performance test."
    ],
    "code_examples": [
      {
        "description": "Shell script to run performance tests on multiple nightly containers.",
        "code": "for m in 7 8 9; do\n    for d in `seq -w 1 30`; do\n      docker run -v $PWD:/dir --gpus=all ghcr.io/nvidia/jax:nightly-2023-0${m}-${d} /bin/bash /dir/test.sh &> OUT-0${m}-${d}\n    done\n  Done\nfor m in 7 8 9; do\n    for d in `seq -w 1 30`; do\n      docker run -v $PWD:/dir --gpus=all ghcr.io/nvidia/jax:nightly-2023-0${m}-${d} /bin/bash /dir/test.sh &> OUT-0${m}-${d}\n    done\n  Done"
      },
      {
        "description": "Shell script to install dependencies and run the performance test using MLUPS3d.",
        "code": "pip\ninstall\njmp\npyvista\nnumpy\nmatplotlib\nRtree\ntrimesh\njmp\ntermcolor\norbax\ngit\nclone\nhttps\n:\n//\ngithub\n.\ncom\n/\nAutodesk\n/\nXLB\ncd\nXLB\nexport\nPYTHONPATH\n=.\nexport\nCUDA_VISIBLE_DEVICES\n=\n0\n# only 1 GPU is needed\npython3\nexamples\n/\nperformance\n/\nMLUPS3d\n.py\n256\n200\npip\ninstall\njmp\npyvista\nnumpy\nmatplotlib\nRtree\ntrimesh\njmp\ntermcolor\norbax\ngit\nclone\nhttps\n:\n//\ngithub\n.\ncom\n/\nAutodesk\n/\nXLB\ncd\nXLB\nexport\nPYTHONPATH\n=.\nexport\nCUDA_VISIBLE_DEVICES\n=\n0\n# only 1 GPU is needed\npython3\nexamples\n/\nperformance\n/\nMLUPS3d\n.py\n256\n200"
      }
    ]
  },
  {
    "title": "Analyzing Output and Identifying the Regression",
    "concepts": [
      "Use `grep` to search output files for performance metrics (e.g., MLUPS).",
      "Compare the performance metrics across different nightly builds.",
      "Identify the specific day or range of days where the regression occurs.",
      "The example shows a regression between August 24th and August 26th."
    ],
    "code_examples": []
  },
  {
    "title": "Hourly Investigation and Commit Isolation",
    "concepts": [
      "Perform hourly testing between the identified dates to narrow down the regression.",
      "Checkout JAX and XLA at each hour between the two dates.",
      "Rebuild JAX and XLA for each hourly test.",
      "Use `test_runner2.sh` and `test2.sh` to automate hourly testing.",
      "Leverage incremental XLA builds to accelerate the testing process.",
      "Check the JAX and XLA history between those hours."
    ],
    "code_examples": [
      {
        "description": "Shell script to run performance tests on hourly builds between specified dates. This script should be executed inside a docker container.",
        "code": "# Execute this script inside the container:\n  # docker run -v $PWD:/dir --gpus=all ghcr.io/nvidia/jax:nightly-2023-08-24 /bin/bash\n  cd /opt/xla-source\n  git remote update\n  cd /opt/jax-source\n  git remote update\n  pip install jmp pyvista numpy matplotlib Rtree trimesh jmp termcolor orbax\n  cd /tmp\n  git clone https://github.com/Autodesk/XLB\n  cd XLB\n\n  for d in `seq -w 24 26`; do\n      for h in `seq -w 0 24`; do\n          echo $m $d $h\n          /bin/bash /dir/test2.sh Aug $d 2023 $h:00:00 &> OUT-08-${d}-$h\n      done\n  done"
      },
      {
        "description": "Shell script to checkout specific JAX and XLA commits, rebuild JAX and run the performance test.",
        "code": "echo \"param: $@\"\n  cd /opt/xla-source\n  git checkout `git rev-list -1 --before=\"$@\" origin/main`\n  git show -q\n  cd /opt/jax-source\n  git checkout `git rev-list -1 --before=\"$@\" origin/main`\n  git show -q\n\n  rm /opt/jax-source/dist/jax*.whl\n  build-jax.sh # The script is in the nightly container\n\n  export PYTHONPATH=.\n  export CUDA_VISIBLE_DEVICES=0 # only 1 GPU is needed\n\n  python3 examples/performance/MLUPS3d.py 256 200"
      }
    ]
  },
  {
    "title": "Final Steps and Considerations",
    "concepts": [
      "Analyze the output files from the hourly tests.",
      "Check the JAX and XLA history between the specific hours where the regression appeared.",
      "Manually test a few commits or use `git bisect` for more precise isolation.",
      "Be aware that `git bisect` might not always be ideal for speed regressions as it may hide information about multiple regressions."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to JAX Extensions",
    "concepts": [
      "jax.extend module provides access to JAX internal machinery.",
      "This module is related to JEP #15856.",
      "JAX offers other extensibility mechanisms, like customizing derivatives and registering custom pytree definitions.",
      "The jax.extend module has no compatibility guarantee across releases.",
      "Breaking changes will be announced in the JAX project changelog."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Functions",
    "concepts": [
      "A function is a reusable block of code.",
      "Functions are defined using the `def` keyword.",
      "Functions can take arguments as input.",
      "Functions can return a value using the `return` keyword.",
      "Functions help to organize code and make it more readable."
    ],
    "code_examples": [
      {
        "description": "A simple function that adds two numbers and returns the result.",
        "code": "def add(x, y):\n  return x + y"
      },
      {
        "description": "Calling the add function with arguments 5 and 3.",
        "code": "result = add(5, 3)\nprint(result)"
      },
      {
        "description": "A function without a return statement implicitly returns None.",
        "code": "def greet(name):\n  print(f\"Hello, {name}!\")\n\ngreet(\"Alice\")"
      }
    ]
  },
  {
    "title": "GPU Memory Preallocation in JAX",
    "concepts": [
      "JAX preallocates 75% of GPU memory by default.",
      "Preallocation minimizes overhead but can cause OOM errors.",
      "Environment variables can override the default preallocation behavior.",
      "Disabling preallocation can reduce memory usage but increases fragmentation risk.",
      "Lowering the preallocation fraction can fix OOM errors at startup.",
      "Allocating memory on demand is very slow but minimizes memory footprint."
    ],
    "code_examples": []
  },
  {
    "title": "Environment Variables for Memory Management",
    "concepts": [
      "XLA_PYTHON_CLIENT_PREALLOCATE=false disables preallocation.",
      "XLA_PYTHON_CLIENT_MEM_FRACTION sets the preallocation percentage.",
      "Using CPU-only TensorFlow can avoid GPU memory conflicts.",
      "TensorFlow also preallocates memory by default.",
      "TensorFlow has configuration options like gpu_memory_fraction and allow_growth (TF1) to manage GPU memory.",
      "Use XLA_PYTHON_CLIENT_MEM_FRACTION or XLA_PYTHON_CLIENT_PREALLOCATE."
    ],
    "code_examples": []
  },
  {
    "title": "TensorFlow Configuration",
    "concepts": [
      "Prevent TensorFlow from using the GPU with tf.config.experimental.set_visible_devices([], \"GPU\")"
    ],
    "code_examples": [
      {
        "description": "Prevents TensorFlow from using the GPU.",
        "code": "tf.config.experimental.set_visible_devices([], \"GPU\")"
      }
    ]
  },
  {
    "title": "Rematerialization Optimization",
    "concepts": [
      "Disabling the automatic rematerialization HLO pass can improve memory usage.",
      "The pass can be enabled/disabled by setting jax.config.update('enable_remat_opt_pass', True) or jax.config.update('enable_remat_opt_pass', False).",
      "Manual rematerialization with jax.remat can provide better trade-offs between compute and memory."
    ],
    "code_examples": [
      {
        "description": "Enables the rematerialization optimization pass.",
        "code": "jax.config.update('enable_remat_opt_pass', True)"
      },
      {
        "description": "Disables the rematerialization optimization pass.",
        "code": "jax.config.update('enable_remat_opt_pass', False)"
      }
    ]
  },
  {
    "title": "cudaMallocAsync Memory Allocator",
    "concepts": [
      "This feature is experimental and should be used with caution.",
      "Replacing XLA\u2019s BFC memory allocator with cudaMallocAsync removes the fixed pre-allocation and uses a growing memory pool.",
      "The expected benefit is to avoid needing to set XLA_PYTHON_CLIENT_MEM_FRACTION.",
      "Memory fragmentation may behave differently.",
      "The allocation time may be incurred when the memory pool needs to be increased.",
      "Pre-allocating a significant chunk of memory can mitigate the risks using TF_CUDA_MALLOC_ASYNC_SUPPORTED_PREALLOC=N.  If N is -1 it will preallocate the same as what was allocated by default. Otherwise, it is the size in bytes that you want to preallocate."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of jnp.arctanh",
    "concepts": [
      "Calculates the element-wise inverse hyperbolic tangent of an input array.",
      "It is a JAX implementation of NumPy's arctanh.",
      "The inverse hyperbolic tangent is defined as (1/2) * [ln(1 + x) - ln(1 - x)].",
      "The output array has the same shape as the input array.",
      "For real-valued inputs outside the range [-1, 1], jnp.arctanh returns NaN.",
      "It follows the branch cut convention of numpy.arctanh for complex inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Real-valued Input Examples",
    "concepts": [
      "Demonstrates the use of jnp.arctanh with a real-valued JAX array.",
      "Shows how the function handles values outside the range [-1, 1] by returning NaN and inf.",
      "Demonstrates use of jnp.printoptions for formatted output."
    ],
    "code_examples": [
      {
        "description": "Calculates the inverse hyperbolic tangent for a real-valued JAX array, showcasing the output for values outside the domain [-1, 1].",
        "code": "x = jnp.array([-2, -1, -0.5, 0, 0.5, 1, 2])\nwith jnp.printoptions(precision=3, suppress=True):\n  print(jnp.arctanh(x))"
      },
      {
        "description": "Calculates the inverse hyperbolic tangent for a real-valued JAX array, showcasing the output for values outside the domain [-1, 1].",
        "code": "x = jnp.array([-2, -1, -0.5, 0, 0.5, 1, 2])\nwith jnp.printoptions(precision=3, suppress=True):\n  print(jnp.arctanh(x))"
      }
    ]
  },
  {
    "title": "Complex-valued Input Examples",
    "concepts": [
      "Demonstrates the use of jnp.arctanh with a complex-valued JAX array.",
      "Shows how the function handles complex numbers.",
      "Demonstrates the resulting complex output.",
      "Demonstrates use of jnp.printoptions for formatted output."
    ],
    "code_examples": [
      {
        "description": "Calculates the inverse hyperbolic tangent for a complex-valued JAX array.",
        "code": "x1 = jnp.array([-2 + 0j, 3 + 0j, 4 - 1j])\nwith jnp.printoptions(precision=3, suppress=True):\n  print(jnp.arctanh(x1))"
      },
      {
        "description": "Calculates the inverse hyperbolic tangent for a complex-valued JAX array.",
        "code": "x1 = jnp.array([-2 + 0j, 3 + 0j, 4 - 1j])\nwith jnp.printoptions(precision=3, suppress=True):\n  print(jnp.arctanh(x1))"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.broadcast_arrays",
    "concepts": [
      "The function broadcasts arrays to a common shape, similar to NumPy's broadcast_arrays.",
      "It uses NumPy-style broadcasting rules.",
      "The function takes array-like objects as input.",
      "It returns a list of arrays containing broadcasted copies of the inputs."
    ],
    "code_examples": []
  },
  {
    "title": "Broadcasting a 1D array with a scalar",
    "concepts": [
      "Broadcasting a 1D array with a scalar using jnp.broadcast_arrays."
    ],
    "code_examples": [
      {
        "description": "This example shows how to broadcast a 1D JAX array with a scalar. The scalar is broadcast to the shape of the 1D array.",
        "code": "x = jnp.arange(3)\ny = jnp.int32(1)\njnp.broadcast_arrays(x, y)"
      },
      {
        "description": "This example shows how to broadcast a 1D JAX array with a scalar. The scalar is broadcast to the shape of the 1D array.",
        "code": "x = jnp.arange(3)\ny = jnp.int32(1)\njnp.broadcast_arrays(x, y)"
      }
    ]
  },
  {
    "title": "Broadcasting a 2D array with another 2D array",
    "concepts": [
      "Broadcasting two 2D arrays with compatible shapes using jnp.broadcast_arrays."
    ],
    "code_examples": [
      {
        "description": "This example demonstrates broadcasting two 2D JAX arrays. The arrays are broadcast to a common shape, which is (2, 3) in this case.",
        "code": "x = jnp.array([[1, 2, 3]])\ny = jnp.array([[10],\n                [20]])\nx2, y2 = jnp.broadcast_arrays(x, y)\nx2\ny2"
      },
      {
        "description": "This example demonstrates broadcasting two 2D JAX arrays. The arrays are broadcast to a common shape, which is (2, 3) in this case.",
        "code": "x = jnp.array([[1, 2, 3]])\ny = jnp.array([[10],\n                [20]])\nx2, y2 = jnp.broadcast_arrays(x, y)\nx2\ny2"
      }
    ]
  },
  {
    "title": "JAX Scalar Constructor (complex64)",
    "concepts": [
      "JAX has a scalar constructor of type complex64.",
      "NumPy defines scalar types for each data type.",
      "JAX represents scalars as zero-dimensional arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Einstein Summation with JAX",
    "concepts": [
      "JAX implementation of numpy.einsum() is used.",
      "einsum is a generic API for computing reductions, inner/outer products, and axis reorderings.",
      "The API is overloaded and can have a complicated calling convention.",
      "The subscripts argument is a string containing axes names separated by commas.",
      "The operands are a sequence of one or more arrays corresponding to the subscripts.",
      "Optimization can be specified with the optimize argument, defaulting to 'auto'.",
      "Precision can be controlled with the precision argument using Precision enum values.",
      "The preferred element type for accumulation can be specified.",
      "The out argument is unsupported by JAX.",
      "The _dot_general argument allows overriding the dot_general callable.",
      "The function returns an array containing the result of the Einstein summation."
    ],
    "code_examples": []
  },
  {
    "title": "Vector Product Examples",
    "concepts": [
      "Demonstrates vector product using einsum.",
      "Shows both explicit and implicit forms of einsum for vector product.",
      "Uses jnp.vecdot for comparison."
    ],
    "code_examples": [
      {
        "description": "Computes the vector product of two vectors x and y using jnp.einsum with explicit subscript notation.",
        "code": ">>> M = jnp.arange(16).reshape(4, 4)\n>>> x = jnp.arange(4)\n>>> y = jnp.array([5, 4, 3, 2])\n>>> jnp.einsum('i,i', x, y)\nArray(16, dtype=int32)\n>>> jnp.vecdot(x, y)\nArray(16, dtype=int32)"
      },
      {
        "description": "Demonstrates alternative einsum calling conventions for vector product, including explicit and implicit forms using indices.",
        "code": ">>> M = jnp.arange(16).reshape(4, 4)\n>>> x = jnp.arange(4)\n>>> y = jnp.array([5, 4, 3, 2])\n>>> jnp.einsum('i,i->', x, y)  # explicit form\nArray(16, dtype=int32)\n>>> jnp.einsum(x, (0,), y, (0,))  # implicit form via indices\nArray(16, dtype=int32)\n>>> jnp.einsum(x, (0,), y, (0,), ())  # explicit form via indices\nArray(16, dtype=int32)"
      }
    ]
  },
  {
    "title": "Matrix Product Examples",
    "concepts": [
      "Demonstrates matrix product using einsum.",
      "Shows both explicit and implicit forms of einsum for matrix product.",
      "Uses jnp.matmul for comparison."
    ],
    "code_examples": [
      {
        "description": "Computes the matrix product of a matrix M and a vector x using jnp.einsum with explicit subscript notation.",
        "code": ">>> M = jnp.arange(16).reshape(4, 4)\n>>> x = jnp.arange(4)\n>>> jnp.einsum('ij,j->i', M, x)  # explicit form\nArray([14, 38, 62, 86], dtype=int32)\n>>> jnp.matmul(M, x)\nArray([14, 38, 62, 86], dtype=int32)"
      },
      {
        "description": "Demonstrates alternative einsum calling conventions for matrix product, including explicit and implicit forms using indices.",
        "code": ">>> M = jnp.arange(16).reshape(4, 4)\n>>> x = jnp.arange(4)\n>>> jnp.einsum('ij,j', M, x)  # implicit form\nArray([14, 38, 62, 86], dtype=int32)\n>>> jnp.einsum(M, (0, 1), x, (1,), (0,))  # explicit form via indices\nArray([14, 38, 62, 86], dtype=int32)\n>>> jnp.einsum(M, (0, 1), x, (1,))  # implicit form via indices\nArray([14, 38, 62, 86], dtype=int32)"
      }
    ]
  },
  {
    "title": "Outer Product Examples",
    "concepts": [
      "Demonstrates outer product using einsum.",
      "Shows both explicit and implicit forms of einsum for outer product.",
      "Uses jnp.outer for comparison."
    ],
    "code_examples": [
      {
        "description": "Computes the outer product of two vectors x and y using jnp.einsum with explicit subscript notation.",
        "code": ">>> x = jnp.arange(4)\n>>> y = jnp.array([5, 4, 3, 2])\n>>> jnp.einsum(\"i,j->ij\", x, y)\nArray([[ 0,  0,  0,  0],\n       [ 5,  4,  3,  2],\n       [10,  8,  6,  4],\n       [15, 12,  9,  6]], dtype=int32)\n>>> jnp.outer(x, y)\nArray([[ 0,  0,  0,  0],\n       [ 5,  4,  3,  2],\n       [10,  8,  6,  4],\n       [15, 12,  9,  6]], dtype=int32)"
      },
      {
        "description": "Demonstrates alternative einsum calling conventions for outer product, including explicit and implicit forms using indices.",
        "code": ">>> x = jnp.arange(4)\n>>> y = jnp.array([5, 4, 3, 2])\n>>> jnp.einsum(\"i,j\", x, y)  # implicit form\nArray([[ 0,  0,  0,  0],\n       [ 5,  4,  3,  2],\n       [10,  8,  6,  4],\n       [15, 12,  9,  6]], dtype=int32)\n>>> jnp.einsum(x, (0,), y, (1,), (0, 1))  # explicit form via indices\nArray([[ 0,  0,  0,  0],\n       [ 5,  4,  3,  2],\n       [10,  8,  6,  4],\n       [15, 12,  9,  6]], dtype=int32)\n>>> jnp.einsum(x, (0,), y, (1,))  # implicit form via indices\nArray([[ 0,  0,  0,  0],\n       [ 5,  4,  3,  2],\n       [10,  8,  6,  4],\n       [15, 12,  9,  6]], dtype=int32)"
      }
    ]
  },
  {
    "title": "1D Array Sum Example",
    "concepts": [
      "Demonstrates summing the elements of a 1D array using einsum.",
      "Shows how to use explicit form of einsum for array sum.",
      "Uses jnp.sum for comparison."
    ],
    "code_examples": [
      {
        "description": "Computes the sum of elements in the array x using jnp.einsum.",
        "code": ">>> x = jnp.arange(4)\n>>> jnp.einsum(\"i->\", x)  # requires explicit form\nArray(6, dtype=int32)\n>>> jnp.einsum(x, (0,), ())  # explicit form via indices\nArray(6, dtype=int32)\n>>> jnp.sum(x)\nArray(6, dtype=int32)"
      }
    ]
  },
  {
    "title": "Sum Along an Axis Example",
    "concepts": [
      "Demonstrates summing along an axis of a matrix using einsum.",
      "Shows how to use explicit form of einsum for summing along an axis.",
      "Uses M.sum(-1) for comparison."
    ],
    "code_examples": [
      {
        "description": "Computes the sum along the last axis of matrix M using jnp.einsum.",
        "code": ">>> M = jnp.arange(16).reshape(4, 4)\n>>> jnp.einsum(\"...j->...\", M)  # requires explicit form\nArray([ 6, 22, 38, 54], dtype=int32)\n>>> jnp.einsum(M, (..., 0), (...,))  # explicit form via indices\nArray([ 6, 22, 38, 54], dtype=int32)\n>>> M.sum(-1)\nArray([ 6, 22, 38, 54], dtype=int32)"
      }
    ]
  },
  {
    "title": "Matrix Transpose Examples",
    "concepts": [
      "Demonstrates matrix transpose using einsum.",
      "Shows both explicit and implicit forms of einsum for transpose.",
      "Uses jnp.transpose for comparison."
    ],
    "code_examples": [
      {
        "description": "Transposes a matrix y using jnp.einsum with explicit subscript notation.",
        "code": ">>> y = jnp.array([[1, 2, 3],\n...               [4, 5, 6]])\n>>> jnp.einsum(\"ij->ji\", y)  # explicit form\nArray([[1, 4],\n       [2, 5],\n       [3, 6]], dtype=int32)\n>>> jnp.einsum(\"ji\", y)  # implicit form\nArray([[1, 4],\n       [2, 5],\n       [3, 6]], dtype=int32)\n>>> jnp.einsum(y, (1, 0))  # implicit form via indices\nArray([[1, 4],\n       [2, 5],\n       [3, 6]], dtype=int32)\n>>> jnp.einsum(y, (0, 1), (1, 0))  # explicit form via indices\nArray([[1, 4],\n       [2, 5],\n       [3, 6]], dtype=int32)\n>>> jnp.transpose(y)\nArray([[1, 4],\n       [2, 5],\n       [3, 6]], dtype=int32)"
      }
    ]
  },
  {
    "title": "Matrix Diagonal Examples",
    "concepts": [
      "Demonstrates extracting the diagonal of a matrix using einsum.",
      "Uses jnp.diagonal for comparison."
    ],
    "code_examples": [
      {
        "description": "Extracts the diagonal elements of matrix M using jnp.einsum.",
        "code": ">>> M = jnp.arange(16).reshape(4, 4)\n>>> jnp.einsum(\"ii->i\", M)\nArray([ 0,  5, 10, 15], dtype=int32)\n>>> jnp.diagonal(M)\nArray([ 0,  5, 10, 15], dtype=int32)"
      }
    ]
  },
  {
    "title": "Matrix Trace Examples",
    "concepts": [
      "Demonstrates calculating the trace of a matrix using einsum.",
      "Uses jnp.trace for comparison."
    ],
    "code_examples": [
      {
        "description": "Calculates the trace of matrix M using jnp.einsum.",
        "code": ">>> M = jnp.arange(16).reshape(4, 4)\n>>> jnp.einsum(\"ii\", M)\nArray(30, dtype=int32)\n>>> jnp.trace(M)\nArray(30, dtype=int32)"
      }
    ]
  },
  {
    "title": "Tensor Product Examples",
    "concepts": [
      "Demonstrates tensor product using einsum.",
      "Shows both explicit and implicit forms of einsum for tensor product.",
      "Uses jnp.tensordot for comparison."
    ],
    "code_examples": [
      {
        "description": "Computes the tensor product of two tensors x and y using jnp.einsum with explicit subscript notation.",
        "code": ">>> x = jnp.arange(30).reshape(2, 3, 5)\n>>> y = jnp.arange(60).reshape(3, 4, 5)\n>>> jnp.einsum('ijk,jlk->il', x, y)  # explicit form\nArray([[ 3340,  3865,  4390,  4915],\n       [ 8290,  9940, 11590, 13240]], dtype=int32)\n>>> jnp.tensordot(x, y, axes=[(1, 2), (0, 2)])\nArray([[ 3340,  3865,  4390,  4915],\n       [ 8290,  9940, 11590, 13240]], dtype=int32)"
      },
      {
        "description": "Demonstrates alternative einsum calling conventions for tensor product, including explicit and implicit forms using indices.",
        "code": ">>> x = jnp.arange(30).reshape(2, 3, 5)\n>>> y = jnp.arange(60).reshape(3, 4, 5)\n>>> jnp.einsum('ijk,jlk', x, y)  # implicit form\nArray([[ 3340,  3865,  4390,  4915],\n       [ 8290,  9940, 11590, 13240]], dtype=int32)\n>>> jnp.einsum(x, (0, 1, 2), y, (1, 3, 2), (0, 3))  # explicit form via indices\nArray([[ 3340,  3865,  4390,  4915],\n       [ 8290,  9940, 11590, 13240]], dtype=int32)\n>>> jnp.einsum(x, (0, 1, 2), y, (1, 3, 2))  # implicit form via indices\nArray([[ 3340,  3865,  4390,  4915],\n       [ 8290,  9940, 11590, 13240]], dtype=int32)"
      }
    ]
  },
  {
    "title": "Chained Dot Products Examples",
    "concepts": [
      "Demonstrates chained dot products using einsum.",
      "Shows how to perform chained matrix multiplication using einsum and other methods.",
      "Uses jnp.linalg.multi_dot for comparison."
    ],
    "code_examples": [
      {
        "description": "Computes a chained dot product of four matrices using jnp.einsum with explicit subscript notation.",
        "code": ">>> w = jnp.arange(5, 9).reshape(2, 2)\n>>> x = jnp.arange(6).reshape(2, 3)\n>>> y = jnp.arange(-2, 4).reshape(3, 2)\n>>> z = jnp.array([[2, 4, 6],\n...                [3, 5, 7]])\n>>> jnp.einsum('ij,jk,kl,lm->im', w, x, y, z)\nArray([[ 481,  831, 1181],\n       [ 651, 1125, 1599]], dtype=int32)\n>>> jnp.einsum(w, (0, 1), x, (1, 2), y, (2, 3), z, (3, 4))  # implicit, via indices\nArray([[ 481,  831, 1181],\n       [ 651, 1125, 1599]], dtype=int32)\n>>> w @ x @ y @ z  # direct chain of matmuls\nArray([[ 481,  831, 1181],\n       [ 651, 1125, 1599]], dtype=int32)\n>>> jnp.linalg.multi_dot([w, x, y, z])\nArray([[ 481,  831, 1181],\n       [ 651, 1125, 1599]], dtype=int32)"
      }
    ]
  },
  {
    "title": "JAX NumPy frombuffer Function",
    "concepts": [
      "Converts a buffer into a 1-D JAX array.",
      "Implements NumPy's frombuffer function in JAX.",
      "The buffer can be a bytes object or an object exporting the Python buffer interface.",
      "Specifies the data type of the array using the dtype argument.",
      "The count argument specifies the number of items to read from the buffer.",
      "The offset argument specifies the number of bytes to skip at the beginning of the buffer."
    ],
    "code_examples": [
      {
        "description": "Example of using jnp.frombuffer() with a bytes buffer and specifying the dtype as uint8.",
        "code": "buf = b\"\\x00\\x01\\x02\\x03\\x04\"\njnp.frombuffer(buf, dtype=jnp.uint8)"
      },
      {
        "description": "Example of using jnp.frombuffer() with a bytes buffer, specifying the dtype as uint8 and offset as 1.",
        "code": "buf = b\"\\x00\\x01\\x02\\x03\\x04\"\njnp.frombuffer(buf, dtype=jnp.uint8, offset=1)"
      },
      {
        "description": "Example demonstrating constructing a JAX array via the Python buffer interface, using Python\u2019s built-in array module.",
        "code": "from array import array\npybuffer = array('i', [0, 1, 2, 3, 4])\njnp.frombuffer(pybuffer, dtype=jnp.int32)"
      },
      {
        "description": "Example demonstrating constructing a JAX array via the Python buffer interface, using Python\u2019s built-in array module.",
        "code": "from array import array\npybuffer = array('i', [0, 1, 2, 3, 4])\njnp.frombuffer(pybuffer, dtype=jnp.int32)"
      }
    ]
  },
  {
    "title": "Unimplemented jnp.fromiter Wrapper",
    "concepts": [
      "jnp.fromiter is deliberately unimplemented in JAX.",
      "The function may be non-pure and unsafe for JIT and JAX transformations.",
      "Consider using jnp.asarray(np.fromiter(...)) as an alternative.",
      "Using np.fromiter within JAX transformations can have side effects due to consuming the iterable object."
    ],
    "code_examples": []
  },
  {
    "title": "Description of jax.numpy.full_like()",
    "concepts": [
      "Creates an array filled with a specified value.",
      "The created array has the same shape and dtype as a given array.",
      "It is a JAX implementation of numpy.full_like().",
      "The shape and dtype can be optionally overridden.",
      "The array can be placed on a specific device."
    ],
    "code_examples": [
      {
        "description": "Example using jnp.full_like() to create an array with the same shape and dtype as an existing array, filled with the value 2.",
        "code": ">>> x = jnp.arange(4.0)\n>>> jnp.full_like(x, 2)\nArray([2., 2., 2., 2.], dtype=float32)"
      },
      {
        "description": "Example using jnp.full_like() to create an array with a specified shape, filled with the value 0, and using the dtype from the input array.",
        "code": ">>> x = jnp.arange(4.0)\n>>> jnp.full_like(x, 0, shape=(2, 3))\nArray([[0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)"
      },
      {
        "description": "Example using jnp.full_like() with a fill_value that is an array, which is broadcast to the specified shape.",
        "code": ">>> x = jnp.arange(6).reshape(2, 3)\n>>> jnp.full_like(x, fill_value=jnp.array([[1],[2]]))\nArray([[1, 1, 1],\n       [2, 2, 2]], dtype=int32)"
      }
    ]
  },
  {
    "title": "Greatest Common Divisor (GCD) in JAX",
    "concepts": [
      "Computes the greatest common divisor of two arrays element-wise.",
      "Uses JAX implementation of numpy.gcd().",
      "Accepts two input arrays, x1 and x2, with integer dtypes.",
      "Returns an array containing the GCDs of the corresponding elements from the absolute values of x1 and x2.",
      "See also jax.numpy.lcm() for least common multiple.",
      "The function supports scalar inputs, array inputs, and broadcasting."
    ],
    "code_examples": [
      {
        "description": "GCD with scalar inputs.",
        "code": "jnp.gcd(12, 18)"
      },
      {
        "description": "GCD with array inputs.",
        "code": "x1 = jnp.array([12, 18, 24])\nx2 = jnp.array([5, 10, 15])\njnp.gcd(x1, x2)"
      },
      {
        "description": "GCD with broadcasting.",
        "code": "x1 = jnp.array([12])\nx2 = jnp.array([6, 9, 12])\njnp.gcd(x1, x2)"
      }
    ]
  },
  {
    "title": "Hanning Window Function in JAX",
    "concepts": [
      "The function returns a Hanning window of a specified size.",
      "The function is a JAX implementation of NumPy's hanning function.",
      "The input 'M' determines the window size.",
      "The output is an array of size M representing the Hanning window."
    ],
    "code_examples": [
      {
        "description": "Example usage of jnp.hanning() with M=4, demonstrating the output array of the Hanning window. Uses jnp.printoptions to format the output for better readability.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.hanning(4))"
      },
      {
        "description": "Example usage of jnp.hanning() with M=4, demonstrating the output array of the Hanning window. Uses jnp.printoptions to format the output for better readability.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(jnp.hanning(4))"
      }
    ]
  },
  {
    "title": "Least Common Multiple (LCM) Definition",
    "concepts": [
      "Computes the least common multiple of two arrays using JAX.",
      "The input arrays must have integer dtypes.",
      "Returns an array containing the LCM of corresponding elements from the absolute values of the input arrays.",
      "The function is a JAX implementation of NumPy's lcm function."
    ],
    "code_examples": []
  },
  {
    "title": "LCM with Scalar Inputs",
    "concepts": [
      "Demonstrates the use of jnp.lcm() with scalar inputs."
    ],
    "code_examples": [
      {
        "description": "Calculates the LCM of 12 and 18.",
        "code": "jnp.lcm(12, 18)"
      },
      {
        "description": "Calculates the LCM of 12 and 18.",
        "code": "jnp.lcm(12, 18)"
      }
    ]
  },
  {
    "title": "LCM with Array Inputs",
    "concepts": [
      "Demonstrates the use of jnp.lcm() with array inputs.",
      "jnp.array() is used to define the arrays."
    ],
    "code_examples": [
      {
        "description": "Calculates the LCM of two arrays x1 and x2.",
        "code": "x1 = jnp.array([12, 18, 24])\nx2 = jnp.array([5, 10, 15])\njnp.lcm(x1, x2)"
      },
      {
        "description": "Calculates the LCM of two arrays x1 and x2.",
        "code": "x1 = jnp.array([12, 18, 24])\nx2 = jnp.array([5, 10, 15])\njnp.lcm(x1, x2)"
      }
    ]
  },
  {
    "title": "LCM with Broadcasting",
    "concepts": [
      "Demonstrates the broadcasting feature of jnp.lcm().",
      "jnp.lcm() automatically broadcasts arrays with compatible shapes."
    ],
    "code_examples": [
      {
        "description": "Calculates the LCM of an array x1 and x2 with broadcasting.",
        "code": "x1 = jnp.array([12])\nx2 = jnp.array([6, 9, 12])\njnp.lcm(x1, x2)"
      },
      {
        "description": "Calculates the LCM of an array x1 and x2 with broadcasting.",
        "code": "x1 = jnp.array([12])\nx2 = jnp.array([6, 9, 12])\njnp.lcm(x1, x2)"
      }
    ]
  },
  {
    "title": "Introduction to lexsort",
    "concepts": [
      "The function `jax.numpy.lexsort()` sorts a sequence of keys in lexicographic order.",
      "It is a JAX implementation of NumPy's `lexsort()` function.",
      "The last key in the sequence is used as the primary key for sorting.",
      "The `axis` parameter specifies the axis along which to sort.",
      "It returns an array of indices that represent the lexicographically sorted order of the input arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Lexsort with a Single Key",
    "concepts": [
      "Using lexsort with a single key is equivalent to using argsort.",
      "argsort returns the indices that would sort an array."
    ],
    "code_examples": [
      {
        "description": "Demonstrates lexsort with a single key, which is equivalent to argsort.",
        "code": "key1 = jnp.array([4, 2, 3, 2, 5])\nprint(jnp.lexsort([key1]))\nprint(jnp.argsort(key1))"
      }
    ]
  },
  {
    "title": "Lexsort with Multiple Keys",
    "concepts": [
      "When using multiple keys, `lexsort` sorts primarily by the last key provided.",
      "Secondary keys are used to break ties in the primary key."
    ],
    "code_examples": [
      {
        "description": "Demonstrates lexsort with multiple keys. The last key (key2) is the primary key, and key1 is used to break ties.",
        "code": "key1 = jnp.array([4, 2, 3, 2, 5])\nkey2 = jnp.array([2, 1, 1, 2, 2])\nprint(jnp.lexsort([key1, key2]))"
      },
      {
        "description": "Demonstrates how the indices returned by lexsort can be used to print the sorted keys.",
        "code": "key1 = jnp.array([4, 2, 3, 2, 5])\nkey2 = jnp.array([2, 1, 1, 2, 2])\nindices = jnp.lexsort([key1, key2])\nprint(f\"{key1[indices]}\n{key2[indices]}\")"
      }
    ]
  },
  {
    "title": "Lexsort with Multi-Dimensional Arrays",
    "concepts": [
      "For multi-dimensional arrays, lexsort defaults to sorting along the last axis (axis=-1).",
      "The axis along which to sort can be specified using the `axis` keyword argument."
    ],
    "code_examples": [
      {
        "description": "Demonstrates lexsort with multi-dimensional arrays, sorting along the default last axis.",
        "code": "key1 = jnp.array([[2, 4, 2, 3],\n                [3, 1, 2, 2]])\nkey2 = jnp.array([[1, 2, 1, 3],\n                [2, 1, 2, 1]])\nprint(jnp.lexsort([key1, key2]))"
      },
      {
        "description": "Demonstrates lexsort with multi-dimensional arrays, sorting along the leading axis (axis=0).",
        "code": "key1 = jnp.array([[2, 4, 2, 3],\n                [3, 1, 2, 2]])\nkey2 = jnp.array([[1, 2, 1, 3],\n                [2, 1, 2, 1]])\nprint(jnp.lexsort([key1, key2], axis=0))"
      }
    ]
  },
  {
    "title": "Logarithm of Sum of Exponentials in Base-2",
    "concepts": [
      "Computes log2(2^{x1} + 2^{x2}) element-wise, avoiding overflow.",
      "JAX implementation of numpy.logaddexp2.",
      "Inputs x1 and x2 should have the same shape or be broadcast compatible.",
      "Related functions: jax.numpy.logaddexp() and jax.numpy.log2()."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the usage of jnp.logaddexp2 and compares it with the equivalent calculation using jnp.log2 and jnp.exp2.",
        "code": "x1 = jnp.array([[3, -1, 4],\n                [8, 5, -2]])\nx2 = jnp.array([2, 3, -5])\nresult1 = jnp.logaddexp2(x1, x2)\nresult2 = jnp.log2(jnp.exp2(x1) + jnp.exp2(x2))\njnp.allclose(result1, result2)"
      },
      {
        "description": "Demonstrates the usage of jnp.logaddexp2 and compares it with the equivalent calculation using jnp.log2 and jnp.exp2.",
        "code": "x1 = jnp.array([[3, -1, 4],\n                [8, 5, -2]])\nx2 = jnp.array([2, 3, -5])\nresult1 = jnp.logaddexp2(x1, x2)\nresult2 = jnp.log2(jnp.exp2(x1) + jnp.exp2(x2))\njnp.allclose(result1, result2)"
      }
    ]
  },
  {
    "title": "Introduction to jnp.nanvar",
    "concepts": [
      "Calculates the variance of array elements along a specified axis, while ignoring NaN values.",
      "It is a JAX implementation of numpy.nanvar().",
      "The function can handle different data types for the output array.",
      "It allows specifying the degrees of freedom (ddof) for the variance computation.",
      "It supports keeping the reduced axes in the result with size 1 (keepdims=True).",
      "It allows including specific elements of the array in the variance computation using the `where` parameter.",
      "If all elements along the given axis are NaNs, the function returns nan."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage: Variance Along All Axes",
    "concepts": [
      "By default, jnp.nanvar computes the variance along all axes of the input array.",
      "The function handles NaN values in the array by ignoring them during the calculation."
    ],
    "code_examples": [
      {
        "description": "Demonstrates calculating the variance of an array containing NaN values along all axes.",
        "code": "import jax.numpy as jnp\n\nnan = jnp.nan\nx = jnp.array([[1, nan, 4, 3],\n               [nan, 2, nan, 9],\n               [4, 8, 6, nan]])\n\njnp.nanvar(x)"
      }
    ]
  },
  {
    "title": "Variance Along a Specific Axis",
    "concepts": [
      "The `axis` parameter allows specifying the axis along which to compute the variance.",
      "When `axis=1`, the variance is computed along the rows of the array."
    ],
    "code_examples": [
      {
        "description": "Calculates the variance along axis 1 (rows), ignoring NaN values, and prints the result with specified precision and suppression of scientific notation.",
        "code": "import jax.numpy as jnp\n\nnan = jnp.nan\nx = jnp.array([[1, nan, 4, 3],\n               [nan, 2, nan, 9],\n               [4, 8, 6, nan]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.nanvar(x, axis=1))"
      }
    ]
  },
  {
    "title": "Preserving Dimensions with keepdims",
    "concepts": [
      "The `keepdims` parameter can be set to `True` to preserve the dimensions of the input array in the output.",
      "This results in the reduced axes having a size of 1."
    ],
    "code_examples": [
      {
        "description": "Calculates the variance along axis 1 (rows), ignoring NaN values, and keeps the dimensions of the original array by setting keepdims=True. The result is printed with specified precision and suppression of scientific notation.",
        "code": "import jax.numpy as jnp\n\nnan = jnp.nan\nx = jnp.array([[1, nan, 4, 3],\n               [nan, 2, nan, 9],\n               [4, 8, 6, nan]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.nanvar(x, axis=1, keepdims=True))"
      }
    ]
  },
  {
    "title": "Degrees of Freedom (ddof)",
    "concepts": [
      "The `ddof` parameter specifies the degrees of freedom for the variance calculation.",
      "The divisor in the variance computation is N - ddof, where N is the number of elements along the given axis.",
      "Setting `ddof=1` changes the variance calculation."
    ],
    "code_examples": [
      {
        "description": "Calculates the variance along axis 1 (rows), ignoring NaN values, keeps the dimensions of the original array, and sets ddof=1. The result is printed with specified precision and suppression of scientific notation.",
        "code": "import jax.numpy as jnp\n\nnan = jnp.nan\nx = jnp.array([[1, nan, 4, 3],\n               [nan, 2, nan, 9],\n               [4, 8, 6, nan]])\n\nwith jnp.printoptions(precision=2, suppress=True):\n  print(jnp.nanvar(x, axis=1, keepdims=True, ddof=1))"
      }
    ]
  },
  {
    "title": "Conditional Variance Calculation with where",
    "concepts": [
      "The `where` parameter allows including only specific elements of the array in the variance calculation.",
      "It takes a boolean array that is broadcast compatible with the input array.",
      "Elements corresponding to `True` in the `where` array are included, while others are excluded."
    ],
    "code_examples": [
      {
        "description": "Calculates the variance along axis 1 (rows), ignoring NaN values, keeps the dimensions of the original array, and uses the `where` parameter to include only specific elements in the calculation.",
        "code": "import jax.numpy as jnp\n\nnan = jnp.nan\nx = jnp.array([[1, nan, 4, 3],\n               [nan, 2, nan, 9],\n               [4, 8, 6, nan]])\n\nwhere = jnp.array([[1, 0, 1, 0],\n                   [0, 1, 1, 0],\n                   [1, 1, 0, 1]], dtype=bool)\n\njnp.nanvar(x, axis=1, keepdims=True, where=where)"
      }
    ]
  },
  {
    "title": "Introduction to jax.numpy.polyadd()",
    "concepts": [
      "jax.numpy.polyadd() returns the sum of two polynomials.",
      "It is a JAX implementation of numpy.polyadd().",
      "It accepts two ArrayLike objects as input, representing polynomial coefficients.",
      "The function returns an array containing the coefficients of the sum of the input polynomials.",
      "Unlike numpy.polyadd(), jax.numpy.polyadd() only accepts arrays as input.",
      "It computes the sum of two polynomials represented by their coefficients."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage Examples",
    "concepts": [
      "Demonstrates basic usage with 1D arrays.",
      "Shows the addition of two polynomials represented as arrays of coefficients."
    ],
    "code_examples": [
      {
        "description": "Adds two polynomials represented by 1D jax.numpy arrays x1 and x2.",
        "code": "x1 = jnp.array([2, 3])\nx2 = jnp.array([5, 4, 1])\njnp.polyadd(x1, x2)"
      },
      {
        "description": "Adds two polynomials represented by 1D jax.numpy arrays x1 and x2.",
        "code": "x1 = jnp.array([2, 3])\nx2 = jnp.array([5, 4, 1])\njnp.polyadd(x1, x2)"
      }
    ]
  },
  {
    "title": "Examples with Multidimensional Arrays",
    "concepts": [
      "Demonstrates usage with multidimensional arrays.",
      "Illustrates how jax.numpy.polyadd() handles arrays with different shapes.",
      "Shows element-wise addition of polynomial coefficients in arrays."
    ],
    "code_examples": [
      {
        "description": "Adds two polynomials represented by 2D jax.numpy arrays x3 and x4.",
        "code": "x3 = jnp.array([[2, 3, 1]])\nx4 = jnp.array([[5, 7, 3],\n               [8, 2, 6]])\njnp.polyadd(x3, x4)"
      },
      {
        "description": "Adds two polynomials represented by 2D jax.numpy arrays x3 and x4.",
        "code": "x3 = jnp.array([[2, 3, 1]])\nx4 = jnp.array([[5, 7, 3],\n               [8, 2, 6]])\njnp.polyadd(x3, x4)"
      }
    ]
  },
  {
    "title": "Broadcasting and ValueError",
    "concepts": [
      "Illustrates a ValueError due to incompatible array shapes.",
      "Demonstrates the limitations of broadcasting in jax.numpy.polyadd().",
      "Shows an example where the arrays cannot be broadcast to a compatible shape."
    ],
    "code_examples": [
      {
        "description": "Attempts to add x5 and x6, which results in a ValueError because the arrays cannot be broadcast.",
        "code": "x5 = jnp.array([1, 3, 5])\nx6 = jnp.array([[5, 7, 9],\n               [8, 6, 4]])\njnp.polyadd(x5, x6)"
      },
      {
        "description": "Adds x6 and x7 successfully after broadcasting.",
        "code": "x7 = jnp.array([2])\njnp.polyadd(x6, x7)"
      },
      {
        "description": "Attempts to add x5 and x6, which results in a ValueError because the arrays cannot be broadcast.",
        "code": "x5 = jnp.array([1, 3, 5])\nx6 = jnp.array([[5, 7, 9],\n               [8, 6, 4]])\njnp.polyadd(x5, x6)"
      },
      {
        "description": "Adds x6 and x7 successfully after broadcasting.",
        "code": "x7 = jnp.array([2])\njnp.polyadd(x6, x7)"
      }
    ]
  },
  {
    "title": "Overview of jax.numpy.reshape",
    "concepts": [
      "The function reshapes an array using JAX.",
      "It is a JAX implementation of numpy.reshape.",
      "It uses jax.lax.reshape internally.",
      "The function always returns a copy of the array.",
      "The function supports C-style and Fortran-style ordering.",
      "The function can infer one dimension size using -1.",
      "jax.Array.reshape() provides equivalent functionality as an array method."
    ],
    "code_examples": []
  },
  {
    "title": "Reshaping with jax.numpy.reshape",
    "concepts": [
      "Reshape an array to a 1D array.",
      "Reshape an array to a 2D array."
    ],
    "code_examples": [
      {
        "description": "Reshaping a 2x3 array to a 1D array of size 6 and to a 3x2 array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\nprint(jnp.reshape(x, 6))\nprint(jnp.reshape(x, (3, 2)))"
      },
      {
        "description": "Inferring a dimension size using -1 during reshaping.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\nprint(jnp.reshape(x, -1))\nprint(jnp.reshape(x, (-1, 2)))"
      }
    ]
  },
  {
    "title": "Specifying Order During Reshape",
    "concepts": [
      "Reshaping with C-style (row-major) ordering (default).",
      "Reshaping with Fortran-style (column-major) ordering."
    ],
    "code_examples": [
      {
        "description": "Reshaping an array with Fortran-style ordering.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\nprint(jnp.reshape(x, 6, order='F'))\nprint(jnp.reshape(x, (3, 2), order='F'))"
      }
    ]
  },
  {
    "title": "Reshaping with jax.Array.reshape",
    "concepts": [
      "Using the reshape method of a JAX array.",
      "The array method provides the same functionality as jax.numpy.reshape."
    ],
    "code_examples": [
      {
        "description": "Reshaping a JAX array using the array's reshape method.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\n\nprint(x.reshape(3, 2))"
      }
    ]
  },
  {
    "title": "Introduction to jnp.size",
    "concepts": [
      "jnp.size returns the number of elements along a given axis of a JAX array.",
      "It is a JAX implementation of numpy.size.",
      "It raises a TypeError if the input is a collection such as a list or tuple, unlike numpy.size.",
      "The axis argument is optional; by default, it returns the total number of elements."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jnp.size with arrays",
    "concepts": [
      "Demonstrates using jnp.size with JAX arrays to get the total number of elements.",
      "Demonstrates using jnp.size to get the number of elements along a specified axis."
    ],
    "code_examples": [
      {
        "description": "Example showing the size of a 1D JAX array.",
        "code": "x = jnp.arange(10)\njnp.size(x)"
      },
      {
        "description": "Example showing the size of a 2D JAX array.",
        "code": "y = jnp.ones((2, 3))\njnp.size(y)"
      },
      {
        "description": "Example showing the size along a specific axis of a 2D JAX array.",
        "code": "y = jnp.ones((2, 3))\njnp.size(y, axis=1)"
      },
      {
        "description": "Example showing the size of a 1D JAX array.",
        "code": "x = jnp.arange(10)\njnp.size(x)"
      },
      {
        "description": "Example showing the size of a 2D JAX array.",
        "code": "y = jnp.ones((2, 3))\njnp.size(y)"
      },
      {
        "description": "Example showing the size along a specific axis of a 2D JAX array.",
        "code": "y = jnp.ones((2, 3))\njnp.size(y, axis=1)"
      }
    ]
  },
  {
    "title": "jnp.size with Scalars",
    "concepts": [
      "jnp.size returns 1 for scalar values."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating jnp.size with a scalar value.",
        "code": "jnp.size(3.14)"
      },
      {
        "description": "Example demonstrating jnp.size with a scalar value.",
        "code": "jnp.size(3.14)"
      }
    ]
  },
  {
    "title": "Accessing Size via Array Property",
    "concepts": [
      "The size of a JAX array can also be accessed using the .size property."
    ],
    "code_examples": [
      {
        "description": "Example accessing the size of a JAX array using the .size property.",
        "code": "y.size"
      },
      {
        "description": "Example accessing the size of a JAX array using the .size property.",
        "code": "y.size"
      }
    ]
  },
  {
    "title": "Introduction to JAX Array Splitting",
    "concepts": [
      "Splitting an array into sub-arrays is possible using JAX.",
      "The jnp.split() function is a JAX implementation of numpy.split().",
      "The function takes an array, indices/number of sections, and an axis as input.",
      "It returns a list of sub-arrays.",
      "The 'indices_or_sections' parameter can be an integer or a sequence of integers specifying split points.",
      "If 'indices_or_sections' is an integer, the array is divided into that many equally sized chunks along the specified axis."
    ],
    "code_examples": []
  },
  {
    "title": "Splitting a 1D Array into Equal Sections",
    "concepts": [
      "Demonstrates splitting a 1D JAX array into a specified number of equal sections.",
      "The jnp.split() function is used with the array and the number of sections as arguments."
    ],
    "code_examples": [
      {
        "description": "Splits a 1D JAX array into three equal sections and prints the resulting chunks.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\nchunks = jnp.split(x, 3)\nprint(*chunks)"
      }
    ]
  },
  {
    "title": "Splitting a 1D Array by Index",
    "concepts": [
      "Demonstrates splitting a 1D JAX array using specific index values.",
      "The jnp.split() function is used with the array and a list of indices as arguments.",
      "The indices specify the boundaries between the sub-arrays."
    ],
    "code_examples": [
      {
        "description": "Splits a 1D JAX array into sections defined by the indices [2, 7].",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\nchunks = jnp.split(x, [2, 7])\nprint(*chunks)"
      }
    ]
  },
  {
    "title": "Splitting a 2D Array Along an Axis",
    "concepts": [
      "Demonstrates splitting a 2D JAX array along a specified axis.",
      "The 'axis' parameter of the jnp.split() function is used to specify the axis along which to split.",
      "The example splits the array along axis 1 (horizontally)."
    ],
    "code_examples": [
      {
        "description": "Splits a 2D JAX array into two sections along axis 1 and prints the resulting sub-arrays.",
        "code": "import jax.numpy as jnp\n\nx = jnp.array([[1, 2, 3, 4],\n              [5, 6, 7, 8]])\n\nx1, x2 = jnp.split(x, 2, axis=1)\nprint(x1)\nprint(x2)"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.numpy.array_split() is similar to split, but handles cases where indices_or_sections does not evenly divide the array size.",
      "jax.numpy.vsplit() splits vertically along axis=0.",
      "jax.numpy.hsplit() splits horizontally along axis=1.",
      "jax.numpy.dsplit() splits depth-wise along axis=2."
    ],
    "code_examples": []
  },
  {
    "title": "JAX Implementation of tril()",
    "concepts": [
      "The function returns the lower triangle of an array.",
      "The input array must have a dimension of 2 or greater.",
      "The 'k' parameter specifies the sub-diagonal above which elements are set to zero. k=0 is the main diagonal.",
      "k < 0 refers to sub-diagonals below the main diagonal.",
      "k > 0 refers to sub-diagonals above the main diagonal.",
      "The output array has the same shape as the input array.",
      "Elements above the specified sub-diagonal are set to zero.",
      "jax.numpy.triu() returns the upper triangle of an array.",
      "jax.numpy.tri() returns an array with ones on and below the diagonal and zeros elsewhere."
    ],
    "code_examples": [
      {
        "description": "Example usage of jnp.tril() with a 2D array, showing the default behavior (k=0).",
        "code": "x = jnp.array([[1, 2, 3, 4],\n               [5, 6, 7, 8],\n               [9, 10, 11, 12]])\n\njnp.tril(x)"
      },
      {
        "description": "Example usage of jnp.tril() with a 2D array and k=1, showing how to include the diagonal above the main diagonal.",
        "code": "x = jnp.array([[1, 2, 3, 4],\n               [5, 6, 7, 8],\n               [9, 10, 11, 12]])\n\njnp.tril(x, k=1)"
      },
      {
        "description": "Example usage of jnp.tril() with a 2D array and k=-1, showing how to exclude the main diagonal.",
        "code": "x = jnp.array([[1, 2, 3, 4],\n               [5, 6, 7, 8],\n               [9, 10, 11, 12]])\n\njnp.tril(x, k=-1)"
      },
      {
        "description": "Example demonstrating batch-wise operation of jnp.tril when the input array has more than 2 dimensions.",
        "code": "x1 = jnp.array([[[1, 2],\n                [3, 4]],\n\n               [[5, 6],\n                [7, 8]]])\n\njnp.tril(x1)"
      }
    ]
  },
  {
    "title": "Description of jax.numpy.triu_indices",
    "concepts": [
      "This function returns the indices of the upper triangle of an array.",
      "It is a JAX implementation of NumPy's `triu_indices()` function.",
      "The function takes the number of rows `n` as input.",
      "The optional argument `k` specifies the sub-diagonal from which to start.",
      "The optional argument `m` specifies the number of columns, defaulting to `n` if not provided.",
      "The function returns a tuple of two arrays representing the row and column indices of the upper triangle."
    ],
    "code_examples": []
  },
  {
    "title": "Examples of jax.numpy.triu_indices usage",
    "concepts": [
      "Demonstrates the usage of `jax.numpy.triu_indices()` with different parameters.",
      "Shows how to get the indices of the upper triangle for a square matrix when only `n` is provided.",
      "Shows how to get the indices of the upper triangle for a non-square matrix when both `n` and `m` are provided.",
      "Illustrates how to use the `k` parameter to select a specific sub-diagonal.",
      "Shows the result of setting k to 1 to retrieve the indices above the main diagonal.",
      "Shows the result of setting k to -1 to retrieve the indices starting from the diagonal below the main one."
    ],
    "code_examples": [
      {
        "description": "Returns the indices of the upper triangle of a (3, 3) array.",
        "code": "jnp.triu_indices(3)\n(Array([0, 0, 0, 1, 1, 2], dtype=int32), Array([0, 1, 2, 1, 2, 2], dtype=int32))"
      },
      {
        "description": "Returns the indices of the upper triangle of a (3, 2) array.",
        "code": "jnp.triu_indices(3, m=2)\n(Array([0, 0, 1], dtype=int32), Array([0, 1, 1], dtype=int32))"
      },
      {
        "description": "Returns the indices on and above the first sub-diagonal above the main diagonal for a (3, 3) array.",
        "code": "jnp.triu_indices(3, k=1)\n(Array([0, 0, 1], dtype=int32), Array([1, 2, 2], dtype=int32))"
      },
      {
        "description": "Returns the indices on and above the first sub-diagonal below the main diagonal for a (3, 3) array.",
        "code": "jnp.triu_indices(3, k=-1)\n(Array([0, 0, 0, 1, 1, 1, 2, 2], dtype=int32), Array([0, 1, 2, 0, 1, 2, 1, 2], dtype=int32))"
      }
    ]
  },
  {
    "title": "JAX true_divide Function",
    "concepts": [
      "Calculates element-wise division of two arrays.",
      "It is a JAX implementation of NumPy's true_divide function.",
      "The function always performs floating-point division.",
      "Input arrays can be of type ArrayLike.",
      "The function supports complex number division."
    ],
    "code_examples": [
      {
        "description": "Demonstrates true_divide with integer array and scalar.",
        "code": "x1 = jnp.array([3, 4, 5])\nx2 = 2\njnp.true_divide(x1, x2)"
      },
      {
        "description": "Demonstrates true_divide with scalar and complex array.",
        "code": "x1 = 24\nx2 = jnp.array([3, 4, 6j])\njnp.true_divide(x1, x2)"
      },
      {
        "description": "Demonstrates true_divide with complex array and scalar.",
        "code": "x1 = jnp.array([1j, 9 + 5j, -4 + 2j])\nx2 = 3j\njnp.true_divide(x1, x2)"
      }
    ]
  },
  {
    "title": "JAX uint64 Scalar Constructor",
    "concepts": [
      "JAX provides a scalar constructor of type uint64.",
      "NumPy defines scalar types for each data type.",
      "JAX represents scalars as zero-dimensional arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to jnp.union1d",
    "concepts": [
      "Computes the set union of two 1D arrays using JAX.",
      "It is a JAX implementation of numpy.union1d().",
      "The function is not typically compatible with jit() due to data-dependent output size.",
      "JAX version requires a static 'size' argument for use with jit()."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Usage of jnp.union1d",
    "concepts": [
      "Demonstrates the basic usage of jnp.union1d to compute the union of two arrays.",
      "The function returns a sorted array containing the unique elements from both input arrays."
    ],
    "code_examples": [
      {
        "description": "Compute the union of two JAX arrays.",
        "code": "import jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njnp.union1d(ar1, ar2)"
      },
      {
        "description": "Compute the union of two JAX arrays.",
        "code": "import jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njnp.union1d(ar1, ar2)"
      }
    ]
  },
  {
    "title": "Incompatibility with JIT without size argument",
    "concepts": [
      "Illustrates the error that occurs when using jnp.union1d with jax.jit() without specifying the size argument.",
      "The error is a ConcretizationTypeError due to the dynamic output shape."
    ],
    "code_examples": [
      {
        "description": "Attempting to use jnp.union1d with jax.jit() without specifying the size argument results in an error.",
        "code": "import jax\nimport jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njax.jit(jnp.union1d)(ar1, ar2)"
      },
      {
        "description": "Attempting to use jnp.union1d with jax.jit() without specifying the size argument results in an error.",
        "code": "import jax\nimport jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njax.jit(jnp.union1d)(ar1, ar2)"
      }
    ]
  },
  {
    "title": "Using jnp.union1d with jax.jit() and size argument",
    "concepts": [
      "Shows how to use jnp.union1d with jax.jit() by providing the static 'size' argument.",
      "This ensures a statically-known output shape, making it compatible with JIT compilation."
    ],
    "code_examples": [
      {
        "description": "Demonstrates using jnp.union1d with jax.jit() and the static size argument to ensure a statically-known output shape.",
        "code": "import jax\nimport jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njit_union1d = jax.jit(jnp.union1d, static_argnames=['size'])\njit_union1d(ar1, ar2, size=6)"
      },
      {
        "description": "Demonstrates using jnp.union1d with jax.jit() and the static size argument to ensure a statically-known output shape.",
        "code": "import jax\nimport jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njit_union1d = jax.jit(jnp.union1d, static_argnames=['size'])\njit_union1d(ar1, ar2, size=6)"
      }
    ]
  },
  {
    "title": "Truncation and Padding with size and fill_value",
    "concepts": [
      "Illustrates the behavior when the provided 'size' is smaller than the actual union size, resulting in truncation.",
      "Shows how to use 'fill_value' to pad the output when 'size' is larger than the actual union size."
    ],
    "code_examples": [
      {
        "description": "Demonstrates the truncation of the union when the specified size is smaller than the actual number of unique elements.",
        "code": "import jax\nimport jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njit_union1d = jax.jit(jnp.union1d, static_argnames=['size'])\njit_union1d(ar1, ar2, size=4)"
      },
      {
        "description": "Demonstrates the truncation of the union when the specified size is smaller than the actual number of unique elements.",
        "code": "import jax\nimport jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njit_union1d = jax.jit(jnp.union1d, static_argnames=['size'])\njit_union1d(ar1, ar2, size=4)"
      },
      {
        "description": "Shows how to pad the output with a specified fill_value when the specified size is larger than the actual number of unique elements.",
        "code": "import jax\nimport jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njit_union1d = jax.jit(jnp.union1d, static_argnames=['size'])\njit_union1d(ar1, ar2, size=8, fill_value=0)"
      },
      {
        "description": "Shows how to pad the output with a specified fill_value when the specified size is larger than the actual number of unique elements.",
        "code": "import jax\nimport jax.numpy as jnp\n\nar1 = jnp.array([1, 2, 3, 4])\nar2 = jnp.array([3, 4, 5, 6])\n\njit_union1d = jax.jit(jnp.union1d, static_argnames=['size'])\njit_union1d(ar1, ar2, size=8, fill_value=0)"
      }
    ]
  },
  {
    "title": "Introduction to `jax.numpy.linalg.vecdot`",
    "concepts": [
      "The function performs conjugate multiplication of two batched vectors.",
      "It is a JAX implementation of numpy.vecdot().",
      "The size of b[axis] must match the size of a[axis], and remaining dimensions must be broadcast-compatible.",
      "The `axis` parameter specifies the axis along which to compute the dot product.",
      "The `precision` parameter allows specifying the precision of the computation.",
      "The `preferred_element_type` parameter specifies the accumulation datatype."
    ],
    "code_examples": []
  },
  {
    "title": "1D Array Conjugate Dot Product Example",
    "concepts": [
      "Demonstrates how to compute the conjugate dot product of two 1D arrays using `jax.numpy.linalg.vecdot`."
    ],
    "code_examples": [
      {
        "description": "Computes the conjugate dot product of two complex 1D arrays.",
        "code": "a = jnp.array([1j, 2j, 3j])\nb = jnp.array([4., 5., 6.])\njnp.linalg.vecdot(a, b)"
      },
      {
        "description": "Computes the conjugate dot product of two complex 1D arrays.",
        "code": "a = jnp.array([1j, 2j, 3j])\nb = jnp.array([4., 5., 6.])\njnp.linalg.vecdot(a, b)"
      }
    ]
  },
  {
    "title": "2D Array Batched Vector Dot Product Example",
    "concepts": [
      "Demonstrates how to compute the batched vector dot product of two 2D arrays using `jax.numpy.linalg.vecdot`.",
      "The `axis` parameter is used to specify the axis along which the dot product is computed."
    ],
    "code_examples": [
      {
        "description": "Computes the batched vector dot product of two 2D arrays along the last axis (axis=-1).",
        "code": "a = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\nb = jnp.array([[2, 3, 4]])\njnp.linalg.vecdot(a, b, axis=-1)"
      },
      {
        "description": "Computes the batched vector dot product of two 2D arrays along the last axis (axis=-1).",
        "code": "a = jnp.array([[1, 2, 3],\n               [4, 5, 6]])\nb = jnp.array([[2, 3, 4]])\njnp.linalg.vecdot(a, b, axis=-1)"
      }
    ]
  },
  {
    "title": "Cross Product of 3D Vectors in JAX",
    "concepts": [
      "Computes the cross-product of two 3D vectors.",
      "JAX implementation of numpy.linalg.cross().",
      "Arrays x1 and x2 should be N-dimensional with shape[axis] == 3.",
      "The `axis` parameter specifies the axis along which the cross product is computed (default: -1)."
    ],
    "code_examples": [
      {
        "description": "Showing that x_hat x y_hat = z_hat",
        "code": "x = jnp.array([\n    1.,\n    0.,\n    0.\n])\ny = jnp.array([\n    0.,\n    1.,\n    0.\n])\njnp.linalg.cross(\n    x,\n    y\n)"
      },
      {
        "description": "Cross product of x_hat with all three standard unit vectors, via broadcasting",
        "code": "xyz = jnp.eye(3)\njnp.linalg.cross(\n    x,\n    xyz,\n    axis=-1\n)"
      }
    ]
  },
  {
    "title": "Introduction to multi_dot",
    "concepts": [
      "The jnp.linalg.multi_dot() function efficiently computes matrix products between a sequence of arrays.",
      "It uses opt_einsum library to determine the most efficient order of operations.",
      "The function aims to minimize the computational cost associated with multiple matrix multiplications."
    ],
    "code_examples": []
  },
  {
    "title": "Computational Cost of Matrix Multiplication",
    "concepts": [
      "The order of matrix multiplication can significantly impact computational cost.",
      "Associativity allows for different evaluation orders of matrix products, yielding equivalent results.",
      "Different evaluation orders can lead to drastically different numbers of floating-point operations (flops)."
    ],
    "code_examples": [
      {
        "description": "Estimates the number of floating point operations (flops) required for a single matrix multiplication.",
        "code": "def approx_flops(x, y):\n    # for 2D x and y, with x.shape[1] == y.shape[0]\n    return 2 * x.shape[0] * x.shape[1] * y.shape[1]"
      },
      {
        "description": "Demonstrates the computational cost difference between two ways of computing x @ y @ z.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef approx_flops(x, y):\n    # for 2D x and y, with x.shape[1] == y.shape[0]\n    return 2 * x.shape[0] * x.shape[1] * y.shape[1]\n\nkey1, key2, key3 = jax.random.split(jax.random.key(0), 3)\nx = jax.random.normal(key1, shape=(200, 5))\ny = jax.random.normal(key2, shape=(5, 100))\nz = jax.random.normal(key3, shape=(100, 10))\n\nresult1 = (x @ y) @ z\nresult2 = x @ (y @ z)\n\nprint(\"(x @ y) @ z flops:\", approx_flops(x, y) + approx_flops(x @ y, z))\nprint(\"x @ (y @ z) flops:\", approx_flops(y, z) + approx_flops(x, y @ z))"
      },
      {
        "description": "Compares the output of (x @ y) @ z,  x @ (y @ z), and jnp.linalg.multi_dot([x,y,z]).",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef approx_flops(x, y):\n    # for 2D x and y, with x.shape[1] == y.shape[0]\n    return 2 * x.shape[0] * x.shape[1] * y.shape[1]\n\nkey1, key2, key3 = jax.random.split(jax.random.key(0), 3)\nx = jax.random.normal(key1, shape=(200, 5))\ny = jax.random.normal(key2, shape=(5, 100))\nz = jax.random.normal(key3, shape=(100, 10))\n\nresult1 = (x @ y) @ z\nresult2 = x @ (y @ z)\nresult3 = jnp.linalg.multi_dot([x, y, z])\n\njnp.allclose(result1, result3, atol=1E-4)"
      },
      {
        "description": "Uses JAX's Ahead-of-time compilation to estimate the flops of each approach and confirms multi_dot is choosing the efficient one.",
        "code": "import jax\nimport jax.numpy as jnp\n\ndef approx_flops(x, y):\n    # for 2D x and y, with x.shape[1] == y.shape[0]\n    return 2 * x.shape[0] * x.shape[1] * y.shape[1]\n\nkey1, key2, key3 = jax.random.split(jax.random.key(0), 3)\nx = jax.random.normal(key1, shape=(200, 5))\ny = jax.random.normal(key2, shape=(5, 100))\nz = jax.random.normal(key3, shape=(100, 10))\n\nprint(jax.jit(lambda x, y, z: (x @ y) @ z).lower(x, y, z).cost_analysis()['flops'])\nprint(jax.jit(lambda x, y, z: x @ (y @ z)).lower(x, y, z).cost_analysis()['flops'])\nprint(jax.jit(jnp.linalg.multi_dot).lower([x, y, z]).cost_analysis()['flops'])"
      }
    ]
  },
  {
    "title": "Tensor Inverse Computation",
    "concepts": [
      "Computes the tensor inverse of an array.",
      "JAX implementation of numpy.linalg.tensorinv().",
      "Computes the inverse of the tensordot() operation.",
      "The input array must satisfy the condition prod(a.shape[:ind]) == prod(a.shape[ind:]).",
      "The 'ind' parameter specifies the number of indices in the tensor product.",
      "The function returns the tensor inverse of the input array."
    ],
    "code_examples": [
      {
        "description": "This example demonstrates how to compute the tensor inverse of a random array and verifies the result by performing a tensordot product and comparing it to an identity matrix.",
        "code": "key = jax.random.key(1337)\nx = jax.random.normal(key, shape=(2, 2, 4))\nxinv = jnp.linalg.tensorinv(x, 2)\nxinv_x = jnp.linalg.tensordot(xinv, x, axes=2)\njnp.allclose(xinv_x, jnp.eye(4), atol=1E-4)"
      },
      {
        "description": "This example demonstrates how to compute the tensor inverse of a random array and verifies the result by performing a tensordot product and comparing it to an identity matrix. This example is repeated in the document.",
        "code": "key = jax.random.key(1337)\nx = jax.random.normal(key, shape=(2, 2, 4))\nxinv = jnp.linalg.tensorinv(x, 2)\nxinv_x = jnp.linalg.tensordot(xinv, x, axes=2)\njnp.allclose(xinv_x, jnp.eye(4), atol=1E-4)"
      }
    ]
  },
  {
    "title": "Overview of rsf2csf Function",
    "concepts": [
      "The function converts a real Schur form to a complex Schur form.",
      "It is a JAX implementation of scipy.linalg.rsf2csf().",
      "It takes the real Schur form (T) and the corresponding Schur transformation matrix (Z) as input.",
      "It returns the complex Schur form and the associated Schur transformation matrix as a tuple of arrays (T, Z).",
      "jax.scipy.linalg.schur() can be used to compute the Schur decomposition."
    ],
    "code_examples": []
  },
  {
    "title": "Example Usage of rsf2csf",
    "concepts": [
      "Demonstrates how to use jax.scipy.linalg.schur() to obtain the real Schur form.",
      "Demonstrates how to use jax.scipy.linalg.rsf2csf() to convert the real Schur form to complex Schur form.",
      "Shows that both real and complex Schur forms can be used to reconstruct the original matrix.",
      "Illustrates the quasi-upper-triangular nature of the real Schur form and the upper-triangular nature of the complex Schur form."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the conversion from real to complex Schur form and reconstruction of the original matrix.",
        "code": "A = jnp.array([[0., 3., 3.],\n              [0., 1., 2.],\n              [2., 0., 1.]])\nTr, Zr = jax.scipy.linalg.schur(A)\nTc, Zc = jax.scipy.linalg.rsf2csf(Tr, Zr)\n\njnp.allclose(Zr @ Tr @ Zr.T, A, atol=1E-5)\njnp.allclose(Zc @ Tc @ Zc.conj().T, A, atol=1E-5)\n\njnp.allclose(Zr @ Tr @ Zr.T, A, atol=1E-5)\njnp.allclose(Zc @ Tc @ Zc.conj().T, A, atol=1E-5)"
      },
      {
        "description": "Example demonstrating the quasi-upper-triangular nature of the real Schur form.",
        "code": "with jax.numpy.printoptions(precision=2, suppress=True):\n  print(Tr)"
      },
      {
        "description": "Example demonstrating the upper-triangular nature of the complex Schur form.",
        "code": "with jnp.printoptions(precision=2, suppress=True):\n  print(Tc)"
      }
    ]
  },
  {
    "title": "Singular Value Decomposition (SVD) Overview",
    "concepts": [
      "SVD decomposes a matrix A into U, \u03a3, and V^H.",
      "U contains the left singular vectors and is unitary (U^H * U = I).",
      "V contains the right singular vectors and is unitary (V^H * V = I).",
      "\u03a3 is a diagonal matrix containing the singular values.",
      "The input array 'a' has a shape of (..., N, M).",
      "The 'full_matrices' parameter determines the shape of U and V.",
      "The 'compute_uv' parameter determines whether to return the full SVD (U, s, V^H) or just the singular values (s)."
    ],
    "code_examples": []
  },
  {
    "title": "SVD Examples",
    "concepts": [
      "Demonstrates how to compute the SVD of a small real-valued array using jax.scipy.linalg.svd().",
      "Shows how to verify that the singular vectors are orthonormal.",
      "Illustrates how to reconstruct the original matrix from the SVD components."
    ],
    "code_examples": [
      {
        "description": "Compute the SVD of a 2x3 matrix and print the singular values.",
        "code": "x = jnp.array([[1., 2., 3.],\n               [6., 5., 4.]])\nu, s, vt = jax.scipy.linalg.svd(x, full_matrices=False)\nprint(s)"
      },
      {
        "description": "Verify that the singular vectors are orthonormal.",
        "code": "x = jnp.array([[1., 2., 3.],\n               [6., 5., 4.]])\nu, s, vt = jax.scipy.linalg.svd(x, full_matrices=False)\n\njnp.allclose(u.T @ u, jnp.eye(2), atol=1E-5)\nv = vt.T\njnp.allclose(v.T @ v, jnp.eye(2), atol=1E-5)"
      },
      {
        "description": "Reconstruct the original matrix from its SVD.",
        "code": "x = jnp.array([[1., 2., 3.],\n               [6., 5., 4.]])\nu, s, vt = jax.scipy.linalg.svd(x, full_matrices=False)\n\nx_reconstructed = u @ jnp.diag(s) @ vt\njnp.allclose(x_reconstructed, x)"
      }
    ]
  },
  {
    "title": "Introduction to Toeplitz Matrices",
    "concepts": [
      "A Toeplitz matrix has equal diagonals.",
      "The matrix is defined by its first column (c) and first row (r).",
      "The element Aij is equal to c[i-j] if i >= j, and r[j-i] if i < j.",
      "If r is not specified, it defaults to the conjugate of c.",
      "r[0] is ignored when both c and r are specified."
    ],
    "code_examples": []
  },
  {
    "title": "Specifying Only the First Column (c)",
    "concepts": [
      "When only 'c' is specified, the first row is the conjugate transpose of 'c'."
    ],
    "code_examples": [
      {
        "description": "Example showing toeplitz matrix creation when only the first column 'c' is provided.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\nc = jnp.array([1, 2, 3])\ntoeplitz_matrix = jax.scipy.linalg.toeplitz(c)\nprint(toeplitz_matrix)"
      },
      {
        "description": "Example showing toeplitz matrix creation when only the first column 'c' is provided.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\nc = jnp.array([1, 2, 3])\ntoeplitz_matrix = jax.scipy.linalg.toeplitz(c)\nprint(toeplitz_matrix)"
      }
    ]
  },
  {
    "title": "Specifying Both First Column (c) and First Row (r)",
    "concepts": [
      "Both the first column 'c' and the first row 'r' can be specified.",
      "The element r[0] is ignored."
    ],
    "code_examples": [
      {
        "description": "Example showing toeplitz matrix creation with both first column 'c' and first row 'r'.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\nc = jnp.array([1, 2, 3])\nr = jnp.array([-1, -2, -3])\ntoeplitz_matrix = jax.scipy.linalg.toeplitz(c, r)\nprint(toeplitz_matrix)"
      },
      {
        "description": "Example showing toeplitz matrix creation with both first column 'c' and first row 'r'.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\nc = jnp.array([1, 2, 3])\nr = jnp.array([-1, -2, -3])\ntoeplitz_matrix = jax.scipy.linalg.toeplitz(c, r)\nprint(toeplitz_matrix)"
      }
    ]
  },
  {
    "title": "Complex-Valued First Column (c) and Hermitian Matrices",
    "concepts": [
      "If 'c' is complex-valued and 'r' is not specified, 'r' defaults to the conjugate of 'c'.",
      "If c[0].imag == 0, the resulting Toeplitz matrix is Hermitian."
    ],
    "code_examples": [
      {
        "description": "Example demonstrating the creation of a Hermitian Toeplitz matrix with a complex-valued first column 'c'.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\nimport jax\n\nc = jnp.array([1, 2 + 1j, 1 + 2j])\nM = jax.scipy.linalg.toeplitz(c)\nprint(M)\nprint(\"M is Hermitian:\", jnp.all(M == M.conj().T))"
      },
      {
        "description": "Example demonstrating the creation of a Hermitian Toeplitz matrix with a complex-valued first column 'c'.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\nimport jax\n\nc = jnp.array([1, 2 + 1j, 1 + 2j])\nM = jax.scipy.linalg.toeplitz(c)\nprint(M)\nprint(\"M is Hermitian:\", jnp.all(M == M.conj().T))"
      }
    ]
  },
  {
    "title": "Batched Toeplitz Matrices",
    "concepts": [
      "For N-dimensional 'c' and/or 'r', the result is a batch of Toeplitz matrices."
    ],
    "code_examples": [
      {
        "description": "Example showing the creation of a batch of Toeplitz matrices using a 2D array for the first column 'c'.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\nc = jnp.array([[1, 2, 3], [4, 5, 6]])\ntoeplitz_matrix = jax.scipy.linalg.toeplitz(c)\nprint(toeplitz_matrix)"
      },
      {
        "description": "Example showing the creation of a batch of Toeplitz matrices using a 2D array for the first column 'c'.",
        "code": "import jax.numpy as jnp\nimport jax.scipy.linalg\n\nc = jnp.array([[1, 2, 3], [4, 5, 6]])\ntoeplitz_matrix = jax.scipy.linalg.toeplitz(c)\nprint(toeplitz_matrix)"
      }
    ]
  },
  {
    "title": "Cross Power Spectral Density (CSD) Estimation using Welch's Method",
    "concepts": [
      "Estimates the cross power spectral density of two input signals using Welch's method.",
      "It is a JAX implementation of scipy.signal.csd().",
      "Similar to jax.scipy.signal.welch() but operates on two input signals.",
      "If only one input signal x is provided, it computes the power spectral density (PSD) of x.",
      "The function returns a tuple containing the sample frequencies and the cross spectral density."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "x: Input time series data.",
      "y: Second time series data (optional). If None, computes PSD of x.",
      "fs: Sampling frequency (default: 1.0).",
      "window: Data tapering window (default: 'hann').",
      "nperseg: Length of each segment (default: 256).",
      "noverlap: Number of overlapping points (default: nperseg // 2).",
      "nfft: Length of the FFT used, zero-padded if desired (default: nperseg).",
      "detrend: Method for detrending each segment (default: False).",
      "return_onesided: Return one-sided spectrum for real inputs (default: True).",
      "scaling: Selects between power spectral density ('density') or power spectrum ('spectrum') (default: 'density').",
      "axis: Axis along which the CSD is computed (default: -1).",
      "average: Type of averaging to use on the periodograms ('mean' or 'median') (default: 'mean')."
    ],
    "code_examples": []
  },
  {
    "title": "Returns",
    "concepts": [
      "Returns a tuple of arrays: (f, Pxy).",
      "f is the array of sample frequencies.",
      "Pxy is the cross spectral density of x and y."
    ],
    "code_examples": []
  },
  {
    "title": "Notes",
    "concepts": [
      "The JAX implementation behaves like csd(x, x.copy()) in SciPy.",
      "To replicate the behavior of csd(x, None) in SciPy, call the function as csd(x, None)."
    ],
    "code_examples": []
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.scipy.signal.welch(): Power spectral density.",
      "jax.scipy.signal.stft(): Short-time Fourier transform."
    ],
    "code_examples": []
  },
  {
    "title": "Natural Log of Beta Function (betaln)",
    "concepts": [
      "The betaln function calculates the natural logarithm of the absolute value of the beta function.",
      "The betaln function is a JAX implementation of scipy.special.betaln.",
      "betaln(a, b) = log B(a, b), where B is the beta function.",
      "Parameter 'a' is an array-like, real-valued parameter of the beta distribution.",
      "Parameter 'b' is an array-like, real-valued parameter of the beta distribution.",
      "The function returns an array containing the values of the log-beta function."
    ],
    "code_examples": []
  },
  {
    "title": "Bernoulli Probability Mass Function Definition",
    "concepts": [
      "The Bernoulli PMF is defined for k=0 and k=1.",
      "For k=0, the PMF returns 1-p.",
      "For k=1, the PMF returns p.",
      "Otherwise the PMF returns 0.",
      "k represents the value at which to evaluate the PMF.",
      "p represents the distribution shape parameter.",
      "loc represents the distribution offset.",
      "The function returns an array of PMF values."
    ],
    "code_examples": []
  },
  {
    "title": "Cauchy Cumulative Distribution Function",
    "concepts": [
      "Defines the Cauchy cumulative distribution function (CDF).",
      "The CDF is the integral of the Cauchy probability distribution function (PDF).",
      "It uses jax.scipy.stats.cauchy.pdf() to calculate the PDF.",
      "The function takes x, loc, and scale as input parameters.",
      "It returns an array of CDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Generalized Normal Log Probability Distribution Function",
    "concepts": [
      "The document describes the generalized normal log probability distribution function.",
      "The generalized normal probability distribution function is defined by the formula: f(x, \u03b2) = (\u03b2 / (2\u0393(1/\u03b2))) * exp(-|x|^\u03b2)",
      "\u0393 represents the gamma function, and \u03b2 must be greater than 0.",
      "The function takes x (the value at which to evaluate the PDF) and beta (the distribution shape parameter) as input.",
      "The function returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Normal Probability Distribution Function",
    "concepts": [
      "JAX implementation of the normal distribution probability density function (PDF).",
      "The PDF formula is f(x) = (1 / sqrt(2*pi)) * e^(-x^2 / 2).",
      "The function takes x (value at which to evaluate the PDF), loc (distribution offset), and scale (distribution scale) as input.",
      "It returns an array of PDF values."
    ],
    "code_examples": []
  },
  {
    "title": "Uniform Distribution Percent Point Function (PPF)",
    "concepts": [
      "The percent point function (PPF) is the inverse of the cumulative distribution function (CDF).",
      "This document describes the JAX implementation of the uniform distribution's PPF.",
      "The `q` parameter represents the value at which to evaluate the PPF.",
      "The `loc` parameter represents the distribution's offset.",
      "The `scale` parameter represents the distribution's scale."
    ],
    "code_examples": []
  },
  {
    "title": "Von Mises Log Probability Distribution Function",
    "concepts": [
      "This section describes the von Mises log probability distribution function.",
      "It provides a JAX implementation of the scipy.stats.vonmises logpdf.",
      "The von Mises probability distribution function is defined as f(x, \u03ba) = (1/(2\u03c0I0(\u03ba)))e^(\u03bacos x).",
      "I0 is the modified Bessel function i0() and \u03ba >= 0.",
      "The distribution is normalized in the interval -\u03c0 <= x <= \u03c0.",
      "x is the value at which to evaluate the PDF.",
      "kappa is the distribution shape parameter.",
      "The function returns an array of logpdf values."
    ],
    "code_examples": []
  },
  {
    "title": "Overview of approx_max_k",
    "concepts": [
      "The function `approx_max_k` finds the approximate max k values and their indices in an array.",
      "It uses an approximate algorithm, detailed in https://arxiv.org/abs/2206.14286.",
      "The function returns a tuple of two arrays: the max k values and their corresponding indices.",
      "The `operand` argument is the input array and must be a floating-point type.",
      "The `k` argument specifies the number of maximum values to find.",
      "The `reduction_dimension` argument specifies the dimension along which to search (defaults to -1).",
      "The `recall_target` argument is the target recall for the approximation.",
      "The `reduction_input_size_override` argument overrides the size determined by operand[reduction_dim] for evaluating the recall.",
      "The `aggregate_to_topk` argument controls whether the results are aggregated to the top-k in sorted order or returned unsorted."
    ],
    "code_examples": []
  },
  {
    "title": "MIPS Example with approx_max_k",
    "concepts": [
      "Example usage of `approx_max_k` in a maximal inner product search (MIPS) scenario.",
      "The example shows how to use `jax.jit` to compile the MIPS function for performance.",
      "The function calculates dot products between query vectors and a database and then uses `approx_max_k` to find the nearest neighbors.",
      "The `k` and `recall_target` arguments are passed as static arguments to `jax.jit`."
    ],
    "code_examples": [
      {
        "description": "MIPS function definition with jax.jit and approx_max_k",
        "code": "import functools\nimport jax\nimport numpy as np\n\n@functools.partial(jax.jit, static_argnames=[\"k\", \"recall_target\"])\ndef mips(qy, db, k=10, recall_target=0.95):\n    dists = jax.lax.dot(qy, db.transpose())\n    # returns (f32[qy_size, k], i32[qy_size, k])\n    return jax.lax.approx_max_k(dists, k=k, recall_target=recall_target)"
      },
      {
        "description": "Example usage of the MIPS function with random data",
        "code": "qy = jax.numpy.array(np.random.rand(50, 64))\ndb = jax.numpy.array(np.random.rand(1024, 64))\ndot_products, neighbors = mips(qy, db, k=10)"
      }
    ]
  },
  {
    "title": "Duplicated MIPS Example with approx_max_k",
    "concepts": [
      "Example usage of `approx_max_k` in a maximal inner product search (MIPS) scenario.",
      "The example shows how to use `jax.jit` to compile the MIPS function for performance.",
      "The function calculates dot products between query vectors and a database and then uses `approx_max_k` to find the nearest neighbors.",
      "The `k` and `recall_target` arguments are passed as static arguments to `jax.jit`."
    ],
    "code_examples": [
      {
        "description": "MIPS function definition with jax.jit and approx_max_k",
        "code": "import functools\nimport jax\nimport numpy as np\n\n@functools.partial(jax.jit, static_argnames=[\"k\", \"recall_target\"])\ndef mips(qy, db, k=10, recall_target=0.95):\n    dists = jax.lax.dot(qy, db.transpose())\n    # returns (f32[qy_size, k], i32[qy_size, k])\n    return jax.lax.approx_max_k(dists, k=k, recall_target=recall_target)"
      },
      {
        "description": "Example usage of the MIPS function with random data",
        "code": "qy = jax.numpy.array(np.random.rand(50, 64))\ndb = jax.numpy.array(np.random.rand(1024, 64))\ndot_products, neighbors = mips(qy, db, k=10)"
      }
    ]
  },
  {
    "title": "dynamic_update_index_in_dim Function Overview",
    "concepts": [
      "dynamic_update_index_in_dim is a convenience wrapper around dynamic_update_slice.",
      "It updates a slice of size 1 in a single axis of an array.",
      "It takes an array (operand), update values (update), a scalar index (index), and an axis (axis) as input.",
      "It also takes a boolean (allow_negative_indices) to specify whether negative indices are allowed.",
      "If allow_negative_indices is true, negative indices are taken relative to the end of the array.",
      "If allow_negative_indices is false, negative indices are considered out of bounds.",
      "The function returns the updated array."
    ],
    "code_examples": []
  },
  {
    "title": "1D Array Update Examples",
    "concepts": [
      "Demonstrates updating a 1D JAX array with a scalar value at a specific index.",
      "Shows how the dynamic_update_index_in_dim function modifies the array in place (conceptually, as JAX arrays are immutable).",
      "Illustrates updating a 1D JAX array with a JAX array of size 1 at a specific index.",
      "If the specified index is out of bounds, the index will be clipped to the valid range."
    ],
    "code_examples": [
      {
        "description": "Updating a 1D array with a scalar value.",
        "code": "import jax.numpy as jnp\n\nx = jnp.zeros(6)\ny = 1.0\ndynamic_update_index_in_dim(x, y, 2, axis=0)"
      },
      {
        "description": "Updating a 1D array with a scalar value (repeated example).",
        "code": "import jax.numpy as jnp\n\nx = jnp.zeros(6)\ny = 1.0\ndynamic_update_index_in_dim(x, y, 2, axis=0)"
      },
      {
        "description": "Updating a 1D array with a JAX array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.zeros(6)\ny = jnp.array([1.0])\ndynamic_update_index_in_dim(x, y, 2, axis=0)"
      },
      {
        "description": "Updating a 1D array with a JAX array (repeated example).",
        "code": "import jax.numpy as jnp\n\nx = jnp.zeros(6)\ny = jnp.array([1.0])\ndynamic_update_index_in_dim(x, y, 2, axis=0)"
      },
      {
        "description": "Handling out-of-bounds indices by clipping to valid range.",
        "code": "import jax.numpy as jnp\n\nx = jnp.zeros(6)\ny = jnp.array([1.0])\ndynamic_update_index_in_dim(x, y, 10, axis=0)"
      },
      {
        "description": "Handling out-of-bounds indices by clipping to valid range (repeated example).",
        "code": "import jax.numpy as jnp\n\nx = jnp.zeros(6)\ny = jnp.array([1.0])\ndynamic_update_index_in_dim(x, y, 10, axis=0)"
      }
    ]
  },
  {
    "title": "2D Array Update Examples",
    "concepts": [
      "Demonstrates updating a 2D JAX array along a specified axis.",
      "Illustrates how the update array's shape affects the update in the target array.",
      "Shows that the shape of the additional axes in update need not match the associated dimensions of the operand."
    ],
    "code_examples": [
      {
        "description": "Updating a 2D array with a 1D array.",
        "code": "import jax.numpy as jnp\n\nx = jnp.zeros((4, 4))\ny = jnp.ones(4)\ndynamic_update_index_in_dim(x, y, 1, axis=0)"
      },
      {
        "description": "Updating a 2D array with a 1D array (repeated example).",
        "code": "import jax.numpy as jnp\n\nx = jnp.zeros((4, 4))\ny = jnp.ones(4)\ndynamic_update_index_in_dim(x, y, 1, axis=0)"
      },
      {
        "description": "Updating a 2D array with a 2D array where the shapes of additional axes don't match.",
        "code": "import jax.numpy as jnp\n\nx = jnp.zeros((4, 4))\ny = jnp.ones((1, 3))\ndynamic_update_index_in_dim(x, y, 1, 0)"
      },
      {
        "description": "Updating a 2D array with a 2D array where the shapes of additional axes don't match (repeated example).",
        "code": "import jax.numpy as jnp\n\nx = jnp.zeros((4, 4))\ny = jnp.ones((1, 3))\ndynamic_update_index_in_dim(x, y, 1, 0)"
      }
    ]
  },
  {
    "title": "See Also",
    "concepts": [
      "jax.numpy.ndarray.at for indexing and updating array elements.",
      "jax.lax.dynamic_update_slice() for updating slices of arrays.",
      "jax.lax.dynamic_update_index_in_dim() itself.",
      "jax.lax.dynamic_index_in_dim() for dynamic indexing into arrays."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise isfinite",
    "concepts": [
      "The function lowers directly to the stablehlo.is_finite operation.",
      "The input array must have a floating-point type.",
      "The output is a boolean array with the same shape as the input.",
      "The output array contains False where the input is \u00b1\u221e or NaN, and True otherwise."
    ],
    "code_examples": []
  },
  {
    "title": "Description of Bitwise XOR Reduction",
    "concepts": [
      "Computes the bitwise XOR of elements over specified array axes.",
      "The input array must have a boolean or integer data type.",
      "The axes argument specifies the axes to reduce over.",
      "The output array has the same data type as the input.",
      "The output array's shape is the input array's shape with the specified axes removed.",
      "jax.numpy.bitwise_xor.reduce() is a more flexible NumPy-style logical reduction API."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters",
    "concepts": [
      "operand: The input array-like object for reduction.",
      "operand must have a boolean or integer dtype.",
      "axes: A sequence of integers specifying the axes for reduction.",
      "Each axis must be within the valid range of the operand's dimensions (0 <= axis < operand.ndim)."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "Returns an array with the same dtype as the input operand.",
      "The returned array's shape corresponds to the input operand's shape, with the specified axes removed."
    ],
    "code_examples": []
  },
  {
    "title": "Related Functions",
    "concepts": [
      "jax.numpy.bitwise_xor.reduce(): A more flexible NumPy-style logical reduction API.",
      "jax.lax.reduce_xor():  The underlying JAX primitive for XOR reduction.",
      "Other low-level jax.lax reduction operators: jax.lax.reduce_sum(), jax.lax.reduce_prod(), jax.lax.reduce_max(), jax.lax.reduce_min(), jax.lax.reduce_and(), jax.lax.reduce_or()."
    ],
    "code_examples": []
  },
  {
    "title": "Elementwise Logical Right Shift",
    "concepts": [
      "Elementwise logical right shift is denoted as x >> y.",
      "This function lowers to stablehlo.shift_right_logical.",
      "Input arrays x and y must have matching integer dtypes.",
      "If neither x nor y is a scalar, they must have the same number of dimensions and be broadcast compatible.",
      "The output is an array of the same dtype as x and y.",
      "The output contains the element-wise logical right shift of each pair of broadcasted entries."
    ],
    "code_examples": []
  },
  {
    "title": "Collective Permutation",
    "concepts": [
      "Performs a collective permutation on an array or pytree.",
      "The permutation is defined by a list of (source, destination) index pairs.",
      "It's an analog to CollectivePermute HLO.",
      "Unspecified destination indices are filled with zeros."
    ],
    "code_examples": []
  },
  {
    "title": "ppermute Convenience Wrapper",
    "concepts": [
      "This is a convenience wrapper for jax.lax.ppermute.",
      "If x is a pytree, the function is mapped to each leaf.",
      "x is the input array(s) with a mapped axis named axis_name.",
      "axis_name is a hashable Python object used to name a pmapped axis.",
      "perm is a list of ints encoding sources for the permutation.",
      "The output at axis index i comes from the input at axis index perm[i].",
      "Every integer in [0, N) should be included exactly once for axis size N.",
      "Returns array(s) with the same shape as x with slices along the axis axis_name gathered from x according to the permutation perm."
    ],
    "code_examples": []
  },
  {
    "title": "Cholesky Rank-1 Update",
    "concepts": [
      "Cholesky decomposition is given by A = R.T * R.",
      "The function computes the Cholesky decomposition of A + w * w.T in O(N^2) time.",
      "r_matrix is an upper-triangular matrix R.",
      "w_vector is the vector w for rank-1 update.",
      "The function returns a new upper-triangular matrix R defining the Cholesky decomposition of A + w * w.T."
    ],
    "code_examples": []
  },
  {
    "title": "Creating Legacy PRNG Keys",
    "concepts": [
      "This function creates old-style legacy PRNG keys as uint32 arrays.",
      "It is recommended to use jax.random.key() instead when possible.",
      "The resulting key does not carry a PRNG implementation.",
      "The implementation is determined by the `impl` argument or the `jax_default_prng_impl` config flag.",
      "Callers must ensure the correct PRNG implementation is set as the default.",
      "The seed can be a 64- or 32-bit integer.",
      "The function returns a PRNG key that can be used by random functions, split, and fold_in."
    ],
    "code_examples": []
  },
  {
    "title": "Beta Random Value Generation",
    "concepts": [
      "Generates Beta distributed random numbers.",
      "Uses the probability density function f(x; a, b) \u221d x^(a - 1)(1 - x)^(b - 1).",
      "The domain of the distribution is 0 <= x <= 1.",
      "Requires a PRNG key, alpha (a), beta (b) parameters, optional shape, and optional dtype.",
      "Returns an array of random numbers with the specified dtype and shape.",
      "The shape is determined by broadcasting a and b if not provided."
    ],
    "code_examples": []
  },
  {
    "title": "Cauchy Random Values Generation",
    "concepts": [
      "Generation of Cauchy random values.",
      "Uses a probability density function proportional to 1/(x^2 + 1).",
      "Accepts a PRNG key, shape, and dtype as input.",
      "Returns a random array with specified shape and dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Maxwell Distribution Sampling",
    "concepts": [
      "Sampling from a double-sided Maxwell distribution.",
      "The distribution's probability density function is f(x; \u03bc, \u03c3) \u221d z^2 e^{-z^2 / 2}, where z = (x - \u03bc) / \u03c3.",
      "The 'loc' parameter specifies the center (\u03bc) of the distribution.",
      "The 'scale' parameter specifies the scale (\u03c3) of the distribution.",
      "The 'key' parameter is a PRNG key for random number generation.",
      "The 'shape' parameter determines the shape of the output array.",
      "The 'dtype' parameter specifies the data type of the samples."
    ],
    "code_examples": []
  },
  {
    "title": "Standard Normal Random Values Generation",
    "concepts": [
      "Generation of random values following a standard normal distribution.",
      "The values are generated using a PRNG key.",
      "The function allows specifying the shape and dtype of the output array.",
      "The probability density function is f(x) = (1/sqrt(2*pi))*e^(-x^2/2)."
    ],
    "code_examples": []
  },
  {
    "title": "Pareto Distribution Overview",
    "concepts": [
      "Pareto distribution probability density function is defined as f(x; b) = b / x^(b + 1).",
      "The domain of the Pareto distribution is 1 <= x < infinity, with b > 0.",
      "The function samples Pareto random values with given shape and float dtype."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters of the Pareto Sample Function",
    "concepts": [
      "key: A PRNG key used as the random key.",
      "b: A float or array of floats broadcast-compatible with shape, representing the parameter of the distribution.",
      "shape: Optional, a tuple of nonnegative integers specifying the result shape. Must be broadcast-compatible with b. The default (None) produces a result shape equal to b.shape.",
      "dtype: Optional, a float dtype for the returned values (default float64 if jax_enable_x64 is true, otherwise float32)."
    ],
    "code_examples": []
  },
  {
    "title": "Return Value",
    "concepts": [
      "Returns a random array with the specified dtype.",
      "The shape of the returned array is given by shape if shape is not None, or else by b.shape."
    ],
    "code_examples": []
  },
  {
    "title": "Sparse Sigmoid Activation Function",
    "concepts": [
      "Sparse sigmoid is a piecewise function.",
      "The function outputs 0 for x <= -1.",
      "The function outputs 1 for x >= 1.",
      "The function outputs (x+1)/2 for -1 < x < 1.",
      "It's the twin function of the sigmoid activation.",
      "It is the derivative of sparse_plus."
    ],
    "code_examples": []
  },
  {
    "title": "Scaled Exponential Linear Unit (SELU) Activation",
    "concepts": [
      "SELU activation function is defined piecewise.",
      "For positive inputs, SELU returns the input value.",
      "For non-positive inputs, SELU returns alpha * (exp(x) - 1) scaled by lambda.",
      "lambda and alpha are predefined constants.",
      "SELU is designed for self-normalizing neural networks."
    ],
    "code_examples": []
  },
  {
    "title": "Softmax Function Definition",
    "concepts": [
      "Softmax function rescales elements to the range [0, 1].",
      "The sum of elements along a specified axis equals 1 after applying the softmax function.",
      "The softmax function is defined as exp(x_i) / sum(exp(x_j)).",
      "The input 'x' is an array-like object.",
      "The 'axis' parameter specifies the axis or axes along which the softmax is computed.",
      "The 'where' parameter selects elements to include in the softmax.",
      "The result will be all NaN if any input values are +inf."
    ],
    "code_examples": []
  },
  {
    "title": "Building a treedef from a Nested Structure",
    "concepts": [
      "A treedef (PyTreeDef) represents the structure of a PyTree.",
      "The `build_tree` function reconstructs a PyTree from a treedef and a nested iterable structure.",
      "The nested iterable structure must match the arity defined by the treedef.",
      "`jax.tree_util.tree_unflatten()` is an alternative function that reconstructs a tree from a treedef and a flat list of values."
    ],
    "code_examples": [
      {
        "description": "Example showing how to build a treedef from a nested data structure and how to reconstruct the tree using both build_tree and tree_unflatten.",
        "code": "import jax\n\ntree = [(1, 2), {'a': 3, 'b': 4}]\ntreedef = jax.tree.structure(tree)\n\nprint(jax.tree_util.build_tree(treedef, [[10, 11], [12, 13]]))\nprint(jax.tree_util.tree_unflatten(treedef, [10, 11, 12, 13]))"
      }
    ]
  },
  {
    "title": "Custom PyTree Node Registration",
    "concepts": [
      "Registers a custom PyTree node for serialization and deserialization.",
      "This is necessary for types not natively supported for serialization.",
      "It affects the serialization of PyTree nodes in the `in_tree` and `out_tree` fields of `Exported` objects.",
      "The function should be called after `jax.tree_util.register_pytree_node`, except for `collections.namedtuple`.",
      "Requires the node type, a unique serialized name, and functions for serializing and deserializing auxiliary data.",
      "An optional `from_children` function can be provided to construct instances from deserialized auxiliary data and children.",
      "The `from_children` argument is similar to the `unflatten_func` passed to `jax.tree_util.register_pytree_node`.",
      "If `from_children` is not provided, it looks up and uses the `unflatten_func`.",
      "The registered type is returned, enabling its use as a class decorator."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to Pydantic",
    "concepts": [
      "Pydantic is a data validation and settings management library using Python type annotations.",
      "It enforces type hints at runtime, and provides user friendly errors when data is invalid.",
      "Pydantic models can be used to define the structure and types of data.",
      "Pydantic models are created by inheriting from the `BaseModel` class."
    ],
    "code_examples": []
  },
  {
    "title": "Basic Pydantic Model Definition",
    "concepts": [
      "A Pydantic model is defined as a class that inherits from `pydantic.BaseModel`.",
      "Fields are defined as class attributes with type annotations.",
      "Default values can be provided for fields.",
      "Type annotations are used for validation."
    ],
    "code_examples": [
      {
        "description": "Defines a simple Pydantic model named `User` with fields `id`, `name`, and `signup_ts`.",
        "code": "from datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    id: int\n    name = 'John Doe'\n    signup_ts: Optional[datetime] = None\n    friends: list[int] = []"
      }
    ]
  },
  {
    "title": "Creating Instances and Accessing Data",
    "concepts": [
      "Instances of Pydantic models are created like regular Python classes.",
      "Data can be accessed using attribute access.",
      "Pydantic enforces type constraints and raises errors if data doesn't match the defined types.",
      "Pydantic converts data types when possible."
    ],
    "code_examples": [
      {
        "description": "Creates an instance of the `User` model and accesses its attributes.",
        "code": "from datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    id: int\n    name = 'John Doe'\n    signup_ts: Optional[datetime] = None\n    friends: list[int] = []\n\n\nuser = User(id=123, signup_ts='2019-06-01 12:22', friends=[1, '2', b'3'])\nprint(user.id)  # 123\nprint(user.signup_ts)  # datetime.datetime(2019, 6, 1, 12, 22)\nprint(user.friends)  # [1, 2, 3]\nprint(user.name)\n"
      },
      {
        "description": "Illustrates accessing model data as a dictionary.",
        "code": "from datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    id: int\n    name = 'John Doe'\n    signup_ts: Optional[datetime] = None\n    friends: list[int] = []\n\n\nuser = User(id=123, signup_ts='2019-06-01 12:22', friends=[1, '2', b'3'])\nprint(user.dict())\n"
      }
    ]
  },
  {
    "title": "Handling Invalid Data",
    "concepts": [
      "Pydantic raises `ValidationError` when data does not conform to the defined type annotations.",
      "`ValidationError` contains information about the validation errors.",
      "The `errors()` method of `ValidationError` returns a list of error dictionaries."
    ],
    "code_examples": [
      {
        "description": "Demonstrates how Pydantic raises a `ValidationError` when invalid data is provided and how to catch it.",
        "code": "from datetime import datetime\nfrom typing import Optional\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass User(BaseModel):\n    id: int\n    name = 'John Doe'\n    signup_ts: Optional[datetime] = None\n    friends: list[int] = []\n\n\ntry:\n    User(id='not an int', signup_ts='broken', friends=[1, 2, 'not a number'])\nexcept ValidationError as e:\n    print(e.errors())\n"
      }
    ]
  },
  {
    "title": "Compiler Parameters Overview",
    "concepts": [
      "Compiler parameters control the behavior of the Triton compiler.",
      "Key parameters include the number of warps, number of stages, and serialized metadata."
    ],
    "code_examples": []
  },
  {
    "title": "Parameter Definitions",
    "concepts": [
      "num_warps: Specifies the number of warps to use for the kernel (each warp is 32 threads).",
      "num_stages: Controls the level of software pipelining optimization.",
      "serialized_metadata: Allows passing additional, unstable metadata to the compiler."
    ],
    "code_examples": []
  },
  {
    "title": "Attributes and Methods",
    "concepts": [
      "The class has attributes such as PLATFORM, num_stages, num_warps, and serialized_metadata.",
      "The __init__ method is used to initialize the compiler parameters."
    ],
    "code_examples": []
  },
  {
    "title": "Introduction to named_call in JAX",
    "concepts": [
      "JAX stages computations for JIT compilation.",
      "Function names and metadata are not preserved by default.",
      "Debugging staged out programs can be difficult without context.",
      "named_call stages a function out as a subcomputation with a specific name.",
      "Named subcomputations are preserved when compiled with XLA and are visible in debugging tools like TensorFlow Profiler.",
      "Names are also preserved when staging out JAX programs to TensorFlow using experimental.jax2tf.convert()."
    ],
    "code_examples": []
  },
  {
    "title": "Parameters and Return Value of named_call",
    "concepts": [
      "fun (F) is the function to be wrapped.",
      "name (str | None) is the optional prefix to name subcomputations within the name scope.",
      "If name is not specified, fun.__name__ is used.",
      "The function returns a version of fun that is wrapped in a name_scope."
    ],
    "code_examples": []
  }
]